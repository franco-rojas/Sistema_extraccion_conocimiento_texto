id_doc|tema|transcription
DOC0001|Internet de las cosas|Bienvenidos a nuestro canal de YouTube, Internet de las Cosas y a nuestro primer curso en línea. Soy José Antonio y a nombre de nuestro equipo les doy una cordial bienvenida y los invito a aprender y a crear la Internet de las Cosas. El curso será en formato de videos que iremos subiendo periódicamente. El objetivo del curso es enseñar a programar y a crear dispositivos inteligentes. Nuestro punto de reunión será el canal de YouTube, pero también pueden visitar nuestra página web y realizar consultas, ya sea en los comentarios o en nuestro correo electrónico. Cada video tratará de un tema específico. Primero veremos una parte teórica y luego una parte práctica en videos que llamaremos laboratorios. Comencemos con los temas que trataremos en este video. Primero hablaremos de la Internet de las Cosas y de los tres principales conceptos, sensores, actuadores y controladores. Luego aprenderemos qué es la plataforma Arduino. Comencemos. ¿Qué es la Internet de las Cosas? La Internet de las Cosas es un concepto que como su nombre lo indica, son cosas conectadas a Internet. Para explicarlo de forma simple, podemos decir que estas cosas son las mismas que ya conocemos, por ejemplo lámparas, procesadoras de alimento, cámaras o lavadoras. Pero de alguna forma estas cosas ahora son más inteligentes, están dotadas de sensores y pueden hablar en red con otras cosas o con un cerebro o nube central. El concepto en sí no es nuevo, pero hoy está de moda. Se habla de una segunda revolución industrial o de la revolución de las cosas. Las nuevas tecnologías han abaratado los costos de los componentes necesarios para crear dispositivos inteligentes y estos se están haciendo cada vez más presentes en nuestra vida diaria. Los teléfonos celulares ya son inteligentes, nuestros televisores también y nuestros vehículos y así seguirán incorporándose cada vez más dispositivos o cosas. ¿Pero qué es un dispositivo inteligente? ¿De qué está compuesto? Veamos el ejemplo de la cafetera. Una cafetera inteligente tiene un conjunto de sensores que le permiten conocer el nivel del agua, la temperatura del agua y la presencia de la taza de café. Al mismo tiempo tiene actuadores, que en este caso son las válvulas que liberan el agua caliente, el café y el azúcar para dejarlos caer sobre la taza. Todo el proceso está supervisado por una pequeña computadora o controlador que al mismo tiempo es capaz de conectarse a la red internet para enviar los datos en forma inalámbrica. ¿Y dónde está la inteligencia? Una cafetera internet de las cosas aprenderá cómo te gusta el café y las horas en que lo consumes, de tal forma que después de una semana de entrenamiento ya no tendrás que presionar botones. Simplemente le darás una orden verbal y el café será tal como te gusta. Veamos qué es un sensor. Un sensor es un dispositivo normalmente mecánico electrónico que puede detectar y medir magnitudes físicas o químicas, transformándolas en señales eléctricas. Hay varios tipos de sensores. Entre los más usados veremos los de temperatura, humedad, presión, visión, presencia o proximidad. Estos sensores los podemos encontrar en las alarmas de las casas para detectar movimiento o en las cámaras de video, que realmente son sensores de visión. También hay sensores que detectan movimiento, posición y aceleración, como el GPS o el acelerómetro interno de nuestro teléfono inteligente que permite orientar la pantalla de acuerdo a su posición vertical u horizontal. En otros videos trabajaremos con sensores pequeños y muy diferentes entre sí, pero que en general tendrán tres partes que debemos identificar. La placa donde está montado el sensor, el sensor propiamente tal y los pines o patas para alimentación de energía y de comunicación de la señal que miden. El número de pines dependerá del tipo de sensor y de si éste necesita o no alimentación externa. Los más complejos por lo general tienen más pines. Debo mencionar también que hay dos tipos de sensores, los análogos y los digitales, pero ya veremos de qué se trata esta clasificación. Los actuadores. Un actuador es un dispositivo generalmente mecánico que recibe una señal eléctrica y la transforma en una acción o movimiento. Esta definición puede ser confusa, así que veremos algunos ejemplos. El actuador más conocido es el motor. Nosotros trabajaremos con pequeños motores que normalmente podemos encontrar en juguetes. También trabajaremos con servos y motores paso a paso que son motores un poco más avanzados a los cuales se les puede especificar la cantidad de grados que deseamos que se mueva. Dentro de los actuadores también están los relés que encienden y apagan luces, las luces o LED, los displays y también las pantallas. Otro actuador bien conocido son los ventiladores que ocupamos cuando hace mucho calor y los califactores eléctricos que ocupamos en invierno para calefaccionar nuestros hogares. Finalmente veremos qué son los controladores. Un controlador es un dispositivo que recibe la señal de uno o varios sensores, la procesa de acuerdo a su programación interna y luego activa o acciona actuadores. Se puede decir que el controlador es el cerebro de un dispositivo inteligente. Por la funcionalidad propia de los controladores, estos siempre tienen muchos puertos o pines de entrada y son la parte principal de un dispositivo internet de las cosas. Si el controlador falla, todo falla. Los controladores que veremos en este curso son Arduino, Pigalbone y Raspberry Pi. ¿Qué es Arduino? Arduino en realidad son tres cosas. Una placa como la que ven en pantalla, el lenguaje con el que se programa esta placa o el software que se utiliza para programar esta placa. Nosotros definiremos Arduino como una plataforma para realizar prototipos y nos enfocaremos en la potencialidad que tiene para desarrollar dispositivos para el internet de las cosas. La placa Arduino que tenemos en pantalla es la versión Arduino Uno. Si observamos con detención veremos que tiene un procesador. Puertas de entrada que están marcados en tres distintas categorías, Power o poder, Analog In o entradas analógicas y Digital o entradas digitales. También veremos un conector para alimentarlo con energía, un puerto serial para comunicaciones y programación y un botón reset para reiniciarlo en caso de error. El Arduino Uno no está solo, es parte de una familia de placas, cada una con características bien definidas. Arduino Uno se caracteriza por ser ideal para aprender, es fácil conectarlo y agregarle capas o chills. El Arduino Mega es el hermano mayor de Arduino Uno, es más poderoso en capacidad de memoria y en puertos de conexión y por lo mismo es un poco más grande que el Arduino Uno. El Arduino Micro, como su nombre lo indica, es un Arduino en un formato pequeño, es fácil de montar en protoboard y es útil para proyectos donde el espacio, tamaño y peso es muy importante. Dispone de un puerto micro USB para programación. El Arduino Mini es una de las versiones más reducidas de la plataforma Arduino, no posee mucha memoria y por su tamaño no dispone de un puerto USB y debe ser programado mediante una interfaz serial de datos. El Arduino Gema está diseñado para ser utilizado en proyectos vestibles o wearables. El Arduino literalmente se cosa en la ropa y los mismos hilos de costura hacen la conexión eléctrica con sensores y actuadores. Dispone de una interfaz micro USB para programación. El Arduino Jun es una de las placas más avanzadas de la plataforma Arduino. Su principal característica es que es capaz de correr Linux. Viene con un puerto de red listo para conectarse en la red Ethernet y también dispone de un puerto USB host. Se programa mediante un puerto micro USB. Y con esto terminamos la introducción teórica a Internet de las Cosas. No se olviden de suscribirse a nuestro canal y permanecer atentos a los videos de nuestros laboratorios donde iremos mostrando paso a paso cómo crear dispositivos Internet de las Cosas. Si te gustó este primer video o tienes alguna sugerencia, escríbenos en un comentario. No te olvides de compartirlo también en tus redes sociales. Un abrazo y nos vemos en el siguiente video. ¡Suscríbete al canal!
DOC0002|Internet de las cosas|Bienvenidos al curso Internet de las Cosas. Mi nombre es José Antonio y como siempre les doy la bienvenida en nombre del equipo Internet de las Cosas.cl a nuestro curso en línea. En este segundo video aprenderemos un poco más de la plataforma Arduino, viendo las placas Arduino más conocidas y algunas que no lo son tanto, pero que están pensadas para dispositivos Internet de las Cosas. Revisemos la agenda. Veremos en este video el Arduino Uno y sus Chills. También veremos el Arduino Redboard y el Arduino Micro. Dentro de los sensores comenzaremos a familiarizarnos con un sensor GPS, un sensor de temperatura, un sensor PIR y un joystick. Finalmente veremos cómo descargar e instalar el software para programar una placa Arduino y cargar el primer programa Hola Mundo versión Arduino. Comencemos. Lo primero que vamos a explicar son estas líneas que están en nuestra mesa de trabajo. La distancia entre cada línea es de 1 cm, por lo que ustedes pueden ver fácilmente cuál es el tamaño de cada dispositivo. En este caso el Arduino Uno tiene un tamaño de 7 cm por 5 cm y medio. El Arduino Uno tiene una característica muy importante que son sus conectores laterales, los cuales permiten agregar chills o capas a la placa principal. La capa simplemente se posiciona encima del Arduino haciendo coincidir sus conectores y luego se presiona para que los conectores hagan contacto eléctrico. En este caso hemos conectado a la Ethernet Shield que le permite al Arduino conectarse a la red internet mediante un conector RJ45. Existen muchas capas para Arduino, incluso puedes hacer tus propios diseños. Para mostrar lo versátil de la plataforma conectaremos ahora una capa XB la que le permite al Arduino conectarse en forma inalámbrica con otro dispositivo XB. Arduino es una plataforma abierta por lo cual existen muchas placas compatibles con Arduino. Uno de los Arduino compatibles más importantes es el Arduino Redboard de la empresa SparkFan. La principal diferencia entre el Arduino Redboard y el Arduino Uno es el conector de programación. Mientras que el Arduino Uno tiene un conector USB, el Arduino Redboard posee un conector micro USB como pueden apreciar al colocar las dos placas paralelas. Otra placa Arduino que es bien interesante es el Arduino Pluno fabricado por la empresa TF Robot. Este Arduino al igual que el Redboard tiene un conector micro USB y posee una tarjeta Bluetooth integrada que le permite comunicarse utilizando este protocolo. El hermano del Arduino Pluno es el Arduino Wido que en este caso está dentro de una caja de plástico. Este Arduino posee una placa WiFi integrada compatible con los protocolos de seguridad actuales. También posee un lector de tarjetas micro SD en el cual se pueden leer y escribir datos. La familia Arduino no solo tiene placas de formato grande, también hay placas más pequeñas como el Arduino Micro. Y si ustedes colocan atención a las líneas pueden ver que es bien pequeño. También existe el Arduino Flora diseñado por la empresa Adafruit. Este Arduino redondo está pensado para dispositivos vestibles y teóricamente se puede lavar sin que se oxide. Yo personalmente no lo he intentado. Veamos algunos sensores. En nuestra mesa de trabajo tenemos un sensor GPS, un acelerómetro, un sensor de temperatura y un sensor integrado de temperatura y humedad. Noten que cada sensor posee pines o patas de conexión. Algunos pines están en la placa donde están montados y los otros forman parte de la carcajera. Estos pines nos permitirá entregar la energía a cada sensor y obtener los datos medidos por él. Otro sensor bien conocido es el sensor PIR o sensor infrarrojo pasivo de acuerdo a sus siglas en inglés. Este sensor se usa para detectar movimiento y lo podemos encontrar en todas las alarmas instaladas en la mesa de trabajo. El sensor PIR es un sensor que se utiliza para detectar movimientos y las alarmas que se están instalando en la mesa de trabajo. Este sensor se usa para detectar movimiento y lo podemos encontrar en todas las alarmas instaladas en nuestras casas o lugares de trabajo. El sensor PIR posee tres pines, uno para alimentación, otro para datos y el último para tierra. Hay un sensor que es muy conocido pero que no todos lo reconocen como un sensor. Se llama joystick y está presente en la mayoría de las consolas de juego modernas. Hay un sensor que es muy conocido pero que no todos lo reconocen como un sensor. Se llama joystick y está presente en la mayoría de las consolas de juego modernas. Básicamente este sensor está formado por dos resistencias variables que detectan los movimientos de la pequeña palanca que acciona el jugador. Pasando al tema de los actuadores, los más utilizados son los motores. Aquí tenemos un microcervo el cual posee tres cables y dos motores reciclados de juguetes rotos que solo usan dos cables. Veamos ahora la instalación del software Arduino. El software Arduino es gratuito y se puede descargar desde la página web Arduino.cc Una vez cargada la página principal debe ir a la sección download o descargas. En esta sección encontrarán las versiones para los distintos sistemas operativos como Windows, Mac OS X o Linux. En este caso descargaremos la versión para Windows. Hay dos versiones para Windows, la versión para instalación y la versión que se ejecuta sin instalarse y se usa cuando no se tienen los permisos de administración en el computador. Recomiendo la versión para instalación. Antes de comenzar la descarga la fundación Arduino solicita una donación para mantener los costos de desarrollo. Si no desean donar pueden ir a la parte de abajo de la página y presionar just download para comenzar la descarga. Después de algunos minutos dependiendo de la velocidad de su conexión a internet deben ir a la sección descarga de su navegador y ejecutar el archivo .exe. Windows puede solicitar validar la ejecución del instalador de Arduino. Si esto ocurre deben presionar aceptar. Una vez que el instalador se ejecuta solicita aceptar los términos y condiciones de la licencia. Para esto se debe presionar I agree. Luego solicita seleccionar los componentes y la ubicación donde se instalará en su disco duro. Deben dejar todo por defecto y presionar install. El proceso de instalación durará varios minutos y termina cuando cambia el estado a completed. Para cerrar la ventana deben presionar close. Finalmente en su escritorio quedará el icono de Arduino que deben ejecutar cuando quieran utilizar la interfaz de desarrollo. Los materiales para el laboratorio de hoy serán un cable USB. Este cable es bien especial porque en sus extremos tiene distintos conectores. Por un lado tiene uno del tipo B y por el otro tiene uno del tipo A. También vamos a utilizar un Arduino Uno y un diodo emisor de luz o LED. Comenzaremos conectando el extremo del cable USB tipo A en el computador. Y luego conectamos el otro extremo del cable, el que tiene el conector del tipo B, al Arduino Uno. Podemos ver que el Arduino Uno enciende algunas luces porque ya está obteniendo energía desde el computador. Con la placa Arduino conectada al computador ejecutamos el software de programación. Lo primero que haremos será cargar un código de ejemplo. Para esto vamos al menú Archivo. Luego vamos al menú Ejemplos, seleccionamos 01 Basics y luego Blink. El código de ejemplo del programa Blink se muestra en una nueva ventana. Cerramos la ventana inicial y vamos a expandir la ventana para ver todo el código. Antes de cargar el programa en el Arduino debemos asegurarnos que está siendo reconocido por el software. Esto se ve en el menú Herramientas, puerto, que en mi caso es el puerto COM3. También debemos asegurarnos que la placa esté correctamente configurada. Para esto vamos a Herramientas y dentro del menú Placas seleccionamos Arduino Genuino 1. Y ya estamos listos para cargar el programa en el Arduino. Esto se hace con el segundo botón de izquierda a derecha. La barra de estado nos indicará el avance del proceso. Primero se compilará el programa y luego se cargará en la placa Arduino. El proceso termina cuando el estado cambia a subido. La barra de estado muestra adicionalmente información del Sketch, que es como se llama un programa en Arduino. Esta información incluye la memoria utilizada por el Sketch y por las variables globales. El programa que acabamos de cargar envía 5 volts al pin 13 por un segundo y luego envía 0 volts. El Arduino 1 tiene un pequeño LED amarillo conectado al pin 13, así que podemos ver directamente en la placa como este LED se enciende y apaga. Pero como el LED es pequeño, vamos a conectar un LED más grande al pin 13. El LED posee dos patas o pines. Debemos conectar la pata más larga o pata positiva al pin 13 y la pata más corta o tierra a ground, simbolizado por las letras GND en la placa Arduino, justo al lado del pin 13. Comenzamos a ver que el LED se enciende y apaga. Este es el hola mundo que realiza la placa Arduino, obviamente utilizando un lenguaje minario en una secuencia de ceros y unos. Y hemos llegado al final de este video y como lo prometimos tenemos un concurso. En el video hay un personaje de película oculto. Sortearemos un Arduino entre los suscriptores que escriban en los comentarios de este video el nombre correcto del personaje. No se olviden de suscribirse al canal y compartir este video en sus redes sociales. Un abrazo y nos vemos en el siguiente video. Subtítulos por la comunidad de Amara.org
DOC0003|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, hoy comenzamos con una nueva sección en el canal. En esta sección explicaré conceptos básicos que debes saber y entender para tu aprendizaje dentro del campo de la Inteligencia Artificial. En este primer video, por supuesto, teníamos que comenzar con lo más básico, que es la Inteligencia Artificial. Por lo tanto, empecemos con el video. Seguramente cuando piensas en la Inteligencia Artificial, lo primero que se te viene a la mente son los robots, las máquinas con cerebro e inclusive Matrix Terminator o Ex Machina. Esta es una apropiada pero vaga comprensión de la Inteligencia Artificial, por lo que en este video veremos qué es realmente la Inteligencia Artificial y cómo ha cambiado la definición en el pasado. En ocasiones cuando se empieza a investigar sobre este término no hay una sola definición generalmente aceptada, si preguntas a 10 expertos dentro del área te darán 10 respuestas diferentes y todas ellas son correctas. Esto demuestra lo complejo que es el campo y cuántas facetas tiene la Inteligencia Artificial. Definiciones La Inteligencia Artificial se considera popularmente como la Inteligencia exhibida por las máquinas. Aunque es absolutamente correcto, no es como la mayoría de los expertos de la Inteligencia Artificial piensan sobre el tema. Aquí hay un par de ejemplos de definiciones de Inteligencia Artificial que usan este modelo de Inteligencia. Modelado de Inteligencia La Inteligencia Artificial es una constelación evolutiva de tecnologías que permiten a las computadoras simular procesos cognitivos, como elementos del pensamiento humano, pero también formas no humanas de cognición. La Inteligencia Artificial es un sistema que piensa como los humanos, o un sistema que actúa como los humanos, o un sistema que piensa racionalmente, o un sistema que actúa racionalmente. Estas definiciones funcionan bien, pero en realidad son demasiado estrechas y demasiado amplias para ser útiles por sí mismas. Los expertos de la Inteligencia Artificial tienen una visión diferente del modelo de Inteligencia Artificial y piensan que la Inteligencia Artificial es como una disciplina o campo de problemas a resolver como la física o la química. Disciplina de campo La Inteligencia Artificial es una disciplina centrada en la creación de máquinas que pueden tomar decisiones bajo la incertidumbre. La Inteligencia Artificial es un campo centrado en problemas de diseño de agentes que perciben y actúan para satisfacer algún objetivo, a menudo sin que se programe explícitamente cómo hacerlo. Se puede ver como el modelo de Inteligencia Artificial podría no clasificar un sistema robótico como Inteligencia Artificial, sino parece inteligente, pero la disciplina de campo podría ver el mismo robot SOSO como un agente que está tomando una decisión o satisfaciendo un objetivo y por lo tanto un tema para la investigación de la Inteligencia Artificial. La disciplina de campo se centra menos en las definiciones subjetivas de inteligencia y más en la percepción, la toma de decisiones y el logro autónomo de resultados. No queremos quedar atrapados en esto, así que recuerda la Inteligencia Artificial puede ser tanto el software o la máquina que exhibe un comportamiento inteligente, como el campo que estudia la toma de decisiones y acciones autónomas. Entre las disciplinas de la Inteligencia Artificial se encuentra Machine Learning y a su vez una subdisciplina de Machine Learning se encuentra Deep Learning o Aprendizaje Profundo. La Inteligencia Artificial estrecha y general La primera generación de científicos y visionarios de la Inteligencia Artificial creían que eventualmente podríamos crear una Inteligencia a nivel humano, pero varias décadas de investigación de la Inteligencia Artificial han demostrado que replicar la compleja resolución de problemas y el pensamiento abstracto del cerebro humano es sumamente difícil. Por lo tanto, los humanos somos muy buenos para generalizar el conocimiento y aplicar los conceptos que aprendemos en un campo a otro. También podemos tomar decisiones relativamente fiables basados en la intuición y con poca información. A lo largo de los años, la Inteligencia Artificial a nivel humano se ha conocido como Inteligencia General Artificial o Inteligencia Artificial Fuerte. El alboroto inicial y el entusiasmo que rodeó la Inteligencia Artificial atrajo el interés y la financiación de los organismos gubernamentales y las grandes empresas, pero pronto se evidenció que, contraria a las primeras percepciones, la Inteligencia a nivel humano no estaba a la vuelta de la esquina y los científicos tenían dificultades para reproducir las funcionalidades más básicas de la mente humana. En la década de 1970, las promesas y expectativas incumplidas acabaron provocando el invierno de la Inteligencia Artificial, un largo periodo durante el cual el interés público y la financiación de la Inteligencia Artificial se debilitaron. Se necesitaron muchos años de innovación y una revolución en la tecnología de aprendizaje profundo para reavivar el interés por la Inteligencia Artificial, pero incluso ahora, a pesar de los enormes avances, ninguno de los enfoques actuales de la Inteligencia Artificial puede resolver los problemas de la misma manera que lo hace la mente humana, y la mayoría de los expertos creen que la Inteligencia Artificial está al menos a décadas de distancia. La Inteligencia Estrecha o Débil no tiene como objetivo reproducir la funcionalidad del cerebro humano, sino que se centra en la optimización de una sola tarea. La Inteligencia Artificial Estrecha ya ha encontrado muchas aplicaciones en el mundo real, como el reconocimiento de rostros, la transformación de audio a texto, la recomendación de videos en YouTube y la visualización de contenido personalizado Muchos científicos creen que con el tiempo crearemos la Inteligencia Artificial, pero algunos tienen una visión distópica de la era de las máquinas pensantes. En 2014, el renombrado físico inglés Stephen Hawking describió la Inteligencia Artificial como una amenaza existencial para la humanidad, advirtiendo que la Inteligencia Artificial podría significar el fin de la raza humana. En 2015, el presidente de Convainidor Y. Sam Alman y el director general de Tesla, Elon Musk, otros dos creyentes de la Inteligencia Artificial, cofundaron OpenAI, un laboratorio de investigación sin ánimo de lucro que tiene como objetivo crear Inteligencia General Artificial de manera que beneficie a toda la humanidad. Científicos como Norvig creen que la Inteligencia Artificial Estrecha puede ayudar a automatizar tareas repetitivas y laboriosas y ayudar a los humanos a ser más productivos. Por ejemplo, los doctores pueden usar algoritmos de Inteligencia Artificial para examinar escaneos de rayos X a altas velocidades, permitiéndoles ver más pacientes. Por ejemplo, otro ejemplo de Inteligencia Artificial Estrecha es la lucha contra las amenazas cibernéticas. Los analistas de seguridad pueden usar la Inteligencia Artificial para encontrar señales de violaciones de datos que se transfieren a través de las redes de sus empresas. Aplicaciones de la Inteligencia Artificial Hay muchas aplicaciones de la Inteligencia Artificial y las empresas de nueva creación se apresuran a construir chips de Inteligencia Artificial para centros de datos, robóticas, teléfonos inteligentes, aviones no tripulados y otros dispositivos. Gigantes de las tecnologías como Apple, Google, Facebook y Microsoft ya han creado productos interesantes aplicando software de Inteligencia Artificial al reconocimiento de voz, la búsqueda en Internet y la clasificación de imágenes. La destreza de la Inteligencia Artificial de Amazon abarca servicios de computación en la nube y asistentes digitales caseros activados por voz. Alexa y Amazon Echo Aquí hay algunas otras aplicaciones interesantes de la Inteligencia Artificial. Veículos sin conductor, asesores, pueden recomendar inversiones, reequilibrar los ratios de acciones bonus y hacer recomendaciones personalizadas para clientes individuales. El reconocimiento de imágenes puede ayudar al personal encargado de hacer cumplir la ley e identificar a los delincuentes. El futuro de la Inteligencia Artificial En nuestra búsqueda por descifrar el código de la Inteligencia Artificial y crear máquinas pensantes, hemos aprendido mucho sobre el significado de la Inteligencia y el razonamiento. Y gracias a los avances de la Inteligencia Artificial estamos realizando tareas junto a nuestros computadores que una vez fueron consideradas como el dominio exclusivo del cerebro humano. Algunos de los campos emergentes en los que la Inteligencia Artificial está haciendo incursiones incluye la música y las artes, donde los algoritmos de Inteligencia Artificial están manifestando su propio tipo de creatividad. También hay esperanza de que la Inteligencia Artificial ayude a luchar contra el cambio climático, a cuidar a los ancianos y, finalmente, crear un futuro utópico en el que los humanos no tengan que trabajar en absoluto. También se teme que la Inteligencia Artificial cause un desempleo altera el equilibrio económico, provoque otra guerra mundial y finalmente lleve a los humanos a la esclavitud. Todavía no sabemos qué dirección tomará la Inteligencia Artificial, pero a medida que la ciencia y la tecnología de la Inteligencia Artificial continúen mejorando a un ritmo constante, nuestras expectativas y la definición de la Inteligencia Artificial cambiará y lo que hoy consideramos la Inteligencia Artificial podría convertirse en las funciones mundanas de las computadoras del mañana. Con esto finalizamos la explicación. Si quieres aprender más sobre Inteligencia Artificial puedes visitar nuestra página web AprendeIA en donde encontrarás mucha más información como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto te dejo la pregunta del video, ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1. Todo lo que vemos en las películas sobre Inteligencia Artificial es cierto. Opción 2. Ya hemos llegado a la Inteligencia Artificial fuerte o general. Opción 3. En estos momentos estamos realizando tareas junto a nuestros computadores que una vez fueron consideradas como el dominio exclusivo del cerebro humano. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial, solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0004|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata Machine Learning, por lo tanto empecemos con el video. El hombre siempre ha estado enamorado y desconcertado por la inteligencia humana. La habilidad de aprender y entender patrones en cosas aparentemente no relacionadas ha contribuido mucho a nuestro éxito aquí en este planeta. Los investigadores siempre han estado a la casa del Santo Grial, la capacidad de replicar esta inteligencia en una máquina y en el proceso de entender qué es lo que hace que la mente humana sea lo que es. Ahora bien, Machine Learning o aprendizaje automático se ha hecho popular en los últimos años y está ayudando a las computadoras a resolver problemas que antes se consideraban del dominio exclusivo de la inteligencia humana. Y aunque todavía está muy lejos de la visión general de la inteligencia artificial, Machine Learning nos ha acercado mucho más al objetivo final de crear máquinas pensantes. Machine Learning vs. Inteligencia Artificial Las personas tienden a utilizar los términos de Machine Learning e Inteligencia Artificial como si fueran intercambiables, pero no lo son, más bien, Machine Learning es un enfoque muy exitoso en el campo más amplio de la Inteligencia Artificial. La Inteligencia Artificial es esencialmente la simulación de la inteligencia en las computadoras, como tal es algo en lo que la gente ha estado trabajando durante décadas. Abarca una gama de enfoques que van desde lo simple a lo increíblemente complejo. Si codificas un conjunto de reglas simples que permiten a un computador no perder nunca el Tic Tac Toe es una forma básica de Inteligencia Artificial. Machine Learning, por otra parte, es una forma de Inteligencia Artificial en la que el computador aprende por sí mismo cómo completar una tarea. Y esto es lo que ha estado en el corazón de muchos de los enormes desarrollos recientes en el campo de la Inteligencia Artificial, lo que podría explicar por qué el interés en ella ha crecido tanto en los últimos años. ¿Por qué es importante? Como se mencionó anteriormente, Machine Learning y la Inteligencia Artificial existen desde hace tiempo, pero recientemente ha empezado a acelerarse a un ritmo que ha sorprendido a mucha gente. En 2014, la mayoría de los expertos pensaron que pasarían diez años antes de que una máquina venciera a los mejores jugadores del mundo en el Goat. DeepMind demostró que estaban equivocados. Cada vez es más evidente que muchas de las tareas que una vez pensamos que serían dominio exclusivo de los humanos en un futuro previsible, si no para siempre, se llevarán a cabo por sistemas de Machine Learning mucho antes de lo esperado. Esto va a tener un enorme efecto en la política, la economía y la sociedad en su conjunto. Industrias enteras serán automatizadas, dejando a millones de personas sin trabajo e incapaces de estudiar lo suficientemente rápido para mantenerse a la vanguardia de las mejores tecnologías cada vez más rápidas. Esto, a su vez, es probable que conduzca a un descontento a una escala nunca antes pista. Los Trastornos Políticos de 2016 son un signo temprano de ello. Los movimientos populistas que han dado la vuelta al mundo que tiende al aislamiento y al rechazo de los enormes cambios sociales de los últimos años tienen sus raíces en las comunidades más inmediatamente amenazadas por la automatización masiva del trabajo. Esta insatisfacción solo va a aumentar a medida que los sistemas de Machine Learning sean más y más capaces. Y si piensas que hay un límite a lo que estos sistemas pueden lograr, ten en cuenta que la mayoría de los expertos creen que la Inteligencia Artificial será capaz de realizar cualquier tarea intelectual que los humanos puedan llevar a cabo para el 2050. Vale la pena decir que Machine Learning, si se maneja correctamente, traerá enormes beneficios a la humanidad en su conjunto. Un mundo de máquinas que pueden trabajar incansablemente, innovar y mejorar en el que los avances en la eficiencia económica hace que todo lo que viene antes se parezca a la edad media. Pero el camino hacia esta revolución de la eficiencia económica estará asambrado de industrias desaparecidas y borradores desechados. Llenos de subestimaciones de lo que nosotros como especie creemos que es tecnológicamente posible, Machine Learning va a cambiar la forma que los humanos operan más que cualquier otra tecnología que haya existido. ¿Cuáles son las condiciones necesarias para el éxito de Machine Learning? Machine Learning y Big Data o grandes datos se han hecho más conocidos y han generado mucha prensa en los últimos años. Como resultado de ello, muchas personas y organizaciones están considerando cómo y si podría aplicarse a su situación específica y si hay un valor que se puede obtener de ello. Sin embargo, la creación de capacidades internas para el éxito de Machine Learning o el uso de expertos externos puede ser costosa. Antes de consumir este reto, es conveniente evaluar si existen las condiciones adecuadas para que la organización tenga posibilidades de éxito. Las principales consideraciones en este caso se refieren a los datos y a la perspicacia humana. Hay tres requisitos importantes en materia de datos para que Machine Learning sea eficaz. A menudo no todos los requisitos pueden cumplirse satisfactoriamente, y las deficiencias de uno de ellos pueden compensarse a veces con uno o ambos. Estos requisitos son Cantidad Los algoritmos de Machine Learning necesitan un gran número de ejemplos para proporcionar los resultados más fiables. La mayoría de los conjuntos de entrenamiento para Machine Learning implicarán miles o decenas de miles de ejemplos. Variabilidad Machine Learning tiene como objetivo observar las similitudes y diferencias en los datos. Si los datos son demasiado similares o demasiado aleatorios, no podrán aprender eficazmente de ellos. En el aprendizaje por clasificación, por ejemplo, el número de ejemplos de cada clase en los datos de capacitación es fundamental para las posibilidades de éxito. Dimensión Los problemas de Machine Learning suelen operar en un espacio multidimensional en el que cada dimensión está asociada a una determinada variable de entrada. Cuanto mayor sea la cantidad de información que falta en los datos, mayor será la capacidad de espacio pasivo que impide el aprendizaje. Por lo tanto, el nivel de completitud de los datos es un factor importante en el éxito del proceso de aprendizaje. Tipos de Machine Learning Machine Learning puede clasificarse en tres categorías principales. Los algoritmos de aprendizaje supervisado hacen uso de un conjunto de datos de entrada y de salida. El algoritmo aprende una relación entre los datos de entrada y salida del conjunto de entrenamiento y luego utiliza esta relación para predecir la salida de nuevos datos. Uno de los objetivos del aprendizaje supervisado más comunes es la clasificación. El aprendizaje de clasificación tiene por objeto utilizar la información aprendida para predecir la pertenencia a una determinada clase. El ejemplo de la puntuación de créditos representa el aprendizaje de la clasificación en el sentido de que predice las personas que no cumplan con los préstamos. El aprendizaje no supervisado El aprendizaje no supervisado tiene por objeto hacer observaciones en los datos en los que no hay ningún resultado o resultado conocido mediante la deducción de las pautas y la estructura subyacente en los datos. El aprendizaje de asociación es una de las formas más comunes de aprendizaje no supervisado en la que el algoritmo busca asociaciones entre los datos de entrada. El ejemplo del análisis de la cesta de la compra representa el aprendizaje de asociación. El aprendizaje de refuerzo es una forma de aprendizaje de ensayo y error en la que los datos de entrada estimulan el algoritmo en una respuesta y en la que el algoritmo es castigado o recompensado dependiendo de si la respuesta es la deseada. La robótica y la tecnología autónoma hacen un gran uso de esta forma de aprendizaje. Ejemplo de Machine Learning Los siguientes son ejemplos de más desarrollados de Machine Learning que puedes encontrar en tu vida cotidiana. Puntoación de crédito Las instituciones financieras recopilan información detallada sobre sus clientes a lo largo del tiempo, por ejemplo, ingresos activos, trabajo, edad, historial financiero. Estos datos pueden analizarse para identificar qué características están más asociadas a resultados negativos como el incumplimiento de los préstamos, o cuáles impulsan resultados positivos como el reembolso puntual de los préstamos. Así pues, se puede establecer una relación de predicción que permita clasificar a los clientes en función de su probabilidad de impago y la institución financiera que puede utilizarse para tomar decisiones más eficientes sobre los préstamos. Ciencia genética Las pruebas de ADN proporcionan información personal y de salud. Los códigos genéticos de las personas que reportan condiciones o rasgos de salud similares pueden ser analizados sobre un gran número de individuos para encontrar las cadenas o sectores más frecuentes. Si se descubre tales cadenas o sectores, pueden utilizarse para predecir los rasgos o posibles problemas médicos que puedan surgir. Este tipo de aprendizaje también puede utilizarse para algunos casos, reuniendo a miembros de la familia que han sido separados por adopción y otras circunstancias. Otras aplicaciones comunes son el diagnóstico médico, la conversión de la escritura en texto, el reconocimiento de voz, el reconocimiento facial, la comprensión de imágenes, la robótica, los vehículos autónomos y muchos otros usos. Los límites de Machine Learning Machine Learning ha dado grandes pasos hacia la resolución de problemas complejos. Todavía está muy lejos de crear las máquinas de pensamientos previstas por los pioneros de la Inteligencia Artificial. Además de aprenderte la experiencia, la verdadera inteligencia requiere razonamiento, sentido común y pensamiento abstracto, áreas en los que los modelos de Machine Learning funcionan muy mal. Por ejemplo, mientras que Machine Learning es bueno en tareas complicadas de reconocimiento de patrones, como predecir el cáncer de mamá con cinco años de antelación, lucha con tareas lógicas y razonamientos más simples, como resolver problemas matemáticos de la escuela secundaria. La falta de poder de razonamiento de Machine Learning lo hace malo para generalizar su conocimiento. Por ejemplo, un agente de Machine Learning que pueda jugar a Super Mario 3 como un profesional no dominará otro juego de plataforma, como otra versión de Super Mario, tendría que ser entrenado desde cero. Sin el poder de extraer el conocimiento conceptual de la experiencia, los modelos de Machine Learning requieren toneladas de datos de entrenamiento para funcionar. Desafortunadamente, muchos dominios carecen de suficientes datos de entrenamiento o no tienen los fondos para adquirir más. Afortunadamente, se están haciendo esfuerzos para superar los límites de Machine Learning. Un ejemplo notable es una amplia iniciativa de DARPA, el brazo de investigación del Departamento de Defensa, para crear modelos de Inteligencia Artificial explicables. Otros proyectos tienen por objeto reducir la excesiva dependencia de Machine Learning de los datos anotados y hacer que la tecnología sea accesible a los dominios con datos de formación limitados. Queda por ver si la evolución de Machine Learning nos ayudará finalmente a alcanzar el objetivo, siempre difícil de crear una Inteligencia Artificial a nivel humano, por lo que sabemos con seguridad es que gracias a los avances en Machine Learning, los dispositivos que se encuentran en nuestro escritorio y descansan en nuestros bolsillos son cada vez más inteligentes. Con esto finalizamos esta explicación. Si quieres aprender más sobre Inteligencia Artificial, puedes visitar la página web de AprendeIA en donde encontrarás más información, como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto, te dejo la pregunta del video, ¿cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1. Los términos de Machine Learning en Inteligencia Artificial son los mismos. Opción 2. Los algoritmos de Machine Learning necesitan un gran número de ejemplos para proporcionar los resultados más fiables. Opción 3. El aprendizaje supervisado, no supervisado y el reforzado son los tipos de Machine Learning. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial, solamente tienes que presionar el botón rojo que se encuentra debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0005|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata Deep Learning. Por lo tanto, empecemos con el video. Deep Learning o Aprendizaje Profundo está a la vanguardia de lo que las máquinas pueden hacer y los desarrolladores y líderes de negocios necesitan absolutamente entender lo que es y cómo funciona. Este tipo de algoritmo ha superado con creces cualquier punto de referencia anterior para la clasificación de imágenes, texto y voz. También potencia algunas de las aplicaciones más interesantes del mundo, como los vehículos autónomos y la traducción en tiempo real. Ciertamente hubo mucha emoción cuando el AlphaGo de Google basado en Deep Learning venció al mejor jugador de Go del mundo, pero las aplicaciones comerciales de esta tecnología son más inmediatas y potencialmente más impactantes. El primer paso para entender cómo funciona Deep Learning es comprender la diferencia entre los términos importantes. Inteligencia Artificial vs. Machine Learning La Inteligencia Artificial es la réplica de la Inteligencia Humana en las computadoras. Cuando la investigación de la Inteligencia Artificial comenzó, los investigadores trataban de replicar la Inteligencia Humana para las tareas específicas, como jugar a un juego. Introdujeron un gran número de reglas que el ordenador debía respetar. La computadora tenía una lista específica de posibles acciones y tomaba decisiones basadas en esas reglas. Machine Learning, por su parte, se refiere a la capacidad de una máquina para entender utilizando grandes conjuntos de datos en lugar de reglas de código puro. Machine Learning permite a las computadoras aprender por sí mismas. Este tipo de aprendizaje aprovecha la potencia de procesamiento de las computadoras modernas que pueden procesar fácilmente grandes conjuntos de datos. Cómo funciona Deep Learning Deep Learning es en realidad solo un subconjunto de Machine Learning que ha recibido una atención significativa recientemente debido a su rendimiento estelar en muchas de las tareas que hemos desarrollado anteriormente. Nos permite entrenar una Inteligencia Artificial para predecir las salidas, dado un conjunto de entradas. Tanto el aprendizaje supervisado como el no supervisado pueden ser usados para entrenar la Inteligencia Artificial. Deep Learning es solo un tipo de algoritmo que puede funcionar muy bien para predecir las cosas. Deep Learning y las redes neuronales para la mayoría de los propósitos son efectivamente sinónimos. Las redes neuronales se inspiran en la estructura de la corteza cerebral. En el nivel básico está el perceptron, la representación matemática de una neurona biológica. Como en la corteza cerebral, puede haber varias capas de percepciones interconectadas. Las neuronas se agrupan en tres tipos diferentes de capas, capa de entrada, capas ocultas y capa de salida. La capa de entrada recibe datos de entrada. Desde esta capa pasan las entradas a la primera capa oculta. Las capas ocultas realizan cálculos matemáticos en nuestras entradas. Uno de los retos en la creación de redes neuronales es decidir el número de capas ocultas, así como el número de neuronas de cada capa. Las capas ocultas de una red neuronal realizan modificaciones en los datos para eventualmente sentir cuál es la relación con la variable objetivo. Cada nodo tiene un peso y multiplica su valor de entrada por ese peso. Si se hace esto en unas pocas capas diferentes, la red es capaz de manipular los datos para convertirlos en algo significativo. Para averiguar cuáles deberían ser estos pequeños pesos, típicamente usamos el algoritmo llamado backpropagation. La capa de salida devuelve los datos de salida que vendría haciendo nuestra predicción. Puede ser un nodo si el modelo solo produce un número o unos pocos nodos si se trata de un problema de clasificación multiclásica. Cada neurona tiene una función de activación, pero ¿cuál es el punto de esta? Una función de activación proporciona cierta no linealidad a la función que necesitamos para representar funciones complejas. El poder de las redes neuronales es que en la práctica resultan tener la flexibilidad de representar bien aproximadamente las relaciones subyacentes entre la entrada y la salida de la tarea. Entrenar la red neuronal El entrenamiento de la red neuronal es la parte más difícil de deep learning. Esto es porque necesitas un gran conjunto de datos y necesitas una gran cantidad de potencia de cálculo. Para entrenar una red neuronal necesitamos darles las entradas de nuestro conjunto de datos y comparar sus salidas con las salidas del conjunto de datos. Como la red aún no está entrenada, sus salidas serán erróneas. Una vez que revisemos todo el conjunto de datos, podemos crear una función que nos muestre cuán equivocados fueron los resultados de la red con respecto a los resultados reales. Esta función se llama función de costo. Idealmente queremos que nuestra función de costo sea cero. Es cuando los resultados de nuestra red neuronal son los mismos que los del conjunto de datos. Para reducir la función de costo, cambiamos los pesos entre las neuronas. Podríamos cambiarlos al azar hasta que nuestra función de costo sea baja, pero eso no es muy eficiente. En su lugar usaremos una técnica llamada descenso del gradiente. El descenso del gradiente es una técnica que nos permite encontrar el mínimo de una función. En nuestro caso buscamos el mínimo de la función de costo. Funciona cambiando los pesos en pequeños incrementos después de cada iteración del conjunto de datos. Al calcular la derivada o gradiente de la función de coste en un cierto de conjuntos de pesos, somos capaces de ver en qué dirección está el mínimo. Para minimizar la función de costo es necesario iterar a través de su conjunto de datos muchas veces. Por eso necesitas una gran cantidad de potencia de cálculo. Si hacemos un resumen de todos los pasos explicados acá tenemos Deep Learning utiliza una red neuronal para imitar la inteligencia humana. Hay tres tipos de capas de neuronas en una red neuronal, la capa de entrada, las capas ocultas y la capa de salida. Las conexiones entre las neuronas están asociadas a un peso que dicta la importancia del valor de entrada. Las neuronas aplican una función de activación sobre los datos para estandarizar la salida que sale de la neurona. Para entrenar una red neuronal se necesita un gran conjunto de datos. Iterando a través del conjunto de datos y comparando las salidas se producirá una función de costo, indicando cuanto se aleja la predicción de las salidas reales. Después de cada iteración a través del conjunto de datos, los pesos entre las neuronas se ejecutan usando el descenso del gradiente para reducir la función de costo. ¿Por qué es importante Deep Learning? Deep Learning es importante por una razón. Hemos sido capaces de lograr una precisión significativa y útil en tareas importantes. Deep Learning se ha utilizado para la clasificación de imágenes y texto durante décadas, pero se ha esforzado para cruzar el umbral. Hay una precisión básica que los algoritmos deben tener para trabajar en entornos empresariales. Deep Learning nos permite finalmente cruzar esa línea en lugares que antes no podíamos. La visión por computador es un gran ejemplo de una tarea que Deep Learning ha transformado en algo realista para las aplicaciones de negocios. El uso de Deep Learning para clasificar y etiquetar imágenes no solo es mejor que cualquier otro algoritmo tradicional, está empezando a ser mejor que los humanos reales. Facebook ha tenido un gran éxito en la identificación de rostro en las fotografías mediante el uso de Deep Learning. Si se pregunta si dos fotos desconocidas de rostros muestran a la misma persona, un humano lo hará bien el 97.53% de las veces, mientras que el software desarrollado por Facebook puede obtener una puntuación del 97.25% en el mismo desafío, independientemente de las variaciones en la iluminación o de si la persona en la foto está directamente frente a la cámara. El reconocimiento de voz es otra área que ha sentido el impacto de Deep Learning. Los idiomas hablados son vastos y ambivos. Baidu, uno de los principales motores de búsqueda de China, ha desarrollado un sistema de reconocimiento de voz que es más rápido y preciso que el de los humanos para producir texto en un teléfono móvil, tanto en inglés como en mandarin. Google está usando ahora Deep Learning para manejar la energía de los centros de datos de la compañía. Han reducido sus necesidades de energía para la refrigeración en un 40%. Eso se traduce en un 15% de mejora en la eficiencia del uso de la energía por la compañía y cientos de millones de dólares en ahorros. Deep Learning es importante porque finalmente hace estas tareas accesibles, trae cargas de trabajos previamente y relevantes al ámbito de Machine Learning. Estamos justo en la cúspide de los desarrolladores y líderes empresariales que entienden cómo pueden utilizar Machine Learning para impulsar los resultados empresariales y tener más tareas disponibles al alcance de la mano. Gracias a Deep Learning va a transformar la economía en las próximas décadas. Con esto finalizamos la explicación. Si quieres aprender más sobre Inteligencia Artificial puedes visitar nuestra página web Aprendía en donde encontrarás más información como eBooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto te dejo la pregunta del video, ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1 Deep Learning no es un subconjunto de Machine Learning. Opción 2 Las neuronas se agrupan en tres tipos diferentes de capas, capa de entrada, capas ocultas y capa de salida. Opción 3 Para entrenar una red neuronal no se necesita un gran conjunto de datos. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial. Simplemente tienes que presionar el botón rojo de este video. Nos vemos en el siguiente video. Chao.
DOC0006|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré las diferencias entre Inteligencia Artificial, Machine Learning y Deep Learning. Por lo tanto, empecemos con el video. La tecnología está cada vez más integrada en nuestra vida por minuto y para mantener el ritmo de las expectativas de los consumidores, las empresas confían más en los algoritmos de aprendizaje para hacer las cosas más fáciles. Se puede ver su aplicación en los medios sociales a través del reconocimiento de objetos en las fotos o en hablar directamente a los dispositivos como Alexa o Siri. Estas tecnologías se asocian comúnmente con la Inteligencia Artificial, el Machine Learning, el Deep Learning o Aprendizaje Profundo y las redes neuronales. Y aunque todas ellas desempeñan un papel, estos términos tienen a utilizarse indistintamente en las conversaciones, lo que provoca cierta confusión en torno a los matices entre ellas. Con suerte podemos usar este video para aclarar algunas de las ambigualidades aquí. Empezaré dando una rápida explicación de lo que significan los términos de Inteligencia Artificial, Machine Learning y Deep Learning para luego explicar cómo son diferentes unas con otras. Inteligencia Artificial Acuñada por primera vez en 1956 por John McCarthy, la Inteligencia Artificial implica máquinas que pueden realizar tareas características de la inteligencia humana. Aunque esto es bastante general, incluye cosas como la planificación, la comprensión del lenguaje, el reconocimiento de objetos y sonidos, el aprendizaje y la resolución Podemos poner la Inteligencia Artificial en dos categorías, general y estrecha. La Inteligencia Artificial general tendría todas las características de la inteligencia humana, incluyendo las capacidades mencionadas anteriormente. La Inteligencia Artificial estrecha exhibe algunas facetas de la inteligencia humana y puede hacer esa faceta extremadamente bien, pero carece de otras áreas. Una máquina que es genial para reconocer imágenes, pero nada más. Sería un ejemplo de Inteligencia Artificial estrecha. Machine Learning Machine Learning o aprendizaje automático es el término general para cuando las computadoras aprenden de los datos. Describe la intersección de la informática y la estadística donde los algoritmos se utilizan para realizar una tarea específica sin ser programados explícitamente. En cambio, reconocen patrones en los datos y hacen predicciones una vez que llegan nuevos datos. Así que, en lugar de rutinas de software de codificación con instrucciones precisas para realizar una tarea particular, Machine Learning es una forma de entrenar un algoritmo para que puedan aprender cómo. En general, el proceso de aprendizaje de estos algoritmos puede ser supervisado o no supervisado, dependiendo de los datos que se utilizan para alimentar los algoritmos. Deep Learning Los algoritmos de Deep Learning pueden ser considerados como una evolución sofisticada y matemáticamente compleja de los algoritmos de Machine Learning. El campo ha recibido mucha atención últimamente y por una buena razón. Los desarrollos recientes han llevado a resultados que antes no se creían posibles. Deep Learning describe algoritmos que analizan datos con una estructura lógica similar a la de un humano para sacar conclusiones. Nota que esto puede ocurrir tanto a través de un aprendizaje supervisado como no supervisado. Para lograrlo, las aplicaciones de Deep Learning utilizan una estructura en capas de algoritmos llamadas Red Neuronal Artificial. El diseño de dicha red se inspira en la red neuronal biológica del cerebro humano, lo que conduce a un proceso de aprendizaje mucho más capaz que el de los modelos estándar de Machine Learning. Diferencia entre Inteligencia Artificial y Machine Learning Es normal que se utilice ambos términos de manera discriminada, por lo que a continuación se presentan varias diferencias. La Inteligencia Artificial es una tecnología que permite a una máquina simular el comportamiento humano. Machine Learning es un subconjunto de la Inteligencia Artificial que permite a una máquina aprender automáticamente de los datos del pasado sin necesidad de programarlos explícitamente. El objetivo de la Inteligencia Artificial es hacer un sistema informático inteligente, como los humanos, para resolver problemas complejos. El objetivo de Machine Learning es permitir que las máquinas aprendan de los datos para que puedan dar resultados precisos. En la Inteligencia Artificial hacemos sistemas inteligentes para realizar cualquier tarea como humano. En Machine Learning, enseñamos a las máquinas con datos a realizar una tarea particular y dar un resultado preciso. Machine Learning y Deep Learning son los principales subconjuntos de la Inteligencia Artificial. Deep Learning es un subconjunto principal de Machine Learning. La Inteligencia Artificial tiene un alcance muy amplio. Machine Learning tiene un alcance limitado. La Inteligencia Artificial está trabajando para crear un sistema inteligente que pueda realizar varias tareas complejas. Machine Learning trabaja para crear máquinas que puedan realizar solo aquellas tareas específicas para las que están entrenadas. El sistema de Inteligencia Artificial se preocupa por maximizar la posibilidad de éxito. Machine Learning se preocupa principalmente por la precisión y los patrones. En función de las capacidades, la Inteligencia Artificial puede dividirse en tres tipos, que son Inteligencia Artificial débil, general y fuerte. Machine Learning también puede dividirse principalmente en tres tipos, que son el Aprendizaje Supervisado, No Supervisado y Reforzado. Diferencias entre Machine Learning y Deep Learning. Esta es una pregunta común y si has llegado hasta acá probablemente ya sepas que no debería ser preguntada de esa manera. Los algoritmos de Deep Learning son algoritmos de Machine Learning, por lo tanto, sería mejor pensar en lo que hace que Deep Learning sea especial dentro del campo de Machine Learning. Las respuestas, la estructura del algoritmo de redes neuronales, la menor necesidad de intervención humana, los mayores requerimientos de datos. En primer lugar, mientras que los algoritmos tradicionales de Machine Learning tienen una estructura bastante simple como la Regresión Lineal o un árbol de decisiones, Deep Learning se basa en una red neuronal de varias capas, como el cerebro humano, compleja y entrelazada. En segundo lugar, los algoritmos de Deep Learning requieren mucho menos intervención humana, las características se extraen automáticamente y el algoritmo aprende de sus propios errores. En tercer lugar, Deep Learning requiere muchos más datos que un algoritmo tradicional de Machine Learning para funcionar correctamente. Machine Learning funciona con mil puntos de datos, Deep Learning a menudo solo con millones. Debido a la compleja estructura multi capa, un sistema de Deep Learning necesita un gran conjunto de datos para eliminar las fluctuaciones y hacer interpretaciones de alta calidad. Deep Learning está todavía en su infancia, en algunas áreas, pero su poder es ya enorme. Se aprovecha principalmente por las grandes empresas con vastos recursos financieros y humanos, ya que la construcción de algoritmos de Deep Learning solía ser compleja y costosa. Pero esto está cambiando. Diferencias entre Deep Learning y Redes Neuronales Lo profundo en Deep Learning se refiere a la profundidad de las capas en una red neuronal, una red neuronal que constate más de tres capas que incluirían las entradas y la salida. Puede considerar un algoritmo de Deep Learning. La mayoría de las redes neuronales profundas son de alimentación, lo que significa que fluyen en una sola dirección de la entrada a la salida. Sin embargo, también puede entrenar el modelo a través de la retro propagación, es decir, moverse en dirección opuesta de la salida a la entrada. La retro propagación nos permite calcular y atribuir el error asociado a cada neurona, permitiéndonos ajustar y encajar el algoritmo apropiadamente. En resumen, de todo lo explicado acá podemos desclosar lo siguiente. Deep Learning es un subconjunto especializado de Machine Learning que a su vez es un subconjunto de la Inteligencia Artificial. En otras palabras, Machine Learning es un tipo de Inteligencia Artificial y Deep Learning es una parte especialmente compleja de Machine Learning. Con esto finalizamos la explicación. Si quieres aprender más sobre Inteligencia Artificial puedes visitar nuestra página web Aprende IA en donde encuentras más información, como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto, te dejo la pregunta del video ¿Cuáles de las siguientes afirmaciones crees tú que sea cierto? Opción 1 Podemos poner la Inteligencia Artificial en dos categorías, general y estrecha. Opción 2 Los algoritmos tradicionales de Machine Learning tienen una estructura bastante simple, mientras que Deep Learning se basa en una red neuronal artificial de varias capas. Opción 3 Lo profundo en Deep Learning se refiere a la profundidad de las capas en una red neuronal. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial. Solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0007|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata la visión computacional, por lo tanto empecemos con el video. La fantasía de que una máquina es capaz de simular el sistema visual humano es antigua. Hemos recorrido un largo camino desde que aparecieron los primeros trabajos universitarios en la década de 1960, como lo demuestra la llegada de los sistemas modernos trivialmente integrados en las aplicaciones móviles. Hoy en día la visión por computador es uno de los subcampos más importantes de la Inteligencia Artificial y Machine Learning, dada su amplia variedad de aplicaciones y su tremendo potencial. Su objetivo es replicar las poderosas capacidades de la visión humana. ¿Qué es la visión por computador? La visión computacional es el subcampo de la Inteligencia Artificial que intenta imitar las capacidades de la visión humana, y por visión humana no nos referimos solo a los ojos o a la capacidad de ver imágenes. No es tan trivial como simplemente tomar una foto con el teléfono. El propósito no es imitar solo la vista, sino imitar la percepción, la capacidad de los humanos de dar sentido a lo que ven. La visión por computador se centra en la creación de sistemas digitales que pueden procesar, analizar y dar sentido a los datos visuales, imágenes o videos, de la misma manera que los humanos. El concepto de visión computacional se basa en enseñar a los computadores a procesar una imagen a nivel de pixel y a entenderla. Técnicamente, las máquinas intentan recuperar la información visual, manejarla e interpretar los resultados a través de algoritmos de software especiales. ¿Cómo funciona la visión computacional? La tecnología de visión computacional tiende a imitar la forma en que funciona el cerebro humano. Pero ¿cómo resuelve nuestro cerebro el reconocimiento visual de objetos? Una de las hipótesis populares afirma que nuestro cerebro depende de patrones para decodificar objetos individuales. Este concepto se utiliza para crear sistemas de visión por computador. Los algoritmos de visión computacional que usamos hoy en día se basan en el reconocimiento de patrones. Entrenamos a los computadores en una gran cantidad de datos visuales. Los computadores procesan imágenes, etiquetan los objetos en ellos y encuentran patrones en esos objetos. Por ejemplo, si enviamos un millón de imágenes de flores, la computadora las analizará, identificará patrones que son similares a todas las flores y, al final de este proceso, creará un modelo flor. Como resultado, la computadora será capaz de detectar con precisión si una imagen en particular es una flor cada vez que les enviamos imágenes. Los métodos y técnicas de deep learning han transformado profundamente la visión computacional junto con otras áreas de la Inteligencia Artificial, hasta tal punto que para muchas tareas su uso se considera estándar. En particular, las redes neuronales convolucionales han logrado resultados más allá del estado de la técnica utilizando técnicas tradicionales de visión por computadora. Estos cuatro pasos esbozan un enfoque general para construir un modelo de visión por computadora utilizando redes neuronales convolucionales. Crear un conjunto de datos compuesto de imágenes anotadas o utilizar uno ya existente. Las anotaciones pueden ser la categoría de la imagen para un problema de clasificación, pares de cajas delimitadoras y clases para un problema de detección de objetos o una segmentación de píxeles para cada objeto de interés presente en una imagen. Para problemas de segmentación de instancia, extraer de cada imagen las características pertinentes a la tarea en cuestión. Este es un punto clave en el modelado del problema. Por ejemplo, los rasgos utilizados para reconocer rostros, rasgos basados en criterios faciales, obviamente no son los mismos que los utilizados para reconocer atracciones turísticas u órganos Entrenar un modelo de deep learning basado en los rasgos aislados. Entrenar significa alimentar el modelo de machine learning con muchas imágenes y aprenderá basándose en esos rasgos, cómo resolver la tarea en cuestión. Evaluar el modelo utilizando imágenes que no se utilizaron en la fase de entrenamiento. Al hacerlo, la precisión del modelo de entrenamiento puede ser probada. La estrategia es muy básica, pero sirve bien al propósito. Este enfoque, conocido como aprendizaje supervisado, requiere un conjunto de datos que abarque el fenómeno que el modelo tiende para aprender. Tareas típicas de la visión computacional. La visión por computadora se basa en un extenso conjunto de tareas diversas combinadas para lograr aplicaciones altamente sofisticadas. Las tareas más frecuentes son el reconocimiento de imágenes y de video, que básicamente consisten en determinar los diferentes objetos que contiene una imagen. Clasificación de imágenes. Probablemente una de las tareas más conocidas en la visión computacional es la clasificación de imágenes. Permite clasificar una imagen TADAP como perteneciente a una de un conjunto de categorías predefinidas. Tomemos un simple ejemplo binario. Queremos categorizar las imágenes según el contenido, una atracción turística o no. Supongamos que se construye un clasificador para este propósito y se proporciona una imagen. El clasificador responderá que la imagen pertenece al grupo de imágenes que contiene atracciones turísticas. Esto no significa que haya reconocido necesariamente el lugar, sino más bien que ha visto previamente fotos del sitio y que se le ha dicho que esas imágenes contienen una atracción turística. Una versión más ambiciosa del clasificador podría tener más de dos categorías. Por ejemplo, podría haber una categoría para cada tipo específico de atracción turística que queramos conocer. En tal escenario, las respuestas por entrada de imagen podrían ser múltiples. Detección de objetos. La detección de objetos es similar a la clasificación de imágenes, excepto que con la detección de objetos la imagen puede contener muchos objetos que se localizan y clasifican. En este patrón de código de detección de objetos, un modelo está entrenado no solo para clasificar imágenes de Coca-Cola, sino también para localizar cada botella dentro de la imagen. En la siguiente se ve que reconocieron tres categorías diferentes de productos de Coca-Cola. Además de devolver las etiquetas y la confianza, proporciona coordenadas que permite a la aplicación dibujar cajas delimitadoras alrededor de cada objeto identificado. La detección de objetos se utiliza en una amplia variedad de casos de uso, en los que lo interesante no es una única clasificación que describa toda la imagen, sino que pueden ser muchos objetos de diferentes categorías en la imagen. Dando a una aplicación la capacidad de identificar, localizar y contar los objetos, incrementando las posibles aplicaciones de este tipo de visión computacional. Identificación de objetos La identificación de objetos es ligeramente diferente de la detección de objetos, aunque a menudo se utilizan técnicas similares para lograr ambas. En este caso, dado un conjunto específico, el objetivo es encontrar instancias de dicho objeto en imágenes. No se trata de clasificar una imagen como vimos anteriormente, sino de determinar si el objeto aparece o no en una imagen, y si aparece, especificar el lugar o los lugares donde aparece. Un ejemplo puede ser la búsqueda de imágenes que contenga el logotipo de una empresa determinada. Otro ejemplo es la supervisión de imágenes en tiempo real de cámaras de seguridad para rostros de una persona específica. Seguimiento de objetos de videos Cuando se utiliza la detección de objetos en los videos, a menudo se quiere seguir el rastro de los objetos de un fotograma a otro. La detección inicial de objetos se puede hacer extrayendo un fotograma del video y detectando los objetos en el fotograma. Además de contar los objetos y localizar cada uno de ellos, el seguimiento de los objetos a medida que se desplazan de un fotograma a otro añade otra dimensión a lo que puede hacer. Casos de uso comercial Las empresas utilizan cada vez más las aplicaciones de visión computacional para responder a las preguntas comerciales o para mejorar sus productos. Probablemente ya forman parte de su vida cotidiana, sin que te des cuenta. A continuación se presentan algunos casos de uso popular. Organización de contenidos Los sistemas de visión computacional ya nos ayudan a organizar nuestro contenido. Apple Photos y Google Photos son un excelente ejemplo. Estas aplicaciones tienen acceso a nuestras colecciones de fotos y añaden automáticamente etiquetas a las fotos, y nos permiten navegar por una colección de fotografías más estructuradas. Estas aplicaciones crean una vista curada de tus mejores momentos para ti. Motores de búsqueda visual La tecnología de búsqueda visual se puso a disposición del público con la aparición de Google Images en 2001. Un motor de búsqueda visual es capaz de recuperar imágenes que cumplen con ciertos criterios de contenido. La búsqueda de palabras clave es un caso de uso común, pero a veces podemos presentar una imagen de origen y solicitar que se encuentren imágenes similares. Reconocimiento facial La tecnología de reconocimiento facial se utiliza para hacer coincidir las fotos de los rostros de las personas con sus identidades. Esta tecnología está integrada en los principales productos que usamos a diario. Por ejemplo, Facebook está usando la visión computarizada para identificar a las personas en las fotos. El reconocimiento facial es una tecnología crucial para la autentificación biométrica. Muchos dispositivos móviles disponibles en el mercado hoy en día permiten a los usuarios desbloquear los dispositivos mostrando sus caras. Para el reconocimiento facial se utiliza una cámara frontal. Los dispositivos móviles procesan esta imagen y, basándose en el análisis, pueden decir si la persona que tiene el dispositivo está autorizada en él. Lo importante de esta tecnología es que funciona realmente rápido. Realidad aumentada La visión computacional es un elemento central de las aplicaciones de realidad aumentada. Esta tecnología ayuda a estas aplicaciones a detectar objetos físicos, tanto superficies como objetos individuales dentro de un espacio físico determinado en tiempo real, y a utilizar esta información para colocar objetos virtuales dentro del entorno físico. Automóviles autoconductores La visión computacional permite a los autos dar sentido a su entorno. Un vehículo inteligente tiene unas cuantas cámaras que capturan videos desde diferentes ángulos y los envías como señal de entrada al software de visión computacional. El sistema procesa el video en tiempo real y detecta objetos como marcas en la carretera, objetos cercanos al auto como peatones u otros autos, semáforos, entre otros. Uno de los ejemplos más notables de las aplicaciones de esta tecnología es el piloto automático en los automóviles Tesla. Salud La información de las imágenes es un elemento clave para el diagnóstico en medicina porque representa el 90% de todos los datos médicos. Muchos diagnósticos en la salud se basan en el procesamiento de imágenes, rayos X, resonancia magnética y mamografía, solo por nombrar algunos. Y la segmentación de las imágenes demostró la eficacia durante el análisis de las exploraciones médicas. Por ejemplo, los algoritmos de visión computacional pueden detectar la retinopatía diabética, la causa de ceguera de más rápido crecimiento. La visión por computador puede procesar imágenes de la parte posterior del ojo y clasificarlas según la presencia y la gravedad de la información. Agricultura Muchas organizaciones agrícolas emplean la visión computacional para vigilar la cosecha y resolver los problemas agrícolas comunes como la aparición de malas hierbas o la deficiencia de nutrientes. Estos sistemas procesan imágenes de satélites, aviones no tripulados o aviones e intentan detectar los problemas en una fase temprana, lo que ayuda a evitar pérdidas financieras innecesarias. La visión computacional es un tema popular en los artículos sobre nuevas tecnologías. Un enfoque diferente de la utilización de los datos es lo que hace que esta tecnología Tremendas cantidades de datos que creamos diariamente se utilizan en realidad para nuestro beneficio. Los datos pueden enseñar a los computadores a ver y comprender los objetos. Esta tecnología también demuestra un importante paso hacia la creación de una inteligencia artificial que será tan sofisticada como la de los humanos. Con esto finalizamos la explicación. Si quieres aprender más sobre inteligencia artificial puedes visitar nuestra página web AprendeIA en donde encontrarás más información, como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto te dejo la pregunta del video ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1 La visión computacional es un subconjunto de Machine Learning. Opción 2 Los métodos y técnicas de deep learning han transformado profundamente la visión computacional. Opción 3 Las tareas más frecuentes de visión computacional son el reconocimiento de imágenes y video. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de la que tenemos publicada acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial, solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0008|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata el procesamiento de lenguaje natural, por lo tanto empecemos con el video. Todo lo que expresamos ya sea verbalmente o por escrito conlleva enormes cantidades de El tema que elegimos, nuestro tono, nuestra selección de palabras, todo añade algún tipo de información que se puede interpretar y extraer valor de ella. En teoría, podemos entender e incluido predecir el comportamiento humano utilizando esa información. Pero hay un problema, una persona puede generar cientos o miles de palabras en una declaración y en cada frase tiene su correspondiente complejidad. Los datos generados a partir de conversaciones, declaraciones o inclusive tweets son ejemplos de datos no estructurados. Los datos no estructurados no encajan perfectamente en la estructura tradicional de filas y columnas de las bases de datos relacionales y representan la gran mayoría de datos disponibles en el mundo real. Son desordenados y difíciles de manipular, sin embargo, gracias a los avances en disciplinas como Machine Learning se está produciendo una gran revolución de este tema. Hoy en día ya no se trata de intentar interpretar un texto o discurso a partir de sus palabras, la antigua forma mecánica sino de entender el significado que hay detrás de esas palabras, la forma cognitiva. De esta manera es posible detectar figuras del lenguaje como la ironía o incluso realizar un análisis de sentimientos. ¿Qué es el procesamiento del lenguaje natural? El procesamiento del lenguaje natural es la rama de la informática y más específicamente de la inteligencia artificial que se ocupa de dotar a los computadores de la capacidad de entender textos y palabras habladas de forma muy similar a la de los seres humanos. El procesamiento del lenguaje natural combina la lingüística computacional, el modelado del lenguaje humano basado en reglas con modelos estadísticos de Machine Learning y de Deep Learning. Juntas estas tecnologías permiten a las computadoras procesar el lenguaje humano en forma de texto o datos de voz y entender su significado completo, con la intención y el sentimiento del hablante o escritor. ¿Cómo funciona el procesamiento del lenguaje natural? El lenguaje humano está repleto de ambiguidades que hacen increíblemente difícil escribir un software que determine con precisión el significado deseado de los datos de texto o de voz. Por lo tanto, el procesamiento del lenguaje natural permite a las computadoras entender el lenguaje natural como lo hacen los humanos, tanto si el lenguaje es hablado como escrito. El procesamiento del lenguaje natural utiliza la Inteligencia Artificial para tomar datos del mundo real, procesarlos y darles un sentido que el computador pueda entender. Al igual que los humanos tienen diferentes sensores como los oídos para oír y los ojos para ver, las computadoras tienen programas para leer y micrófonos para recoger el audio, y al igual que los humanos tienen cerebro para procesar esa entrada, las computadoras tienen un programa para procesar sus respectivas entradas. En algún momento del procesamiento, la entrada se convierte en un código que el computador puede entender. El procesamiento del lenguaje natural consta de dos fases principales, el procesamiento de datos y el desarrollo de algoritmos. El preprocesamiento de datos consiste en preparar y limpiar los datos de texto para que las máquinas puedan analizarlos. El procesamiento pone los datos de forma visible y destaca las características del texto con las que puede trabajar un algoritmo. Hay varias formas de hacerlo, entre ellas tokenización, se trata de dividir el texto en unidades más pequeñas con las que trabajar. Eliminación de las palabras de parada, en este caso se eliminan las palabras comunes del texto para que queden las palabras únicas que ofrecen más información sobre el texto. Lematización y stinging, en este caso las palabras se reducen a sus raíz para poder procesarlas. Etiquetado de parte del discurso, es cuando las palabras se marcan en función de la parte del discurso que son como sustantivos, verbos y adjetivos. Una vez procesados los datos, se desarrolla un algoritmo para procesarlos. Hay muchos algoritmos diferentes del procesamiento del lenguaje natural, pero se suelen utilizar dos tipos principales. Sistema basado en reglas, este sistema utiliza reglas lingüísticas cuidadosamente diseñadas, este enfoque se utilizó al principio del desarrollo del procesamiento del lenguaje natural y todavía se utiliza. Sistema basado en Machine Learning, los algoritmos de Machine Learning utilizan métodos estadísticos, aprenden a realizar tareas basándose en los datos de entrenamiento que reciben y ajustan sus métodos a medida que se procesan más datos. Mediante una combinación de Machine Learning, Deep Learning y redes neuronales, los algoritmos de procesamiento del lenguaje natural perfeccionan sus propias reglas a través del procesamiento y el aprendizaje repetitivo. ¿En dónde se utiliza el procesamiento del El procesamiento del lenguaje natural es el motor de la inteligencia artificial en muchas aplicaciones modernas del mundo real. A continuación te dejo varios ejemplos. Detección de spam Es posible que no pienses en la detección de spam como la solución del procesamiento del lenguaje natural, pero las mejores tecnologías de detección de spam utilizan las capacidades de clasificación de texto de procesamiento del lenguaje natural para escanear los correos electrónicos, en busca del lenguaje que suele indicar spam o phishing. Estos indicadores pueden incluir el uso excesivo de términos financieros, la mala gramática característica, el lenguaje amenazante, la urgencia inapropiada, los nombres de empresas mal escritos, etc. La detección de spam es uno de los problemas del procesamiento del lenguaje natural que los expertos consideran mayormente resueltos, aunque en estos momentos pueden argumentar que no se ajusta a tu experiencia con el correo electrónico. Traducción automática Google Translate es una de las tecnologías de procesamiento del lenguaje natural ampliamente disponible. La traducción automática verdaderamente útil implica algo más que la sustitución de palabras de un idioma por otras de otro. Una traducción eficaz tiene que cantar con precisión el significado y el tono de la lengua de entrada y traducirlo a un texto con el mismo significado y el mismo impacto deseado en la lengua de salida. Una buena manera de poner en práctica cualquier herramienta de traducción automática es traducir un texto de una lengua y luego volver al original. Asistentes virtuales o chatbots Los asistentes virtuales como Siri de Apple y Alexa de Amazon utilizan el reconocimiento de voz para reconocer patrones en los comandos de voz y la generación de lenguaje natural para responder con acciones apropiadas o comentarios útiles. Los chatbots realizan la misma magia en respuesta a entradas de texto escritas. Los mejores también aprenden a reconocer pistas contextuales sobre las solicitudes humanas y las utilizan para ofrecer respuestas u opciones aún mejores con el tiempo. La siguiente mejora para estas aplicaciones es la respuesta a preguntas con la capacidad de responder a nuestras preguntas anticipadas o no con respuestas relevantes y útiles en sus propias palabras. Análisis del sentimiento en las redes sociales El procesamiento del lenguaje natural se ha convertido en una herramienta empresarial esencial para descubrir los datos ocultos de los canales de las redes sociales. El análisis de sentimientos puede analizar el lenguaje utilizado en las publicaciones de las redes sociales, las respuestas, los comentarios, etcétera, para extraer actitudes y emociones en respuesta a los productos, las promociones y los eventos, información que las empresas pueden utilizar en el diseño de productos, las campañas publicitarias, etcétera. Resumen de textos El resumen de texto utiliza técnicas de procesamiento del lenguaje natural para diferir enormes volúmenes de texto digital y crear resúmenes y sinopsis para índices, base de datos de investigación o lectores ocupados que no tienen tiempo de leer el texto completo. Las mejores aplicaciones de resumen de texto utilizan el razonamiento semántico y la generación del lenguaje natural para añadir contexto y conclusiones útiles a los resúmenes. ¿Cuáles son los beneficios del procesamiento del lenguaje natural? El principal beneficio del procesamiento del lenguaje natural es que mejora la forma en que los humanos y las computadoras se comunican entre sí. La forma más directa de manipular un computador es a través de código, el lenguaje del computador. Al permitir que las computadoras entiendan el lenguaje humano, la interacción de las computadoras se vuelve mucho más intuitiva para los humanos. Otras ventajas son mayor precisión y eficacia de la documentación, capacidad de hacer automáticamente un resumen legible de un texto original más grande y complejo. Utiliza para asistentes personales como Alexa al permitirle entender la palabra hablada. Permite a una organización utilizar chatbots para la atención al cliente. Facilita la realización de análisis de sentimientos. Proporciona conocimientos avanzados de análisis que antes eran inalcanzables debido al volumen de datos. ¿Qué desafíos presenta el procesamiento del lenguaje natural? El procesamiento del lenguaje natural presenta una serie de retos, la mayoría de los cuales se reducen al hecho de que el lenguaje natural está en constante evolución y siempre es algo ambiguo. Entre ellos se encuentra precisión. Tradicionalmente las computadoras requieren que los humanos les hablen en un lenguaje de programación preciso, sin ambigüedades y muy estructurado mediante un número limitado de órdenes claramente enunciadas. Sin embargo, el habla humana no siempre es precisa, a menudo es ambigua y la estructura lingüística puede depender de muchas variables complejas como la jerga, los dialectos regionales y el contexto social. Tono de voz e inflexión El procesamiento del lenguaje natural aún no se ha perfeccionado, por ejemplo, el análisis semántico puede seguir siendo un reto. Otras dificultades son el hecho de que el uso abstracto del lenguaje suele ser difícil de entender para los programas. Por ejemplo, el procesamiento del lenguaje natural no capta fácilmente el sarcasmo, estos temas suelen requerir la compresión de las palabras que se utilizan y su contexto en una conversación. Otro ejemplo es una frase que puede cambiar de significado dependiendo de la palabra o sílaba en la que el hablante ponga el acento. Los algoritmos de procesamiento del lenguaje natural pueden pasar por alto los sutiles pero importantes cambios de tono en la voz de una persona al realizar el reconocimiento del habla. El tono y la inflexión del habla también pueden variar entre diferentes acentos, lo que puede ser un reto para un algoritmo. Evolución del uso del lenguaje El procesamiento del lenguaje natural también se ve afectado por el hecho de que el lenguaje y la forma en que lo utilizan las personas cambian continuamente. Aunque hay reglas para el lenguaje, ninguna está escrito en piedra y está sujeta a cambios con el tiempo. Las reglas computacionales que funcionan ahora pueden quedar obsoletas cuando las características del lenguaje del mundo real cambien con el tiempo. El procesamiento del lenguaje natural desempeña un papel fundamental en la tecnología y en la forma en que los seres humanos interactúan con ella. Se utilizan en muchas aplicaciones del mundo real, tanto en el ámbito empresarial como en el de los consumidores, como los chatbots, la cyberseguridad, los motores de búsqueda y el análisis de grandes datos. Aunque no está exenta de desafíos, se espera que el procesamiento del lenguaje natural siga siendo una parte importante tanto de la industria como la vida cotidiana. Con esto finalizamos la explicación. Si quieres aprender más sobre inteligencia artificial puedes visitar nuestra página web AprendeIA en donde encontrarás más información como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto te dejo la pregunta del video ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1 El procesamiento del lenguaje natural permite a las computadoras entender el lenguaje natural como lo hacen los humanos. Opción 2 El procesamiento del lenguaje natural consta de dos fases principales, el preprocesamiento de datos y el desarrollo de algoritmos. Opción 3 El principal beneficio del procesamiento del lenguaje natural es que mejora la forma en que los humanos y las computadoras se comunican entre sí. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la inteligencia artificial. Solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0009|Introduccion IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata la ciencia de datos, por lo tanto empecemos con el video. Hace un tiempo se calificó la ciencia de datos como uno de los trabajos más sexys del siglo XXI, pero para muchos es un misterio que significa esto. ¿Qué hace exactamente un científico de datos y qué hace realmente esta persona en su trabajo cada día? Esto es precisamente lo que quiero aclararte con este video. La ciencia de los datos es un enfoque multidisciplinario para extraer información útil de los grandes y crecientes volúmenes de datos que recopilan y crean las organizaciones de hoy en día. La ciencia de datos abarca la preparación de los datos para su análisis y la realización de análisis de datos avanzados y la presentación de los resultados para revelar patrones y permitir a las partes interesadas sacar conclusiones fundamentales. La preparación de los datos puede implicar su limpieza, agregación y manipulación para que estén listos para determinados tipos de procesamiento. El análisis requiere el desarrollo y el uso de algoritmos, análisis y modelos de inteligencia artificial. Está impulsado por un software que revisa los datos para encontrar patrones y transformarlos en predicciones que apoyen la toma de decisiones empresariales. La precisión de estas predicciones deben validarse mediante pruebas y experimentos diseñados científicamente y los resultados deben compartirse mediante el uso hábil de herramientas de visualización que permitan a cualquiera ver los patrones y comprender las tendencias. En consecuencia, los científicos de datos, como se denomina a los profesionales de la ciencia de datos, requieren conocimientos de informática y ciencias puras que van más allá de los de un típico analista de datos. Un científico de datos debe ser capaz de hacer lo siguiente. Aplicar las matemáticas, la estadística y el método científico. Utilizar una amplia gama de herramientas y técnicas para evaluar y preparar los datos, desde SQL hasta la minería de datos y los métodos de integración de datos. Extraer información de los datos utilizando análisis predictivos e inteligencia artificial, incluyendo modelos de Machine Learning y Deep Learning. Escribir aplicaciones que automaticen el procesamiento de datos y los cálculos. Contar e ilustrar historias que transmitan claramente el significado de los resultados a los responsables de la toma de decisiones y a las partes interesadas en todos los niveles de conocimiento técnico y comprensión. Explicar cómo se pueden utilizar estos resultados para resolver problemas empresariales. Esta combinación de habilidades es poco frecuente, y no es de extrañar que los científicos de datos estén actualmente muy solicitados. Según una encuesta de IBM, el número de puestos de trabajo en este campo sigue creciendo a un ritmo superior al 5% anual. La ciencia en la ciencia de datos El término ciencia suele ser sinónimo de método científico, y algunos de ustedes habrán notado que el proceso descrito anteriormente es muy similar al proceso caracterizado por la expresión método científico. En general, todos los científicos tradicionales como los científicos de datos se plantean preguntas y o definen un problema, recogen y aprovechan los datos para dar respuesta o soluciones, prueban la solución para ver si el problema está resuelto e iteran según sea necesario para mejorar o finalizar la solución. Ciclo de vida de la ciencia de datos El ciclo de vida de la ciencia de datos incluye entre 5 y 16, dependiendo de a quién se le pregunte, procesos continuos y superpuestos. Los procesos comunes a la definición del ciclo de vida de casi todo el mundo son los siguientes. Captura Se trata de la recopilación de datos brutos estructurados y no estructurados de todas las fuentes relevantes a través de cualquier método, desde la entrada manual o recogidos desde la web hasta la captura de datos de sistemas y dispositivos en tiempo real. Preparar y mantener Esto implica poner los datos brutos en un formato consistente para los análisis o modelos de Machine Learning o Deep Learning. Esto puede incluir todo, desde la limpieza, la eliminación de datos duplicados y el escalamiento de los datos, hasta el uso de ETL, extraer, transformar y cargar, u otras tecnologías de integración para combinar los datos en un almacén de datos u otro medio para ser unificados para el análisis. Pre-Procesamiento Aquí los científicos de datos examinan los sesgos, los patrones, los rangos y las distribuciones de los valores dentro de los datos para determinar la idoneidad de los datos para su uso con algoritmos de análisis predictivo, Machine Learning y o Deep Learning u otros métodos analíticos. Analizar Aquí es donde se produce el descubrimiento. Los científicos de datos realizan análisis estadísticos, análisis predictivos, regresión, Machine Learning y algoritmos de Deep Learning, entre otros, para extraer información de los datos preparados. Comunicar Por último, los conocimientos se presentan en forma de informes, gráficos u otras visualizaciones de datos que hacen que los conocimientos y su impacto en el negocio sean más fáciles de entender para los responsables de la toma de decisiones. Un lenguaje de programación de ciencia de datos como R o Python incluye componentes para generar visualizaciones. Alternativamente, los científicos de datos pueden utilizar herramientas de visualizaciones específicas. Herramientas de ciencia de datos Dado que la programación informática es un componente importante, los científicos de datos deben dominar lenguajes de programación como Python, R, SKL, Java, Julia y Scala. Normalmente no es necesario ser programadores expertos en todos ellos, pero Python, R y SQL son definitivamente clave. Para las estadísticas, las matemáticas, los algoritmos, el modelado y la visualización de datos, los científicos de datos suelen utilizar paquetes y bibliotecas preexistentes cuando es posible. Algunos de los más populares basados en Python son Scikit Learn, TensorFlow, PyTorch, Pandas, NumPy y MapLog. Para la investigación y los informes reproducibles, los científicos de datos suelen utilizar cuadernos o marcos como Jupyter y JupyterLab. Estos son muy potentes ya que el código y los datos pueden entregarse junto con los resultados clave, de modo que cualquiera puede realizar el análisis y basarse en él si lo desea. Hoy en día los científicos de datos pueden utilizar cada vez más herramientas asociadas a los grandes datos. Algunos de los ejemplos más populares son Hadoop, Spark, Kafka, Hive, entre otros. También deben saber cómo acceder y consultar muchos de los principales sistemas de gestión de base de datos. Por último, la computación en la nube y los servicios basados en la nube y las APIs son una parte importante de la caja de herramientas de los científicos de datos, especialmente en términos de almacenamiento y acceso a los datos. Machine Learning e Inteligencia Artificial Los proveedores de servicios en la nube más comunes son Amazon Web Service, Microsoft Azure y Google Cloud Compute. La orquestación y el despliegue de DevOps y DataOps implican cada vez más tecnologías basadas en contenedores, junto con herramientas de infraestructura. Casos de uso de la ciencia de datos No hay límite para el número o el tipo de empresas que podrían beneficiarse de las oportunidades que está creando la ciencia de datos. Casi todos los procesos empresariales pueden ser más eficientes gracias a la optimización basada en datos y casi todos los tipos de experiencia el cliente pueden mejorarse con mejor orientación y personalización. He aquí algunos casos de usos representativos de la ciencia de datos y la Inteligencia Artificial. Un banco internacional crea una aplicación móvil que ofrece decisiones en el momento a los solicitantes de préstamo utilizando modelos de riesgo crediticio impulsados por Machine Learning y una estructura de computación en la nube híbrida que es potente y segura. Una empresa electrónica está desarrollando sensores ultra potentes impresos en 3D que guiarán los vehículos sin conductor del futuro. La solución se basa en herramientas de ciencia de datos y análisis para mejorar sus capacidades de detección de objetos en tiempo real. Un proveedor de soluciones de automatización de procesos robóticos han desarrollado una solución cognitiva de minería de procesos empresariales que reduce los tiempos de gestión de incidentes entre un 15% y un 95% para sus empresas clientes. La solución está capacitada para entender el contenido y el sentimiento de los correos electrónicos de los clientes, dirigiendo a los equipos de servicio para priorizar los más relevantes y urgentes. Una empresa de tecnología de medios digitales creó una plataforma de análisis de audiencias que permite a sus clientes ver qué es lo que atrae a las audiencias de televisión cuando se les ofrece una creciente gama de canales digitales. La solución emplea la Anaditica Profunda y Machine Learning para recopilar información en tiempo real sobre el comportamiento de los espectadores. Un departamento de policía urbano crea herramientas de análisis estadísticos de incidentes para ayudar a los agentes a entender cuándo y dónde desplegar los recursos para prevenir la delincuencia. La solución basada en datos crea informes y cuadros de mando para aumentar el conocimiento de la situación de los agentes sobre el terreno. Una empresa de asistencia sanitaria inteligente ha desarrollado una solución que permite a las personas mayores vivir de forma independiente durante más tiempo. Combinando sensores, Machine Learning, análisis y procesamiento basado en la Nude, el sistema supervisa los comportamientos inusuales y alerta a los familiares y cuidadores, al tiempo que se ajusta a las estrictas normas de seguridad obligatorias en el sector sanitario. Harvard tenía razón sobre los científicos de datos. Es un papel extremadamente importante y de gran demanda que puede tener un impacto significativo en la capacidad de una empresa para alcanzar sus objetivos, ya sean financieros, operativos, estratégicos, etc. Las empresas recopilan una gran cantidad de datos y la mayoría de las veces se descuidan. Estos datos a través de la extracción de información significativa y el descubrimiento de perspectiva procesables pueden utilizarse para tomar decisiones empresariales críticas e impulsar un cambio empresarial significativo. También pueden utilizarse para optimizar el éxito de los clientes y la posterior adquisición, retención y crecimiento. Como se ha mencionado, los científicos de datos pueden tener un gran impacto positivo en el éxito de una empresa y, a veces, causar inadvertidamente perdidas financieras, que es una de las muchas razones por las que contratar a un científico de datos de primera categoría es fundamental. Con esto finalizamos la explicación. Si quieres aprender más sobre Inteligencia Artificial puedes visitar nuestra página web AprendeIA en donde encontrarás más información, como también ebooks y recursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto, te dejo la pregunta del video ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1 La ciencia de los datos es un enfoque multidisciplinario para extraer información útil de los grandes datos. Opción 2 Las matemáticas, la estadística y el método científico no son conocimientos necesarios para un científico de datos. Opción 3 Los científicos de datos deben dominar todos los lenguajes de programación como Python, R, SQL, Java, Julia y Scala. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de lo que tenemos publicado acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial. Solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0010|Introduccion IA|Las redes neuronales han sido creadas para simular el cerebro humano, la propiedad más importante que tiene es que tienen la capacidad de entender según los datos que se les entreguen y en los últimos años han tenido un gran avance que lo podemos ver en nuestro día a día. Existen numerosos tipos de redes neuronales, entre ellos se encuentran el perceptron simple y el perceptron multicapa. Hola a todos, soy Ligthy González de Aprendida y en el video de hoy te explicaré qué es el perceptron, su funcionamiento y algunos detalles que debes saber. Por lo tanto, empecemos con el video. Comencemos definiendo qué es el perceptron. Un perceptron es un modelo matemático que tiene la estructura y la funcionalidad de una neurona biológica. Es una sección de Machine Learning que se utiliza para entender el concepto de los clasificadores binarios. Forma parte del sistema de redes neuronales, inclusive se puede decir que tanto las redes neuronales como el perceptron se encuentran interconectadas entre sí. El perceptron constituye el fundamento básico de la red neuronal que forma parte de Deep Learning, se considera como bloques de construcción de una sola capa de la red neuronal. Una red neuronal formada por el perceptron puede definirse como un enunciado complejo con una compresión muy profunda de las ecuaciones lógicas. Un enunciado neuronal que sigue al perceptron es verdadero o falso, pero nunca puede ser ambas cosas a la vez. El objetivo del perceptron es identificar las entradas que intervienen en él, hay que identificar si las características son verdaderas o no. Origen del perceptron El perceptron fue inventado por Frank Rosenblatt en 1957 en Nueva York. Este proyecto formó parte de una investigación de 5 años en donde su objetivo era diseñar un cerebro electrónico con la capacidad de analizar y entender patrones. El objetivo con esta investigación era construir un dispositivo que tuviese funciones similares a la de los humanos, como la percepción, la capacidad de entender conceptos, la orientación, entre muchas otras. Sin embargo, para llevar a cabo todas estas operaciones, un sistema informático convencional tendría que almacenar miles o millones de patrones y luego, siempre que fuera necesario, se buscaría este número excesivo de patrones para reconocer un patrón no visto. Lo que era computacionalmente muy caro y no era una forma económica de identificar un patrón u objeto. Para resolver este problema, Frank Rosenblatt propuso un sistema que podría funcionar según los principios del cerebro biológico y reconocer las similitudes entre los patrones utilizando un enfoque probabilístico en lugar del enfoque determinista. El modelo del perceptron cobró vida gracias a la construcción de un hardware personalizado llamado Mark I Perceptron, diseñado principalmente para el reconocimiento de imágenes. Era una caja negra muy parecida a la red neuronal actual, con una capa de entrada, capas ocultas y una capa de salida. Avanzado unas décadas llegamos a 1986 cuando Geoffrey Hinton presentó un nuevo procedimiento llamado retro propagación, el cual se ha convertido en la columna pértebra de la gran mayoría de investigaciones basadas en redes neuronales de la actualidad. Esta técnica funciona minimizando entre los valores reales y los deseados mediante el ajuste de los pesos del modelo de red neuronal. Hace que la red neuronal aprenda o extraiga características y generalice un patrón o secuencia de entradas para hacer predicciones bastante precisas sobre representaciones de datos no vistas. Componentes principales del perceptron El perceptron está formado por varios componentes que te explico a continuación. Entradas Las entradas en el algoritmo del perceptron se entienden como x1, x2, x3, x4 y así sucesivamente. Todas estas entradas denotan los valores del perceptron de características y la ocurrencia total de las características. Pesos Se observan como valores que se planifican a lo largo de la sesión de preparación del perceptron. Los pesos ofrecen un valor preliminar en el inicio del aprendizaje del algoritmo. Con la ocurrencia de cada inexactitud de entrenamiento, los valores de los pesos se actualizan. Estos se representan principalmente como w1, w2, w3, w4 y así sucesivamente. Suma ponderada Es la proliferación de cada valor de entrada o características asociadas con el valor de paso correspondiente. Función de activación Cada función de activación o no lineal toma un único número y realiza una determinada operación matemática fija sobre él. Hay varias funciones de activación que se pueden encontrar en la práctica. Las más comunes son la simmoide o la relu o unidad lineal rectificada. Salida La suma ponderada se pasa a la función de activación y cualquier valor que obtengamos después del cálculo es nuestra salida predicha. Si te interesa profundizar sobre estos temas de Inteligencia Artificial, en nuestro canal de YouTube podrás encontrar bastante contenido relacionado a estos temas. Por lo tanto te recomiendo que los revises y a su vez que te suscribas al canal si aún no lo has hecho. Hablemos ahora del modelo de Persextron de una capa. Un modelo de Persextron de una capa incluye una Red Feed Forward que depende de una función de transferencia de umbral en su modelo. Es el tipo más sencillo de Red Neuronal Artificial que puede analizar solo objetos linealmente separables con resultados binarios, es decir, 1 y 0. Si hablamos del funcionamiento del modelo de Persextron de una capa, su algoritmo no tiene información previa, por lo que inicialmente los pesos se asignan de forma inconstante. Entonces el algoritmo suma todas las entradas ponderadas. Si el valor añadido es más que algún valor predeterminado o valor umbral, entonces el Persextron de una capa se declara activo y entrega la salida como más 1. En palabras sencillas, los valores de entradas que han sido multiplicados entran al modelo de Persextron y estos son los datos que se utiliza para ejecutar el mismo. Una vez que se hayan obtenido los valores de salida se comparan con los valores que se estima se debe obtener. Si no hay diferencia entre ellos podemos decir que nuestro modelo está perfecto y no hay necesidad de modificar los pesos. Si por el contrario, los valores de salida que hemos obtenido con nuestro modelo son diferentes a los valores estimados se deben ajustar los pesos para poder obtener un mejor resultado. Expliquemos ahora el modelo de Persextron multi capa. Un modelo de Persextron multi capa tiene una estructura similar a la de un modelo de Persextron de una sola capa con más número de capas ocultas. También se denomina algoritmo de retro propagación. Se ejecuta en dos etapas, la etapa hacia adelante y la etapa hacia atrás. En la etapa hacia adelante las funciones de activación se originan desde la capa de entrada a la salida y en la capa hacia atrás el error entre el valor observado y el valor demandado se origina hacia atrás en la capa de salida para modificar los pesos. En términos sencillos, el Persextron multi capa está formado por numerosas redes neuronales sobre distintas capas. Acá la función de activación será no lineal, por lo tanto debemos aplicar funciones de activaciones no lineales como la sigmoide, la relu entre muchas otras. Los modelos de Persextron son el tipo más simplista de red neuronal en la que llevan una entrada. El peso de cada entrada toma la suma de la entrada ponderada y se aplica una función de activación. Aceptan y construye solo valores binarios, es decir, el Persextron solo se implementa para la clasificación binaria con la limitación de que solo son aplicables para objetos linealmente separables. El Persextron es la base de las redes neuronales, por lo tanto es muy importante entender cómo funciona el Persextron para poder ir avanzando con redes neuronales mucho más profundas. Te dejo la siguiente pregunta para comprobar lo que has aprendido con este contenido. De las siguientes afirmaciones ¿Cuál crees que es cierta? Opción 1 El Persextron es un modelo matemático inspirado en una estructura y función simplificadas de una única neurona biológica. Opción 2 El Persextron se puede utilizar para proyectos tanto de clasificación como de regresión. Opción 3 Los componentes del Persextron son la entrada, pesos, función de activación, suma ponderada y la salida. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. No te olvides de pasarte por la cajita de información debajo de este video en donde te dejo varios enlaces que seguramente serán de tu interés sobre publicaciones relacionadas a la Inteligencia Artificial y si aún no lo has hecho, suscríbete a nuestro canal y presiona en la campanita de notificación para que estés al tanto de todos los contenidos que vamos publicando. También puedes escribir en los comentarios cualquier duda o sugerencia que tengas y estaré encantada de responderla. Te veo en el siguiente video. Chao.
DOC0011|Introduccion IA|Hay muchas cosas que las computadoras pueden hacer, pero hay otras que nosotros los humanos le llevamos ventajas, como por ejemplo el sentido común, la inspiración, la imaginación. Basándose en el cerebro humano, las redes neuronales artificiales lo que quieren es parecerse un poco más a nosotros y justamente de eso es que te explicaré en este video. Hola a todos, soy Ligli González de Aprendía y en el video de hoy te explicaré cómo funcionan las redes neuronales artificiales, cómo se pueden implementar y mucho más. Por lo tanto, empecemos con este video. Comencemos con lo más fundamental que tenemos que entender, ¿qué son las redes neuronales artificiales? Las redes neuronales artificiales son un tipo especial de algoritmos de Machine Learning y se modelan a partir del cerebro humano. Igual que nosotros, las redes neuronales artificiales aprende de datos pasados y en función de ellas toma una decisión para realizar una predicción. Las redes neuronales artificiales son modelos estadísticos no lineales que muestran una relación compleja entre las entradas y las salidas para descubrir un nuevo patrón. Una gran variedad de tareas como el reconocimiento de imágenes, el reconocimiento del habla, la traducción automática y el diagnóstico médico hacen uso de estas redes. Una ventaja importante que tienen es que aprenden a partir de conjuntos de datos de ejemplo. El uso más habitual es el de la aproximación de funciones aleatorias. Las redes neuronales artificiales también son capaces de tomar muestras de los datos en vez de tomar todo el conjunto de datos. Con ellas puede proporcionar una salida. Con estas redes se puede aumentar el nivel de procesamiento y obtener mejores resultados debido a esta capacidad de análisis. Continuemos hablando sobre la historia de las redes neuronales. Aunque la gran mayoría crea que las redes neuronales artificiales son recientes, en realidad se remontan a la antigua Grecia. Pero no nos iremos tan lejos para analizar su historia, veamos los últimos acontecimientos en su desarrollo. 1943. Dos investigadores publicaron un documento que pretendía comprender cómo el cerebro humano podía producir patrones complejos a través de las células cerebrales conectadas o neuronas. Una de las principales ideas que surgieron de este trabajo fue la comparación de las neuronas con un umbral binario a la lógica boleana, es decir, afirmaciones cero y uno o verdadero falso. 1958. Se atribuye a Frank Rosenblatt el desarrollo del perceptron documentado en su investigación. Lleva el trabajo anterior un paso más allá al introducir pesos en la ecuación. Aprovechando un IBM 704, Rosenblatt consiguió que un computador aprendiera a distinguir las cartas marcadas a la izquierda de las marcadas a la derecha. 1974. Aunque numerosos investigadores contribuyeron a la idea de la retropropagación, Paul Werbos fue la primera persona en Estados Unidos en anotar su aplicación en las redes neuronales dentro de su tesis doctoral. 1989. Jean Lee Kuhn publicó un artículo en el que ilustra cómo el uso de restricciones en la retropropagación y su integración en la arquitectura de la red neuronal puede utilizarse para entrenar algoritmos. Esta investigación aprovechó con éxito una red neuronal para reconocer los dígitos de los códigos postales escritos a mano por el Servicio Postal de Estados Unidos. Explicemos ahora los componentes de las redes neuronales artificiales. Hay diferentes tipos de redes neuronales, pero sus componentes más importantes son neuronas, pesos y vías. Neuronas. Todas las neuronas de una red se dividen en tres grupos. Neuronas de entradas que reciben información del mundo exterior, neuronas ocultas que procesan esa información y neuronas de salidas que producen una conclusión. En una gran red neuronal con muchas neuronas y conexiones entre ellas, las neuronas se organizan en capas. Hay una capa de entrada que recibe información, un número de capas ocultas y la capa de salida que proporciona resultados valiosos. Cada neurona realiza una transformación sobre la información de entrada. Las neuronas solo operan con números en el rango de 0 y 1 o –1,1. Pesos. Los pesos también se suman a los cambios en la información de entrada. Los resultados de la neurona con mayor peso serán dominantes en la siguiente neurona, mientras que la información de las neuronas con menos peso no se transmitirá. Se puede decir que la matriz de pesos gobierna todo el sistema neuronal, pero ¿cómo se sabe qué neurona tiene el mayor peso? Durante la inicialización, primera puesta en marcha de la red neuronal, los pesos se asignan aleatoriamente, pero luego habrá que optimizarlos. Vías. Una neurona de vías permite almacenar más variaciones con pesos. Las vías añade una representación más rica del espacio de entrada a los pesos del modelo. En el caso de las redes neuronales se añade una neurona de vías a cada capa. Esta desempeña un papel fundamental al hacer posible el desplazamiento de la función de activación hacia la izquierda o a la derecha en el gráfico. Es cierto que las redes neuronales artificiales pueden funcionar sin neuronas de polarización, sin embargo casi siempre se añade y se considera una parte indispensable del modelo ¿Cómo funcionan las redes neuronales artificiales? El componente fundamental de las redes neuronales artificiales son precisamente las neuronas. Las neuronas reciben los datos de entradas, los cuales son multiplicados por su respectivo peso y luego son sumados. Una vez realizada esta operación pasa esta información a la siguiente neurona. En ocasiones se utiliza una función de activación en la salida antes de pasarlo a la siguiente variable. En ocasiones esto puede parecer operaciones matemáticas triviales, pero cuando le agregamos cientos, miles o millones de neuronas aquí la operación se vuelve un poco más compleja, lo que hace que el procesamiento sea mucho mayor. Las redes neuronales artificiales se componen de una capa de entrada que recibe datos de fuentes externas, archivos de datos, imágenes, sensores de hardware, micrófono, entre otros. Una o varias capas ocultas que procesan los datos y una capa de salida que proporciona uno o varios puntos de datos basándose en la función de la red. Por ejemplo, una red neuronal que detecte personas, coches y animales tendrá una capa de salida con tres nodos. Una red que clasifique transacciones bancarias entre seguras y fraudulentas tendrá una única salida. Si te interesa profundizar sobre estos temas de Inteligencia Artificial en nuestro canal de YouTube podrás encontrar bastante contenido relacionado a estos temas, por lo tanto te recomiendo que los revises y a su vez que te suscribas al canal si aún no lo has hecho. Entrenamiento de redes neuronales artificiales Las redes neuronales artificiales comienzan asignando valores aleatorios a los pesos de las conexiones entre neuronas. La clave para que la red neuronal realice su tarea de forma correcta y precisa es ajustar estos pesos a los números adecuados. Pero encontrar los pesos adecuados no es muy fácil, sobre todo cuando se trata de múltiples capas y miles de neuronas. Esta calibración se realiza entrenando la red con ejemplos anotados. Por ejemplo, si se quiere entrenar un clasificador de imágenes se le proporcionan múltiples fotos. Cada una etiqueta con su clase corresponde a una persona, auto o animal. A medida que le proporcionas más y más ejemplos de entrenamiento la red neuronal ajusta gradualmente sus pesos para asignar cada entrada a las salidas correctas. Básicamente lo que ocurre durante el entrenamiento es que la red se ajusta para encontrar patrones dentro de los datos. De nuevo, en el caso de una red clasificadora de imágenes, cuando se entrena el modelo de inteligencia artificial con ejemplos de calidad cada capa detecta una clase específica de características. Por ejemplo, la primera capa puede detectar los bordes horizontales y verticales. Las siguientes capas pueden detectar esquinas y formas redondas. Más adelante las capas más profundas empezarán a detectar características más avanzadas como caras y objetos. Cuando se pasa una nueva imagen por una red neuronal bien entrenada los pesos ajustados de las neuronas podrán extraer las características adecuadas y determinar con precisión a qué clase de salida pertenece la imagen. Veamos ahora los tipos de redes neuronales artificiales. Las redes neuronales se clasifican en diferentes tipos que pueden ser utilizados en distintas aplicaciones. A continuación te presento las más importantes de ellas. El Persextron es la red neuronal más antigua, creada por Frank Rosenblatt en 1958. Tiene una sola neurona y es la forma más simple de una red neuronal. Las redes neuronales de avance o Persextron multicopa son las que nos hemos centrado principalmente en este video. Se compone de una capa de entrada, una o varias capas ocultas y una capa de salida. Es importante señalar que en realidad están compuestas por neuronas sigmoides, no por Persextron, ya que la mayoría de los problemas del mundo real no son lineales. Los datos suelen introducirse en estos modelos para entrenarlos y son la base de la visión por computador, el procesamiento del lenguaje natural y otras redes neuronales. Las redes neuronales convolucionales son similares a las redes de alimentación, pero suelen utilizarse para el reconocimiento de imágenes, el reconocimiento de patrones y o la visión computacional. Estas redes aprovechan los principios del álgebra lineal, especialmente la multiplicación de matrices, para identificar patrones dentro de una imagen. Las redes neuronales recurrentes se identifican por sus bucles de retroalimentación. Estos algoritmos de aprendizaje se aprovechan principalmente cuando se utilizan datos de series temporales para hacer predicciones sobre resultados futuros, como las predicciones del mercado de valores o la previsión de ventas. Redes neuronales frente a Deep Learning El término de Deep Learning y el de redes neuronales artificiales se utilizan indistintamente en las conversaciones. Lo importante es que tengas claro que el término de Deep Learning se refiere a la profundidad que se le pueden dar a las capas. Una red neuronal que consta de más de tres capas, que incluirá las entradas y la salida, puede considerarse como un algoritmo de Deep Learning. Una red neuronal que solo tiene dos o tres capas es solo una red neuronal básica. ¿Cuáles son los límites de las redes neuronales artificiales? A pesar de que la intención con las redes neuronales artificiales es simular el cerebro humano, en realidad están muy lejos de lo grave. Y no sabemos cuándo podrás ser capaz. A continuación te presento algunas de las limitancias que se tienen con las redes neuronales artificiales. Las redes neuronales necesitan muchos datos. A diferencia del cerebro humano que puede aprender a hacer cosas con muy pocos ejemplos, las redes neuronales necesitan miles y millones de ejemplos. Las redes neuronales son malas para generalizar. Una red neuronal puede realizar con precisión una tarea para lo que ha sido entrenada, pero muy mal cualquier otra cosa. Inclusive, si es similar al problema original. Un clasificador de gatos entrenado con miles de fotos de gatos no podrá detectar perros. Para ello necesitará miles de imágenes nuevas. A diferencia de los humanos, las redes neuronales no desarrollan conocimientos en términos de símbolos, orejas, ojos, bigotes, sino que procesan valores de píxeles. Por eso no podrán aprender sobre nuevos objetos en términos de características de alto nivel y tendrán que volver a ser entrenados desde cero. Las redes neuronales son opacas. Como las redes neuronales expresan su comportamiento en términos de peso y activaciones de las neuronas, es muy difícil determinar la lógica que hay detrás de sus decisiones. Por eso suelen describirse como cajas negras. Esto hace que sea difícil de averiguar si están tomando decisiones basadas en factores erróneos. ¿Para qué se utilizan las redes neuronales artificiales? Las siguientes son las aplicaciones más importantes de las redes neuronales artificiales. Reconocimiento de características escritas a mano. Las redes neuronales se entrenan para reconocer los caracteres manuscritos que pueden ser letras o dígitos. Reconocimiento del habla. Los primeros modelos de reconocimiento de habla se basan en modelos estadísticos. Con la llegada de deep learning, varios tipos de redes neuronales son la opción absoluta para obtener una clasificación precisa. Clasificación de firmas. Para reconocer firmas y categorizarlas, se utilizan redes neuronales artificiales para construir estos sistemas de autenticación. Además, las redes neuronales también pueden clasificar si la forma es falsa o no. Reconocimiento facial. Para reconocer los rostros en función de la identidad de la persona, se recurren a las redes neuronales. Son las más utilizadas en áreas donde los usuarios requieren un acceso de seguridad. Las redes neuronales convolucionales son el tipo más popular utilizado en este campo. Aunque hemos avanzado con las redes neuronales artificiales, aún nos falta un largo camino por recorrer. Inclusive va a ser interesante ver las siguientes innovaciones que se va a poder lograr con esta tecnología. Con esto finalizamos la explicación. Por lo tanto, te dejo la siguiente pregunta para comprobar lo que has aprendido con este contenido. De las siguientes afirmaciones, ¿cuál crees que es cierta? Opción 1. La historia de las redes neuronales artificiales es muy reciente. Desde hace unos pocos años se viene desarrollando. Opción 2. Las redes neuronales artificiales se componen de una capa de entrada, capas ocultas y una capa de salida. Opción 3. El profundo en deep learning solo se refiere a la profundidad de las capas en una red neuronal. Deja en los comentarios cuál crees que es la respuesta correcta. Puede ser una o más las respuestas correctas. No te olvides de pasarte por la cajita de información debajo de este video en donde te dejo varios enlaces que seguramente serán de tu interés sobre publicaciones relacionadas a la inteligencia artificial. Y si aún no lo has hecho, suscríbete a nuestro canal y presiona la campanita de notificación para que estés al tanto de todos los contenidos que vamos publicando. También puedes escribir en los comentarios cualquier duda o sugerencia que tengas y estaré encantada de responderla. Te veo en el siguiente video. Chao. Chau.
DOC0012|Datalakes|Hola a todos y bienvenidos de nuevo a este canal. Hoy veremos lo que es un Data Lake. Un Data Lake es un sistema o almacén centralizado de datos que te permite almacenar todos tus datos estructurados, semiestructurados, no estructurados y binarios en formato nativo y sin procesar. Los datos estructurados pueden incluir tablas de base de datos relacionales. Los datos semiestructurados incluyen archivos CSV, archivos XML, registros JSON, etc. Los datos no estructurados pueden incluir archivos PDF, documentos de Word, archivos de texto, correos electrónicos, etc. Y los datos binarios pueden incluir archivos de audio, video e imágenes. Como un Data Lake puedes almacenar todos los datos de tu empresa tal como están en un solo lugar y sin necesidad de estructurar los datos. Puedes ejecutar directamente los diversos tipos de análisis en él, incluidos el aprendizaje automático, el análisis en tiempo real, el movimiento de datos en las instalaciones, el movimiento de datos en tiempo real, los cuadros de mando y las visualizaciones. El Data Lake mantiene los datos en su forma original y supone que el análisis se realizará más tarde bajo demanda. Analogía del Data Lake. Os dejo aquí una imagen que ahora os explicaré. El término Data Lake fue acuñado por James Dixon, el entonces CTO de Pentaho, herramienta de la que si quieres saber un poco más te dejo aquí arriba a la derecha un vídeo. Dixon defiende que el Datamar, un subconjunto de un Data Warehouse, es similar a una botella de agua llena de agua limpia y destilada, empaquetada y estructurada para un uso directo y fácil. Por otro lado, el término Data Lake es análogo a un cuerpo de agua en su forma natural. Los datos fluyen desde los flujos hasta el lago. En el Data Lake, los usuarios tienen acceso al lago para analizar, examinar, recolectar muestras y sumergirse. Al igual que el agua, en el lago satisface las diferentes necesidades de las personas, como pescar, navegar, proporcionar agua potable, etc., de manera similar, la arquitectura del Data Lake sirve para múltiples propósitos. Un Data Science puede usarlo para explorar los datos. Ofrece una oportunidad para que los analistas de datos analicen datos y descubran patrones. Proporciona un modo para que los usuarios comerciales y las partes interesadas exploren los datos. También ofrece una oportunidad para que los analistas diseñen informes y los presenten al negocio. Por el contrario, el Data Warehouse tiene datos empaquetados para propósitos bien definidos. El mercado del Data Lake se divide en función del propósito, solución o servicio, implementación, local o en la nube, industria de los clientes, venta minoritaria, banca, servicios públicos, seguros, TI, atención médica, telecomunicaciones, publicaciones, fabricación, según el informe publicado por Mordor Intelligence. Resumen del mercado actual. El mercado de los Data Lakes se evaluó en 3.740 millones de dólares en 2019 y se prevé que alcance los 17.600 millones de dólares para 2025, con una tasa de crecimiento anual compuesta del 29,9% durante el período de proyección 2020-2025. Estos almacenes de datos se están convirtiendo cada vez más en una opción económica para muchas organizaciones en lugar de los Data Warehouse. A diferencia de los Data Lakes, el almacenamiento de datos requiere un procesamiento adicional de los datos antes de ingresar en el almacén. El coste de administrar un Data Lake es menor en comparación con un Data Warehouse debido a que se requiere mucho procesamiento y espacio para crear la base de datos para los almacenes. Empresas principales. Se prevé que el mercado de Data Lakes será un mercado consolidado dominado por las cinco empresas claves, como se ve en la imagen a Microsoft, Amazon, Oracle, Campec Mini y Teradata. Tendencias claves. Se espera que su uso crezca considerablemente en el sector bancario. Los bancos están adaptando Data Lakes para ofrecer análisis sobre la marcha. Además, está ayudando a disolver muchos hilos en el sector bancario. Dado que hay un gran aumento en los pagos digitales de billeteras móviles, en todo el mundo el alcance del análisis de Big Data y por lo tanto la oportunidad para ellos está aumentando. Se anticipa que América del Norte tendrá una gran adopción de Data Lakes. Un estudio realizado por Campec Mini dice que más del 60% de las organizaciones financieras en Estados Unidos piensan que el análisis de Big Data actúa como un diferenciador para los negocios y les brinda una ventaja competitiva. Más del 90% de las organizaciones sienten que invertir en proyectos de Big Data aumenta las posibilidades de éxito en el futuro. ¿Por qué son necesarios los Data Lakes? El objetivo de un Data Lake es brindar una vista sin procesar de los datos. Datos en su forma más pura. Por ejemplo, hoy en día muchas grandes empresas, incluidas Google, Amazon, Cluvera, Oracle, Microsoft y alguna más, tienen ofertas de Data Lake. Muchas organizaciones utilizan servicios de almacenamiento en la nube como Azura Data Lake o Amazon S3. Las empresas también están utilizando un sistema de archivos distribuido como Apache Hadoop. El concepto de un Data Lake personal que te permita administrar y compartir tu propio Big Data también ha evolucionado. Si hablamos de usos industriales, es muy adecuado para el ámbito sanitario, debido al formato no estructurado de una gran cantidad de datos en el cuidado de la salud. Por ejemplo, notas médicas, datos clínicos, historial de enfermedades del paciente, etc. Y el requisito de información en tiempo real. Un Data Lake es una excelente opción en lugar de un Data Warehouse. También ofrece soluciones flexibles en el sector de la educación, donde los datos son muy amplios y muy crudos. En el sector del transporte, principalmente en la gestión de la cadena de suministro o logística, ayuda a hacer predicciones y obtener beneficios de reducción de costes. Las industrias de la aviación y la energía eléctrica también utilizan la Data Lakes. Un ejemplo de su implementación es GEpPredix desarrollado por General Electric, que es una plataforma de Data Lakes industriales que ofrece sólidas competencias de gobierno de datos para crear, implementar y gobernar aplicaciones industriales que se vinculan con actos industriales, recopilan y analizan datos y brindan información en tiempo real. Diferencias entre Data Warehouse y Data Lake. A menudo a las personas les resulta difícil entender en qué se diferencia un Data Lake de un Data Warehouse. También argumentan que es lo mismo que el Data Warehouse, pero esto no es la realidad. El único punto en común entre el Data Lake y el Data Warehouse es que ambos son repositorios de almacenamiento de datos. Tienen diferentes casos de uso y propósitos. Por ejemplo, un Data Lake mantendrá en él los datos sin procesar. Puede ser estructurado, no estructurado o semiestructurado. Es posible que algunos de los datos del Data Lake nunca se utilicen, mientras que un Data Warehouse incorpora solo los datos que se procesan y refinan, es decir, los datos estructurados que se requieren para informar y resolver problemas comerciales específicos. Datos analíticos que ayudan a dar respuesta a preguntas propias de la empresa. Generalmente, los usuarios de un Data Lake son científicos de datos, data science y desarrolladores de datos, data engineer, mientras que generalmente los usuarios del Data Warehouse son profesionales de negocio, usuarios operativos y analistas. El Data Lake es altamente accesible, fácil y rápido de actualizar porque no tiene ninguna estructura, mientras que el Data Warehouse, actualizar los datos es una operación más complicada y costosa porque los almacenes de datos están estructurados por diseño. El esquema del Data Lake es basado en escritura, es decir, diseñado antes de la implementación del Data Warehouse, mientras que el esquema del Data Warehouse es basado en lectura, escrito en el momento del análisis. La arquitectura del Data Lake es plana, mientras que la del Data Warehouse es una arquitectura jerárquica. El propósito de los datos sin procesar, almacenados en el Data Lake, no es fijo o no está determinado. A veces los datos pueden fluir hacia un Data Lake con algún uso futuro específico en mente o simplemente para tener los datos a mano. El Data Lake tiene datos menos organizados y menos filtrados, mientras que los datos tratados y almacenados en el Data Warehouse tienen una finalidad específica y definida. Un Data Warehouse tiene datos organizados y filtrados, por lo tanto requiere menos espacio de almacenamiento que en el Data Lake. Un Data Lake se puede utilizar para el aprendizaje automático, el perfilado de los datos de descubrimiento de datos y el análisis proyectivo, mientras que un Data Warehouse se puede utilizar para Business Intelligent, visualizaciones e informes modo batch. El almacenamiento en el Data Lake está diseñado para utilizar un almacenamiento de bajo coste. El hardware del Data Lake es muy diferente al hardware del Data Warehouse. Utiliza servidores listos para usar, combinados con almacenamiento económico. Esto hace que el Data Lake sea bastante económico y altamente escalable a terabytes y petabytes. Esto se hace para mantener todos los datos en un Data Lake para que pueda volver a la hora en cualquier momento para reutilizar el análisis. El Data Warehouse es caro para grandes volúmenes de datos. El Data Warehouse tiene un almacenamiento en disco costoso para que tenga un alto rendimiento. Por lo tanto, para conservar el espacio, el modelo de datos se simplifica y solo los datos que realmente se requieren para tomar decisiones comerciales se mantienen en él. Un Data Lake admite muy bien los tipos de datos no tradicionales, como registros del servidor, datos de sensores, actividades de redes sociales, texto y imágenes multimedia, etc. Todos los datos se conservan independientemente de la fuente y la estructura. Mientras que generalmente un Data Warehouse consta de datos obtenidos del sistema transaccionales, no soportan muy bien los tipos de datos no tradicionales. Almacenar y consumir los datos no tradicionales puede ser costoso y difícil con el Data Warehouse. La seguridad de los Data Lakes se encuentra en una etapa de maduración, ya que esto es un concepto relativamente nuevo. Asimismo, la seguridad de los Data Warehouse también se encuentra en una etapa de maduración. Un Data Lake es altamente ágil. Configurar y reconfigurar según sea necesario no es un problema, mientras que un Data Warehouse es mucho menos ágil, ya que su configuración es fija. Arquitectura del Data Lake. Aquí te dejo la siguiente imagen. En ella puedes ver el diagrama de arquitectura conceptual de un Data Lake. En la parte más a la izquierda puedes ver que tenemos las fuentes de datos que pueden ser estructuradas, semiestructuradas o no estructuradas. Estas fuentes de datos se combinan en un almacén de datos sin procesar que utiliza los datos en su forma sin procesar, es decir, datos sin transformaciones. Este es un almacenamiento de bajo coste, permanente y escalable. A continuación, tenemos sandboxes analíticos que se pueden usar para el descubrimiento de datos, en análisis exploratorio de datos y en moderado predictivo. Básicamente, los científicos de datos lo utilizan para explorar datos, construir nuevas hipótesis y definir casos de uso. Luego, hay un motor de procesamiento por lotes que procesa los datos sin procesar en una forma utilizable por el consumidor, es decir, en un formato estructurado que se puede usar para informar a los usuarios finales. Por último, tenemos un motor de procesamiento en tiempo real que se toma en la transmisión de datos y los transforma. Características clave del Data Lake. Para ser clasificado como Data Lake, un repositorio de Big Data debe poseer los siguientes tres atributos. Un único almacen común de datos que normalmente se encuentra dentro de un sistema de archivos distribuido, HDFS. Los Data Lakes de Hadoop mantienen los datos en su forma nativa y capturan los cambios en los datos y la semática relativa durante el ciclo de vida de los datos. Este enfoque es particularmente beneficioso para los controles de cumplimiento y las auditorias internas. Esta es una mejora sobre el almacén de datos empresarial convencional en el que cuando los datos pasan de transformaciones, agregaciones y modificaciones es difícil colocar los datos como un todo cuando es necesario y las empresas se esfuerzan para descubrir la fuente origen de los datos. Incorpora capacidades de planificación y programación de jobs, por ejemplo, a través de cualquier herramienta de programación como Yarn. La ejecución de la carga de jobs es una necesidad esencial para la empresa Hadoop y Yarn, que ofrece gestión de recursos y una plataforma central para proporcionar procesos constantes, seguridad y herramientas de gobierno de datos en los clásteres de Hadoop, asegurándose de que los flujos de trabajo analíticos posean el nivel requerido de acceso a datos y poder de cómputo. Comprende el conjunto de utilidades y funciones requeridas para consumir, procesar o trabajar con los datos. La accesibilidad fácil y rápida para los usuarios es una de las características claves de un Data Lake, ya que las organizaciones almacenan los datos en su forma nativa o pura. Independientemente de la forma en que se encuentren los datos, es decir, estructurados, no estructurados o semiestructurados, se insertan tal como están en el Data Lake. Permite a los propietarios de datos combinar datos de clientes, proveedores y operaciones, eliminando cualquier barrera técnica o política para compartir datos. Tiene diferentes beneficios, por ejemplo, es versátil lo suficientemente competente para almacenar datos, tipo de datos, estructurados o no estructurados, desde datos de CRM hasta actividades de redes sociales. Más flexibilidad de esquema. No necesita planificación ni conocimientos previos de análisis de datos. Almacena todos los datos tal como están en su forma original y supone que el análisis se realizará más tarde bajo demanda. Esto es muy útil para el app. Por ejemplo, el Data Lake de Hadoop le permite estar libre de esquema donde puede desacoplar el esquema de los datos. Análisis de decisiones en tiempo real. Disfrutan del beneficio de una gran cantidad de datos consistentes y algoritmos de aprendizaje profundo para alcanzar análisis de decisiones en tiempo real, capaz de obtener valor de tipos de datos ilimitados. Escalabilidad. Son mucho más escalables que los almacenes de datos tradicionales y también son menos costosos. Análisis avanzado, compatibilidad con SQL y otros lenguajes. Con los Data Lakes existen numerosas formas de consultar los datos. A diferencia de los almacenes de datos tradicionales que solo admiten SQL para análisis simple, le brindan muchas otras opciones y soporte de idiomas para analizar datos. También son compatibles con herramientas de aprendizaje automático como Spark y MLLiv. Aceso de los datos. Aceso democratizado a los datos a través de una vista única e integrada de los datos en toda la organización mientras se utiliza una plataforma de gestión de datos eficaz. Esto asegura la disponibilidad total de los datos. Mejor calidad de datos. En general obtiene una mejor calidad de datos con Data Lakes a través de beneficios tecnológicos como el almacenamiento de datos en formato nativo, escalabilidad, versatilidad, flexibilidad de esquema, compatibilidad con SQL y otros lenguajes y análisis avanzados. Desafíos y riesgos. Los Data Lakes ofrecen muchas ventajas, pero si también hay algunos desafíos y riesgos asociados con ellos que una organización debe abordar con cuidado. Si no se diseñan correctamente, pueden convertirse en pantanos de datos. A veces las organizaciones simplemente terminan arrojando datos ilimitados en estos Data Lakes sin ninguna estrategia ni propósito en mente. A veces los analistas que quieren usar los datos no saben cómo hacerlo, ya que es bastante desafiante hacer minería en Data Lakes. Por lo tanto, pierden relevancia e impulso después de un tiempo. Las organizaciones deben trabajar para eliminar esta barrera para los analistas. Como tenemos una gran cantidad de datos desorganizados en los Data Lakes, no son lo suficientemente frescos o autorizados para usarlos en producción. Por lo tanto, los datos en estos lagos permanecen en modo piloto y nunca se ponen en producción. Los datos no estructurados pueden conducir a datos inutilizables. En ocasiones las organizaciones experimentan que no están teniendo un impacto significativo en el negocio con respecto a las inversiones realizadas. Esto requiere un cambio de mentalidad. Para que se produzcan impactos, las empresas deben adentrar a los gerentes y líderes a tomar decisiones basadas en los análisis derivados de estos depósitos de datos. La seguridad y el control de acceso también son uno de los riesgos cuando se trabaja con ellos. Algunos de los datos que pueden tener privacidad y regulaciones requeridas se colocan en lagos de datos sin supervisión. Implementación. En una empresa es bastante sensato realizar la implementación del Data Lake de manera ágil. Es decir, para implementar primero un MVP de Data Lake, los usuarios lo prueban con respeto a la calidad, la facilidad de acceso, el almacenamiento y las capacidades analíticas. Reciben comentarios y luego agregan los requisitos y características complejas para agregar valor al Data Lake. En general, una organización pasa por las siguientes cuatro etapas básicas de implementación. Nivel 1. El Data Lake básico. En esta etapa el equipo establece la arquitectura básica, la tecnología basada en la nube o heredada y las prácticas de seguridad y gobierno para el Data Lake. Es capaz de almacenar todos los datos sin procesar provenientes de varias fuentes empresariales y combinar los datos internos y externos para brindar información enriquecida. Etapa 2. El sandbox. Mejora la capacidad analítica. En esta etapa los científicos de data o data science acceden al depósito de datos para ejecutar experimentos preliminares para utilizar datos sin procesar y diseñar modelos analíticos para satisfacer las necesidades comerciales. Etapa 3. Colaboración entre almacenes de datos y Data Lakes. En esta etapa la organización comienza a utilizar un Data Lake en sinercia con los almacenes de datos existentes. Los datos de baja prioridad se les envían para que no se exceda el límite de almacenamiento de los almacenes de datos. Presenta una perspectiva para producir información a partir de datos fríos o consultarlos para descubrir información que no está indeseada en la base de datos convencionales. Etapa 4. Adopción de extremo extremo del Data Lake. Esta es la última etapa de adquisición de madurez en la que se convierte en un elemento clave de la arquitectura de datos de la organización y dirige de manera efectiva la operación de búsqueda. En ese momento el Data Lake habría sustituido al Data Warehouse y se convertiría en la única fuente de datos de los datos de la empresa. Una organización puede hacer lo siguiente a través del Data Lake. Crear soluciones complejas de modelado y análisis de datos para diferentes necesidades comerciales, diseñar cuadros de mandos que consoliden la comprensión del Data Lake más varias aplicaciones y fuentes de datos, implementar programas de análisis o roboteca avanzada ya que maneja operaciones computacionales. En este punto también tienen fuertes medidas de seguridad y de gobierno de datos. Hay diferentes proveedores que ofrecen herramientas de Data Lakes en la industria. Si miramos a las grandes empresas nos encontramos con informática proporcionada una herramienta de Data Lake inteligente, BDM, Big Data Management. Hay un proveedor llamado Locker que también proporciona la herramienta. La empresa Talent que es popular por sus herramientas ETL también proporciona la herramienta Data Lake. Luego tenemos una herramienta de código abierta llamado Kylo de la empresa Teradata. El equipo llamado ZingBing de la empresa Teradata ha desarrollado esta herramienta. La empresa Cash Dataing también brinda estos servicios. Desde Microsoft puedes encontrar el Data Lake de Azure disponible por la industria. Snowflake también tiene un producto de Data Lake y Zaloni es una empresa de Data Lakes que maneja grandes cantidades de datos utilizando Big Data. Por lo tanto todos estos son los proveedores de servicios populares así como los proveedores de dichas herramientas. Como conclusión en este vídeo hemos comentado el concepto del Data Lake en detalle. Analizamos la idea básica detrás del Data Lake, su arquitectura, características claves, beneficios, con sus ejemplos y casos de uso. También vimos cómo un Data Lake es diferente al Data Workhouse cubriendo a los principales proveedores que brindan servicios relacionados. Muchas gracias por llegar hasta el final del vídeo. Si te ha gustado te agradecería que te suscribieras al canal y le dieras a like. Si tienes alguna duda puedes dejarme en los comentarios y te
DOC0013|Datalakes|¿Cuál es el poder de la inteligencia aplicada? El acceso a la información y datos se restringía exclusivamente a una sola fuente a la que se podía acceder y obtener métricas puntuales, donde pocos usuarios interactuaban con la aplicación y su base de datos, generando información y reportes gracias a estas interacciones. El modelo de data warehouse permitió que una mayor cantidad de usuarios interactuarán no con una, sino con docenas de aplicaciones, de las cuales extraían datos transaccionales que se almacenaban en el data warehouse de la compañía y de la cual se obtenían indicadores y otras utilidades de información y funcionalidades. Finalmente, el Data Lake permitió que millones de usuarios interactúen con cientos de aplicaciones, las cuales interaccionan con la Data Lake, que ya contiene datos tanto básicos como procesados, a la que se le aplica una capa de análisis que, alimentada por esta big data, brinda un potencial enorme para la toma de decisiones permitiendo no solo acceder a los datos, sino administrarlos, añadir nuevos, gobernarlos y asegurar la información. Además de analizar grandes cantidades de datos, permite generar modelos predictivos para aplicaciones de machine learning, entender y predecir comportamientos de consumidores, entre otros casos de uso. Este es el poder de la inteligencia aplicada.
DOC0014|Datalakes|Así está la cosa, los datos son el petróleo de esta época, pero existen muchas formas de almacenar y analizar estos datos. Si la organización elige mal entre las alternativas, podría enfrentarse un problema muy costoso. Hola, yo soy Piamis Treta y esto es IT Masters News. No olvides suscribirte al canal, hoy vamos a platicar de Data Warehouse, Data Lake y Data Mart, que son, para qué sirven y cómo se deben utilizar. En términos generales, los datos de una organización son generados por distintos sistemas como CRM o RP, entre muchos otros. Pero para pasar de sólo ser información a generar inteligencia de negocio, deben centralizarse en un solo lugar, desde el que se puede ejecutar análisis de datos de todo tipo para así descubrir tendencias que ayuden a la toma de decisiones. Este lugar puede ser principalmente un lake o un warehouse. En un Data Lake se invierten todas las formas de datos que se han generado a través de la empresa. Esto incluye puentes de datos estructurados, registros de conversaciones, correos electrónicos, imágenes, audio, videos y prácticamente todo lo que se les ocurra. Por lo mismo, esto resulta en una cantidad muy grande de información acumulada. Todo suena muy agradable en papel, pero los Data Lakes manejan una cantidad apabullante de información. Los volúmenes son tan altos que las bases de datos tradicionales pueden tardar días en ejecutar una sola petición, por lo que hardware especializado y fuertes inversiones en almacenamiento son inseparables en los lagos de información. Por otro lado, y a diferencia de las oscuras profundidades que hay dentro de los datos sin estructurar de un Data Lake, un Data Warehouse tiene sus anaqueles limpios y sus datos ordenados para extraer información mucho más rápido. Pero no cualquier dato, los Data Warehouse suelen guardar información que ya ha sido estructurada. Y bueno, ¿qué pasó con el Data Mart? Y esta tercera estrategia de análisis y almacenamiento suele considerarse una subsección de Data Warehouse, o para una necesidad muy puntual de un departamento. A diferencia de un Warehouse y un Lake, donde la información es almacenada en un archivo único y centralizado, los Data Marts cuentan con una fuente diferenciada y descentralizada de datos. Así ganan un nivel de seguridad mayor para la organización en general, ya que la unidad que usa el Data Mart solo tendrá acceso a los datos previamente cargados en su base. O sea, no ve los datos del resto de la compañía, lo mismo aplica a la eficiencia. Las cargas de trabajo en un entorno aislado no utilizan recursos de otros sectores o departamentos. En términos generales, Data Mart es la aproximación más pequeña de las tres y está orientada a proyectos a corto plazo. La elección entre estos tres sistemas dependerá de las necesidades presentes y futuras del negocio. Es posible que muchas organizaciones no vayan a necesitar un Data Mart para operar con éxito. Sin embargo, diversos analistas recomiendan que el Warehouse y el Lake se desplieguen de manera paralela. Puede que hoy esos grandes volúmenes de datos parezcan basura, pero nada dice que no puedan ser una nueva fuente de ingresos. De todas formas, debido a los altos costos de los Data Lakes, es un lujo que solamente algunas empresas podrían darse. ¿Les quedó claro que es un Data Lake, un Data Warehouse o un Data Mart? Déjenme sus comentarios, yo soy Pia Mistreta y nos vemos en la próxima edición. ¡Suscríbete al canal!
DOC0015|Datalakes|¿Has escuchado el término Lakehouse? Desde hace un par de años, esta arquitectura de datos ronda los foros y confunde a uno que otro especialista en cloud. Para no hacer el cuento largo, un Data Lakehouse es el resultado de un proceso de hibridación de dos enfoques distintos, el Data Warehouse y el Data Lake. En términos sencillos, un Data Lakehouse une lo mejor de ambos mundos. Se trata de un sistema de almacenamiento masivo y exponencial que recibe todo tipo de datos, estructurados, semiestructurados o sin estructura, y está provisto de herramientas altamente sofisticadas que permiten resolver las necesidades de cualquier tipo de empresa sin importar su tamaño. Este sistema es tan eficiente que ya es visto como el nuevo paradigma de inteligencia y analytics a gran escala. Pero para entender cómo funciona, primero debemos conocer a sus antecesores, los Data Warehouses y los Data Lakes. En la historia del procesamiento de datos, primero llegaron los Data Warehouses, que comenzaron almacenando información en servidores físicos y después se mudaron a la nube utilizando una infraestructura elástica que disminuyo tiempos y costos de procesamiento. Tiempo después nacieron los Data Lakes, que permiten almacenar todo tipo de información en crudo y proporcionan arquitecturas especializadas para hacer análisis en batch y tiempo real y así generar conocimiento a partir de los datos oscuros o dark data. Y aunque pueden parecer la misma cosa, en realidad los Data Warehouses y los Data Lakes son muy diferentes. Su diversidad recae precisamente en la forma en que procesan la información. Mientras que el Data Warehouse proporciona un control más estructurado de los datos, un Data Lake facilita el almacenamiento central masivo, manteniendo la coherencia y eliminando los hilos de información. Si el Data Warehouse se basa en el Business Intelligence, los Data Lakes centran su poder en los algoritmos y el Machine Learning. Sin embargo, los tiempos modernos tienen nuevas necesidades, así que ambos sistemas se complementan en el flamante y novedoso Data Lake House, que provee una arquitectura centrada en sus fortalezas para elevar la eficacia de toda la estructura cloud. Y nos encanta porque es escalable, robusto, tiene un alto rendimiento y se adapta bien a los cambios de una industria altamente competitiva. Todo un acontecimiento para los sistemas de datos. Como dije antes, un Data Lake House es lo mejor de ambos mundos y ya está en el camino de convertirse en el nuevo status quo de almacenamiento cloud.
DOC0016|Datalakes|¿Cómo se utiliza un Lago de Datos? Un data leico Lago de Datos es un repositorio centralizado que permite almacenar, compartir, gobernar y descubrir todos los datos estructurados y no estructurados a cualquier escala. Las características más importantes de los Lagos de Datos es que no requieren un esquema predefinido, incorporan mecanismos de seguridad, protecciones de acceso y catálogos de datos, tienen conexión con herramientas analíticas de reporting, de procesamiento y de inteligencia artificial y son la base para obtener valor de los datos. Los Datas y Los ocurren cuando no existe un lugar o un sistema centralizado en el que almacenar todos los datos de una organización. Cada uno de ellos está controlado por un departamento independiente, con diferentes políticas o tecnologías. Difultan extraer valor y descubrir nuevos datos. Un Data Warehouse suele ser un subconjunto de las tecnologías involucradas en el despliegue de data lakes. Comprende los componentes de data lakes encargados de tratar datos estructurados. Los componentes más comunes en un data lake de Azure son ADLS, HD Insight y Azure Data Factory. Los componentes más comunes en un data lake de Amazon son S3, Redshift y AWS Glue. Claude también permite desplegar lagos de datos con tecnologías Hadoop y Open Source.
DOC0017|Datalakes|Antes de hablar de la diferencia entre una base de datos, un data warehouse y un data lake, primero debemos entender un concepto muy importante. Y ese concepto es la diferencia entre datos estructurados y datos no estructurados. Los datos estructurados son datos que tienen un esquema conocido y están organizados en filas y columnas en una tabla. Casi como el típico Excel con el que estamos acostumbrados a trabajar todos. Los datos estructurados suelen estar almacenados en bases de datos relacionales ya que los datos tienen relaciones entre sí. Los datos no estructurados no tienen esquema. Pueden ser, por ejemplo, texto, imágenes, diferentes tipos de archivos, etc. No están estructurados en filas y columnas como los datos estructurados. Pueden venir de cualquier aplicación que usamos como por ejemplo las redes sociales, artículos, correos electrónicos, audio y video. Ahora que ya sabemos la diferencia entre datos estructurados y no estructurados, vamos a hablar de lo que es una base de datos. Una base de datos es un almacén de datos. Hay muchos tipos de bases de datos, pero las bases de datos más comunes son las bases de datos relacionales, que trabajan con datos estructurados. Estas suelen ser de tipo SQL, con sus diferentes sabores como son MySQL, SQL Server, PostgreSQL, SQLite, Amazon Aurora y muchos más. Todos estos son motores de bases de datos. También existen bases de datos para datos no estructurados, que solemos llamar no SQL o Not Only SQL. Otras de los ejemplos más comunes son MongoDB, Cassandra, DynamoDB, etc. Otra cosa que debemos resaltar es que las bases de datos normalmente son para datos transaccionales y no están diseñados para analítica. Y aquí entra el término Data Warehouse o almacén de datos. Un Data Warehouse es un sistema de almacenamiento de datos creado específicamente para el análisis de datos o BI, inteligencia de negocios. Un Data Warehouse puede tener muchas fuentes de datos, pero suelen ser datos estructurados, bien organizados y normalmente accesibles a través de SQL. El Data Warehouse suele ser construido a través de lo que llamamos Data Pipelines, utilizando ETL, Extract, Transform y Load, extraer, transformar y cargar. De esta forma tenemos un Data Warehouse listo con los datos actualizados para nuestras tareas de análisis de datos y inteligencia de negocios. Vuelvo a repetir, ya que es importante que un Data Warehouse está optimizado para el análisis de datos y inteligencia de negocios. Vale, pues ahora que ya sabemos lo que son los datos estructurados y no estructurados, sabemos que son las bases de datos y los diferentes tipos de bases de datos que existen y hasta lo que es un Data Warehouse o un almacén de datos, nos podemos hacer la pregunta ¿Qué es un Data Lake? Un Data Lake o Lago de Datos es un repositorio central donde almacenamos todo tipo de datos sin que nos importe su estructura ni su tipo y no le hacemos ningún tipo de transformación. Almacenamos todo tipo de datos en su forma cruda. Se mantienen todos los datos por si los necesitaríamos en algún momento más adelante. Una de las ventajas de tener un Data Lake es que son más flexibles. Mientras un Data Warehouse es estructurado y tiene que ser diseñado de antemano. Requiere trabajo para hacer cambios y conseguir más datos de los que estaban previstos. Un Data Lake es mucho más flexible porque los datos ya están ahí para poder utilizarlos cuando queramos. Eso sí, tendremos que extraer y transformarlos para nuestro uso específico. El ejemplo típico sería S3, el servicio de almacenamiento de objetos de AWS en la nube más popular, Amazon Web Services. Pues ya está, ya sabéis la diferencia entre una base de datos, un Data Warehouse y un Data Lake. Espero que os haya gustado el video. Si os ha gustado no os olvidéis de darle me gusta, suscribiros al canal para más videos como este. En Datademia trabajamos para crear el mejor contenido de datos en español, incluyendo la ciencia de datos, inteligencia de negocios y la programación. Nuestro objetivo es transformarte en un experto en datos, aprendiendo desde cualquier parte del mundo y a tu ritmo. En Datademia ofrecemos tres bootcamps en los que te puedes convertir en un analista, un científico o un ingeniero de datos. Al ser parte de esta comunidad podrás utilizar el cupón YouTube para tener un 10% de descuento en cualquiera de los cursos y bootcamps de Datademia. Te invito a que entres en nuestra web www.datademia.es y empieces a aprender algo relacionado al mundo de los datos en Datademia, la mejor academia de datos en español. ¡Suscríbete!
DOC0018|Datalakes|Hola de nuevo, soy Ferregrino y hoy les voy a hablar sobre la diferencia entre un Data Lake y un Data Warehouse. Tanto los Data Lakes como los Data Warehouses son usados por compañías para almacenar enormes cantidades de datos, es una gran similitud, pero esto no quiere decir que sean o que deban ser usados para lo mismo. Un Data Lake es un repositorio de datos crudos o no procesados, raw se diría en inglés. Como el nombre lo sugiere, puedes imaginarte un lago en donde los datos están ahí, aún sin tener ningún propósito en específico, mientras que un Data Warehouse es lo contrario, un lugar para datos que ya han sido procesados y estructurados con un propósito en mente. El saber diferenciarlos es importante, ya que la forma de almacenamiento de cada uno necesita distintas habilidades para ser propiamente aprovisionada, configurada, mantenida y usada. Datos crudos o procesados. Esta es quizá la diferencia más grande entre las dos soluciones de las que les estoy hablando el día de hoy, Data Lake y Data Warehouse. Cuando hablo de datos crudos, me refiero a datos que no han sido procesados para ningún propósito. Son precisamente estos los datos que son almacenados en un Data Lake. Por ejemplo, si parte de tu negocio es trabajar con audios de grabaciones telefónicas o grabaciones de entrevistas, tendrás estas grabaciones en algún formato como .WAV o .MP3. Estas grabaciones podrían ir dentro de un Data Lake. Asimismo, si trabajas recopilando información de diferentes fuentes, puedes poner en el Data Lake todos los archivos de Excel, LibreOffice y inclusive archivos de CSV que te envían tus proveedores. O imagínate que estás en el negocio de web scraping. El Data Lake es un buen lugar para poner los archivos .html que vas descargando de internet. Por el contrario, el Data Warehouse está hecho para almacenar información que ya ha sido previamente procesada y refinada, teniendo un objetivo en mente. Acá es en donde podrías extraer de alguna forma los diálogos de tus archivos .MP3 de las entrevistas o las llamadas telefónicas. Es un lugar en donde podrías unificar los datos de todos esos archivos de Excel y todos esos archivos CSV en una sola tabla para poder crear un modelo a partir de ellos. Y si estás en el web scraping, aquí es donde tú podrías extraer únicamente el texto de las páginas web para analizar su contenido. ¿Si te das cuenta de la diferencia? Debido a estas diferencias, el Data Lake suele requerir mayor capacidad de almacenamiento que un Data Warehouse. Y este es otro aspecto a tomar en cuenta cuando vas a aprovisionarlo. Y obviamente, al momento de calcular el presupuesto, puedes darse el caso que la organización y la forma del... puede darse el caso que debido a su organización y que almacena los datos procesados, los Data Warehouses pueden ser más baratos o pueden resultar más caros y lo que quieres es optimizar la velocidad para al momento de consultar los datos. Hay que tenerlo en cuenta. Aquí también quisiera hacer un paréntesis y es que hay que tener cuidado porque si no lo tienes, entre tantos datos, tu lago de datos, tu Data Lake, se puede convertir en realidad en un pantano sin control. Blob controtabular. Siguiendo en el tema del almacenamiento, el formato en el que la información es almacenada en un Data Lake y en un Data Warehouse también es diferente. Los datos en un Data Lake suelen estar almacenados como archivos individuales, usualmente en forma de blobs o estos famosos binary large objects, desde gigabytes de archivos en punto de CSV, comprimidos a veces en formato tar, hasta grabaciones en punto mp3, videos en punto jpg o videos en mp4, en fin, como archivos binarios. Los Data Lakes almacenan información de todo tipo, como ya lo mencioné, estructuradas como un archivo CSV, semiestructuradas como un archivo HTML y no estructuradas como un archivo de audio o de video. Es por eso que muchas veces estos tipos de sistemas, los Data Lakes, suelen asemejarse al sistema de archivos que estás utilizando en tu día a día, aunque claro, la implementación detrás de las cortinas es muy diferente. Los Data Warehouses suelen almacenar la información en bases de datos, en tablas, usualmente cada uno de estos warehouses contiene múltiples bases de datos o esquemas, cada esquema contiene diferentes tablas y cada tabla contiene múltiples columnas. A lo que voy es, un Data Warehouse se parece mucho a los sistemas relacionales de bases de datos que tal vez te sean muy familiares, aunque no te queda el clave mucho en esto de las relaciones, a veces tener relaciones entre las tablas no es tan importante en un Data Warehouse. Sin propósito o con propósito. Otra de las diferencias es que la información que llega al Data Lake puede ser reutilizada fácilmente para múltiples fines, sus usos están abiertos a cualquier propósito de la organización en la que estés trabajando. Y es más, puede darse el caso en que dentro de tu compañía estén almacenando información para la que aún no tengan un uso o que inclusive nunca jamás vayan a volver a utilizar. Por el contrario, el Data Warehouse rara vez contiene información sin propósito alguno, si la información está ahí es muy muy seguro que alguien, algún equipo decidió procesarla y hacerla disponible para sus propósitos, para sus intenciones y sus aplicaciones. Esta es otra de la razón por la que los Data Warehouses suelen ser también más eficientes. Los usuarios. El público objetivo de cada uno de estos métodos de almacenamiento también suele ser muy diferente, mientras que Data Scientists y Data Engineers pueden estar más familiarizados con trabajar con información no estructurada y no procesada, como la que nosotros solemos guardar en un Data Lake. Otros usuarios como analistas o aquellas personas encargadas de la generación de reportes y toma de decisiones a más alto nivel dentro de una organización estén más acostumbradas a trabajar con información tabular, como la que existe en un Data Warehouse. Obviamente para nutrir estos dos sistemas existen Data Engineers, sin embargo las habilidades que sirven para mantener uno no son 100% transferibles a la manutención del otro, es por eso que también debes saber distinguir cuál necesitas o cuál es el que ocupas en tu organización o en tu proyecto. Flexibilidad. También la flexibilidad varía entre soluciones. Un Data Lake es más amigable a cambios de la información. Como no hay reglas de negocio y no hay esquemas para modificar, en realidad lo único que estás almacenando son archivos individuales, solamente estás limitado en cuanto al almacenamiento que tienes disponible en el disco. Por otro lado, el Data Warehouse requiere de esquemas e inclusive de relaciones entre las diferentes tablas y bases de datos. A veces un cambio en los datos, inclusive en los tipos de datos con los que vas a trabajar, requiere de migraciones de información que pueden llegar a ser pesadas y muy complicadas. Y esto es lo que hace que los Data Warehouses sean un poco lentos o más bien sean lentos en cuanto a adaptación de cambios se refieren. Sé que te di mucha información acá, pero para resumir te lo pongo en una tabla. Mientras que el Data Lake almacena datos sin procesar, el Data Warehouse lo hace con datos procesados. Los datos en un Data Lake son almacenados sin un propósito específico en mente, mientras que por el contrario la información que vive en el Data Warehouse fue procesada con una idea ya en mente. El Data Lake suele almacenar objetos binarios, mientras que el Data Warehouse tiene un poco más de estructura, los almacena de forma tabular. Por otro lado, tanto los Data Scientists como Data Engineers son los usuarios más comunes de un Data Lake, mientras que analistas de datos y gente más orientada al negocio suelen ser los principales clientes de un Data Warehouse. Y para terminar, cambiar la información que almacena un Data Lake es más rápido, por lo que no, como ya lo mencioné, no existe algún esquema, no existen tantas reglas, mientras que hacerlo dentro de un Data Warehouse es muy complicado y puede llevar un gran proceso de migración. Ahora, tal vez te estás preguntando cuál debo elegir, y bueno, espero que para este momento, en este punto del video, ya te estés dando una idea de cuál de estos dos sistemas es el que tu compañía necesita o tu proyecto. Aunque la verdad es que no me sorprenderías si llegarás a la conclusión de que necesitas ambos. Usualmente, esto es lo que llega a suceder en las compañías, tienen tanto un Data Lake, un Data Lake del cual extraen la información para nutrir el Data Warehouse, es decir, un lugar en donde vas a poner la información que vas recolectando de diferentes fuentes de diferentes sistemas, tan solo para después moverla al Data Warehouse y pueda ser utilizada por usuarios dedicados a la visualización, análisis y toma de decisiones dentro del negocio. Así que no te puedo decir cuál es el que tienes que elegir, varía de proyecto a proyecto, pero de nuevo, no te sorprendería que no te sorprenda si necesitas ambos. Esto es un patrón muy común dentro de las compañías. Y pues bueno, esas fueron las diferencias entre Data Lake y Data Warehouse. Espero les haya sido de utilidad este video, ya saben que si así fue, le pueden dar un me gusta, les pido por favor que lo compartan si es que les resultó útil y me digan en los comentarios si tienen alguna duda o comentario sobre este tema. Ustedes ya han utilizado alguna de estas tecnologías? Por ahí tengo un video en donde les hablé de Snowflake, que es precisamente una solución para un sistema de Data Warehouse, aunque ellos también dicen ser un Data Lake, la verdad es que es más útil como un Data Warehouse. Entonces les dejo el enlace en la descripción. Como sea, si tienen temas o algunas sugerencias sobre algún tema que quisieran que yo les platicara, dejen en los comentarios también, les agradezco mucho su participación. No quisiera irme antes sin pedirles que se suscriban y sin agradecerle a los miembros de este canal, sin su apoyo no sería nada a este canal, muchas gracias. Y pues bueno, cuídense mucho, tomen agua y nos vemos la próxima.
DOC0019|Datalakes|Bienvenidos a todos, este es el canal de Data Pro Ratan. Mi nombre es Vladimir Fussman y en este laboratorio vamos a ver cómo crear un Azure Data Lake Store generación 2. Para eso primero debemos definir qué es Azure Data Lake. Básicamente es una solución de almacenamiento altamente escalable, reside en la nube, eso es muy importante, lo cual nos permite crecer rápidamente y está diseñada para hacer análisis de Big Data. Una de las grandes ventajas es que no estamos amarrados al desarrollo con algún framework específico, existen diferentes frameworks del cual se puede conectar al Azure Data Lake Store, tenemos Databricks, podemos usar Spark, podemos usar Erre, podemos consumir los datos desde Machine Learning, entonces esa es una de las grandes ventajas. Para poder crear nuestro Azure Data Lake, lo primero que debemos hacer es registrarnos, logearnos en nuestro portal de Azure, en este caso, aquí estamos usando mi cuenta y este es mi portal principal. Lo que vamos a hacer en este caso es crear un grupo de recursos para poder tener todos nuestros componentes en el mismo grupo. Vamos a darle agregar. Aquí lo que escogemos es nuestra suscripción, creamos nuestro nombre para nuestro resource group, le vamos a poner Azure Data Lake Resource Group, esta validad y la ubicación en la cual queremos nuestro grupo de recursos, en este caso voy a escoger este Estados Unidos Versión 2, le vamos a poner Azure Data Lake Resource Group, aquí vamos a poner Azure Data Lake Resource Group, aquí vamos a poner Azure Data Lake Resource Group, aquí vamos a revisar y crear y le vamos a crear. A este momento solamente hemos creado nuestro grupo de recursos, aquí vamos a ir a nuestro grupo de recursos y ahora sí, para poder crear nuestro Azure Data Lake dentro de nuestro grupo de recursos le vamos a agregar y buscamos la opción de Storage. Escogemos Storage Account y comenzamos a configurar nuestro Storage Account, lo que nos pide es básicamente nuestra suscripción, el nombre del resource group en el cual se va a encontrar, ahora sí, nuestro Storage Account, a este le vamos a poner Azure Data Lake Storage Account Lab, le vamos a poner Data Pro, todo en minúscula, la ubicación en la cual va a pertenecer nuestro Azure Data Lake, cogemos igual la misma en el resource group, estándar o premio básicamente es para poder tener el tipo de servicio que tenemos, escogemos que es versión 2 y le damos Next, aquí nos pregunta si va a ser pública, si va a ser privado nuestro Data Lake o cómo va a tener acceso, lo dejamos configurado como viene, le damos Next y aquí es la parte importante, lo que hacemos es tenemos que activar esta opción que dice Data Lake Storage Gen 2 para que active la opción de Geretical Naming Space, le damos Enable y ya con esto le estamos diciendo que nuestro Storage Account va a contener un Azure Data Lake, va a poder tener y manejar contenedores de Azure Data Lake, le damos Next y le vamos Revisar y crear. Y una vez que termina la validación le damos Crear, en este momento él ya está creando nuestro Azure Data Lake, en este momento está provisionando en la nube y esta es una de las grandes ventajas de los servicios Azure y los servicios nube, que no necesitamos montar, crear o comprar, adquirir infraestructura para poder desarrollar nuestras soluciones, dado que estos ya proveen los diferentes medios de almacenamiento y como pudieron ver en cuestión de segundos pudimos crear y aprovisionar nuestro Azure Data Lake, entonces eso nos da una ventaja competitiva muy grande, dado que en cuestión de segundos podemos tener recursos en la nube rápidamente. Ya él finalizó, vean que fue bastante rápido y ahora sí vamos a ir a nuestro grupo de recursos. En este momento estamos viendo ya nuestra configuración del Azure Data Lake, tenemos varias configuraciones por el lado izquierdo y tenemos la parte de cómo se va a comportar este storage account. Para nosotros la parte importante es esta que es la de contenedores, aquí es donde se va a manejar todos nuestros contenedores de Azure Data Lake, vamos a ingresar a contenedores y debemos tener un contenedor como mínimo para poderlo manejar y repositar información. En este caso le vamos a poner nuestro contenedor de retail, ¿verdad? Iremos a decir que es privado, crear. Una de las ventajas de Azure Data Lake o funcionalidades es que nosotros podemos repositar, podemos hacer, copiar archivos, insertar archivos en su formato natural, es decir, que no necesitamos pasar por una depuración de datos, sino más bien se utiliza para el modelo de ingesta, para ingestar datos en la manera tal como vinieron originalmente. Es decir, si son archivos XML, se depositan archivos XML, si son JSON, se depositan los archivos JSON, si son archivos CSV, que son archivos de texto separados por comas, por tabuladores, se depositan de la forma natural de la cual fue traído del sistema transaccional. Entonces eso es muy importante porque nos permite hacer una carga masiva de datos. Para poder subir datos, vamos a probarlo rápidamente y vamos a utilizar nuestra herramienta de Azure Storage Explorer. Si no la tienen, descarguenla desde la página de Microsoft y aquí vamos a darle clic derecho, vamos a darle clic derecho, refrescar, y ya debería aparecer. Aquí está nuestro Azure Data Lake Storage de Dataprona TAM. Como pueden observar, es un ADLS, Azure Data Lake Storage Generación 2. Expandimos y ahí vamos a ver nuestros contenedores. Expandimos nuestros contenedores y en este momento solamente nos aparece el que dice Retail. A partir de aquí, esto se comporta como un sistema de archivos, ¿verdad?, en el cual yo puedo crear diferentes folders y subir la información a los folders. Para este laboratorio vamos a hacer solamente un Upload, un Upload File. Voy a buscar mi archivo, en este caso en mi Desktop, tengo este archivo que se llama Stop.txt y le digo Upload. En este momento ya estamos subiendo información desde mi computadora, desde mi Desktop, un archivo directamente a nuestro Azure Data Lake Storage. Esperamos unos minutos y ya el archivo subió. Vamos a verificarlo, vamos a volver a nuestra página. Aquí estamos otra vez en la nube, vamos a entrar en nuestro folder de Retail y en este caso nuestro archivo Stop ya se encuentra subido a nuestro repositorio en la nube. Podemos darle clic, es un archivo TXT, es un archivo separado por tabulaciones, le podemos dar clic y aquí podemos tener una previsualización del archivo. Ahí estamos observando la información que tiene mi archivo, básicamente es un archivo de productos muy básico. Ok, con esto vamos por cerrar nuestro laboratorio el día de hoy. En siguientes laboratorios estaremos viendo cómo nos conectamos, cómo vamos a poder utilizar este Data Lake Store como repositorio de fuente de datos y vamos a generar transformaciones con diferentes herramientas. Muchas gracias, hasta el próximo video.
DOC0020|Datalakes|¿Cuentas con una gran cantidad de datos, pero aún no sabes cómo transformarlos en valor? Sumérgete en el lago de datos con Plug & Play Data Lake. Desde Nuviral, comandamos la expedición. Plug & Play Data Lake permite etiquetar, buscar, compartir, transformar, analizar y administrar fácilmente sus conjuntos de datos en una empresa o con usuarios externos. ¿Por qué es necesario que todas las empresas cuenten con esta solución? Porque exporta y almacena datos de forma segura, a gran escala y bajo costo. Realiza transformaciones de datos de manera sencilla. Proporciona data de calidad desde un solo almacén. Garantiza la disponibilidad y el análisis en todo momento. Permite el acceso y análisis de datos desde una misma fuente de información para todos los usuarios. Entre tres paquetes diferentes, puedes elegir el que mejor se adapte a tu empresa. Bronze, Silver o Gold. El momento es ahora. Sumérgete hoy mismo en este lago de oportunidades.
DOC0021|Datalakes|Hola, y en esta clase vamos a hablar de AWS Lake Formation. ¿Y qué es AWS Lake Formation? Básicamente te ayuda a crear un lago de datos. O en las palabras de AWS, te facilita la creación de un lago de datos seguro en cuestión de días. ¿Entonces cómo te ayuda? Bueno, te ayuda con la carga de datos y supervisión de los flujos de datos, la configuración de particiones, el cifrado y gestión de claves, la definición de trabajos de transformación y monitoreo, control de acceso, y todo esto está construido sobre Glue. Entonces todo lo que puedes hacer con Glue lo puedes hacer con Lake Formation, pero este servicio está hecho específicamente para llevarte paso a paso para crear tu lago de datos o data lake. ¿Y cómo funciona? Pues aquí tenemos un esquema un poquito de cómo funciona. Como ves, tenemos nuestros datos en S3, o una base de datos relacional, o una base de datos no relacional como podría ser una no SQL. Y utilizamos Lake Formation, donde puede utilizar crawlers, ETL y Data Prep, Data Catalog, seguridad y control de acceso. Todo esto nos ayuda a crear nuestro data lake en Amazon S3 y luego este data lake puede ser accedido por Amazon Acena, Amazon Redshift o Amazon MR. ¿Y los precios? Bueno, no hay coste para Lake Formation en sí, pero sus servicios tienen coste. Entonces todos los servicios como Glue, S3, MR, Acena y Redshift tienen su coste individual. ¿Y cuál es el proceso para construir un lago de datos? Lo primero que se tiene que hacer es crear un usuario de IAM, luego se crea una conexión de AWS Glue a las fuentes de datos, se crea un bucket S3, se registra la ruta S3 de este bucket en Lake Formation, hay que dar permisos claramente, luego se crea una base de datos en Lake Formation para el catálogo de datos, hay que dar permisos y se puede utilizar lo que llaman un Blueprint, un modelo para crear estos flujos de trabajo, luego se ejecuta el flujo de trabajo y hay que dar permisos Select a quien los necesita leer estos servicios como mencionamos antes, Acena, Redshift, etc. Como veis no es un proceso tan sencillo y por eso nos dicen que AWS Lake Formation es para crear un lago de datos en días y no es todo automático, aún hay algo de trabajo, pero nos llevará paso a paso para crear este lago de datos. ¿Y qué más? Bueno, hay algunas cosas que pueden salir en examen, por ejemplo, los permisos para cuentas múltiples, pues el destinatario debe estar configurado como administrador del lago de datos y para utilizar ese Data Lake con cuentas externas a la organización puedes utilizar el servicio de AWS Resource Access Manager o AWS RAM y también los permisos de IAM tienen que ser configurados para el acceso entre cuentas. Lake Formation no admite manifiestos en las consultas de Acena o Redshift, los permisos de IAM sobre la clave cifrada de KMS son necesarios para los catálogos de datos cifrados en Lake Formation y los permisos IAM también son necesarios para crear Blueprints y flujos de trabajo. Entonces estas cosas últimas pueden salir en examen. Pero ya está, espero que os haya gustado y hayáis entendido como AWS Lake Formation nos ayuda a este servicio a crear estos lagos de datos desde nuestros propios datos en S3, en bases de datos relacionales o no relacionales. Eso es todo y nos vemos en la siguiente clase.
DOC0022|Datalakes|As you wrap up the process of designing the project, we hope traditional outfits will strike your skill Thank you.
DOC0023|Datalakes|Buenas tardes. Me propongo hoy compartir con ustedes una reflexión sobre lo que está ocurriendo con los almacenes de datos, los lagos de datos y una propuesta nueva que es el data mesh. Un poco lo que vamos a hacer es ver bien qué son, qué tienen en común y qué valor tratan de agregar uno sobre otros. Muy bien, se acordaron ustedes que en el principio, cuando éramos jóvenes, las aplicaciones vivían aisladas entre sí, conjuntos de datos que no se hablaban. Quizás los nombres de las calles, de las provincias eran distintos en distintas aplicaciones y si uno quería combinar datos que venían de distinto lado, y era un esfuerzo brutal. Muy bien, la otra cosa que teníamos en este contexto era una importante dificultad para realizar cambios en lo que era la estructura de los datos. Muy bien. Bueno, y entonces, ¿qué pasaba con todo esto? Que era muy difícil que la información resultara disponible para usuarios finales. Requería un procesamiento importante por parte de los equipos de sistemas. Ah, bien. Con lo cual, muchas cosas se demoraban porque entraban en la temida cola de prioridades que siempre crecía. Muy bien. El gran desafío en este momento era normalizar las cosas. La idea era que si yo normalizaba los datos, entonces, sumando a eso interfaces de consulta, iba a poder dejar que los usuarios finales se sirvieran los datos y sacaba estos problemas crecientes, abundantes de la cola de sistemas. Muy bien. Para normalizarla, la idea era, hacemos un almacén de datos en lugar de los datos ya limpios. Pueden ser consultados sin cargar las aplicaciones transaccionales, con lo cual tengo más libertad para dárselas a los usuarios finales. Bien. Pero había un problemita. Necesitábamos que la estructura estuviera bien de una vez y para siempre. Y si algo cambiaba en toda la cañería, porque cambiaba el negocio, porque cambiaban las necesidades, bueno, era muy rígido todo el proceso. Había muchos problemas. Bueno, lo bueno del caso, con lo que sí se cumplió, es con hacer disponible la información mediante interfaces gráficas que permitían hacer consultas sin entender demasiado el base de datos relacionales, SQL y compañía. O sea, algo realmente útil para usuarios finales. Y lo que sí era claro, es que el ETL es un esfuerzo considerable y que cambios llevan mucho tiempo. Ah, bien. Acá el desafío era avanzar con que esto fuera más libre, con que se pudieran aceptar los cambios más rápido. Bueno, muy bien. Nos fuimos entonces al lago de datos. Lago de datos ya lo construimos con una mentalidad de Big Data que pueda crecer todo lo que querramos. Para liberalizar, la idea fue que el formateo de la información se hace en la extracción, no en la carga. Entonces, adentro uno puede guardar cualquier cosa al postergar el momento en el cual le damos estructura a lo que estamos buscando. Entonces, no nos atamos a un formato dado. Podemos tener más alta velocidad de respuesta. Ajá. La gran tentación, entonces, ahora es una gran ingestión de datos. Guardamos por si nos va a hacer falta en el futuro. Dato que pasa, se pierde y no lo guardé, se ve como una desgracia. Y tampoco nos preocupamos demasiado, dado lo barato que está haciendo el espacio de almacenamiento, en ver cómo es el retorno de la inversión de cada dato que nos guardamos por las dudas. O sea, si ustedes quieren, nos convertimos en data hoarders, sí, en este lugar. Muy bien. Pero claro, si tengo que formatear los datos en la extracción, ya eso sí se le escapa a los usuarios finales. Entonces, nos encontramos con un equipo de ingeniero de datos centralizado que tiene su lista de prioridades para cumplir con las necesidades de datos de los usuarios finales. Estamos en un problema. Bien. Entonces, ¿sí? ¿Qué podemos hacer al respecto para justamente no quedar atrapados y torturados por esta circunstancia? Bueno, la solución pasa por productizar. Tenemos que, para empezar a partir este lago de datos, en un montón de dominios de negocio cada uno con su lagunita, si ustedes quieren, que va a tener un equipo mixto de profesionales que se va a encargar de crear productos de datos que van a poder ser consumidos por varias áreas de la organización. A ver, ¿sí? Esta es la visión entonces del data mesh. El data mesh separa ese lago de datos por dominios de negocios, o sea, por grupos de usuarios que están atrás de un proceso punta a punta y sus necesidades de datos y sus necesidades de infesta. Y va a tener eso. Muy bien. Entonces, van a tener propiedad de sus datos y propiedad de sus procesos y van a mantener sus productos. Con lo cual, el equipo está alrededor de ese problema de negocios y, por lo tanto, va a asumir un montón de cosas para ese contexto, que no va a ser el contexto general de la corporación. Con lo cual, ya ahí tenemos una ganancia interpretativa importante. Bien, cuando uno trata de hacer cosas muy en general, lo que le suele pasar es que tiene que poner mucha complejidad para convivir con esa generalidad. Y esa complejidad que agrega hace las cosas más duras de cambiar después. Si algo es sensible al contexto, es más particular, puede ser más simple y, por lo tanto, terminar siendo más flexible y a un costo de propiedad menor. Bien. Sin embargo, alguien podría decir, pero pará, pará, pará, pará. ¿Qué pasa con la consistencia? Porque si vuelvo al modelo en el cual las cosas de distintos dominios van a tener inconsistencias que no les voy a poder juntar, perdí. Bueno, eso no lo tengo que perder. Tengo que mantener maestros de datos generales que sean de aplicación mandatoria para todos los dominios. Ah, bueno, las calles van a tener que llamarse todas iguales. Listo. Además de eso, los equipos van a generar estos productos. Estos productos van a tener que tener algunas características que es interesante señalar. Descubribles. Vamos a necesitar algún catálogo centralizado donde la gente pueda ver qué hay. Tiene que ser direccionable, tiene que estar siempre disponible en un lugar y ese lugar ser alcanzable. Tiene que haber garantías de que tiene un nivel de confiabilidad el dato que está ahí, que cumple con algún estándar que se publica. Hay un compromiso. Muy bien. Que refleja verdaderamente la realidad del negocio que está detrás. Bien. Después tiene que ser algo que se autodescriba. Tiene que ser una interfase que la vemos y la podemos usar. Que no necesitamos irle llevando al usuario de la mano para que avance con eso. Piensen como paradigmas de las cosas que se describen a sí mismas en muchos juegos que hoy están en el celular. La gente no toma un curso para operar el juego, sino que el juego mismo lo va llevando y le va enseñando a jugar. Si no, el juego muere. Muy bien. Interoperable con todas las plataformas que maneje la organización. Si no, te ahuman el horno. Y así como el descubrimiento tenía que venir a partir de un repositorio, un catálogo centralizado, también la seguridad tiene que poderse administrar de manera central. Ellos van a decir quién puede ver qué, quién puede usar qué. Fantástico. Estos equipos por dominio de negocio van a necesitar contener varios roles. Esto está alineado con lo que sugiere de Vox de tener equipos mixtos atrás de un objetivo y no tener sí, los ingenieros de datos por acá, los ingenieros de software por acá. Porque eso genera lentitud en la transmisión de información, pérdida de contexto y multiplicación de las trabas burocráticas. Bien, vamos a necesitar un dueño de producto que va a ser el responsable del crecimiento de ese producto en el tiempo. Vamos a necesitar ingenieros de datos, vamos a necesitar ingenieros de software y en particular vamos a necesitar expertos en confiabilidad que se encarguen de ver que punta a punta este producto entrega lo que se espera de ellos y que identifique dónde están los puntos posibles de falla. Muy bien, muchas gracias por haberse interesado por este tema. Nos seguiremos encontrando en el futuro por este canal y si dejan preguntas sobre lo que les gustaría que les contáramos, maravilloso, lo miramos para ver qué cosas trata. Muchas gracias.
DOC0024|Internet de las cosas|Hola a todos, en este primer video del curso teórico básico de IOT, vamos a definir dicho concepto ¿Qué es el IOT? El Internet de las Cosas o IOT, por sus siglas en inglés, trata básicamente de obtener información del medio físico y llevar esta información a Internet, aprovechando que la red de Internet está ampliamente masificada y es fácilmente accesible. La finalidad de mandar la información a la red de Internet es para poder almacenarla, procesarla, analizarla, automatizarla y representarla. El análisis de la información hace que se genere información útil para tomar decisiones. Información útil o métricas es justamente lo que les interesa a las empresas. Entre la toma de decisiones se puede generar una regla de control o automatización. Bien, ahora vamos a ver la topología del Internet de las Cosas. Hoy en día esta topología está más estandarizada y es la que te presento en la imagen. Entonces, un dispositivo IOT puede leer una variable y generar una radiofrecuencia. Esa señal de radiofrecuencia que transmite la información del dispositivo puede ser detectado por un concentrador, una antena, un gateway o estación base, como prefieras llamarlo. Básicamente es un punto de red que concentra a varios dispositivos IOT conectados. La finalidad de que el gateway detecte los datos es para posteriormente mandarlos a la nube del Internet y que después la información pueda llegar a un servidor de aplicaciones. El servidor de aplicaciones puede ser un software como servicio, una plataforma como servicio, etc. Detallaremos estos temas como hay a profundidad en los próximos videos. Bien, ahora vamos a hablar de las características del IOT. Básicamente las soluciones o proyectos que utilizan el Internet de las Cosas nos proveen de una inteligencia, ya que a partir de la recolección de datos podemos tomar mejores decisiones. El IOT además te permite monitorear un evento sin tener que encontrarte en ese lugar. Lo puedes hacer desde cualquier parte del mundo. También otra característica es que para el Internet de las Cosas no se necesita un gran ancho de banda, ya que generalmente se transmiten datos telemétricos, como la variable de humedad que va de 0 a 100 o la variable de velocidad de un carro que puede ser de 0 a 300. Si sumas el peso de esos datos, no ocupan ni un kilómetro de información. Ahora un aspecto importante en el IOT es la eficiencia energética. Los dispositivos IOT generalmente trabajan con pilas o baterías, ya que si trabajaran conectados a la red eléctrica necesitarían un tomacorriente cerca y al estar siempre conectados generaría un alto coste de electricidad. Los dispositivos IOT son eficientes energéticamente, esto debe ser así ya que estos dispositivos trabajarán de forma aislada y no sería idóneo estar constantemente cambiando de las pilas o baterías. Por lo general estas baterías o pilas pueden durar hasta 5 años. Otra característica del IOT es la escalabilidad, esto es muy importante ya que en el futuro habrá muchos más dispositivos conectados a internet. Además en el IOT se puede ver la lectura de la variable capturada prácticamente al instante, es decir, sin tener demoras mayores a un minuto, lo que comúnmente se conoce como real time. Por ejemplo, con los datos recopilados podemos analizarlos y aplicar machine learning. Por ejemplo, supongamos que estamos midiendo la variable temperatura, con la medición de esa variable durante un mes podríamos predecir la temperatura que tendrá ese lugar dependiendo de la hora. Otra característica importante es que aún no existe un estándar único y propio para el IOT, esto nos lleva a una importante conclusión. No existe una única forma de desarrollar una solución de internet de las cosas, lo que sí existe son muchas herramientas y tecnologías que nos pueden ayudar a generar una solución. El reto mayor consiste en que esa solución sea de bajo costo, que sea lo que el usuario requiere y que se pueda adaptar a las necesidades del caso de uso. Bien, ya para terminar te voy a presentar algunas aplicaciones en las que se utiliza el IOT. Por ejemplo, para hacer mediciones inteligentes, estos medidores enviarán los datos medidos hacia el servidor web de la empresa. En la gráfica podemos ver justamente un medidor de agua inteligente, en varias ciudades de Europa ya se han implementado estos medidores. Otro ejemplo es en la agricultura, se pueden usar dispositivos IOT para medir las condiciones del suelo y meteorológicas y de esa manera podemos monitorizar mejor el desarrollo de las plantas. También podemos aplicarlo para hacer seguimientos o tracking, por ejemplo en los coches, a través del IOT podemos ver los lugares que hemos recorrido y el consumo de combustible. Otra aplicación es en la medicina, actualmente existen dispositivos IOT que pueden medir por ejemplo tu frecuencia cardiaca, esta información puede ser enviada a una clínica para que el doctor lo analice. En la industria el IOT también es bastante importante, ya que podemos obtener datos de interés y poder analizarlos. El IOT aplicado a la industria también se lo puede denotar por sus siglas en inglés IOT. En otro video habíamos hablado con mayor profundidad sobre la aplicación del IOT en la industria, te dejaré el video como sugerencia. Para terminar, otra aplicación importante del IOT es en la domótica, básicamente desde tu celular puedes monitorizar por ejemplo el consumo de electricidad o de agua en tu además de tener control de algunos dispositivos de tu casa como la luz, calefacción, etc. Bien, eso sería todo por este video, en los próximos videos continuaremos con el curso teórico de IOT. Gracias
DOC0025|Internet de las cosas|Hola a todos, continuando con el curso teórico básico del Internet de las Cosas, en este video vamos a hablar de los dispositivos IoT. En un video pasado habíamos visto la topología del Internet de las Cosas. En ese video explicamos de forma muy general los componentes del IoT. Para entenderlo de mejor forma lo podemos dividir en cuatro capas o actores. La primera capa está dedicada a los objetos o dispositivos del Internet de las Cosas. La segunda capa tiene que ver con la conectividad inalámbrica o red. La tercera capa comprende los servidores o plataformas. Y la cuarta capa tiene que ver con la aplicación que le demos a la información. En este video nos vamos a centrar en la capa de dispositivos u objetos IoT. Esta comprende tanto el hardware como los sensores. Nosotros actualmente contamos con muchas opciones de hardware para capturar datos. De los más conocidos encontramos el SCP32, el Arduino, las tarjetas ARM, microchip, etc. En cuanto a los sensores, no es necesario que estos contengan conexión inalámbrica para el envío de datos, ya que la tarjeta lo puede hacer. Solo es necesario que se pueda conectar con el hardware. Los sensores además pueden servir para sistemas embebidos o equipos industriales. Como no tienen algún módulo IoT incorporado, en este video solo nos vamos a centrar en describir el hardware. Pero antes de hablar de las tarjetas más populares para trabajar con IoT, tenemos que explicar qué es un SOC. La palabra SOC viene de las siglas System On Chip. Los SOCs son chips que contienen en su interior un CPU y módulos de conexión inalámbrica como Wifi o Bluetooth. Aparte de ello, también contienen módulos básicos como ADC, UART, etc. Básicamente podemos decir que los SOCs son chips altamente integrados. Ahora sí, procederemos a describir las tarjetas más populares para trabajar con IoT y que utilizan Wifi para la comunicación inalámbrica. Empezamos por el SP8266. Este es una de las primeras tarjetas que venían incorporadas con el módulo Bluetooth y Wifi. Antes, este tipo de tarjetas que usaban Wifi o Bluetooth solo se podían programar con comandos AT. En cambio, el SP8266 se hizo popular porque se podía programar con Arduino IDE y MicroPython. Esta tarjeta contiene un procesador de un núcleo que alcanza una frecuencia de hasta 160 MHz. Estas tarjetas son fabricadas por la compañía china Expressive. Luego de la SP8266 saldría el mercado SP32 fabricado por la misma compañía. Puedo el SP8266 y la SP32 son tarjetas SOC. La diferencia entre estos dos es que el SP32 contiene un procesador de dos núcleos que puede correr hasta 240 MHz. Además, el SP32 contiene un coprocesador de bajo consumo, es decir, que podemos ahorrar energía si no se necesita el procesador principal. El SP32 vendría a ser como una versión mejorada del SP8266 y una gran ventaja si usamos esta tarjeta es que es de bajo costo. El SP32 también, además de Bluetooth y Wifi, contiene protocolos como VLE, EDR, CAM y Ethernet. Además, cuenta con periféricos como ADC, UART, SPI y I2C entre otros. Nosotros vamos a usar el SP32 para hacer algunas pruebas de IoT, así que en otro video explicaremos con mayor detalle todo sobre esta tarjeta. Bien, ahora vamos a hablar de la Raspberry Pi. La Raspberry Pi es una computadora de bajo costo y formato compacto. La ventaja de una computadora es que puede ejecutar diversos programas a la vez. Todos los diseños de la Raspberry Pi se basan en hardware libre, es decir, que sus componentes se pueden encontrar en el mercado de componentes electrónicos. Además, habitualmente se utilizan sistemas operativos libres basados en Linux para trabajar con la Raspberry Pi. Al día de hoy, Raspberry Pi cuenta con muchos modelos. Podemos encontrar el modelo A, modelo B, Raspberry Pi Pico, entre otros. Bien, ahora vamos a hablar de la placa de desarrollo para IoT de Microchip. Básicamente es un módulo de comunicación WiFi, pero que requiere de un microcontrolador para que pueda mandar comandos y para poder interactuar con la comunicación inalámbrica. El microcontrolador que utiliza esta tarjeta es el ATmega 4808. Este microcontrolador puede ser programado en MPLAB o Atmel Studio. Aparte del módulo WiFi, esta tarjeta contiene un sensor de luz, un sensor de temperatura, botones y LEDs. La principal ventaja de este aparato es que son de bajo consumo de energía. Si bien es cierto, el SP32 tiene una configuración para trabajar con poca energía, generalmente su consumo energético es mayor al de esta tarjeta. Aunque en cuanto a precios, el SP32 es mucho más barato. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico básico de IoT. Gracias. ¡Suscríbete al canal!
DOC0026|Internet de las cosas|Hola a todos, en este video vamos a hacer una descripción del SP32. En el video sobre dispositivos IoT ya habíamos descrito este dispositivo, pero de forma muy general. En este video vamos a profundizar más en su estructura y sus modos de funcionamiento para aplicarlo al IoT. Entonces, cuando hablamos del SP32, estamos hablando de tres tipos de productos. Primero tenemos los chips. En estos chips encontramos el procesador que vendría a ser el cerebro del SP32. Además lleva incorporado algunos periféricos. Las características del procesador puede cambiar según el modelo, aunque no son cambios muy significativos. También tenemos los módulos. Estos ya traen en su interior el chip. En estos módulos además, aparte de facilitar la integración del SP32 a una tarjeta, se le añade una antena en la placa para las conexiones por wifi y bluetooth. Por último, tenemos las tarjetas de desarrollo, que son las más conocidas. En estas, el módulo ya está soldado. Además tiene pines diseñados para encajar en una protoboard. Esto facilita la integración del SP32 en prototipos. Además cuenta con un regulador de tensión y un controlador serie USB. Bien, ahora vamos a proceder a describir la tarjeta SP32DeepKit V1. Esta es una de las tarjetas más populares y es la que vamos a estar usando para hacer nuestras distintas pruebas. La SP32DeepKit V1 incorpora el módulo SPROM32 y este módulo además usa el chip SP32DOWTQ6 en su interior. Ahora, en la siguiente imagen podemos ver el diagrama de bloques del chip. Notamos que contiene algunos de los periféricos más populares y comunes. Además tiene periféricos de wifi y bluetooth. También encontramos esquemas de encriptación para proteger los datos. Esto es importante cuando se trabaja con protocolos cifrados como el HTTPS. Bien, ahora te muestro algunas de las características principales del SP32DeepKit V1. Las otras tarjetas pueden que tengan características distintas pero son muy similares. Entonces, el SP32DeepKit V1 en su interior contiene un CPU con dos núcleos que corre hasta 240 MHz. Generalmente un núcleo se encarga de manejar todo el tema de red y el otro núcleo se usa como un procesador de propósitos generales. Además, la CPU es de 32 bits. Esto hace que pueda realizar más cálculos en una sola instrucción. También contiene un coprocesador de ultra bajo consumo de energía. Generalmente, este coprocesador se usa cuando la tarjeta se encuentra en modo sueño o modo suspensión. El SP32 trabaja con 3.3 voltios, pero como la tarjeta contiene un regulador de voltaje, podemos alimentarlo con 5 voltios. También tiene 24 pines digitales. Encontramos además que contiene una memoria ROM de 448 KB, una SRAM de 520 KB. Una memoria flash de 2 MB. También tiene 2 periféricos UART, 3 SPI, 2 i2C, 2 i2S, 2 conversores digital analógico de 8 bits, 18 conversores analógico digital de 12 bits, Bluetooth, WiFi de 2.4 GHz, Bluetooth de bajo consumo de energía, entre otras cosas. Entonces, ya que conocemos el SP32, ¿Cuántos modos de funcionamiento existen para conectarlo al WiFi? Para responder esa pregunta, previamente debemos conocer algunos conceptos teóricos. Primero, los dispositivos que se conectan a una red WiFi se llaman estaciones, y los que generan la red WiFi se les llama punto de acceso o access point. Bien, ahora sí, vamos a hablar de los modos de funcionamiento. Todos estos modos aplican tanto para el SP32 como para el SP8266. El primer modo que encontramos es el modo estación. Básicamente, en este modo de operación, la estación, en nuestro caso el SP32 y el access point, se conectan de forma inalámbrica. De esa forma, se crea un enlace mediante el cual el access point y el SP32 pueden compartir datos. También podemos encontrar el modo estación cliente. En este modo, el dispositivo se conecta a un punto de acceso WiFi y además se conecta con un servidor o servicio dentro de la red. Para ese caso, se puede usar el protocolo HTTP, MQTT, BWS, etc. Para el IoT, esta es la forma más común de utilizar el SP32. Bien, otro modo que podemos encontrar es el modo estación servidor. En este caso, el SP32 va a conectarse a un access point como una estación, pero solo va a recibir conexiones, es decir, va a funcionar como un servidor. En este modo, cualquier dispositivo de red conectado al mismo access point va a poder ingresar al programa de servicio montado al SP32 y puede establecer alguna comunicación o solicitar alguna prestación. Luego, tenemos otro modo llamado access point. En ese modo, el SP32 genera una red WiFi en el cual se pueden conectar otras estaciones. A través del mecanismo DHCP, el SP32 puede habilitar IPs. El rango de IPs generalmente está contenido entre 192.168.4.x, yendo x desde el 1 a 255. Bien, el otro modo que existe es el modo access point más servidor. Este caso combina dos modos de funcionamiento, en la cual el SP32 puede generar una red WiFi y además funcionar como un servidor. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico básico de IoT. Gracias.
DOC0027|Internet de las cosas|Hola a todos, continuando con el curso teórico básico del IoT, en este video vamos a hablar de la capa de conectividad inalámbrica. Durante la última década se ha desarrollado a pasos agigantados el Internet de las Cosas. Al día de hoy existen gran cantidad de dispositivos IoT y existirán aún más en el futuro. Estos dispositivos tienen que estar interconectados. A raíz de esto han surgido tecnologías inalámbricas que cubren esas necesidades. Pero antes de iniciar con esto, tenemos que hablar de las tecnologías o redes LP-1. Como ya lo habíamos dicho en el primer video del curso, en el IoT existen muchas tecnologías para todo tipo de aplicaciones. En las redes LAN, por ejemplo, se pueden implementar tecnologías de rango corto y que sirven para cubrir pequeñas áreas. Estas, por lo general, tienen un buen ancho de banda, aunque su alcance no es muy grande. Estas tecnologías están destinadas para aplicaciones que van a interactuar con el usuario final. Pueden estar presentes en establecimientos, recintos, parques, etc. En este grupo tenemos el WiFi, el Bluetooth, entre otros. Por otro lado, encontramos las redes celulares. Tradicionalmente las redes celulares estaban pensadas para servicios de telecomunicaciones. Pero hoy en día, también puede ser utilizado para el Internet de las Cosas. A partir del 3G ya se podía transmitir paquetes de datos. Pero con el 4G y actualmente con el 5G, se mejora esta característica. Con estas redes se puede transmitir grandes cantidades de datos. Además, no necesitas gastar en implementación, ya que la red celular está masificada. Solo necesitarías pagar una cuota para hacer uso de ella. La desventaja sería que muchos de estos dispositivos de red celular no son de bajo consumo de energía. Por otro lado, tenemos la red LP-1. Que podríamos decir, que combina lo mejor de las redes celulares y la red LAN. Los dispositivos LP-1 consumen poca energía y generalmente trabajan con baterías que pueden durar años. En la siguiente imagen, hacemos una comparación de los tres tipos de tecnologías o redes que existen. Para la comparación, hemos usado un plano cartesiano. En el eje X encontramos el alcance que puede tener. Y en el eje Y se encuentra el ancho de banda o la tasa de datos disponibles. Entonces, la red LP-1 es de largo alcance, aunque tiene un ancho de banda reducido. Pero para las aplicaciones de la IoT, generalmente no se necesitan un gran ancho de banda. Ya que comúnmente se envían datos telemétricos. En el grupo de redes LP-1 encontramos la tecnología LoRa, Sixfox, Narrowband IoT, entre otros. Justamente van a ser estas tres tecnologías de las que vamos a hablar en este video. LTE-M también es un LP-1, pero su descripción lo podremos tocar en otro video. Bien, si nos enfocamos en la tecnología LoRa, esta surgió en Francia, pero fue comprada por la empresa estadounidense Semtensh. LoRa se caracteriza por usar las llamadas bandas libres, también llamadas bandas ISM o bandas no licenciadas. Estas bandas son rangos de frecuencia que para usarlos no requieren permiso. Cualquier persona u organización la puede usar para sus fines. La comunicación LoRa puede cubrir grandes distancias, en el rango de kilómetros. Justamente la palabra LoRa viene de las siglas LONG RANGE, que significa largo alcance. LoRa puede abarcar desde 1 km hasta 15 km si no hay obstáculos, aunque por lo general llega hasta 5 km con obstáculos. La razón por la que LoRa es tan popular es porque la empresa Semtensh, para difundir esta tecnología, creó una organización sin fines de lucro llamada LoRa Alianza o LoRa Aliens. Esta organización engloba a muchas compañías del mundo que aportan a esta y velan para que la tecnología LoRa siga creciendo y sea adoptada de forma masiva. Además tiene muchos dispositivos basados en esta tecnología. La principal característica si utilizamos esta red es que cada persona puede generar su propia cobertura inalámbrica. Para conectar los dispositivos a LoRa se requiere tener los Gateway, que el usuario los puede gestionar con lo que se denomina servidores de red. Esto te permite tener mas control sobre la red, pero también te obliga a utilizar mas recursos. En LoRa se trabaja con el protocolo LoRaWAN. Este protocolo se encarga de encriptar la data, de gestionar la comunicación entre el dispositivo y el Gateway. Además tiene mecanismos para adaptar la potencia que consume el dispositivo para conectarse con el Gateway. Por ejemplo, si el dispositivo está mas cerca al Gateway, el protocolo LoRaWAN se encarga de que el dispositivo consuma menos energía. En cambio, si está mas lejos, LoRaWAN suministra mas energía para contrarrestar la distancia. Mas adelante en otro video vamos a explicar con mayor detalle todo lo que tenga que ver con LoRa y LoRaWAN. Bien, en el caso de SixFox, al ser una red LP1, está orientada al IoT y es de bajo consumo de energía. Al igual que LoRa, opera en bandas libres. Una ventaja de SixFox es que su comunicación es bastante robusta. Tan robusta que no le afecta a los inhibidores de frecuencia. Estos inhibidores se usan para bloquear la señalina alámbrica, ya sea WiFi, red móvil, Bluetooth o cualquiera. Con SixFox puedes conectar medidores de energía, agua, gas o sensores de cualquier tipo. Su principal oferta de valor y que la diferencia de LoRa, es que no debemos preocuparnos por la infraestructura de comunicación. En Latinoamérica existen operadores que se encargan del tema logístico de desplegar la red de intercomunicación de SixFox. En el caso de Perú, México, Colombia y Brasil, el operador SixFox es la empresa Pwnd. Y en el caso de Chile y Argentina, el operador es DATCO. Entonces, ¿cómo funciona SixFox? Pues básicamente, un dispositivo se conecta a una antena SixFox que ya está desplegada. Estas antenas se comunican con el centro de datos del usuario. El usuario tiene una interfaz para acceder a SixFox y poder gestionar de forma remota los dispositivos. Además, el usuario puede integrar esos datos hacia otras plataformas. Bien, luego de LoRa y SixFox apareció la tecnología NarrowBank IoT. Esta tecnología es un estándar de comunicación basado en telefonía móvil, pero orientado a trabajar con dispositivos IoT. Fue estandarizada, de hecho, por la organización 3GPP. Esta organización surgió para poder estandarizar la tecnología 3G y después la 4G y 5G. Entonces, ellos también han sacado este estándar de comunicación inalámbrica, que funciona sobre las empresas y compañías de telecomunicaciones de red móvil. Como ventajas de la NarrowBank IoT, encontramos que es de bajo consumo de energía y de largo alcance, justamente por ser el EP1. Además, es tolerante a una gran cantidad de dispositivos conectados. Permite mayores anchos de banda para la transmisión de datos. Permite comunicación ilimitada entre el dispositivo y la antena. Y también, al igual que todos los estándares IoT, utiliza mecanismos de encriptados para proteger los datos. Como ya dije, a diferencia de LoRa y SixFox, este es un estándar bastante nuevo y además trabaja en bandas licenciadas. NarrowBank IoT aún se encuentra en un proceso de adopción. Por ahora, su mayor cobertura se debe a Europa. Bien, ya para terminar, vamos a hacer una tabla comparativa entre estas tres redes. Como podemos ver, por el lado de la NarrowBank IoT, es altamente escalable. Tiene una baja latencia, tiene una payload o longitud de datos más grandes y tiene una buena calidad de servicio. Es decir, no le afecta las interferencias, por lo mismo que está operado en bandas licenciadas, que son exclusivas para el operador. Ahora, podemos ver que SixFox tiene mayor rango o alcance. Además, tiene mayor cobertura, tiene un costo más eficiente y tiene una duración de batería más larga, ligeramente superior a LoRa. NarrowBank IoT, por lo mismo que es más nueva, aún está en proceso de despliegue y tiene menos cobertura. Por otro lado, LoRa tiene una mayor longitud de datos con respecto a SixFox. El despliegue de LoRa, además, es superior al de SixFox y NarrowBank IoT. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico básico del IoT. Gracias. ¡Suscríbete al canal!
DOC0028|Internet de las cosas|Hola a todos, en este video vamos a revisar todo lo relativo a Lora y LoraOne. En un anterior video sobre la capa de conectividad inalámbrica del IoT ya habíamos visto a grandes rasgos de que se trataba esta tecnología. Ahora vamos a detallarlo con mayor profundidad. En primer lugar, ¿Qué es Lora? Lora es un tipo de modulación inalámbrica utilizada para crear enlaces de comunicación de larga distancia. De hecho, su nombre proviene de LONG RANGE, que significa de largo alcance. Su tipo de modulación es muy parecida a la modulación FCK. Básicamente se basa en la modulación de Spectrum Sanchado CHIRP y tiene la ventaja de que esta modulación es bastante eficiente y que no requiere una gran cantidad de potencia. Lora, al igual que SixFox, Narrowband IoT y LTE-M, es una tecnología LP-1. En el video sobre conectividad inalámbrica también explicamos sobre estas tecnologías. Lora fue comprada por la empresa SEMTECH y la razón por la que Lora es tan popular es porque SEMTECH, para difundir esta tecnología, creó una organización sin fines de lucro llamada Lora Allianza o Lora Allianz. Esta organización engloba a muchas compañías del mundo que aportan a esta y velan para que la tecnología Lora siga creciendo y sea adoptada de forma masiva. En su página web se puede encontrar diferentes tipos de dispositivos Lora, además de información sobre las áreas en las que se puede aplicar esta tecnología. También se encuentran diferentes conferencias y artículos de información. Te dejaré el link de su página web en los comentarios. Ahora vamos a hablar de Lora One. Lora One es un protocolo de comunicación y pertenece a la capa de aplicación, mientras que Lora pertenece a la capa física. De hecho, Lora One se soporta sobre Lora y permite que los dispositivos interactúen a través de un conjunto de reglas. Todo lo que tenga que ver con controlar los nodos conectados, la capacidad de controlar lo que manda uno u otro nodo, determinar la redundancia de los datos, comprobar la conexión, gestionar la red completa, filtrar los paquetes de datos, entre otras cosas que son complejas, está contenida en el servidor de aplicación. Bien, en cuanto a la arquitectura de red, generalmente se utiliza la topología en estrella. Esto ya lo vimos en el primer video del curso. Dentro del protocolo Lora One y trabajando con esta topología, podemos preservar la vida útil de la batería de los dispositivos o nodos, porque estos solo van a consumir energía cuando manden su información. Ya no es necesario que retransmitan los datos de otro nodo. Mientras no manden su información, pueden estar en modo de bajo consumo. Básicamente, los nodos envían datos a un gateway concentrador o pasarela que esté más cerca y donde tenga la mejor potencia de señal. Un gateway puede administrar cientos o miles de nodos. Todo depende de la frecuencia a la que los nodos envíen sus datos. Lora One además tiene un mecanismo que se llama Tasa Adaptativa de Datos. Este mecanismo permite a los nodos poder transmitir sus datos aún en condiciones extremas de baja o mala calidad de señal. Para ello se debe reducir aún más el ancho de banda, pero es mejor tener un ancho de banda reducido que tener un ancho de banda amplio con datos perdidos. Bien, en cuanto a los nodos, con el fin de organizarlos mejor para sus aplicaciones finales, Lora One utiliza diferentes clases de dispositivos divididos en clase A, clase B y clase C. Dependiendo del fabricante, a veces se los puede configurar para que los dispositivos se cambien de clase. Los dispositivos clase A son dispositivos convencionales pensados para telemetría. Estos son nodos diseñados con la finalidad de transmitir datos y tienen una duración de batería alta para que puedan trabajar durante años, aproximadamente entre 5 a 10 años, sin tener que cambiar la batería. Aunque esto también depende de la frecuencia con la que se mande datos. Podemos elegir, por ejemplo, mandar datos cada 10 segundos o mandar datos cada 10 minutos. Esto va a depender de las necesidades que tengamos. Si mandamos datos con un intervalo corto de tiempo, la batería durará menos. Los sensores, generalmente, son configurados como dispositivos de clase A. Los dispositivos de clase C están pensados para el control. Estos dispositivos tienen la capacidad de ser despertados de forma inmediata a través de un comando desde el network server vía Downlink. Los dispositivos envían su paquete de datos o payload a la pasarela o gateway mediante un sistema que se denomina Ablin o enlace ascendente. Después del get-go, el paquete de datos pasa al network server. Y cuando el network server envía datos a los dispositivos se llama Downlink o enlace descendente. Entonces, un dispositivo clase C, después de mandar sus datos, pasa a un estado slip o de reposo. Y si le enviamos un dato, este dispositivo inmediatamente se despierta. Esto no ocurre con los dispositivos clase A. En los nodos clase A, sí pueden aceptar Downlinks, pero tienen un momento específico en el que pueden recibirlo. Justo en el momento en el que se establece un enlace ascendente, se abre una ventana de recepción. Es en esta ventana donde el dispositivo clase A puede recibir un Downlink. Cuando termina el envío de datos, la ventana de recepción se cierra. En el caso de los dispositivos C, la ventana de recepción siempre está abierta. Esto también provoca que la batería dure menos, ya que consume energía. Por eso, normalmente, los dispositivos clase C vienen con alimentación externa. Bien, los dispositivos clase B son una clase intermedia entre la clase A y la clase C. Estos dispositivos trabajan con baterías, pero abren su ventana de recepción con cierta regularidad. No las mantienen siempre abiertas como los dispositivos de clase C, pero tampoco están permanentemente cerradas como los dispositivos de clase A. Para casos prácticos, los nodos clase B no son muy usados. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico básico del IoT. Gracias. ¡Suscríbete al canal!
DOC0029|Internet de las cosas|Hola a todos, continuando con el curso teórico básico de la IoT, en este vídeo vamos a hablar de la capa de conectividad inalámbrica. Yo en un video pasado habíamos hablado de las redes LP-1, ahora hablaremos concretamente de las redes móviles, que es otro tipo de conectividad inalámbrica. Para empezar ¿Qué es una red móvil? Una red móvil es una forma de comunicación entre dos puntos que pueden estar en constante movimiento. La arquitectura móvil sigue el estándar GSM. El estándar se creó para dar una normalización en las comunicaciones móviles para telefonía celular. Uno de sus principales puntos fue que determinó que todos los terminales en el mundo deberían usar un SIM card o tarjeta SIM. Esto con la finalidad de que los arrendados o usuarios puedan cambiar de operador de telefonía móvil de forma fácil sin tener que cambiar el equipo. Con respecto a su área de cobertura de las antenas móviles, estas se representan con un aya hexagonal. El conjunto de estas se conoce como red de celdas. El usuario se puede movilizar entre las celdas y siempre va a tener conexión a la red. Cuando el usuario pasa de una celda a otra se le conoce como Hannover. Además, el estándar GSM utiliza bandas de comunicación. De forma estándar, estas bandas son de 850, 900, 1800 y 1900 MHz. Bien, con respecto a las redes desde 1G hasta 5G, la 1G está prácticamente obsoleta y la 5G está en pleno desarrollo. Esta engloba muchos protocolos de comunicación. Las más conocidas para la IoT son la Narrowband IoT y LTE-M. Sobre Narrowband IoT ya lo hablamos en el video sobre redes LP-1 y LTE-M lo hablaremos en otro video. Ahora vamos a hablar de las redes 2G, 3G y 4G. En el caso de las tecnologías 2G, alcanzan velocidades de 64 kbps y engloban a los protocolos de comunicación de red GPRS y HEDGE. 2GPRS es un servicio de paquetería de datos de red a nivel TCP-IP. Podemos decir que unifica el mundo IP con el mundo de la telefonía móvil. Además, está orientado a la velocidad de datos y tiene velocidades de hasta 144 kbps. A esta tecnología se le agregó la comunicación HEDGE, que puede lograr velocidades de hasta 384 kbps y también el servicio de mensajes SMS que se usa para mandar mensajes de texto. Luego apareció el 3G. Aquí es cuando empieza la era de los smartphones. Estos aparatos utilizaban el protocolo HSPA, HSPA+, y VMTS. La máxima velocidad que pueden alcanzar las redes 3G es de 2 Mbps. Esto ya presenta un gran avance, porque se podía descargar imágenes, fotos, navegar en internet, entre otras cosas. Tiempo después apareció las redes 4G. Estos albergan el estándar LTE. Las velocidades máximas de las redes 4G son de hasta 100 Mbps. Según cómo se muestran los avances, con la red 5G podríamos alcanzar 1 Gbps. Una cosa para tomar en cuenta es que la velocidad de los datos va de la mano con el costo de implementar esa tecnología. Es más caro trabajar con 5G que con 2G. Además, las redes 2G tienen mayor área de cobertura. Si esto lo vemos desde el enfoque en la IoT, nosotros en el internet de las cosas buscamos sobre todo transferir datos telemétricos, como temperatura, humedad, etc. Estos datos no pesan mucho, por lo que nos requerimos un gran ancho de banda. Si queremos, por ejemplo, recopilar los datos de un medidor eléctrico durante 3 veces al día, es suficiente trabajar con la 2G. En el IoT no importa tanto la velocidad de datos, sino la escalabilidad de los dispositivos. Esto quiere decir que la red puede que no maneje una gran cantidad de datos, pero sí debe contener en una celda miles de millones de conexiones. Entre las aplicaciones que se le pueden dar a las redes móviles en el internet de las cosas encontramos sobre todo aplicaciones que necesiten controlar cosas de manera distante, por ejemplo, control a tiempo real de fábricas y procesos industriales y también en vehículos inteligentes. En el caso de Perú existen 4 operadores de telefonía móvil pública, que serían VTEL, Claro, Movistar y Intel. En el caso de VTEL solo tiene implementada 3G y 4G. En el cuadro comparativo que te estoy mostrando se puede ver las bandas en las que trabajan estos 4 operadores para las distintas redes que existen. Bien, las tarjetas más conocidas para trabajar con redes móviles son los módulos SIMCOM. En el mercado ya existen tarjetas que incluyen estos módulos. Por ejemplo, esta tarjeta incluye el SIM900. El SIM900 es uno de los primeros módulos que salió al mercado para trabajar con GCM. Y esta tarjeta se puede conectar fácilmente con Arduino. Y además ya contiene un regulador de voltaje, pines y toda la circuitería para poder conectarlo de forma sencilla a una red móvil. Como el SIM900 podríamos hacer por ejemplo llamadas celulares, mensajes de texto, enviar voz, conectarnos a internet por gprs, etc. Esta otra tarjeta contiene al SIM808. Esta trae lo mismo que el SIM900, pero adicional a eso contiene comunicación GPS y radio FM. Para aplicaciones que tengan que ver con tracking, esta tarjeta es muy interesante. Aquí tenemos una tarjeta que contiene al SIM868. Y este módulo trae lo mismo, pero se le agrega la comunicación Bluetooth. Y una de las tarjetas más populares es esta, que contiene al SIM800L. Este es un módulo bastante práctico que trabaja con 2G. A través de sus pines puedes conectar un parlante y un micrófono para poder hacer llamadas telefónicas. También está enfocado a que se pueda conectar a datos para fines relacionados al internet de las cosas. En cuanto a costo, el SIM800L es bastante barato. De hecho, esta va a ser la tarjeta que vamos a usar para las aplicaciones de internet de las cosas en el curso de la IAT con SP32. Bien, aquí te presento las características que son similares para las tarjetas que trabajan con el SIM900 y SIM800. La diferencia es que el SIM800 es una tecnología más actual. El SIM808 también se parece mucho, solo que esta tiene GPS. Estos módulos trabajan con 4 bandas. Estas son 850, 900, 1800 y 1900 MHz. También incluyen GPRS Clase 10-8 con estación móvil Clase B. Como máximo tienen una potencia de transmisión de 2W, que es bastante comparado con el módulo Wi-Fi que llega a 250mW. Por eso Wi-Fi tiene una cobertura de máximo 100 metros, mientras que con las redes móviles la cobertura puede ser de 15 a 40 km dependiendo de los obstáculos. La desventaja de esto es que consume más energía, por lo mismo que necesita más potencia. A diferencia de las redes LP-1, donde los sensores pueden funcionar con baterías, en las redes móviles el hardware incorporado casi siempre tendrá que ser conectado a la red eléctrica. El control de estos módulos se hace a través de los comandos AT, ya sea, por ejemplo, para hacer llamadas o para operaciones con sockets TCP y P. Para el Internet de las Cosas vamos a utilizar sus comandos para trabajar con TCP y P y generar conexiones con HTTP o MQTT, ya que además incluye protocolos de capa de aplicación implementados mediante comandos AT. En modo bajo consumo llegan a consumir 15 mA. Su temperatura de operación es de menos 40 a 85 ºC. Algo importante por señalar es que se necesita un micro controlador o tarjeta con comunicación UART para poder ser comandado. Para esta acción podemos elegir un Arduino, un SP32, un AVR, un PIC, entre otros. En el curso del IoT con el SP32 usaremos el SP32 para comandarlo, pero en otro próximo video del curso de micro controladores PIC usaremos un PIC para hacer esto, aunque también podemos utilizar un conversor USB serial para conectarlo con la computadora y desde allí comandarlo. Un detalle especial para el SIM 800 es que se lo tiene que alimentar con 3.4 a 4.4 voltios. Con 3.3 voltios o 5 voltios el módulo no va a funcionar. En el caso del SIM 900 se lo alimenta con un voltaje de entre 5 y 12 voltios. Bueno, eso sería todo por este video. Para que esta explicación te quede más clara puedes revisar el próximo video que vamos a sacar en el curso del IoT con SP32 donde usaremos el módulo SIM 800L. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias. ¡Suscríbete al canal!
DOC0030|Internet de las cosas|Hola a todos, continuando con el curso teórico básico del IoT, en esta oportunidad vamos a hablar del Network Server, que está incluida en la tercera capa. Entonces, ¿qué es el Network Server? El Network Server o servidor de red, es un ordenador o equipo informático que ofrece acceso a recursos y servicios compartidos a otros equipos conectados en red. Justamente, se llaman servidores porque sirven y están al servicio de otros ordenadores, denominados clientes. Los recursos compartidos del servidor a los clientes pueden incluir acceso a hardware, como discos por ejemplo, también puede incluir acceso a servicios, como servicios de email o acceso a internet, y puede incluir acceso a software. Esto lo vamos a detallar más adelante. Bien, el modelo o arquitectura que siguen los servidores es el de cliente-servidor, es decir, el cliente o los clientes piden y el servidor proporciona los recursos o servicios. Cabe resaltar algo, un cliente puede estar conectado a varios servidores, así también a un servidor se le puede conectar varios clientes. Los servidores se utilizan para gestionar los recursos de una red, un servidor debe estar siempre encendido, ya que si se apaga o si presenta errores, hace que los demás usuarios de la red tengan problemas porque no disponen de los servicios que proporciona ese servidor. Y la red más conocida y más grande es internet, justamente será esta red la que usaremos para las soluciones del internet de las cosas. El uso de una red de servidores remotos conectados a internet para almacenar, administrar, procesar datos, entre otras cosas, es lo que se conoce como nube o cloud. Para organizar mejor todos los tipos de servicio conectados en la nube, se los puede distribuir en una pirámide. La pirámide de la nube es la distribución de servicios informáticos como servidores, recursos de almacenamiento, bases de datos, redes, software, análisis e inteligencia, a través de internet para proporcionar innovación rápida, recursos flexibles y economías de escala. Solo se paga por los servicios en la nube que se utilizan y se ahorra en costos operativos. Los tres tipos principales de servicios que se pueden entregar en la nube son, infraestructura como servicio, plataforma como servicio y software como servicio. El posicionamiento que cada tipo de servicio ocupa dentro de la pirámide es para representar la interdependencia que existe entre uno y otro. En este sentido, cada software se basa en una plataforma, para lo cual se requiere una infraestructura. Bien, en este video vamos a explicar de forma muy general de que consiste cada tipo de servicio. Empezamos por el tipo de infraestructura como servicio o también conocido por sus siglas IAAS. Cuando hablamos de este servicio nos referimos a la acción de reservar recursos de hardware en la nube. Esto significa que tú determinas una cantidad de recursos físicos como CPU, RAM, red y almacenamiento y luego lo usas con el sistema operativo que deseas. Esto es comparable a comprar físicamente un servidor para instalarlo en una casa o empresa. Pero en lugar de eso puedes decidir reservar toda la infraestructura dentro del servidor de la nube. El usuario puede implementar y ejecutar un software arbitrario que puede incluir sistemas operativos y aplicaciones. Además, el usuario no administra ni controla la infraestructura de la nube subyacente, sino controla todo lo demás. Este tipo de servicio en la nube está en el primer escalón de la pirámide porque los recursos de hardware son la base sobre la que luego se ponen a trabajar para dar vida al resto. Como ejemplo tenemos Amazon Web Services que nos proporciona el EC2 que básicamente es un ordenador en la nube. Nosotros le tenemos que levantar el sistema operativo e instalarle programas como noda de red para hacer soluciones del IoT. En un posterior video hablaremos más sobre Amazon Web Services y en un video anterior de este canal ya habíamos hablado de noda de red. Además, otros ejemplos de infraestructura como servicio son DigitalOcean, Rackspace, KioNetwork, entre otros. Bien, por otro lado tenemos la plataforma como servicio o PAAs. Esto se aleja un poco más de la infraestructura local. En este caso el proveedor de la nube aloja el hardware y el software en su propia infraestructura y ofrece la plataforma al usuario como una solución integrada, una pila de soluciones o un servicio a través de internet. La plataforma como servicio permite al usuario desarrollar, ejecutar y gestionar sus propias aplicaciones sin tener que diseñar ni mantener la infraestructura ni la plataforma. Todo esto es especialmente útil para los desarrolladores y programadores. Entre dos ejemplos podemos encontrar a Ubidots. Cuando adquieres un servicio en Ubidots vas a tener un interfaz donde vas a poder crear tu proyecto. También, en este mismo tipo, podemos encontrar SyncSpeak, Adafruit y Blink. Si bien es cierto que las plataformas como servicio nos reducen el tiempo de estar instalando un sistema operativo y otras cosas, tienen como desventaja que no vamos a poder entrar al terminal del servicio de servidor y no podremos instalar por ejemplo Node.RED o una aplicación de manejo de base de datos. Por último tenemos el software como servicio OSAAS. Este es un modelo de entrega de software en el que el proveedor de la nube desarrolla y mantiene el software de las aplicaciones. Además proporciona actualizaciones automáticas del mismo y lo pone a disposición de sus clientes. El proveedor de la nube pública administra todo el hardware y el software tradicional, incluido software de aplicaciones, seguridad y demás. Podríamos decir que en el caso del IoT, el software como servicio vendrían a ser las aplicaciones finales que serán manejadas por el usuario. Estas aplicaciones finales pueden ser creadas en una plataforma como servicio, por ejemplo con Ubidots, o pueden ser creadas con una herramienta de desarrollo como Node.RED instalada en una F-Structure como servicio. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico del IoT. Gracias. ¡Suscríbete al canal!
DOC0031|Internet de las cosas|Hola a todos, continuando con el curso teórico básico de la IoT, en esta oportunidad vamos a hablar de Amazon Web Services. Amazon Web Services, también conocida como AWS, es un conjunto de herramientas y servicios de computación en la nube proporcionada por Amazon que incluye una combinación de ofertas de infraestructura como servicio, plataforma como servicio y software como servicio. AWS se lanzó en el 2006 y entre las empresas que lo utilizan se encuentran algunas como Netflix, Reddit, Foursquare, Pinterest, la NASA, entre otros. Su uso masivo se debe principalmente a la madurez del servicio frente a otras similares y las amplias herramientas disponibles para empresas y desarrolladores de software que se pueden usar en centros de datos en hasta 190 países. Amazon Web Services ofrece una amplia gama de diferentes servicios o productos globales basados en la nube para fines comerciales. Los productos incluyen almacenamiento, bases de datos, análisis, redes, herramientas de desarrollo, aplicaciones empresariales, aplicaciones para el IoT, entre otros. Bien, en este video vamos a presentar algunos de los servicios mas importantes de AWS relacionados con el internet de las cosas. Primero, entre los servicios informáticos encontramos el S2. S2 es una máquina virtual u ordenador en la nube en la que se tiene en pleno control pudiéndole instalar algún sistema operativo y diferentes aplicaciones. En un próximo video del curso de IoT con SP32 vamos a enseñar a como crear una cuenta AWS para después crear una máquina virtual con S2. También encontramos el LightSide. Esta herramienta implementa y administra automáticamente la computadora, el almacenamiento y las capacidades de red necesarias para ejecutar aplicaciones. Además, con la herramienta Elastic Binstance podemos implementar y aprovisionar automatizaciones de recursos. Y con AWS Lambda podemos ejecutar funciones en la nube. Esta herramienta es un gran ahorro de costos ya que solo pagas cuando se ejecutan las funciones. Bien, entre los servicios de base de datos encontramos Amazon RDS. Este servicio de base de datos de AWS hace fácil configurar, operar y escalar una base de datos relacional en la nube. También está Amazon Dynamo DB. Es un servicio de datos no solo SQL, rápido y totalmente gestionado que permite el almacenamiento y la recuperación rentable de datos, además de atender cualquier nivel de tráfico de solicitudes. Bien, el servicio web Amazon Elastic Ache facilita la implementación, el funcionamiento y el escalado de una cache en memoria en la nube. Además, tenemos el servicio de base de datos gráficos Neptune, que se escala para gestionar miles de millones de relaciones y le permite consultarlas con una latencia de milisegundos. Y Amazon Redshift es la solución de almacenamiento de datos de Amazon que se puede utilizar para realizar consultas o lab complejas. Bien, entre los servicios del Internet de las Cosas tenemos el IoT Core. Este es un servicio administrado en la nube de AWS que permite que los dispositivos conectados como medidores, bombillas, redes de sensores, entre otros, interactúen de forma segura con aplicaciones en la nube y otros dispositivos. Además, con el gestor de dispositivos IoT, podemos administrar los dispositivos IoT. También, con el servicio IoT Analytics, podemos realizar un análisis de todos los datos recopilados por los dispositivos IoT. Y con el sistema operativo en tiempo real para microcontroladores llamado Amazon Free RTS, podemos conectar dispositivos IoT en el servidor local o en la nube. Bien, entre las ventajas de AWS encontramos que permite a las organizaciones utilizar diferentes modelos de programación, sistemas operativos, base de datos y arquitecturas. Además te permite crear una cuenta gratuita para que puedas utilizar algunos servicios por el lapso de un año. Otra ventaja es que no necesitas gastar dinero en ejecutar y mantener centros de datos. También te permite acceder a la nube rápidamente. AWS es un servicio rentable que te permite pagar solo por lo que se usa, sin compromisos iniciales o a largo plazo. Además, puedes agregar o quitar capacidad fácilmente. Ofrece facturación y gestión centralizadas. También, entre otras cosas, el costo total de propiedad es muy bajo en comparación con cualquier servidor privado. Bien, ahora vamos a hablar de las desventajas. Si utilizas AWS y en caso se necesita asistencia más inmediata o intensiva, se debe optar por paquetes de soporte pagados. Además, Amazon Web Services puede tener algunos problemas comunes de computación en la nube. Por ejemplo, tiempo de inactividad, control limitado y protección de respaldo. También, AWS establece límites predeterminados para los recursos que difieren de una región a otra. Estos recursos consisten en imágenes, textos y snapshots. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico de la IoT. Gracias.
DOC0032|Internet de las cosas|Hola a todos, continuando con la serie de videos del curso teórico del IOT, en este video vamos a hablar sobre HTTP. Bien, como ya explicamos en anteriores videos, a través de internet, nosotros podemos unir nuestro dispositivo IOT como un servidor. Este servidor puede estar en nuestra red LAN o instalada en la nube. Pero para enviar y recibir datos, necesitamos usar otro protocolo. Los protocolos más populares para este objetivo son el HTTP, y el MQTT. El MQTT lo explicaremos en otro video. Ahora nos vamos a enfocar en el HTTP. El protocolo HTTP es muy común no solo en el IOT. Básicamente con el HTTP se ha construido toda la web. De hecho, la explicación que te presentaré en este video se puede aplicar para cualquier caso en el que uses el protocolo HTTP. Bien, la ventaja de usar este protocolo es que es altamente masificado, tiene mucho soporte entre los lenguajes de programación y es sencillo utilizar este protocolo para mandar datos de los sensores. En la guía de fundamentos de redes, te explicamos que era el modelo TCP y P. Este modelo se divide en capas. Para describir mejor cada capa, se utiliza el modelo OSI. El protocolo HTTP pertenece a la capa de aplicación del modelo TCP y P. Otra característica es, por ejemplo, puede haber un cliente como una computadora que quiere transmitir información a un servidor. Para que suceda eso, con este protocolo, el cliente realiza una conexión y después una petición HTTP. Como consecuencia, el servidor manda una respuesta HTTP. Por eso se dice que HTTP está basado en un esquema de petición respuesta. Además, HTTP está constituido por mensajes tipo texto plano. Esto tiene la ventaja de que es legible y fácil de depurar, pero su desventaja es que los mensajes son más largos. El mensaje HTTP está compuesto de tres partes, la línea inicial o de estado, la cabecera y el cuerpo. La línea de estado indica qué hacer si es un mensaje de petición o qué ha ocurrido si es un mensaje de respuesta. La cabecera del mensaje contiene los atributos del mensaje. Esta es la unidad fundamental de la comunicación HTTP. En este bloque se dan las condiciones para la conexión con el server. Esta cabecera se termina con una línea en blanco. Y la última parte es el cuerpo del mensaje que puede ser opcional. Su presencia depende de la petición y del resultado. Otra característica es que HTTP es un protocolo sin manejo de estados. Después de que el servidor manda la respuesta, cierra la conexión. Es por eso que se suelen utilizar las cookies para guardar información como contraseñas, correos, etc. Estas cookies están listas para ser usadas cuando se establezca otra conexión con el servidor. Bien, entonces, ¿cómo solicitamos información mediante este protocolo? Eso lo hacemos a través de las urls. Estas urls están compuestas por el protocolo que se utiliza, el nombre del servidor y el camino o la ruta del recurso que queremos utilizar. El puerto que se utiliza para establecer una comunicación HTTP con el servidor es el puerto 80. Los puertos son como canales en las cuales el servidor está prestando algún servicio. Bien, ahora vamos a explicar los llamados métodos de comunicación HTTP. Los dos métodos más usados son los métodos GET y métodos POST. Estos los podemos usar para indicar una petición o respuesta. En cuanto a su uso, los métodos GET son empleados para solicitar o mandar información al servidor, y el método POST se utiliza para mandar información al servidor, pero dentro del cuerpo del mensaje. El método POST se usa, por ejemplo, para rellenar formularios. Entonces, ya conociendo esto, vamos a describir el funcionamiento de la comunicación HTTP. Supongamos que un cliente quiere entrar a este link. Primero, el cliente HTTP inicia una conexión TCP al servidor HTTP llamado educatronicos.blogspot.com. Por defecto, el puerto que se utiliza es el 80. Después, el servidor HTTP, que se encuentra escuchando por el puerto 80, acepta la conexión y se lo notifica al cliente. Seguidamente, el cliente HTTP manda un mensaje de petición GET dentro de la conexión TCP abierta a la página, que son los sistemas embebidos.html. Luego, el servidor HTTP recibe el mensaje de petición y crea un mensaje de respuesta incluyendo el texto HTML de la página solicitada. Después, el servidor HTTP cierra la conexión TCP. Y por último, el cliente recibe el mensaje y presenta la página web. Bueno, eso sería todo por este video. Para que esta explicación teórica quede más clara, próximamente sacaremos un video donde usaremos el protocolo HTTP para transmitir datos desde el SP32 a Nolred. Además, en el próximo video hablaremos del protocolo MQTT. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0033|Internet de las cosas|Hola a todos, continuando con la serie de videos del curso teórico de la IoT, en un video pasado ya hablamos del protocolo HTTP. Ahora, en este video, vamos a hablar sobre el protocolo MQTT. El protocolo MQTT se adapta para los casos en el que queremos mandar o recibir información a tiempo real de forma asíncrona. A diferencia de HTTP, aquí no tenemos que hacer un emparejamiento previo con el servidor. MQTT fue diseñado por ingenieros de IBM y es muy usado para controlar procesos de forma remota utilizando comunicación TCP IP. Entre sus ventajas encontramos que requiere poco ancho de banda, es un protocolo abierto que cualquiera los puede utilizar, consume poca energía, es muy rápido y por eso posibilita un tiempo de respuesta superior al resto de protocolos web actuales, requiere pocos recursos, procesadores y memorias, permite una gran fiabilidad y es ideal para redes inalámbricas. Es justamente por estas ventajas que este protocolo se adapta para trabajar con dispositivos con poca capacidad de cómputo como micro controladores o tarjetas IoT. Este protocolo, al igual que HTTP, se encuentra en la capa de aplicación del modelo TCP IP. En cuanto a la estructura del mensaje del MQTT, podemos identificar tres partes. Primero tenemos un encabezado fijo de 2 bytes como mínimo y 5 bytes como máximo. En este encabezado va los tipos de paquetes, la longitud del payload y el factor de calidad o QoS. El factor de calidad lo vamos a detallar cuando hablemos de las características del MQTT. Bien, continuando con la estructura del MQTT, tenemos otro encabezado de longitud variable. Es variable porque depende del tipo de mensaje, es decir, del ID, el tópico, entre otros. Cuando veamos la topología del MQTT, vamos a explicar estos conceptos. Y por último, tenemos el payload del mensaje que tiene una capacidad de hasta 256 megabytes. Ahora vamos a hablar de la topología del MQTT. MQTT utiliza una topología de tipo estrella, en la cual existe un nodo central que hace de servidor o también llamado broker. Y este es el encargado de gestionar la red y de transmitir los mensajes para mantener activo el canal. Su tarea principal es la de reenviar la información. Los brokers generalmente tienen una capacidad de conectar 10.000 clientes. Los clientes se pueden dividir en suscriptores y publicadores. Los publicadores o publisher son los que mandan información al broker, y los suscriptores o subscribers recogen la información. Cabe indicar que un cliente puede cambiar su estado. Es decir, para algunas ocasiones se puede comportar como un publicador, y para otra ocasión se puede comportar como suscriptor. Si el caso lo amerita, el cliente puede comportarse como suscriptor y publicador al mismo tiempo. Asimismo, la comunicación puede darse de cuatro formas posibles. Primero, de uno a uno. Por ejemplo, un SP32 manda la información del sensor de HT11 a una máquina virtual. Podemos ver que un publicador manda su payload a un suscriptor. Otra forma de comunicación es de uno a muchos. Por ejemplo, un SP32 manda la información del de HT11 a una máquina virtual y a una app en un celular. Aquí podemos ver que un publicador manda su payload a varios suscriptores. La otra forma de comunicación es de muchos a uno. Por ejemplo, un SP32 y un SP8266 mandan información de sus sensores a una máquina virtual. Aquí podemos ver que varios publicadores mandan su payload a un suscriptor. Y también se puede dar el caso en el que muchos publicadores se conectan a muchos suscriptores. Por ejemplo, un SP32 y un SP8266 mandan la información de sus sensores a una máquina virtual y a una app. Aquí podemos ver que varios publicadores mandan su payload a varios suscriptores. En todos los ejemplos observamos que la información pasa por el broker. Entonces, cuando un publicador establece un canal de comunicación, a ese canal se le llama tópico. Al tópico nosotros le podemos poner un nombre. Supongamos que tenemos estos dos suscriptores y dos publicadores. En nuestro caso, a un canal lo llamaremos canal 1 y al otro lo llamaremos canal 2. Consideremos que con el DHT11 estamos midiendo la temperatura del ambiente y con el otro sensor medimos la humedad del suelo. Ese dato que transmitimos lo llamamos payload. En el canal 1, el payload es 27 grados centígrados y en el canal 2, el payload es de 60% que corresponde al porcentaje de humedad de suelo. Bien, digamos que el suscriptor 1, que es el ordenador en la nube, quiere recibir los datos del SP8266. Entonces, se tiene que suscribir a ese canal apuntando al tópico canal 2 y cuando se suscriba recibirá el payload. Lo mismo ocurre con el aplicativo si quiere recibir los datos del SP32. Así también, la máquina virtual o la app podrían recibir el payload de ambas tarjetas. Simplemente se tendrían que suscribir a los dos canales. Además, podría haber un cambio de roles. La SP32 y la SP8266 podrían ser configurados como suscriptores y recibir el payload de la máquina virtual o la app según lo deseen. Entonces, ya que conocemos todo esto, vamos a repasar todas las características más importantes de MQTT. Como vemos, MQTT es un protocolo de publicación-suscripción. Esta comunicación asíncrona resulta útil en aplicaciones donde existan muchos clientes. Además, este protocolo, a diferencia de HTTP, no depende del mensaje que se envíe. En HTTP, el mensaje que se envía depende de los métodos que empleemos y de la cabecera y cuerpo que mandemos. Otra característica es que este protocolo se ejecuta sobre TCP y P y pertenece a la capa de aplicación. También, MQTT presenta tres calidades de servicio, 0, 1 y 2. La calidad de servicio 0 garantiza de que por lo menos se enviará un mensaje desde el publicador al suscriptor. Aunque no garantiza que dicho mensaje lo reciba el suscriptor. Es decir, puede haber mensajes perdidos. La calidad de servicio 1 garantiza de que por lo menos llegara un mensaje al suscriptor desde el publicador, aunque también le puede llegar más de uno, es decir, le pueden llegar mensajes duplicados. La calidad de servicio 2 garantiza de que llegará exactamente un mensaje desde el publicador al suscriptor. No habrá mensajes perdidos ni duplicados. En cuanto al ancho de banda, la calidad de servicio 0 es el que menos ancho de banda usa, y la calidad de servicio 2 es el que más ancho de banda usa. Dependiendo de lo que se necesite, configuraremos la calidad de servicio que más nos convenga. Además, este protocolo dispone de una función de última voluntad. Esto significa que si un cliente se desconectó de la red, ya sea porque se fue el internet u otro problema, automáticamente esta desconexión se notifica al broker. Y el broker notifica esta desconexión a todos los suscriptores de dicho cliente. Esto es bastante útil si estás controlando un proceso de automatización. De esta forma sabrás que un sensor está desconectado. Bien, ahora veremos cuáles son los parámetros para los clientes y el broker. Cuando el publicador quiere mandar mensajes al suscriptor, se tiene que hacer lo siguiente. En el publicador primero, debemos colocar la IP del equipo donde está el broker. También el puerto correspondiente que use el broker, el tópico y finalmente colocar el payload. Además, en el suscriptor, para recibir el mensaje, debemos colocar la IP del broker, el puerto y el tópico. Cuando el suscriptor recoge un mensaje, este no sabe quién lo ha enviado, ya que todo pasa por el broker. El puerto estándar que usa MQTT es el puerto 1883. Y para el MQTT cifrado, o también llamado MQTTS, se usa el puerto 8883. Además, al dispositivo cliente se le debe definir un ID o identificador único. Estos identificadores no se pueden repetir. Otro parámetro es el Keep Alive Time. El broker y el cliente se envían señales para confirmar que los clientes sigan conectados. Este envío de señales se hace de forma automática sin necesidad de que nosotros lo configuremos. El PINREC es el paquete enviado desde el cliente al broker. Y el PINRESP es el paquete enviado desde el broker al cliente. Entonces, el Keep Alive Time es el tiempo en segundos que el broker esperará a que el cliente responda al paquete PINRESP. Si no responde, será considerado como desconectado. Otro parámetro es la calidad de servicio o QoS. Esto ya hablamos en la sección de características. Bien, en el broker un parámetro opcional que se puede configurar es el de la autenticación. La autenticación se pone cuando queramos registrar los dispositivos clientes con un usuario y contraseña. De esta forma se mejora la seguridad para que solo los dispositivos registrados puedan comunicarse. Además, en el broker se configura el cifrado. Este puede ser SSL o TLS. Bien, dentro del protocolo MQTT existen librerías para prácticamente cualquier lenguaje de programación. Eso incluye a los más populares como Arduino, Java, JavaScript, Python, C-Sharp, entre otros. La librería MQTT para Arduino se llama PubSubClient. De hecho, en uno de los videos del IoT con SP32 programamos en Arduino IDE un SP32 utilizando esta librería para realizar una comunicación MQTT con Node-RED. Además, entre los brokers open source más populares que se pueden usar se encuentran Mosquito escrito en C++, también Mosquete escrito en Java y Mosca escrito en JavaScript. Los brokers se pueden instalar en el suscriptor, en el publicador o en un tercer dispositivo. Bueno, eso sería todo por este video. Para que esta explicación teórica te quede más clara, puedes revisar la lista del curso del IoT con SP32, donde usaremos el protocolo MQTT para transmitir datos desde el SP32 a Node-RED. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0034|Internet de las cosas|Hola a todos, continuando con la serie de videos del curso teórico del IoT, en este video vamos a hablar sobre la sintaxis JSON. Esta sintaxis va a estar muy presente en las programaciones de las soluciones IoT que hagamos. Además es muy usada entre los programadores de distintos lenguajes, así que esta explicación también es válida si quieres aplicarlo en otra área que no sea el internet de las cosas. Aparte de los protocolos de comunicación de los que ya hablamos, es importante conocer la sintaxis de transferencia de información. Actualmente la sintaxis que está más estandarizada en el IoT es la sintaxis JSON. Gracias a esto podemos aplicar un formato común para estructuras complejas. Entre sus ventajas encontramos que JSON es un formato de texto sencillo para intercambiar datos. También es ligero e independiente de cualquier lenguaje de programación. Además tiene forma de texto plano de simple lectura, escritura y generación. También ocupa menos espacio que el formato XML. XML es otro formato de texto que se utiliza para almacenar e intercambiar datos estructurados. Y además no es necesario que se construyan parsers personalizados. El parse se usa para evaluar si la sintaxis es correcta según el formato de la sintaxis JSON. Bien, en la siguiente imagen podemos ver un ejemplo de una sintaxis JSON. Como observamos, JSON se representa entre corchetes. Los corchetes indican que todo lo que está dentro es parte de un objeto. Y dentro de los objetos encontramos una clave con su respectivo valor. Si lo vemos de otra forma, la clave podría ser el nombre de una variable y su valor podría representar el contenido de esa variable. El tipo de dato de ese valor podría ser un string, un booleano, un carácter numérico, un array, etc. En JSON también se permite el anidamiento. Esto quiere decir que una clave puede contener como valor una sintaxis JSON. Y esos valores a su vez pueden contener otra sintaxis JSON. Bien, supongamos que el objeto se llame MSG y quiere obtener el valor de la clave type. Para hacer eso tenemos que colocar lo siguiente. Primero definimos una variable. Si estamos usando JavaScript, esa variable se define como var. La variable se llamará dato y esto será igual al nombre del objeto, en nuestro caso MSG, punto y el nombre de la clave, en nuestro caso type. De esa forma la variable dato contendrá el valor de la clave type. Ahora veamos otro ejemplo. Supongamos que quiero obtener el primer valor del array de la clave coordinates. Como podemos observar, coordinates está en una sintaxis JSON contenida dentro de otra sintaxis JSON. Entonces, para hacer esto hacemos lo siguiente. Declaramos una variable var llamada dato que será igual a MSG, punto, geometry, punto, coordinates. Abro corchetes y coloco el índice del valor del array, en nuestro caso el cero. De esa manera obtengo el primer valor del array de coordinates y lo guardo en la variable dato. Bien, si has visto los videos del curso de IoT con SP32 puedes notar que cuando usamos Node.RED todo se encuentra en formato JSON, desde los archivos que descargamos hasta los datos que intercambiamos y además esos datos tienen como nombre de objeto MSG. De hecho, para verificar esto en Node.RED podemos hacer lo siguiente. Le damos doble clic al dado de book, seleccionamos este combo box y le damos clic a esta opción. Ahora le damos clic a DOM, a deploy y clic en el cuadrado azul del nodo INJECT para mandar el mensaje. Como podemos notar, el mensaje es tipo JSON. Para ordenarlo mejor le damos clic al triángulo. Aquí observamos que el objeto se llama MSG. La primera clave es el tópico, la segunda el payload, la tercera el factor de calidad y así sucesivamente. Lo que hace el nodo de book es leer el payload o carga útil. Esto también lo podemos configurar. Para que el nodo de book lea, por ejemplo, el tópico hacemos lo siguiente. Como vemos, ahora se imprimió el nombre del tópico. Bien, siguiendo con la explicación, vamos a comparar un XML con un JSON. Como vemos, la sintaxis JSON es más compacta. Este puede ser parciado usando el método eval de JavaScript y además puede incluir a raíz. Por otra parte, XML utiliza etiquetas igual que HTML. También los nombres son más extensos y puede ser validado bajo un conjunto de reglas. Bien, los lenguajes que soportan JSON son JavaScript, C++, Java, PHP, Python, entre otros. En lenguaje JavaScript, para convertir un string a JSON, se debe colocar al final la función parseJSON. Como dijimos, el parse se usa para evaluar si la sintaxis es correcta según el formato de la sintaxis JSON y si lo es, lo convierte en un objeto. Dentro de Arduino hay una librería llamada ArduinoJSON que nos va a ser de mucha utilidad cuando programemos el SP32 para unirlo a una máquina virtual y aplicar soluciones de la Internet de las Cosas. El link de la documentación de esta librería te la dejaré en la descripción del video. Aquí se muestra el método de serialización y de serialización. Serializar sirve para generar información en un objeto JSON y de serializar es para poder recibir datos de un objeto JSON y extraer la información. Bien, si repasamos las características de la sintaxis JSON, podemos decir lo siguiente. Es independiente de un lenguaje específico, está basado en texto, es de formato ligero, fácil de parsear, no define funciones, no tiene estructuras invisibles, no tiene espacios de nombres, no tiene validator y si usamos HTTP, su MINDTYPE es Application JSON. MINDTYPE es la manera estándar de mandar contenido a través de la red. Bueno, eso sería todo por este video. Para que esta explicación teórica te quede más clara, puedes revisar el próximo video que vamos a sacar sobre Node-RED y SP32 comunicados por el protocolo MQTT usando la sintaxis JSON. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0035|Internet de las cosas|Hola a todos, siguiendo con el curso teórico del IoT, ahora nos toca hablar de todo lo que tenga que ver con fundamentos de redes. Fundamento de redes es un curso separado del IoT, pero están relacionados. Además, conocerlo nos va a ayudar a entender de mejor forma el Internet de las cosas. De todas maneras, puedes ver los videos de fundamentos de redes y aplicarlos en otras áreas, como el curso de redes de datos CCNA que hemos sacado en este canal. En ese curso, usamos el software Cisco Packet Tracer para hacer simulaciones de redes. Te dejaré la lista del curso como sugerencia. Bien, concretamente, en este video vamos a describir los componentes de una red. Entonces, primero, para entender que es una red, debemos saber que es una computadora. Una computadora es una máquina digital capaz de procesar grandes cantidades de datos. Su software comprende varios programas, entre ellos, su sistema operativo, y su hardware contiene elementos como un disco duro, una memoria RAM, una fuente de energía, una tarjeta madre, una tarjeta de red, entre otras cosas. La tarjeta de red es el elemento de la computadora que se usa para conectarla con otras computadoras, de tal forma que se crea una red. Entonces, una red de computadoras es un conjunto de dispositivos que se van a interconectar entre ellos para hacer que la comunicación pueda fluir. Las computadoras, por la descripción que acabamos de hacer, pueden ser una computadora personal, una laptop, una Raspberry, un celular, etc. Entonces, una red empieza en nuestra computadora conectada a la red y termina en la computadora al destino donde enviamos la información. Los componentes de una red son los dispositivos, los medios de transmisión y servicios. Bien, ahora vamos a pasar a describir cada componente de la red. Empezamos por los dispositivos. Los dispositivos, a su vez, se dividen en dos. Tenemos los dispositivos finales o hosts y tenemos los dispositivos intermedios o intermediarios. Si hablamos de los hosts, estos son el origen y destino de un mensaje. Los hosts son los únicos capaces de generar información. Además, se identifican por una dirección y existen de dos tipos, los clientes y servidores. Los clientes son los que necesitan o hacen el requerimiento de un tipo de servicio y necesitan de un sistema operativo, como pueden ser Windows 10, Ubuntu, Linux, etc. En el caso de los servidores, físicamente se parecen a un cliente. Estos tienen sus propios sistemas operativos, como pueden ser Windows Server, Ubuntu Server, etc. Pero a diferencia de los clientes, estos no requieren información, más bien la proveen. Debemos aclarar también que en algunos casos un cliente se puede comportar como un servidor para un servicio, como por ejemplo un servidor web. Aunque esto requiere de otras configuraciones para que esto se pueda dar. Entre los ejemplos de los dispositivos finales encontramos servidores, tablets, laptops, celulares inteligentes, cámaras de seguridad conectados a una computadora, tarjetas IoT como la SP32 o Raspberry, etc. Bien, en cuanto a los dispositivos intermediarios, estos tienen entre sus características que regeneran las señales de datos. Estos dispositivos reciben la señal de los hosts y si se encuentran débil lo regeneran o amplifican. Además, mantienen un registro sobre las rutas a través de la red, con el fin de saber por qué camino se va a enviar esta información una vez que llegue. También pueden elegir rutas alternativas. Los dispositivos intermedios se encargan de que la información sí o sí tenga que llegar a su destino. Si en caso una de las rutas no está disponible, estos dispositivos tienen que buscar otra ruta para mandar la información. Adicionalmente, administran parámetros de seguridad. Entre otras cosas, hacen como una función de guardia de seguridad, donde ven si la información tiene permiso a través de la red o no. Además, gestionan prioridades en el flujo de datos. Estos dispositivos clasifican por prioridad la información que reciben, siempre y cuando el sistema de prioridades esté configurado. Entre los dispositivos intermediarios podemos encontrar el router, el router inalámbrico, los switch LAN, los switch multicapa, etc. Bien, continuando con la descripción de los componentes, ahora vamos a hablar de los medios de transmisión. Entre los medios de transmisión más importantes encontramos el cobre, la fibra óptica y la tecnología inalámbrica. En el cobre, la información viaja por pulsos eléctricos, en la fibra óptica por pulsos de luz y la tecnología inalámbrica se propaga en el espacio. Los criterios para seleccionar un medio de transmisión son los siguientes. Primero, la distancia. Por ejemplo, si en una transmisión la distancia es más de 100 metros, el cobre ya no sería una opción. También se debe tomar en cuenta la velocidad de transmisión de datos. Si se quiere una gran velocidad, la mejor opción sería fibra óptica. Además, tenemos que considerar el entorno de instalación. Por ejemplo, si existe mucho ruido, el cobre no sería una buena opción, ya que el ambiente le causa interferencia. Otro factor sería el costo, aunque esto no debería ser un factor decisivo para elegir un medio de transmisión, ya que se tiene que elegir el medio que sea el más indicado según los requisitos del usuario. Los tres tipos de medios de transmisión que te presenté tienen sus ventajas y desventajas. Por ejemplo, el medio de transmisión más popular en una red es el cobre y el wifi, que están igual de disponibles para el usuario. El wifi sufre más interferencia que el cobre si el cobre está bien aislado, pero no necesita de un medio físico para propagarse. En cambio, la fibra óptica es más caro y no está muy extendida, pero la velocidad es mayor que en los otros dos. Bien, por último, siguiendo con la descripción de los componentes de la red, tenemos los servicios. Los servicios no son algo tangible, el usuario no los puede ver, más bien son un conjunto de actividades que realiza el dispositivo para satisfacer las necesidades del usuario. Entre los más comunes encontramos el de HCP. Nuestros dispositivos a la hora de conectarse a una red logran adquirir una IP de forma inmediata. Esto se da gracias al servicio del de HCP, que cubre la necesidad de conexión. También encontramos el DNS. Si por ejemplo, queremos entrar a YouTube, ponemos www.youtube.com. DNS hace que esa dirección se traduzca a una dirección IP. Con este servicio cubrimos la necesidad de traducción. También podemos encontrar entre otras cosas servicios de correo electrónico y servicios de acceso remoto. Bueno, eso sería todo por este video. En los próximos videos continuaremos explicando otras cosas sobre el fundamento de redes. Además, muchos de estos términos los iremos viendo en el curso de Internet de las Cosas, tanto en el curso del IoT como en el curso práctico del IoT con SP32. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0036|Internet de las cosas|Hola a todos, siguiendo con la guía de fundamentos de redes, en un video pasado ya detallamos que era una red. Ahora, en este video vamos a describir las topologías de red, tipos de redes y ya sabiendo estos conceptos y los conceptos del video pasado responderemos a la pregunta ¿Qué es internet? Bien, empezamos hablando de las topologías de red. Las topologías de red pueden ser de dos tipos, diagrama de topología física y diagrama de topología lógica. En un diagrama de topología física nos concentramos en mostrar la ubicación física en los dispositivos conectados a la red y el medio de transmisión que se esté usando. En la imagen te presento un ejemplo de diagrama de topología física. Podemos ver que se muestran los dispositivos y su interconexión. Cuando vemos líneas continuas, asumiremos que se trata de un cable UTP o coaxial. En la topología física, no se muestra ninguna dirección ni número de puerto. En lugar de eso, aparecen los nombres de los dispositivos y el área donde están instalados. Bien, el otro tipo de diagrama es el de topología lógica. En este diagrama mostramos las conexiones virtuales o lógicas entre los dispositivos. Por ejemplo, qué tipo de direcciones está utilizando, qué sistema operativo se está usando o qué servicios está proveyendo el dispositivo. Podemos decir que la topología lógica identifica dispositivos, puertos y esquemas de direccionamiento. En la imagen que muestro, vemos un ejemplo de diagrama de topología lógica. Aquí sí podemos ver la dirección de cada dispositivo y su puerto. Pueden notar en la imagen que existen conexiones simbolizadas con líneas rectas y negras y también conexiones simbolizadas con líneas rojas y en forma de rayo. Esto se hace para representar dos tipos de redes. Las líneas negras y rectas simbolizan las conexiones en una red LAN y las líneas rojas en forma de rayo simbolizan las conexiones en una red WAN. Entonces, en la red LAN, también llamada local area network, tenemos un dispositivo intermediario que conecta dispositivos finales, es decir, solo se muestra una red local. La característica común de esta red es que está delimitada a una pequeña área. Puede ser una casa, un campus universitario o un colegio. No puede cubrir cientos de kilómetros. Otra característica es que tiene un ancho de banda elevado en el rango de gigas. Bien, por otro lado tenemos la red WAN. Las redes WAN o white area network cubren cientos de kilómetros. Esto es lo que lo distingue de una red LAN y su labor principal es conectar redes LAN a través de los dispositivos intermediarios de cada red LAN. Para este tipo de redes, la conexión por cobre queda descartada, ya que las distancias son muy grandes. Por eso, se usa tecnología inalámbrica o fibra óptica. Bien, otros tipos de redes son las siguientes. Por un lado, tenemos las redes MAN, también llamadas redes metropolitanas. Estas son más pequeñas que las redes WAN, pero también interconectan redes LAN. Por ejemplo, una red MAN podría conectar dos campus universitarios de dos universidades que estén en la misma ciudad. También existe la red WLAN. Esta es una LAN wireless, es decir, que los dispositivos están conectados de forma inalámbrica. Además, tenemos las redes SAN. Estas son infraestructuras que están diseñadas para el almacenamiento de información. Bien, ya con todos estos conceptos de los que hemos hablado, estamos en condiciones de entender qué es Internet. En un video sobre el NetWord Server ya lo habíamos descrito, pero ahora profundizaremos en su concepto. Internet es un conjunto muy grande de dispositivos intermediarios y finales que están interconectados con la finalidad de intercambiar mensajes. También se le conoce como la red de redes. Esta red está descentralizada, es decir, no tiene un dueño. Si bien existen proveedores que se hacen cargo de la administración, si uno de ellos deja de funcionar, se los puede reemplazar por otros. En esta imagen se pueden ver las conexiones WAN del Internet en el mundo. Las líneas de colores que recorren el planeta a través de los océanos, uniendo países, representan los enlaces de fibra óptica. Obviamente, dentro de cada país hay otro conjunto de enlaces. Como pueden ver, todo está conectado. Es por eso que el Internet lo llamamos red de redes. Bueno, eso sería todo por este video. En los próximos videos continuaremos explicando otras cosas sobre el fundamento de redes. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias. ¡Suscríbete al canal!
DOC0037|Internet de las cosas|Hola a todos, siguiendo con la guía de fundamentos de redes, en este video vamos a hablar sobre los modelos TCP IP y OSI. Estos modelos engloban un conjunto de protocolos necesarios para que exista la comunicación entre redes. Bien, para empezar ¿Qué es un protocolo? Un protocolo es una serie de reglas y normas que se deben cumplir en un determinado momento para que la comunicación pueda lograrse. Por lo general estas reglas se crean bajo acuerdos o convenios y una vez instaurados son de cumplimiento obligatorio. Un ejemplo en la vida cotidiana sería el siguiente, cuando dos personas quieren entablar un diálogo ambas personas tienen que hablar el mismo idioma y además una persona tiene que hablar después de la otra. Entonces se puede decir que para iniciar una comunicación se tiene que seguir un protocolo. Bien, ya sabiendo el significado de protocolo ahora pasamos a hablar de las características de los protocolos de red. Los protocolos de red definen el formato y estructuración del mensaje. Cuando hablamos del mensaje nos referimos a la información en general. Puede ser, por ejemplo, un mensaje de messenger o un correo electrónico. También los protocolos definen los procesos que los dispositivos intermediarios tienen que realizar para que la información llegue desde el origen al destino. Además, si ocurriese un error en el camino del mensaje desde el origen al destino, entonces los protocolos de red pueden generar mensajes de error. Y también los protocolos de red nos van a describir los procesos para la configuración y terminación de sesiones de transferencia de datos. Bien, para estructurar de mejor manera los protocolos, estos pueden ser organizados en capas. Vamos a volver a nuestro ejemplo de las dos personas dialogando para hacer una analogía. Supongamos que esas personas son de Hispana América. Para que esta comunicación se desarrolle, vamos a organizarla en capas. La capa física define cuáles son las reglas para que a nivel físico la voz fluya. Entonces, nuestra señal es la voz y el medio de transmisión donde se propaga es el aire. Ahora, en la capa de reglas definimos un idioma en común para que se puedan comunicar. Como son de Hispana América, el idioma será el español. Y para que la comunicación se desarrolle, cada persona tiene que hablar por turnos. Por último, también definimos una capa de contenido. Esta capa está enfocada en la información que se quiere transmitir. En este caso, el mensaje sería Buenos días. Entonces, podemos ver que las condiciones para que se ve la comunicación están bien organizadas en cada capa. El mensaje no hubiera llegado al destino si dichas condiciones no se hubiesen cumplido. Bien, ahora vamos a trasladar ese concepto de capas a redes. En las redes, tenemos dos tipos de modelos que engloban un conjunto de protocolos de capas. Uno de ellos es el modelo OSI y el otro es el modelo TCPIP, siendo el modelo TCPIP el que se implementa en todo dispositivo de red. En cambio, el OSI es un modelo referencial. Se lo sigue usando porque contempla mucho más capas. Y gracias a eso, podemos hacer un estudio del funcionamiento de las redes mucho más detallado. La funcionalidad viene a ser algo similar, porque en el modelo TCPIP la capa de aplicación hace todo lo que hace la capa 5, 6 y 7 del modelo OSI. La capa de transporte es la misma. La capa de internet tiene las mismas funciones que la capa de red. Y la capa acceso de red cumple las funciones de la capa física y enlace de datos. Bien, para diferenciar cuando nos referimos a un modelo u otro, lo que se hace comúnmente es nombrar por su nombre a las capas de los modelos TCPIP y en el modelo OSI utilizamos su número. Si por ejemplo decimos capa 1, nos referimos a la capa física del modelo OSI. Pero si hablamos de la capa aplicación, estamos hablando de la capa aplicación del modelo TCPIP. Entonces, ahora vamos a especificar cada capa del modelo OSI. En la capa 1 y 2 vamos a ver las maneras físicas de mandar los datos a la red. La capa 1 de hecho es más hardware. Esta capa se encarga de las características eléctricas, mecánicas, funcionales y de procedimiento que se requieren para mover los bits de datos. La capa 2 lo que hace es el direccionamiento físico. Es decir, utilizamos las direcciones MAC para identificar a los dispositivos. La capa 3 se va a dedicar a la transmisión de mensajes a través de la red, lo que comúnmente se llama el rutamiento. Es decir, sacar los mensajes de nuestra red a redes remotas buscando el mejor camino. Y para eso también necesitamos que esta capa se dedique al direccionamiento de IPs. El direccionamiento de IPs hace que cada dispositivo tenga una dirección IP para comunicarse con la red. En la capa 4 hay una conectividad de extremo a extremo. Aquí nos aseguramos que la información sea entregada de forma ordenada y confiable. Esta capa es muy importante a nivel de dispositivo final. Porque es en el dispositivo final que la capa de transporte va a definir cuál es el proceso que se necesita hacer para que el destino reciba la información. La capa 5, 6 y 7 son de mucho interés para personas que desarrollan aplicaciones, pero no para un administrador de red. Podemos decir que la capa 5 identifica cada proceso o aplicación que se esté corriendo en el dispositivo final. Gracias a esta capa, los procesos no se van a intermezclar. La capa 6 le va a dar un formato a la información o mensaje que la capa de aplicación le va a pasar. Por ejemplo, si es un video, le puede dar el formato MP4. Y en la capa 7 tenemos las aplicaciones. Esta capa cumple la función de ser la interfaz para que los usuarios puedan acceder a la red a través de aplicaciones y servicios que corren de forma automática. Por ejemplo, entre las aplicaciones, encontramos el navegador. Gracias a esto, podemos acceder a internet. Y entre los servicios, encontramos el DHCP. Gracias a esto, se le asigna un IP a los clientes de la red. Ahora, si nos fijamos en la columna de Data Unit o como los nombra Osi, DDU, podemos ver el nombre que reciben cada uno de los mensajes en cada una de las capas. El nombre va cambiando porque cada vez que el mensaje pasa por una capa, va adquiriendo más información. Y tiene que cambiar de nombre para poder identificarlo. Por ejemplo, si hablamos de paquetes, sabemos que estamos en la capa 3. Bien, ¿y qué protocolos existen en cada capa? Para responder esta pregunta, podemos ver la siguiente imagen que engloba los conjuntos de protocolos TCP y P. En la primera columna entre paréntesis, podemos ver el número de capa Osi a la que corresponde cada capa TCP y P. En la columna del medio, vemos los distintos protocolos para las distintas capas. Además, en la parte derecha, aparecen los dispositivos clasificados, según a la capa a la que pertenecen. Todo esto lo vamos a detallar más en los siguientes videos. Bien, ¿cuáles son los beneficios del uso de un modelo en capas de protocolos? Primero, existe menos complejidad, ya que las capas nos permiten organizar mejor el modo en el que se maneja la información. Además, utilizamos interfaces estándar, es decir, para cada capa, podemos definir estándares que sean abiertos y protocolos que no dependan de un fabricante. Por ejemplo, IP corre en cualquier dispositivo final o intermediario. También es fácil de aprender. Como lo estamos dividiendo por capas, nos va a ser más fácil entenderlo. Otro beneficio es que es fácil de desarrollar. Es más sencillo desarrollar protocolos que se ocupen solo de la función de una capa. Además, existe interoperabilidad entre distintos proveedores. Como no son pilas de protocolos propietarias, es decir, todos son estándares, podemos usar distintos proveedores para distintas capas. Y también, otro beneficio es la modularidad. Cada capa es independiente y no le afecta lo de la otra capa. Bueno, eso sería todo por este video. En los próximos videos continuaremos explicando otras cosas sobre el fundamento de redes. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0038|Internet de las cosas|Hola a todos, siguiendo con la guía de fundamentos de redes, en un video pasado ya habíamos hablado del protocolo OSI y TCP y P. Ahora en este video vamos a hablar del tema del encapsulamiento y desencapsulamiento de datos para que estos lleguen desde el origen hasta el destino. Entonces, ¿cómo fluye la información desde el emisor hasta el receptor? La información fluye de la siguiente manera. Para el receptor, la información pasa de la capa de aplicación hasta la capa física. A este proceso se le llama encapsulamiento de datos. Y para el receptor, la información va desde la capa física hasta la capa de aplicación. A este proceso se le llama desencapsulamiento de datos. Bien, ahora vamos a detallar qué es lo que ocurre con la data en el proceso de encapsulamiento. Por ejemplo, el usuario A envía un correo electrónico al usuario B. Entonces, en las capas superiores o capa de aplicación se le dará un formato, se identifica el proceso y lo preparamos para pasarlo a la capa de transporte, poniéndole un encabezado que se le añade para que el mensaje llegue a su destino. El mensaje junto a esa cabecera lo llamaremos datos. Después pasamos a la capa de transporte, que recibe lo que vendría a hacer los datos. A esos datos, dependiendo si estamos usando el protocolo TCP o MUDEP, le vamos a agregar un encabezado adicional. Entonces, ahora todo el mensaje se pasaría a llamar segmento. Después pasamos a la capa de red o internet para el modelo TCP y P. Esta capa le va a agregar su propio encabezado, que es definido por el protocolo IP. A todo ese conjunto se le llama paquete. Cabe resaltar que ninguna capa va a poder modificar los bits de información que le llegan. Simplemente lo que hacen es añadirle su encabezado. Bien, después de la capa de red, el paquete pasa por la capa 2, donde se le agrega su propio encabezado y todo se pasa a llamar trama. Adicionalmente, se le añade en la parte final un verificador de trama. Este verificador tiene como función asegurarse que la trama llegue al destino de manera íntegra y no haya sufrido alguna modificación. Cuando se haya colocado todo eso, finalmente la trama se envía a la capa 1. Aquí la información está a punto de salir. En esta capa verificamos el medio de transmisión que estemos usando, para que en función de eso vaya la señalización y se definan los bits 1 y 0 que se van a enviar. Bien, ahora vamos a detallar el proceso de desencapsulamiento. Primero se reciben los bits en la capa 1 y se arma todo la trama. En la capa 2 verificamos la secuencia de trama. Si la verificación es válida, se lee el encabezado de la capa 2. Después de procesarlo, subimos a la capa de red. En la capa 3 también se lee el encabezado. Se verifica que el encabezado cumpla con las reglas especificadas y si todo está correcto, pasamos con la siguiente capa que sería la capa de transporte. Aquí verificamos su respectivo encabezado. Y después se pasa a la capa de aplicación, donde finalmente el usuario B puede ver el mensaje enviado por el usuario A. Parece un proceso muy largo, pero realmente es un proceso que toma segundos desde que la información sale del emisor al receptor. Bien, ¿y qué contiene cada uno de los encabezados de cada capa? Primero, en las capas superiores, cuando un usuario envía un mensaje, sus caracteres alfanuméricos se convierten en datos que van a ser codificados. Esta capa no nos va a hacer de mucho interés para el curso, por eso pasamos al siguiente. En la capa de transporte, lo que se envía en el encabezado son los números de proceso de destino y origen, también conocido como puertos. Por ejemplo, un servidor HTTP escucha en un puerto 80 y nosotros podemos tener un puerto 20. Entonces, el puerto destino sería 80 y el puerto origen sería 20. Bien, en el caso de la capa de red, en el encabezado va las direcciones de red de destino y origen, es decir, la IP del dispositivo final que estamos usando y la IP del dispositivo final de destino. Después, en la capa de enlace de datos, en el encabezado van las direcciones físicas de destino y origen, es decir, las direcciones MAC. Estas direcciones se llaman direcciones físicas porque vienen grabadas en las tarjetas. Esto es similar al número de email de los celulares. Si en caso el dispositivo destino no está en mi redlan, ponemos de destino la dirección MAC del Gateway. Después de esto, pasamos a la capa física. En esta capa ponemos bits de temporización y sincronización, para que la información se procese correctamente en el destino. Bien, como conclusión, podemos decir que, toda la información que se ponen estos encabezados es necesaria para que la comunicación sea exitosa, ya que si falta alguno de estos, la comunicación entre emisor y receptor no se podría dar. Bueno, eso sería todo por este video. Cabe resaltar que aquí te he presentado la forma más básica de entender el encapsulamiento y desencapsulamiento. La explicación más minuciosa de este tema, te lo presentamos en uno de los videos del curso de redes de datos CCNA. Te dejaré ese video como sugerencia. Próximamente continuaremos explicando otras cosas sobre el fundamento de redes. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias. Subtítulos realizados por la comunidad de Amara.org
DOC0039|Internet de las cosas|Hola a todos, siguiendo con la guía de fundamentos de redes, en un video pasado ya habíamos hablado del protocolo OSI y TCP y P. Ahora en este video vamos a hablar del tema del encapsulamiento y desencapsulamiento de datos para que estos lleguen desde el origen hasta el destino. Entonces, ¿cómo fluye la información desde el emisor hasta el receptor? La información fluye de la siguiente manera. Para el receptor, la información pasa de la capa de aplicación hasta la capa física. A este proceso se le llama encapsulamiento de datos. Y para el receptor, la información va desde la capa física hasta la capa de aplicación. A este proceso se le llama desencapsulamiento de datos. Bien, ahora vamos a detallar qué es lo que ocurre con la data en el proceso de encapsulamiento. Por ejemplo, el usuario A envía un correo electrónico al usuario B. Entonces, en las capas superiores o capa de aplicación se le dará un formato, se identifica el proceso y lo preparamos para pasarlo a la capa de transporte, poniéndole un encabezado que se le añade para que el mensaje llegue a su destino. El mensaje junto a esa cabecera lo llamaremos datos. Después pasamos a la capa de transporte, que recibe lo que vendría a hacer los datos. A esos datos, dependiendo si estamos usando el protocolo TCP o MUDEP, le vamos a agregar un encabezado adicional. Entonces, ahora todo el mensaje se pasaría a llamar segmento. Después pasamos a la capa de red o internet para el modelo TCP y P. Esta capa le va a agregar su propio encabezado, que es definido por el protocolo IP. A todo ese conjunto se le llama paquete. Cabe resaltar que ninguna capa va a poder modificar los bits de información que le llegan. Simplemente lo que hacen es añadirle su encabezado. Bien, después de la capa de red, el paquete pasa por la capa 2, donde se le agrega su propio encabezado y todo se pasa a llamar trama. Adicionalmente, se le añade en la parte final un verificador de trama. Este verificador tiene como función asegurarse que la trama llegue al destino de manera íntegra y no haya sufrido alguna modificación. Cuando se haya colocado todo eso, finalmente la trama se envía a la capa 1. Aquí la información está a punto de salir. En esta capa verificamos el medio de transmisión que estemos usando, para que en función de eso vaya la señalización y se definan los bits 1 y 0 que se van a enviar. Bien, ahora vamos a detallar el proceso de desencapsulamiento. Primero se reciben los bits en la capa 1 y se arma todo la trama. En la capa 2 verificamos la secuencia de trama. Si la verificación es válida, se lee el encabezado de la capa 2. Después de procesarlo, subimos a la capa de red. En la capa 3 también se lee el encabezado. Se verifica que el encabezado cumpla con las reglas especificadas y si todo está correcto, pasamos con la siguiente capa que sería la capa de transporte. Aquí verificamos su respectivo encabezado. Y después se pasa a la capa de aplicación, donde finalmente el usuario B puede ver el mensaje enviado por el usuario A. Parece un proceso muy largo, pero realmente es un proceso que toma segundos desde que la información sale del emisor al receptor. Bien, ¿y qué contiene cada uno de los encabezados de cada capa? Primero, en las capas superiores, cuando un usuario envía un mensaje, sus caracteres alfanuméricos se convierten en datos que van a ser codificados. Esta capa no nos va a hacer de mucho interés para el curso, por eso pasamos al siguiente. En la capa de transporte, lo que se envía en el encabezado son los números de proceso de destino y origen, también conocido como puertos. Por ejemplo, un servidor HTTP escucha en un puerto 80 y nosotros podemos tener un puerto 20. Entonces, el puerto destino sería 80 y el puerto origen sería 20. Bien, en el caso de la capa de red, en el encabezado va las direcciones de red de destino y origen, es decir, la IP del dispositivo final que estamos usando y la IP del dispositivo final de destino. Después, en la capa de enlace de datos, en el encabezado van las direcciones físicas de destino y origen, es decir, las direcciones MAC. Estas direcciones se llaman direcciones físicas porque vienen grabadas en las tarjetas. Esto es similar al número de email de los celulares. Si en caso el dispositivo destino no está en mi redlan, ponemos de destino la dirección MAC del Gateway. Después de esto, pasamos a la capa física. En esta capa ponemos bits de temporización y sincronización, para que la información se procese correctamente en el destino. Bien, como conclusión, podemos decir que, toda la información que se ponen estos encabezados es necesaria para que la comunicación sea exitosa, ya que si falta alguno de estos, la comunicación entre emisor y receptor no se podría dar. Bueno, eso sería todo por este video. Cabe resaltar que aquí te he presentado la forma más básica de entender el encapsulamiento y desencapsulamiento. La explicación más minuciosa de este tema, te lo presentamos en uno de los videos del curso de redes de datos CCNA. Te dejaré ese video como sugerencia. Próximamente continuaremos explicando otras cosas sobre el fundamento de redes. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias. Subtítulos realizados por la comunidad de Amara.org
DOC0040|Internet de las cosas|Hola a todos, siguiendo con la guía de fundamentos de redes, en un video pasado ya habíamos hablado de la capa física del modelo OSI. Ahora nos toca describir los tres medios de transmisión que existen. Entonces vamos a empezar por el cobre. El cobre es el medio de transmisión que más se ha implementado, ya que entre sus ventajas se encuentra el bajo costo en comparación a los otros medios de transmisión y además tiene baja resistencia a la corriente eléctrica. Bien, entre sus desventajas encontramos que está limitado por la distancia. Con este medio de transmisión no podemos cubrir distancias muy grandes, en muchos casos no pasamos de los 100 metros. Además es sensible a la interferencia electromagnética o de radioflicuencia y también al crustal. La interferencia electromagnética se puede dar debido a motores o luces fluorescentes que estén cerca al cableado, así que la solución para estos casos es poner el cableado lo más lejos posible de estas fuentes de interferencia. El crustal se da específicamente en los cables UTP. Como estos cables están juntos generan interferencia entre ellos mismos. Vamos a ver que también hay métodos para moderar este tipo de interferencia. Entre los tipos de cables que existen tenemos el par trenzado no blindado UTP, el par trenzado blindado STP y el coaxial. De los tres tipos el coaxial es el menos usado. Si comparamos el cable UTP y STP son muy parecidos, con la excepción que el cable STP aísla un par de cables entrelazados, esto le permite aminorar la interferencia electromagnética. Y en el caso del cable coaxial tiene sus propias mallas que si son conectadas a tierra anulamos gran parte de la interferencia. Bien, ahora vamos a hablar del cable UTP. Estos cables como pueden ver están trenzados entre pares. Esto se hace justamente para anular el crostal entre los dos hilos. De esa manera lo que un hilo pueda producir de interferencia es anulado por el otro hilo con el que se ha trenzado. Como podemos ver hay cuatro pares. Si nos fijamos bien vemos que varía el número de vueltas entre cada par. Esto se hace para eliminar el crostal que puede generar un par de cables trenzados a otro par de cables trenzados. En estos cables vamos a cumplir con el estándar DIA-EIA568 que es el estándar más ampliamente utilizado para cuatro pares trenzados UTP. Este estándar nos estipula qué tipos de cables vamos a utilizar, las longitudes de dichos cables, los conectores que vamos a usar, cómo van a ser las terminaciones de dichos cables y los métodos que se tienen que realizar para hacer pruebas en los cables. En estas pruebas se busca que la transmisión a través del cable UTP esté bajo cierto margen de interferencia aceptable que asegure que la transferencia de datos va a ser exitosa. Bien, dentro de la clasificación de los cables vamos a dividirlos por categorías según la capacidad que tengan para transportar datos. A partir de esto encontramos el tipo de servicio que puede proveer. Cabe resaltar que se sigue trabajando para que los cables tengan una mayor velocidad de transferencia de datos, así que en un futuro pueden existir cables de categorías 7 u 8. Los tipos de conectores más comunes, si hablamos de las categorías 5 y 6, son los RJ45. Estos cuentan con un recubrimiento de aluminio que protege aún más el medio de transmisión. Los RJ45 pueden ser macho o hembra. La recomendación del estándar 568 es que la cobertura exterior de plástico ingrese aunque sea medio centímetro al interior del conector para evitar la interferencia entre los hilos. Esto es cumplido por los RJ45. Dentro de este estándar tenemos dos variantes, la A y la B. El que comúnmente se utiliza es el B. La razón de esto es que algunos fabricantes hacen que el cable color marrón sea muy parecido al cable color anaranjado. Como en la opción A ambos cables están juntos, podríamos incurrir en algún error. En función a este estándar hablamos de cables cruzados y directos. Los cables cruzados son cables que tienen terminales con variantes opuestas, es decir, en un terminal un conector variante A y en el otro terminal un conector variante B. Los cables cruzados sirven para conectar dispositivos de la misma capa, por ejemplo un router con otro router. Aunque este cable también se usa para conectar un router con una PC, ya que en cuanto a hardware son muy parecidos. En cuanto a los cables directos estos tienen terminales con conectores de la misma variante, por ejemplo en un terminal tiene un conector variante B y en el otro también tiene un conector variante B. Los cables directos sirven para conectar dispositivos de distintas capas. Otro medio de transmisión es la fibra óptica. Con este medio podemos cubrir grandes distancias. Tenemos anchos de bandas mayores con respecto al cobre y la atenuación es reducida, ya que en el caso de la fibra óptica no tenemos sensibilidad a fuentes de ruido electromagnético o CROSTALC. Entre sus desventajas encontramos que es más costoso. Además existe la dificultad en la implementación. Es más difícil construir un cable de fibra óptica que un cable UTP. Las herramientas con las que se crea la fibra óptica son más difíciles de conseguir. La implementación más común son los enlaces troncales, también el tipo de enlace conocido como Fibertude Home, que son normalmente implementados por los proveedores de servicio, cuyos cables van desde la central hasta el domicilio del cliente. Además encontramos las redes de largo alcance y redes por cable submarino. Las redes por cable submarino son las que vimos en el video sobre topologías de red, tipos de red e internet. Bien, si vemos esta imagen observamos los distintos componentes de la fibra óptica. En el centro tenemos un núcleo que puede ser de vidrio o plástico. Alrededor del núcleo tenemos el revestimiento que aísla la fuente de luz que se está mandando a través del núcleo. También tenemos un protector adicional para evitar que ingrese luz al núcleo, además de la chaqueta externa y el aislante. Existen dos tipos de fibra óptica, la fibra monomodo y multimodo. En el caso de la fibra monomodo el núcleo es más pequeño, por tanto tiene menos pérdida y utiliza un láser como fuente de luz. En la imagen vemos como las de luz va de forma recta. Por otro lado en el caso de las fibra multimodo el núcleo es más amplio, lo que ocasiona mayor dispersión y pérdida de señal. Además utiliza como emisor de luz un LED. Con las multimodo también cubrimos distancias largas pero inferiores a las que cubriríamos con fibra monomodo. En la imagen vemos como la luz se emite por todo el núcleo. Podemos decir que existe una dispersión de la luz en comparación a las monomodo. Bien, ahora pasamos a describir los medios inalámbricos. Entre sus ventajas encontramos la movilidad, es decir, podemos movernos a través de un área de cobertura con mucha facilidad sin desconectarnos de la red. Y otra ventaja es la gran cantidad de dispositivos conectados. Por ejemplo, en una conexión usando cobre estamos limitados por la cantidad de puertos de conexión que existen. Pero si usamos el medio inalámbrico podemos conectar más dispositivos. Para acceder a una red inalámbrica necesitamos una tarjeta de red inalámbrica y un punto de acceso inalámbrico. Entre sus desventajas encontramos las interferencias. Como es un medio que utiliza el espacio, es decir, es un medio abierto, tenemos interferencias de medios que estén corriendo en la misma frecuencia. Además, también está el tema de la seguridad. Por lo mismo que es un medio abierto, otra persona podría interceptar el mensaje. Y otra cosa por tomar en cuenta es que es un medio compartido, al que cualquier dispositivo podría acceder. Entre sus implementaciones encontramos el wifi que está definido por el estándar 802.11. También encontramos el bluetooth definido por el estándar 802.15 y finalmente el Wimax que está regido por el estándar 802.16. Para el internet de las cosas los medios inalámbricos son los que comúnmente se utilizan. A este grupo también pertenecen las redes LP1. Te dejaré ese video como recomendación por si te interesa. Bien, entonces, cuando hablamos de medios de transmisión y capa física, tenemos que tener presente dos términos, ancho de banda y rendimiento. En los medios físicos pueden haber diferentes velocidades de transferencia de datos. Las velocidades no van a ser iguales de forma inalámbrica en el cobre o por fibra óptica. Justamente en esos términos es que hablamos de ancho de banda. El ancho de banda va a medir la cantidad de datos que fluyen desde un origen hacia un destino en un periodo de tiempo determinado. La medida fundamental que usamos es el bit por segundo ó BPS. Entonces, un kilovit por segundo sería mil bits por segundo, un megavit por segundo sería un millón de bits por segundo y así sucesivamente. Los factores que determinan el ancho de banda de una red son propiedades de los medios de transmisión y los estándares de señalización y detección de señales de red. Bien, en el caso de rendimiento es la medida de transferencia de bits a través de un medio de transmisión durante un periodo de tiempo determinado. Es similar al ancho de banda, pero con la diferencia de que en el caso de rendimiento vamos a tomar en cuenta la cantidad de tráfico que se está enviando, la latencia o demora que tenemos a lo largo del camino desde el origen al destino y el tipo de tráfico que se está enviando. Si tomamos en cuenta esos tres puntos que afectan la velocidad de transmisión, vamos a tener un ancho de banda menor. Bueno, eso sería todo por este video. En los próximos videos continuaremos con la guía de fundamentos de redes. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias. ¡Suscríbete al canal!
DOC0041|Internet de las cosas|Hola a todos, continuando con la serie de videos del curso teórico del IOT, en este video vamos a hablar sobre los sistemas de geolocalización y su relación con el IOT. Bien, empezamos hablando sobre el GPS. El GPS, también llamado por sus siglas sistema de posicionamiento global, es un sistema creado por las fuerzas armadas de los Estados Unidos. Este sistema te da la posibilidad que cualquier punto de la tierra sea geolocalizable mediante una red de satélites. Hoy en día este sistema está disponible para uso civil. Los smartphones, por ejemplo, poseen GPS. Como dijimos, el GPS es un sistema de localización mundial y está formada por una constelación de 24 satélites. Estos satélites cuentan con las siguientes características. Se encuentran a unos 20 kilómetros de la tierra. Están situados en seis planos orbitales inclinados 55 grados. De esta forma siempre existen al menos cuatro satélites desde cualquier punto del planeta que pueden realizar la geolocalización. Los satélites recorren la tierra dos veces cada 24 horas. Su vida útil es de 10 años. Básicamente lo que se hace en un sistema GPS es calcular la distancia y tiempo desde los satélites al dispositivo GPS y a partir de eso se deduce la ubicación en la que se encuentra el dispositivo. Cabe resaltar que los dispositivos GPS no envían información a los satélites. Simplemente son receptores pasivos con un oscilador interno que calcula el tiempo para saber las coordenadas en donde se encuentra. La precisión de un dispositivo GPS va a depender bastante de su oscilador interno. El promedio de rango de precisión de un GPS puede variar desde 5 metros hasta 20 metros. Aunque los más avanzados tienen errores de un metro. Lógicamente mientras más precisos sean son más costosos. Las frecuencias en las que trabaja el GPS son dos. Una banda es la de uso civil llamada L1 que está por encima de los 1.5 GHz y la otra banda es la que utiliza las fuerzas armadas de los Estados Unidos llamada L2 que está por encima de los 1.2 GHz. Ahora los dispositivos GPS trabajan con un formato de comunicación estándar. Este protocolo es llamado NMEA 0183. A través de este protocolo los instrumentos marítimos y también la mayoría de dispositivos GPS pueden comunicarse los unos con los otros. En el caso de los GPS trabajan con transmisión serial. Su sintaxis lo puedes ver en la imagen. Primero empieza como el símbolo del dólar y después la trama. En la trama se pone la dirección, orientación, velocidad, coordenadas, latitud, entre otros. Así como Estados Unidos tiene el sistema GPS, Rusia tiene el sistema GLONASS proporcionado por el GNSS ruso. Esta consta de 31 satélites, 24 están activos, 3 son de repuesto, 12 en mantenimiento, uno en servicio y otro en pruebas. Y este sistema también está liberado al público civil. Además existe el GPS chino llamado Beidou y el GPS europeo llamado Galileo. Sus características y funciones son similares a las de GPS. Existen dispositivos que pueden trabajar con estos diferentes sistemas, aunque en la mayoría tienen un módulo GPS y pueden prescindir de otros sistemas de geolocalización. De hecho aquí te presento algunos de los módulos más populares para trabajar con GPS. En el curso del IoT con SP32 vamos a estar sacando un video tutorial sobre la utilización del NEO 6M para hacer proyectos de geolocalización. Bien, en arduino existe la librería TinyGPS que sirve para decodificar la trama NMEA y nos la entrega en formato de coordenadas. Esto lo vamos a poner a prueba cuando conectemos el NEO 6M con la SP32. Además la empresa Ublox que es el fabricante del NEO 6M trae un software llamado U-Center para manipular este dispositivo. En nuestro curso del IoT con SP32 también manipularemos este software. Bueno, eso sería todo por este video. En los próximos videos continuaremos con el curso teórico del IoT. No olvides dejar un like y suscribirte si este video te fue de ayuda. Gracias.
DOC0042|NLP e IA|Este vídeo está patrocinado por la Universitat Politécnica de València. Veamos. Concentración. Si tuvieras que enfrentarte a cómo resolver este problema, ¿cómo lo harías? No digo de saber cómo usar redes neuronales o alguna técnica que conozcas ya, sino realmente, ¿a qué tipo de solución intuitiva hubieras llegado tú si a mediados del siglo XX hubieras tenido que enfrentarte al problema de que una máquina entendiera el contenido de una frase expresada de manera natural? Por ejemplo, para saber si el sentimiento de esta frase es positivo o negativo. En mi caso, mi primer acercamiento a este problema pues sería el siguiente y es que básicamente el lenguaje que estoy utilizando para comunicarme pues está aplicando una serie de reglas gramaticales que podría ser interesante tener en cuenta y codificarlas, programarlas. Y así, poquito a poco y con mucha paciencia, iremos construyendo todo nuestro lenguaje codificándolo en pues cubriendo todas las combinaciones, todos los matices, todas las enviguedades, todos los... Intentar traducir a reglas formales todas nuestra forma de comunicación es una tarea no sólo increíblemente difícil, sino que además deberá de ser actualizada, puesto que el lenguaje está en continua evolución, significando esto que el humano encargado de codificar todas estas reglas pues va a tener que dedicarle bastante tiempo. Pero en este punto ya hemos estado, ¿no? Al final, esto es similar al por qué no tenemos un algoritmo que instrucción a instrucción te vaya describiendo formalmente cuáles son los pasos para conducir correctamente un coche o clasificar un gran dataset de imágenes con gran precisión. A ver, ¿no contaremos ya con algún tipo de tecnología que nos pueda automatizar todo este proceso y que pueda aprender automáticamente todas estas reglas? Dice mi nombre. Machine Learning. ¡Tú eres bien! Y este es el primer punto a entender. Y es que históricamente el campo del Natural Language Processing se ha desarrollado entre dos mundos, en el que por un lado tenemos las técnicas que se apoyan más en la codificación manual de todas aquellas reglas gramaticales que son interesantes a utilizar para la resolución del problema que tengamos. Y por el otro lado contamos con técnicas de Machine Learning que directamente se basan en el análisis de grandes corpus de texto para dejar que todas estas reglas sean aprendidas. Y en medio de estos dos polos lo que nos encontramos pues es un abanico de muchísimas técnicas y algoritmos que se han ido desarrollando a lo largo de las últimas décadas, pero que bueno, como es de esperar, pues en los últimos años ha ido ganando más importancia aquellas más cercanas al campo del Machine Learning. Y son en esta donde vamos a poner nuestro foco. Y es que por suerte en los últimos años hemos ido perfeccionando unos potentes algoritmos, las redes neuronales, que son capaces de tomar datos de entrada, como por ejemplo texto, y mostrándole el resultado que queremos conseguir supervisando su aprendizaje, pues podemos dejar que sea el propio algoritmo el que aprenda. A ver, ¿qué pasa aquí? ¿Por qué no entra el texto? ¡Eh tú! ¿Qué problema tienes con el texto? Eh, que no, que no lo digiero bien. O sea, internamente yo solo proceso cosas numéricas. Ya sabes, vectores bajos en grasas, numeritos más digestivos. ¿Qué es probar uno de estos caracteres? Y es que no sé cómo digerirlos. ¡Todo el día con los parámetros revueltos! Pues... en realidad tiene razón. Y es que, como ya sabemos, pues una red neuronal en realidad es una máquina de analizar datos, representados evidentemente de manera numérica. Es decir, cuando hablamos de dar como input una imagen, realmente lo que le estamos pasando a la red neuronal es una matriz de valores numéricos donde cada número representa las intensidades de un pixel. O cuando insertamos una tabla de datos, pues aquellas variables categóricas representadas con etiquetas también las transformamos a valores numéricos. O en un fichero de audio, las ondas analógicas del sonido pues ya han sido previamente digitalizadas. Es decir, necesitamos representar a nuestro texto de forma numérica para que así nuestras redes neuronales lo puedan procesar. ¿Se te ocurre algún plan? Pues, ¿a poco que sepas cómo funcionan internamente nuestros sistemas operativos, quizá ya tengas una primera idea. Y es que, por ejemplo, sabrás que en nuestros ordenadores cada carácter que tecleamos y almacenamos realmente viene representado por un código numérico donde cada símbolo está asociado con una etiqueta, donde el igual es el 61, la L mayúscula el 76 o este símbolo de aquí el 185. Bueno, y esta estrategia es bastante legítima pues podemos utilizarla para representar también nuestros textos. Pero antes de avanzar voy a aclarar una cosa, y es que antes estábamos hablando de palabras y ahora estamos hablando de codificar caracteres. ¿Cuál de la tos es realmente la buena? Pues realmente depende. Depende del problema que quieras resolver y cómo lo quieras afrontar. Cada una de estas opciones te va a dar ventajas y desventajas. Por ejemplo, como te podrás imaginar, trabajando con un vocabulario de palabras ya formado pues es mucho más fácil generar texto que sea realista. Pero la contraparte es que si directamente te basas en generar cada palabra como una secuencia de caracteres, pues realmente tienes la posibilidad de que tu algoritmo pueda aprender incluso a generar palabras que no existen. Como ves, pues cada una tiene su ventaja. E incluso podemos encontrar estrategias intermedias para dividir a nuestro texto. Por ejemplo, por subpalabra. Método que como curiosidad te cuento es el que utilizan los modelos GPT-2 y GPT-3 que hemos visto en el canal. Es a cada uno de estos bloques que conforman nuestra secuencia de datos, ya sea palabras, caracteres o subpalabras, lo que se le denomina token. Y a este proceso de división, tokenización. En nuestro caso continuaremos el resto del vídeo hablando a nivel de palabra. Es decir, cada token es una palabra. Y como habíamos dicho que lo vamos a representar así, asignándole una etiqueta numérica a cada uno de estos tokens. Así que lo que tenemos que hacer es, eligiendo el criterio que no te la gana, incluso poniendo etiquetas aleatorias, asociar cada una de las palabras con un número. Y lo único que tenemos que tener en cuenta es que cada palabra siempre venga con la misma etiqueta. Y con esto ya habríamos solucionado el problema. Nuestro texto ha sido convertido, ha sido representado a una forma numérica. ¿Verdad? ¿Verdad? ¿Verdad? Pues no. Agárrate porque vienen curvas. Y es que hay un pequeño problema. A ver, no es un problema grave. Si quisieras podrías empezar ya a utilizar esta codificación para analizar tu texto. Pero estamos desvirtuando un poco el problema. Fíjate en lo siguiente. Imagínate que no estuviéramos trabajando con texto, sino con un dataset sobre personas. Tú, yo, el de atrás y el de la esquina. Y uno de los atributos que estuviéramos mirando fuera la edad. Encontraríamos una columna en la tabla de datos donde vendría representada la edad. Y por ejemplo uno podría tener 15 años, otro 20 años y otro 30 años. Y bueno, nuestra red neuronal se comería esos valores y básicamente al analizarlo entendería las diferencias relativas entre un valor y otro sobre la dimensión de esta columna. Es decir, entendería que este valor de aquí es cuantitativamente menor que este otro valor de aquí. Y que por ejemplo este es el doble de este. Y toda esta información sería la que la red utilizaría para resolver cual sea el problema que estuviéramos resolviendo. Pero claro, la red solo ve números. Pero realmente no sabe que esta columna representa la edad o el número de ojos que tiene esa persona o que esto realmente sea la etiqueta asociada a una palabra. Esto genera un problema, porque a lo mejor numéricamente este 15 es el identificador de la palabra teléfono. Y este 20 el de la palabra tostadora y este 30 el de la palabra pangolín. Y claro, haciendo esto lo que le estamos diciendo a la red es que un pangolín esto veces mayor que un teléfono o que una tostadora se encuentra a mitad de camino entre un pangolín y un teléfono. Algo que no tiene realmente sentido. Y si esto es así, ¿se te ocurre otra solución? Pues fíjate. La solución viene por buscar una representación diferente. Donde en vez de asignarle una etiqueta numérica a cada palabra, lo que haremos será asignarle un vector. Un vector con tantas posiciones como palabras queramos tener en el vocabulario de nuestro problema. Mil palabras, veinte mil palabras, cien mil palabras, las que queramos especificar. Cada una de las posiciones de este vector representará a cada una de las palabras de nuestro vocabulario y será marcando con un uno en la posición que queramos con la que especificaremos que es esa la palabra que estamos representando. Era el teléfono el que tenía la etiqueta 15, pues buscamos la componente 15 del vector y la marcamos. Para entender por qué esto es interesante, lo mejor será verlo a nivel geométrico. Imagínate que en nuestro caso, pues todo nuestro vocabulario estuviera conformado solo por estas tres palabras. Teléfono, tostadora y pangolin. En este caso nos limitamos solamente a tres palabras, pues porque geométricamente nuestros obtusos cerebros de primates son incapaces de imaginarse más de tres dimensiones. Así que nos quedaremos con estas tres palabras. Teléfono tendrá la etiqueta cero, tostadora la etiqueta uno y pangolin la etiqueta dos. Y esto reconvertido vector sería su nueva representación. Lo que estamos haciendo ahora, al pasar su representación de esto a esto, es convertirlo en vectores. Vectores de tres componentes que si lo visualizamos en un espacio tridimensional se colocarían de la siguiente manera. ¿Lo ves? Lo que está ocurriendo es que en este caso cada palabra ocupa su propia dimensión. Y ahora si calcularamos la distancia entre cada palabra geométricamente, veríamos que la distancia siempre es la misma. Es constante. Y por tanto ya no tenemos el problema de representación que teníamos anteriormente. ¿Lo entiendes? Y a lo mejor estés pensando, oye Carlos, ¿no es esto un gasto innecesario? Al final, para una frase que contenga veinte palabras en un vocabulario de tamaño mil, pues esto se convierte en una matriz de veinte por mil, veinte mil elementos, donde además la mayoría de números van a ser ceros. Esto sería como querer almacenar en todo un diccionario la definición de una única dejando el resto de páginas sin definiciones, en blanco, un diccionario por palabra. Algo que, bueno, pues parece que es una representación que ocupa demasiado espacio. No lo podemos hacer mejor. Y bueno, no solo eso, está muy bien esta representación para evitar tener algún tipo de ordenación donde tostadora sea menor que pangolín y mayor que teléfono. Pero también es cierto que una tostadora y un teléfono son palabras que conceptualmente están más próximas de lo que lo estarían de un pangolín. Y aquí estamos asumiendo que la diferencia entre todas ellas es la misma. ¿No tendría más sentido algo como esto? Pues sí. Y esta solución vendrá dada por un concepto interesantísimo denominado embeddings. Un concepto que realmente en nuestra serie será el primer punto de encuentro del campo del natural language processing con el campo del deep learning. Un concepto que veremos, sí, pero lo veremos en el próximo vídeo. Video que por cierto se verá muy pronto en el canal. Lo estoy preparando para tenerlo listo este mes. Y de hecho, mira, si conseguimos que este vídeo tenga más de 10 mil likes. Wow, Carlos, eres todo un youtuber. Lo sacaré la próxima semana. Igualmente, si quieres apoyar el canal, ya sabéis que lo podéis hacer a través de Patreon. Tenéis un Patreon hermosísimo disponible donde podéis apoyar todo el contenido que hago y que además si participáis, pues podéis entrar directamente a un grupo de telegram donde hay muchísima gente hablando también de machine learning y discutiendo de muchas cosas que les puede interesar. Por mi parte, nada más. Muchas gracias por ver el vídeo entero y nos vemos en la siguiente parte.
DOC0043|NLP e IA|Este vídeo está patrocinado por la Universitat Politécnica de València. Punto de partida. Queremos trabajar con texto, pero una red neuronal solo procesa números. ¿Cómo lo solucionamos? Pues ya lo sabemos del anterior vídeo. Tenemos que vectorizar el texto. Cambiar su representación a, por ejemplo, One Hot Encoding, que no es más que una forma de convertir a cada palabra en un vector que tiene tantas componentes como palabras tengamos en nuestro vocabulario. Y donde todas estarán a cero, excepto aquella que se corresponda con la palabra que queramos representar. Algo que, como ya vimos, geométricamente nos aporta algo muy beneficioso. Y es que cada palabra, cada vector, es independiente al resto de vectores. O dicho de otra manera, que cada palabra ocupa su propia dimensión. ¿Y qué es lo bueno de esto? Pues que al final cada palabra mantiene la misma distancia con el resto de palabras. Algo que, bueno, también tiene algo malo. Y es que cada palabra tiene la misma distancia con el resto de palabras. Y es que sí, asumir que cada palabra es equidistante al resto de palabras de tu vocabulario es un punto de partida correcto. Pero también es cierto que nosotros como humanos no funcionamos de esta manera. Nosotros entendemos que hay palabras que conceptualmente están más cerca, son más similares, que otras palabras que, bueno, pues, estarán más lejos conceptualmente. Planeta, galaxia o universo sabemos que son palabras que conceptualmente están más cerca que, por ejemplo, pangolin. ¡Sigo siendo pangolin! ¿Verdad? Esto nos habla de un tipo de información que sería interesante codificar en nuestros algoritmos de procesamiento del lenguaje natural. Pero, claro, ¿esto numéricamente cómo se consigue? Pues fácil. Compartando. Planteate la siguiente tarea. Imagínate que te muestro las siguientes imágenes de caras, de caras creadas artificialmente, y te pido que hagas la siguiente tarea. Tu tarea va a ser darle una ordenación. Imagínate que digo que los tienes que ordenar en función de dos atributos, dos propiedades que tú consideras relevantes, para resolver el problema que te propuesto. Ordenarlos. Por ejemplo, tú decides edad y color de piel. Pues venga, probemos. Te pones a ordenar las imágenes en un gradiente de pieles más oscuras y pieles más claras, de rostros más jóvenes a más mayores. ¡Y, boom! Terminado. Pues genial. ¡Mira! ¡Felicidades! Porque ahora, además de tener un bonito collage de caras, lo que has conseguido es reducir la dimensionalidad de los datos de tu problema. De partida, cada una de estas imágenes cuenta con miles y miles de píxeles. Intensidades lumínicas que podemos interpretar como variables de entrada, que tú, con tu intelecto, has sabido percibir, procesar, comprimir y ordenar, hasta armar este mosaico donde cada imagen tiene su posición en base a solo dos atributos. Es decir, tus imágenes han pasado de estar representadas por miles y miles de dimensiones a solo dos dimensiones. Has reducido la dimensionalidad de tus datos. Y en este caso son dos dimensiones porque yo te lo he pedido, pero te podría haber pedido ordenarlo en base a tres atributos, por tanto en tres dimensiones, o en una dimensión, consiguiendo así compactar mucho más la información y armando una taller list como si esto fuera de repente Twitch. Pero bueno, tampoco te creas tan especial porque esto es algo que una red neuronal artificial también sabe hacer bastante bien. De hecho, es un modus operandi habitual. Comprimir datos y ordenarlos de manera inteligente para así poder solucionar de la mejor manera la tarea que tenga que resolver. Fíjate, por ejemplo, cuando nosotros en una red neuronal convolucional tenemos una imagen representada por miles y miles de variables y la vamos pasando capa por capa, esta imagen poco a poco se va comprimiendo, por tanto su dimensión se va reduciendo hasta finalmente alcanzar un vector de predicciones cuyo tamaño pues será de tamaño 10 o tamaño 20. También hemos comprimido nuestros datos. Ahora, el punto a entender es que este proceso de compactación y ordenación que ocurre en una red neuronal no es exclusivo solo de trabajar con imágenes, sino que ocurre de manera general. Es decir, si yo tengo un vector de entrada de tamaño 10 y lo paso por una capa que tiene solamente tres neuronas, el resultado de este procesamiento va a ser un vector que va a tener tamaño 3, un vector tridimensional. Y claro, si podemos conseguir esta compactación y esta ordenación tan interesante, ¿qué pasaría si lo aplicáramos a nuestros vectores de texto? ¿No era eso lo que estábamos buscando en el vídeo anterior? Efectivamente, necesitamos que nuestra red aprenda a comprimir el texto. Y es que sí, trabajar con palabras representadas en One Hot Encoding, donde cada vector es del tamaño de tu vocabulario, pues la verdad que es una representación que apetece que tu red neuronal la comprima un poquito. Y ya te digo, así lo vamos a hacer. ¿Cómo? Pues mira, lo acabamos de ver. Asumamos que tu vocabulario tiene 10.000 palabras diferentes, y esto va a significar que tus palabras de entrada también serán vectores de tamaño 10.000. Ahora, si esto lo pasáramos como input a una red neuronal cuya primera capa tiene, por ejemplo, digamos, 300 neuronas, ¿cuál crees que será el tamaño del vector en este punto? Efectivamente, su tamaño ha cambiado, o dicho de otra manera, su dimensionalidad se ha reducido. Y ojo, porque en un comienzo realmente nuestra red neuronal se va a parecer más a esto. Es bastante estúpida, puesto que aún no ha sido entrenada, y por tanto las palabras tampoco van a estar bien representadas. Pero claro, según empiece el entrenamiento, la red poco a poco irá aprendiendo a saber cómo comprimir y ordenar todas estas palabras de la manera más óptima para resolver el problema que la hayamos planteado. Ojo, esto es importante. ¿Qué estamos prediciendo si el texto tiene un sentimiento positivo o negativo? Pues quizás en esta compactación la red aprenda a ordenar estas palabras que representan sentimientos similares. O si estamos analizando reviews de clientes del campo de la electrónica, a lo mejor este proceso de ordenación y compactación dará prioridad a aquellas palabras del campo semántico de la electrónica. ¿Lo entiendes? Es decir, esta nueva representación que aprende la red dependerá del tipo de tarea que quiera resolver, encontrando aquella codificación de lo tato de entrada que mejor le permitan acercarse a la solución óptima. ¿Lo entiendes? Este proceso que ocurre en la primera capa de la red neuronal, en el que necesariamente tiene que aprender a comprimir vectores de One Hot Encoding a una representación más compacta que le sirva para resolver su tarea, es lo que se conoce como embedding, el tema principal del vídeo de hoy. Pregunta. A ver, si tú por ejemplo quisieras enseñarle a alguien a analizar imágenes médicas, ¿qué preferirías? ¿Coger a una persona que sin saber de medicina, pues por ejemplo, sabe percibir el mundo que le rodea, sabe entender los objetos que está viendo, o coger un trozo de carne visualizado aleatoriamente que solo sabe gritar y llorar? Exactamente. Mejor será partir con algún tipo de conocimiento previo a priori que te aproxime a aprender mejor tu tarea. Esto en el contexto del deep learning significa que si tú por ejemplo tienes algún tipo de red que quieras entrenar para observar imágenes médicas, es mejor partir de una red que ya haya sido pre-entrenada para alguna tarea genérica de visión como sea clasificar un gran conjunto de imágenes variadas. Partir de una red pre-entrenada te dará sustancialmente un mejor rendimiento que comenzar con una red completamente nueva, puesto que parte del conocimiento ya aprendido podrá ser transferido a la nueva tarea. Estamos transfiriendo el aprendizaje. Vale, vale, ok. Otra pregunta. Si tú ahora por ejemplo quieres enseñar a una persona a analizar textos jurídicos, ¿qué preferirías? A una persona que sin saber del tema pueda entender al menos cuál es la relación de palabras dentro de ese vocabulario y más o menos tener un entendimiento del lenguaje o al trozo de carne y... creo que se entiende la respuesta, ¿no? Cogeremos aquel que realmente entienda el vocabulario. Y esta misma idea la encontramos en el campo del natural language processing, porque está muy bien dejar que tu red aprenda la capa de embedding a representar tus palabras en vectores para resolver tu problema, pero... ¿y si contaras ya con algún tipo de embedding pre-entrenado de partida? ¿No sería más interesante contar con algún tipo de modelo que pueda convertir tu texto a vectores comprimidos y que pueda ser aplicado de manera universal a múltiples problemas? Pues sí. Y esta es la tendencia que terminó de despegar en 2013 con la publicación de Word2B, uno de los sistemas de embeddings pre-entrenados más utilizados hasta la fecha desde que Google lo publicara. Y no es el único. La cosa ha evolucionado bastante en la última década, convirtiéndose los embeddings en una herramienta fundamental en este campo. Vale, voy a dar respuesta a esa pregunta que a lo mejor todavía ni siquiera te has planteado. Y es, ¿cómo podemos conseguir entrenar a un embedding que sea lo suficientemente universal como para poder aplicarlo a diferentes tipos de tareas? Bueno, los ingredientes son dos. Por una parte, mucho texto, y por el otro, pues una tarea que sea lo suficientemente genérica, como para no condicionar la estructura que el embedding aprenda. Si la tarea que planteemos es lo suficientemente genérica, la estructura que será aprendida pues también lo será, y este embedding podrá ser utilizado en múltiples tareas. Y esto es súper interesante, porque lo que conseguimos finalmente es construir una estructura de nuestro vocabulario donde aquellas palabras que conceptualmente son similares van a estar representadas de manera próxima, en un espacio vectorial que incluso podemos operar matemáticamente. Esto, por ejemplo, en el caso de Word2vec, lo que hace es transformarte vectores de un vocabulario de tamaño 10.000 a un vector de tamaño 300. Y a lo mejor te estarás preguntando qué tipo de estructuras se forman en estos espacios, cuál es la relación entre palabras. La buena noticia es que los podemos ver. Podemos coger, por ejemplo, estos vectores que Word2vec genera, que son de tamaño 300, y aplicar nuevamente un algoritmo de reducción de la dimensionalidad para convertirlo a vectores tridimensionales, que en este caso sí podríamos visualizar. Y el resultado es el siguiente. Lo que estamos viendo ahora es la estructura tridimensional que surge de haber reducido de nuevo la dimensionalidad sobre las palabras que provienen de Word2vec. Los vectores de tamaño 300 son reconvertidos a vectores tridimensionales que, por tanto, podemos visualizar y nos permiten observar un poco la estructura del texto que el embedding ha aprendido durante su entrenamiento. Podemos encontrarnos diferentes estructuras de palabras que estarían relacionadas por proximidad. Palabras como go, return, wake, run, stood, gone. Podemos, por ejemplo, encontrarnos clúster de palabras de números como 0, 5, 7, 4, 2, todos juntos y muy cerquita. Podemos ver también que hay un clúster que indica meses del año. Marzo, octubre, agosto, junio, diciembre. Recordemos que estos clústers han sido aprendidos de manera no supervisada. Simplemente el algoritmo ha analizado secuencias de texto y, en función de su contexto, ha aprendido que estas palabras tienen que ser palabras similares. Podemos también encontrarnos palabras que, por compartir su misma raíz, también están próximas. Por ejemplo, sugerencia, sugiriendo, sugerir, sugerir. Podemos hacer búsquedas en el mapa. Podemos poner que queremos encontrar la palabra cats y nos marque aquí palabras que podrían estar cerca. Y vemos que, por ejemplo, aquí está cat, sí estaría cerca de elephants, the breeds, the cat, the breed, rat, dogs. Todo lo que tiene que ver con el campo semántico de animales. Salvaje, cabra, alimentar. Incluso podemos buscar por palabras que a nosotros nos apetezca, por ejemplo, python. Y en este caso podemos hacer que nos marque cuáles serían sus vecinos cercanos en el espacio 300 dimensional, que sería el espacio original del embedding. Y nos estaría marcando aquí cuáles son estos puntos, que a lo mejor no se encuentran próximos después de hacer la segunda reducción de dimensionalidad. Pero que sí sabemos que el embedding al menos ha entendido que son palabras relacionadas. Y vemos cómo son, por la ambigüedad de la palabra, cosas como mozilla y deviant del campo de la informática. Pero al mismo tiempo también encontramos aventura, joke. Y esto está relacionado efectivamente también con la palabra monty de monty python. Y vemos film, episodes, sketch, credits. Y podemos encontrarnos con todo el campo de palabras relacionadas que el propio embedding, el propio algoritmo de inteligencia artificial, ha aprendido a encontrar como palabras próximas solamente a partir de analizar cuál es el contexto de dicha palabra. Si queréis jugar con esto, pues lo he dejado abajo en la descripción un enlace para que podáis jugar con esta misma herramienta. Final conclusions. Lo que tenemos que saber es que con un embedding finalmente lo que tenemos es una herramienta para poder representar, para poder codificar a nuestras palabras que inicialmente están representadas en One Hotend Coding. Y que por tanto, pues inicialmente son todas independientes unas con otras. Con un embedding podemos dejar que sea una red neuronal la que aprenda una codificación más inteligente para resolver la tarea que queramos. Y que incluso podemos aprovecharnos de preentrenamientos de embeddings ya preentrenados, que nos puedan dar ese rendimiento extra que buscamos en nuestros problemas de Natural Language Processing. Esta herramienta es fundamental y no es solamente exclusiva del campo del texto, sino que podemos utilizar un embedding para representar de mejor manera pues genes, jugadas de ajedrez, variables categóricas, cualquier tipo de variable que inicialmente viniera representada como un vector One Hotend Coding. De hecho, tienes que saber que esta representación que consiguen los embeddings es tan interesante, es tan interesante que incluso te puede construir un espacio matemático donde se pueden implementar diferentes álgebras. Te estás dando cuenta que no hay tanto tiempo, que el vídeo se va a acabar ya, o sea que esto se va a quedar pendiente para otro vídeo. Una álgebra que cuentan los papers que podríamos utilizar para coger la palabra rey, restarle la palabra hombre, sumarle la palabra mujer y que esto no te vuelve el vector reina. Bueno, hay bastantes matices, pero eso te lo contaré en el próximo vídeo. Y hasta aquí el vídeo de hoy. Muchas gracias por aguantar hasta el final. Continuamos con nuestra serie de Natural Language Processing y tendremos un vídeo muy próximamente. Y en este caso pues no voy a pedir likes, sino voy a pedir visualizaciones, ¿vale? Para liberar la siguiente parte de la serie tenemos que llegar a 75.000 visualizaciones. Y si también queréis ir a la primera parte que se quedó bastante mal porque el algoritmo de YouTube lo cogió por la parte mala, pues podéis volver a ver ese vídeo que nunca está mal repasar los contenidos y así pues duplicamos las visualizaciones. Y en cualquier caso, si queréis mostrar un apoyo más fuerte al contenido que estoy haciendo y para que pueda seguir trabajando en este proyecto, en .csv, recordad que podéis apoyar a través de Patreon, ¿vale? Tenéis el enlace en la descripción y ahora que ya estamos a punto de cerrar temporada, pues es un buen momento para valorar si todo lo que hemos visto este año, biobots, leer la mente, coches autónomos, Nerve, si todos estos vídeos te han valido para algo, si has aprendido cosillas, pues si quieres mostrar ese apoyo extra al canal lo podéis hacer a través de Patreon. Además tendréis acceso a un fantástico grupo donde se discuten cosas bastante chulas relacionadas con el Machine Learning. A veces sí, a veces no. Muchas gracias por ver el vídeo y nos vemos con más Inteligencia Artificial aquí en .csv. .
DOC0044|NLP e IA|Este vídeo está patrocinado por el podcast Cuidado con las macros ocultas de 480. Cuidado con las macros ocultas, un podcast de 480. Veamos, seguramente estarás al día de los increíbles avances que hemos conseguido en el campo del deep learning durante la última década. Un montón de cosas. En los comienzos usábamos arquitecturas de redes neuronales sencillas como las redes neuronales multicapas para construir los primeros modelos que aprendieron a resolver tareas básicas. Y luego los fuimos adaptando y perfeccionando a la naturaleza de los diferentes tipos de datos que usábamos. Redes neuronales convolucionales para entender datos espaciales como imágenes. Redes neuronales recurrentes para entender los datos secuenciales como textos. Redes neuronales que no sólo se utilizaban para aprender a analizar patrones, sino que también eran capaces de generarlos, con resultados que todos vosotros habéis podido ver aquí en el canal. Y si bien el desarrollo de esta tecnología en tan poquito tiempo ha sido impresionante, es en 2017 cuando una nueva publicación empieza literalmente a transformar nuestra concepción de lo que la inteligencia artificial es capaz de hacer. Aparecen los transformers. Y desde ese momento no hemos parado de ver grandísimos logros del deep learning que, en su funcionamiento, se apoyan principalmente en el rendimiento que esta tecnología ofrece. Alpha Fold 2 para analizar secuencias de datos genómicos. Tesla en su sistema de conducción autopilot. GPT-3 para la modelización y generación de texto. O Transformers para que las VQ GANs de este paper de aquí puedan generar estas espectaculares obras de arte. De todo esto ya hemos hablado en el canal. Siempre buena información. Pero hoy os voy a llevar más allá. Hoy, por fin, es el comienzo de una serie de vídeos donde os voy a explicar cuál es la intuición tras el funcionamiento de los Transformers. ¿Y por dónde vamos a empezar? Atención. Para poder entender qué tipo de mejoras nos ofrece un modelo como los Transformers, primero tenemos que entender con qué herramientas contábamos antes. En concreto, para tareas de procesamiento del lenguaje natural que es de donde surge todo esto. Vamos, que la pregunta aquí es, si yo tengo una frase como esta, ¿cómo la analizo? Pues dada la naturaleza secuencial de una frase, donde cada palabra ocupa una posición en el tiempo, una tras otra, la estrategia que se ha venido utilizando desde el campo del deep learning es la siguiente. Tomamos una red neuronal normal y le damos como input la primera palabra. Internamente esta palabra se procesará multiplicándose capa tras capa con los parámetros aprendidos de la red. Vamos, lo típico de siempre. Sin embargo, la novedad viene ahora. Y es que la información que ha sido procesada por la red ahora será agregada a la nueva información que introduciremos en el siguiente paso de nuestra secuencia, a la siguiente palabra. Así, haciendo este proceso de encadenar el output de la red con el input del siguiente paso y dejando que analice todas las palabras, acabaremos en un punto donde toda la información de nuestra secuencia habrá sido procesada y analizada y cuyo resultado tendremos en este punto de aquí. Es esta idea que parece sacada de la película de Cienfieses Humanos de conectar el procesamiento del output anterior con el input del procesamiento actual lo que da nombre a este tipo de redes, las redes neuronales recurrentes. Y estas responden a una idea muy intuitiva y es que tú cuando lees o ahora que me estás escuchando, vas procesando cada palabra individualmente, pero apoyándote del contexto de todas las palabras que he dicho anteriormente. Tú cuando lees seguramente lo haces de manera secuencial, no escaneas automáticamente todas las palabras que aparecen en la página de un libro, ¿verdad? Pues esto es igual. Con este tipo de redes es con lo que en el pasado se construía la mayoría de generadores de texto, traductores neuronales y otras tantas movidas que requerían el análisis de secuencias, como por ejemplo cuál era la secuencia de acciones que un jugador del DOTA hacía con el ratón y el teclado. Como vimos en el vídeo de Tesla, a una día de hoy encontramos proyectos que se siguen basando en este tipo de redes neuronales recurrentes. Y parecería que todo es maravilloso y ideal con este tipo de redes neuronales recurrentes, excepto por un pequeño detalle. Y es que este vídeo no se titula, las redes neuronales recurrentes son maravillosas e ideales. Quiero que me respondas a una pregunta. ¿Cuál ha sido la primera palabra que he pronunciado al comienzo de este vídeo? Si no te acuerdas, no pasa nada. Y es que es normal que después de haber escupido tantas palabras durante el vídeo, pues tú no hayas sido capaz de retenerla. Pero, ¿y si te dijera que este problema no es exclusivo de tu limitado cerebro de primate? No. ¿Y si te dijera que este es el principal problema al que se enfrentan las redes neuronales recurrentes? Y es que está comprobado que uno de los principales problemas de este tipo de redes es que cuando este proceso de nutrir el input con el output anterior se repite durante muchos pasos, el peso que tienen durante el entrenamiento, las primeras palabras respecto a las últimas que hemos agregado es menor. A efectos prácticos, esto es el equivalente a la red olvidando cuáles eran las primeras palabras de nuestra frase. ¿Y esto es un problema? Pues sí. Ya que frases como el pangolín dormía plácidamente colgado de la rama de un árbol usando su cola, no permitiría a la red encontrar una relación que es interesante en este caso, y es que su hace referencia a la posesión que tiene en este caso el sujeto de la frase, el pangolín. ¿Cómo solucionamos este problema? Pues bien, vamos a poner atención. Pero literalmente, vamos a aplicar una serie de mecanismos que sirven de alternativa para dar solución a este problema de falta de memoria. Unos mecanismos que se denominan mecanismos de atención. Y antes de empezar con las matemáticas épicas, dejadme recordar una cosa. Hasta el momento hemos estado hablando de redes que procesan y analizan palabras, pero si habéis seguido la serie de vídeos sobre NLP, Natural Language Processing, tenéis que saber que estas palabras en realidad vienen representadas como vectores numéricos, vectores multidimensionales que capturan gran parte de la información semántica y sintáctica de la palabra que representan, y con los que podemos además operar matemáticamente. Por ejemplo, vectores cuya dirección en este espacio multidimensional sea muy parecida, representarán a conceptos cuyas palabras también sean parecidas, y se alejarán de aquellas palabras que poco tengan que ver. Matemáticamente, este ángulo además lo podríamos calcular, para así estudiar pues cuál es la similitud entre palabras, frases o documentos. Si queréis conocer más en detalle sobre esto, os recomiendo ver estos dos vídeos de aquí que dan comienzo a la serie de NLP. Pero por ahora para este vídeo me vale con que entendáis que aquí cada palabra viene representada por uno de estos vectores matemáticos. Y con ellos vamos a trabajar. Porque aquí nuestro objetivo es buscar una solución a este problema de falta de memoria que parece estar presente en las redes neuronales recurrentes. Esa falta de conexión que parece que existe entre palabras que están muy distanciadas, que no nos permite estudiar cuáles son sus relaciones. Veamos por ejemplo con esta frase de aquí. El pangolín duerme en su árbol. Aquí nuestro objetivo será que cada una de las palabras de nuestra frase, no importa la distancia que haya entre ellas, pueda estudiar cuál es su relación con cada una de las otras palabras de la frase. Estamos buscando cuál es la relación de todas las palabras con todas las palabras. Y te estarás preguntando, ¿qué tipo de relaciones estamos buscando? Bueno, nosotros no. ¿Qué tipo de relaciones está buscando la red neuronal? Pues por ejemplo, exista una relación interesante entre la palabra pangolín y árbol, que semánticamente nos traslada al concepto de naturaleza. O la palabra duerme, que como verbo de la oración estará muy conectado al sujeto, al pangolín, quien es el que realiza la acción. Así es como vemos que cada palabra puede tener una relación interesante o no, con cualquiera de las otras palabras que conforman a nuestra frase. La pregunta es, ¿cómo podemos automatizar este proceso de búsqueda de relaciones? ¿Cómo lo hacemos? Pues la idea es que si esas relaciones existen, tendrán que ser redes neuronales las que aprendan a encontrarlas. Para ello, lo que vamos a hacer será entrenar a dos redes neuronales diferentes para que, con estas palabras dadas como input, aprendan a generar dos vectores distintos. Una de las redes generará un vector que servirá para identificar las propiedades interesantes que caracterizan a dicha palabra. Y por otro lado, la otra red neuronal generará un vector que servirá para describir aquellas propiedades interesantes que esta palabra está buscando. Tal y como suena esto, casi parece que estamos entrenando a estas redes neuronales para generarse un perfil de Tinder. Hola, ¿qué tal? Soy la segunda palabra de esta frase, soy el pangolín. Y esta red de aquí se encargaría de generar su descripción, que diría algo así como... Yo soy un sujeto que tenga una parte muy animal y que estoy interesado por la naturaleza. Y esta otra red de aquí generaría la descripción de lo que está buscando. Pues actualmente lo que estoy buscando es otra palabra interesada por la naturaleza. A poder ser verbos que puedan dar un poco de sentido a lo que hago. Y así, si apareciera otra palabra cuya descripción fuera compatible con lo que busca nuestro pangolín, esto debería de generar un match. Algo que, bueno, como toda cita Tinder, sabemos cómo va a acabar. Esto de aquí es otra forma de entenderlo. Sería mediante la metáfora de una llave y una cerradura. Podríamos decir que con cada palabra de nuestra secuencia, Estas dos redes neuronales se van a encargar de aprender a generar una llave y una cerradura. Llaves que podrán interactuar con el resto de cerraduras de las otras palabras. Claro, tenemos que entender que estas llaves aquí son diferentes a las que solemos utilizar en la vida real. Y es que aquí cada llave puede funcionar en distintas cerraduras con mejor o peor resultado. De hecho, ¿quiere ver cómo funcionan matemáticamente estos candados? Pues en realidad, como ya he dicho antes, aquí no estamos trabajando ni con llaves, ni con cerraduras, ni perfiles de Tinder, sino con vectores. Esto de aquí es un vector. Y esto de aquí es otro vector. Y como vectores que son, nosotros podemos operar matemáticamente con ellos para, por ejemplo, estudiar si su dirección es similar o no. Esto lo podemos hacer a través de una operación matemática denominada producto escalar. O en inglés, dot product, que viene en honor de su creador, que en este caso fui yo. Este producto escalar notará un valor numérico, que será mayor en relación a cuánto coincidan las direcciones de los vectores. Es decir, si las redes que han generado estos vectores quisieran representar que dos palabras son compatibles, pues tendrán que transformar a estas palabras en vectores cuyas direcciones se aproximen en este espacio, donde llave y cerradura encajen. Y creo que estamos en un punto en el que ya podemos empezar a llamar a cada cosa por su nombre. Y es que en el paper original, a lo que aquí estamos llamando cerradura, se le conoce como vector query. Y a lo que estamos llamando llave se le conoce como vector qui, que en este caso tiene más sentido. Estas dos nomenclaturas creo que son interesantes porque además nos acerca a la filosofía de trabajo que encontramos, por ejemplo, en la recuperación de información dentro de bases de datos, donde términos como query o qui también son utilizados. Esta misma idea es la que estamos intentando implementar aquí, pero en este caso utilizando redes neuronales. Como hemos visto, cada palabra obtendrá con estas redes neuronales su vector query y su vector qui. Y ahora, por ejemplo, para esta palabra podremos tomar su vector query y calcular cuál es la compatibilidad que tiene con el resto de palabras. ¿Cómo lo haremos? Pues ya lo sabes, calculando el producto escalar entre el vector query de esta palabra y los vectores qui del resto. Como hemos visto, este producto escalar nos devolverá un valor numérico que entre más alto sea, nos estará indicando que mayor es la compatibilidad. Así podemos ver cuáles son las palabras más compatibles con esta de aquí. Pero realmente son algo más que porcentajes, y es que esto que acabamos de computar aquí es lo que se conoce como el vector de atención. Sí, atención, porque si te fijas, si coloreamos los valores numéricos que hemos calculado para verlos mejor, podemos ver qué importancia le da nuestro modelo de inteligencia artificial al resto de palabras cuando está leyendo esta de aquí. Nos está mostrando qué palabras considera relevantes para dar contexto a su palabra, o dicho de otra forma, a qué parte de la frase está prestando atención. De hecho, si computamos estos vectores para cada palabra de nuestra frase, finalmente acabaremos con lo que se conoce como una matriz de atención, algo como esto, donde podremos ver la importancia que cada palabra está asignando al resto de la frase. Estas son matrices muy interesantes de visualizar, ya que lo que nos van a mostrar son a qué parte de la información dada como input la inteligencia artificial está prestando atención para tomar sus decisiones. En este ejemplo típico de traducción vemos que cuando calculamos la atención entre la frase en inglés y en francés, la inteligencia artificial es capaz de encontrar la relación entre las palabras en distintos idiomas, aún cuando el orden no siempre se preserva, como en este caso de aquí, donde el orden de las palabras zona económica europea varía según el idioma. Pues ahora la pregunta importante que hay que hacerse es ¿para qué? ¿Para qué nos interesa calcular este vector o matriz de atención que nos aporta saber a dónde focaliza su atención la red neuronal en cada momento? ¿Es esto útil? Atiende. Ahora, al igual que utilizamos en un principio, pues vamos a usar una red neuronal para que procese a cada una de nuestras palabras. No la he mencionado hasta ahora, pero su papel es equivalente al procesamiento que hace también la red principal de las redes recurrentes, transformar cada palabra al vector de salida necesario para cumplir la tarea que estamos buscando. Este vector de salida es el que encontramos en el paper denominado como vector valor. Si antes con las redes neuronales recurrentes teníamos el problema de que por usar la recurrencia perdíamos la relación que pudiera existir entre palabras muy distanciadas, ahora hemos creado un mecanismo donde podemos relacionarlas sin importar qué tan lejos estén. Y aquí es donde viene lo interesante, porque aquí es donde las atenciones calculadas previamente van a jugar su principal papel. Y es que en esta frase, como hemos visto antes, la palabra pangolin podría tener una relación interesante con estas dos palabras aquí. Y es por eso que la tensión que presta nuestro sistema cuando el input es la palabra pangolin va a ser muy alta con estas dos palabras de aquí. Así el uso que le daremos a cada una de las atenciones calculadas será el de computar una suma ponderada, donde utilizaremos la tensión que prestamos a cada palabra como factor de mezcla de cada uno de los vectores de valor. Es decir, para la palabra pangolin dada como input, su vector de tensión nos servirá como una receta que nos va a ir indicando en qué porcentaje tenemos que ir mezclando el resto de valores. ¿Entendéis? Así este output, de alguna manera, tendrá el contexto necesario del resto de palabras que conforman a la frase, dando mayor importancia a aquellas a las que le haya prestado atención. Esto repetido para cada una de las palabras de nuestro input notará como resultado unos vectores de palabras cuya transformación ahora recoge el contexto del resto de la frase. Como ves, con este mecanismo hemos encontrado una forma de poder contextualizar a cada una de las palabras de nuestra frase con cualquier otra palabra que se pueda encontrar a cualquier distancia. Hemos resuelto el principal problema al que se enfrentaban las redes neuronales recurrentes, la falta de memoria. Y es que estos mecanismos de atención han sido realmente de ayuda para solucionar este problema que estaba presente en las redes neuronales recurrentes, la falta de memoria a largo plazo. Y ha sido gracias a la combinación de estos mecanismos de atención con este tipo de redes con las que hemos conseguido mejorarlas, potenciarlas y aplicarlas a numerosas tareas de procesamiento del lenguaje natural. Pero lo interesante ocurre en 2017, cuando uno de los papers más influyentes de la época apareció con una idea muy atractiva. Pues que esta idea de querer potenciar a las redes neuronales recurrentes con mecanismos de atención para que funcionen mejor está muy bien. Pero que en realidad las redes recurrentes en todo esto sobran. Que de hecho en aquel paper proponían un nuevo tipo de red neuronal, una arquitectura diferente a la que vamos a llamar transformers. Y donde nos van a demostrar que esto de aplicar mecanismos de atención es suficiente para poder conseguir rendimientos superiores a los de las por todos utilizadas redes neuronales recurrentes. Un paper titulado Attention is all you need. Lo único que necesitas es atención. Ese paper fue el que introdujo el concepto de transformers, un nuevo tipo de red neuronal que venía a sumarse al resto de arquitecturas que ya conocíamos en el campo del deep learning. Redes convulsionales, redes recurrentes y ahora transformers. Una arquitectura que poco a poco vamos conociendo pero que fundamentalmente se basa en los principios de lo que hemos visto hoy, en los mecanismos de atención. Con lo que hemos visto hoy, sin haber hablado de ningún caso de transformers, ya conocéis cuál es la idea clave que hace funcionar a este sistema. Y de dónde surge parte de la ventaja que logran frente a las arquitecturas recurrentes. Pero eso no quita que todavía queden cosas importantes por explicar. Antes en este punto decía que el pangolin se presentaba como, hola soy la segunda palabra de esta frase. Pero cómo puede saber el pangolin que es la segunda palabra de la frase si realmente ya no estamos hablando de secuencias que mediante recurrencia están conectadas. Cómo puede saber cuál es su posición en la frase y también que otros mecanismos y técnicas se aplican dentro del transformer para que todo funcione correctamente. Y por último pero no menos importante, cómo hemos conseguido adaptar a esta arquitectura para que nos sirva tanto para el entrenamiento de un modelo tan impresionante como sería GPT-3. Como para poder también utilizarla en problemas de visión por ordenador con los Visual Transformers. ¿Cómo funciona todo esto? Pues todas las respuestas a estas preguntas las vais a encontrar en un próximo vídeo. Un vídeo que servirá de segunda parte a esta introducción al funcionamiento de los transformers. Y que dará continuación a la serie de NLP, de Procesamiento de Lenguaje Natural que ya introduje con estos vídeos de aquí. Si no habéis visto esta serie pues también os la recomiendo. Voy a dejar en la descripción enlaces para que podáis seguirla entera. Y así tengáis una visión más completa de cómo funciona este fascinante mundo. Por último simplemente deciros que si este vídeo os ha servido, si ha sido útil para vosotros, si habéis aprendido, si queréis apoyar este contenido porque lo veis valioso. Pues podéis apoyarlo a través de Patreon. Voy a explicar un poco qué es esta plataforma para el que no la conozca. Pero es una de las fuentes que tiene este canal para financiarse. Podéis hacer una pequeña aportación mensual. Pues lo que sería una invitación a una cerveza, a una comida o simplemente el apoyo que queráis aportar. Pues será una forma de indicar que mi trabajo para vosotros es de valor. Y es una forma de recompensar el esfuerzo de traeros todo este contenido divulgativo científico aquí a YouTube. Y que esto sirva además para apoyar este contenido en abierto para que lo pueda disfrutar todo el mundo. El que se lo pueda permitir y el que no. Os voy a dejar el enlace abajo en la caja de descripción para que le podáis echar un vistazo. Y si os interesa podáis apoyar. Y por último, último, último recordaros que el patrocinador de este vídeo es el podcast de Cuidado con las Macros Ocultas. Un podcast de 480 donde se va a hablar de tecnología y que creo que os va a interesar. Sobre todo el primer programa donde Andrés Torrubia y un servidor estamos ahí conversando sobre inteligencia artificial. Así que si queréis escucharlo os voy a dejar también el enlace abajo en la cajita de descripción para que le echéis un vistazo. Mientras tanto, nos vemos con más inteligencia artificial la próxima semana.
DOC0045|NLP e IA|Ok, sigamos con los Transformers. Noticia de hace unos días. Nvidia y Microsoft presentan un nuevo modelo del lenguaje que triplica en número de parámetros al famosísimo GPT-3, un modelo que, siguiendo el juego del nombre de los Transformers, ha sido bautizado como Megatron Turin, y cuyas capacidades podemos ver en esta demo en directo del pasado GTC. Y es gracias a una arquitectura como los Transformers que estamos consiguiendo desarrollar tecnologías tan impresionantes como estas. Enormes modelos de lenguaje tipo GPT-3 o Megatron Turin que han revolucionado por completo el campo del procesamiento del lenguaje natural, sistemas como Alphafol para la predicción del plegamento de proteínas, o cosas tan impresionantes como Dali o VQ Gammas Clip. De hecho, casi cualquier revolución que hemos visto en 2021 en el campo del deep learning tiene por detrás funcionando a un Transformer. ¿Por qué? No, de verdad, quiero que lo penséis. ¿Cuál es ese aspecto técnico que hace que los Transformers tengan el rendimiento que realmente demuestran? ¿Qué ingrediente secreto hace que esta tecnología esté suponiendo una revolución dentro de la revolución? En el vídeo anterior de esta serie ya os presenté uno de los componentes principales, los mecanismos de atención. Un mecanismo que lata en el interior de esta arquitectura y que permite a la red poder aprender a prestar atención a los datos que usamos para el entrenamiento. Como digo, este mecanismo de prestar atención es una de las piezas fundamentales en el funcionamiento del Transformer. Pero, ¿y si te dijera que la clave no está ahí? Que en realidad, el motivo por el cual el Transformer es tan espectacular no es porque los mecanismos de atención sean tan buenos que tan bien, sino por otro motivo que hoy te vengo a explicar. Quédate y ponte cómodo porque hoy sí vas a conocer por fin qué aspecto de los Transformers hacen que funcionen tan bien. Aunque para llegar a esto antes tendremos que solucionar unos cuantos problemitas. Si buscáramos el origen de los Transformers en el famoso paper de donde surgió, Attention is all you need, nos encontraríamos con el siguiente diagrama. Un diagrama que creo que ya ha pasado a la historia del universo del deep learning y que hoy, bueno, pues que hoy lo vamos a trocear. Y de todas las partes en la que me quiero centrar hoy es en esta de aquí, la codificación posicional. ¿El motivo? Bueno, pues porque considero que esta piecita de aquí resuelve uno de los procesos fundamentales que hacen del Transformer una arquitectura tan impresionante. ¿Por qué? Bueno, eso ya lo sabrás al final del vídeo. Antes tenemos que entender cómo este módulo de procesamiento actúa exactamente sobre nuestras secuencias de entrada. Ya sabemos que cuando hablamos de Transformers realmente de lo que hablamos es de analizar secuencias de datos. Secuencias de acciones de un jugador que ejecuta en su teclado, secuencias de píxeles que conforman una imagen o secuencias de palabras que conforman a una frase, una frase como esta. Pues vamos a ver dónde comenzaría el viaje dentro de un Transformers. Como ocurre con toda red neuronal, el primer paso lo encontramos en su entrada, donde introducimos el input. Pues aquí, en la entrada del encoder, donde ocurre esta primera transformación. En nuestro caso, la frase que se va a procesar ya sabemos que realmente no viene representada por palabras como tal, sino que estas primeras son reconvertidas a vectores numéricos. Ya sabéis que tenéis este vídeo aquí para saber más sobre esto. Y aquí es donde viene una de las principales diferencias. Y es que si antes, cuando trabajamos con redes neuronales recurrentes, cada palabra era procesada una tras otra, ahora el Transformer se come toda la frase entera al mismo tiempo. Todas las palabras a la vez. Algo así como leer un libro ya no secuencialmente, palabra por palabra, sino absorbiendo de golpe todas las palabras de una página. Claro, siendo esto así, ¿cómo podríamos saber cuál es el orden en el que estas palabras están siendo presentadas? Me explico. Con las redes recurrentes, recordaréis que lo que hacíamos era conectar el output del procesamiento anterior con el input de la siguiente palabra. Así, la palabra número 3 de nuestra frase recibía información de la palabra número 2, que se había procesado anteriormente. Y esta de la primera. Es decir, hay una dependencia entre palabras que ya establece un orden. ¿Lo ves? El problema es que si ahora decidimos que no, que metemos todas las palabras a la vez, este orden se pierde, desaparece. Y lo mismo le daría al Transformer que la frase fuera esta de aquí o esta otra de aquí. Pero claro, creo que todos estaremos de acuerdo en que la forma en la que se ordenan las palabras dentro de una frase es muy importante para poder entender su significado. Entonces, ¿qué hacemos? Pues, puesto que estamos trabajando con vectores numéricos, la cosa no sería tan complicada, ¿no? Por ejemplo, si este vector es la primera palabra, podríamos sumarle otro vector de igual tamaño con todas sus componentes marcando la posición que ocupa. Aquí todos a 1, aquí todos a 2, aquí todos a 3, y así sucesivamente para darle de alguna forma información a la red neuronal de cuál es la posición que ocupa cada palabra. Hemos agregado información posicional y esto es una opción, pero no es la correcta. Lo que hemos propuesto hasta ahora es lo que se conoce como un posicionamiento absoluto, donde cada palabra tiene asociado un vector con justamente la posición que ocupa. 1, 2, 3, 4, 5, 6, y así hasta el número de palabras que tengamos en esa secuencia. Pero claro, esto es un problema porque si tuviéramos secuencias muy largas, por ejemplo de 200 palabras, el último vector que estaríamos sumando con información posicional sería un vector que estaría con todas sus componentes a 200. Estaríamos sumando esto al vector de dicha palabra y si nos damos cuenta por comparación relativa, la información del vector original acabaría siendo poco relevante. Bueno, vale, ok, si sumar vectores que tengan números muy grandes puede ser un problema, vamos a buscar una alternativa. La alternativa podría ser, bueno, pues que si tienes cinco palabras en toda tu secuencia, lo que vamos a hacer ahora es coger las componentes de los vectores posicionales y dividirlo por ese número. En este caso lo que estaríamos haciendo es normalizar estas componentes obteniendo los siguientes porcentajes que irían en el rango de 0 a 1. En la primera palabra sumaríamos 1 partido 5, en la segunda 2 partido 5, en la tercera 3 partido 5 y así sucesivamente. Pues solucionado, ahora los últimos vectores no tendrían valores tan grandes, serían pues cercanos a 1 y ya podríamos tener capturada la información posicional de cada palabra. Perfecto. Pues tampoco y es que siempre hay algún problema perjudicándonos por el camino. En este caso fijaros que para esta secuencia esto funciona perfectamente. Pero y si ahora introducimos una secuencia de longitud diferente, ¿qué pasaría? Para esta estrategia una secuencia de tres palabras tendría una codificación posicional de dos tercios de 0,66 para la segunda palabra. Sin embargo, para una secuencia de seis palabras, pues la palabra en cuarta posición tendría su vector posicional a cuatro sextos que también es 0,66. Entonces ¿qué le estaríamos diciendo a la red? ¿Que 0,66 es la segunda posición o la cuarta? Y así podríamos seguir probando codificaciones encontrándonos diferentes problemas en el camino hasta encontrar a la codificación ideal que sería la siguiente. Ok, volvamos para atrás. Sí, vamos a utilizar posicionamiento absoluto. Vamos a utilizar 1, 2, 3, 4, 5, 6 para identificar cuál es la posición de cada palabra. Pero en este caso si antes el problema era que componentes con valores muy altos podían afectar a la información del vector original, lo que vamos a hacer ahora es trabajar únicamente con unos y ceros. Por ejemplo con codificación binaria. Si antes lo que hacíamos para que por ejemplo este vector representara la posición número 3 era colocar todas sus componentes con dicho número 3, lo que vamos a hacer ahora es que este vector codifique en binario al número 3. 0000000111 ¿Quieres el número 5? Pues 0000000101. Y así lo único que tendríamos que hacer pues sería seguir codificando cada una de estas posiciones en binario. Y aquí cualquiera que haya tenido alguna vez que completar alguna de estas tablas de verdad con muchas posiciones representadas en binario se habrá fijado en el siguiente patrón. Pues que el primer dígito del número en binario, según vamos aumentando posiciones, irá alternando su valor de 0 a 1 y luego a 0 y luego a 1 y así sucesivamente. Y con el segundo dígito ocurre igual. Pero ahora alternando cada dos posiciones. 00110011. Y en la tercera posición lo mismo, pero ahora cada cuatro posiciones. 00001111. 00001111. Y este fue el típico truquillo que a mí me salvó mucho tiempo durante los exámenes de la carrera de informática. Porque al final esta alternancia que se produce a diferentes frecuencias era un atajo muy rápido con el cual podías construir todas las posiciones codificadas en binario. Y con esto ya parecería que lo tenemos todo solucionado. Ya tenemos codificada cuál es la posición de cada palabra. Y además cada vector no incluye números muy grandes que van a distorsionar la información del vector original. Solamente trabajamos con 0 y 1. ¿Es esta la solución correcta? Pues parece que sí. Así que corre, vete a contárselo a todo el mundo, ponlo en redes sociales, llama a tu familia y cuéntase, lo ponlo en los grupos de whatsapp. No hay que ser discreto. Este chiste es muy malo. No hay que ser discreto. ¿A qué me refiero con todo esto? Bueno, que no hay que ser discreto matemáticamente. Es decir, ahora mismo estamos viendo cómo existe un patrón discreto donde cada componente de nuestros vectores posicionales alterna su estado de forma discreta. 0 1 0 1 apagado encendido apagado encendido o en esta columna pues 0 0 1 1 apagado apagado encendido encendido. Estados discretos que se van alternando. Pero la naturaleza continua de las redes neuronales nos invitan a representar estos cambios de estados de otra forma, con ondas. Si te das cuenta, este patrón que hemos identificado en nuestra codificación binaria en realidad se podría representar de forma continua como ondas que vibran a diferentes frecuencias. 0 1 0 1 0 1 es una onda que vibra a una frecuencia muy alta y 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 una onda que vibra a menor frecuencia. Ondas cuya frecuencia de vibración varía según qué componente del vector posicional estemos mirando. Esta intuición que hemos desarrollado hasta ahora nos lleva un resultado compacto y elegante y es que toda la codificación posicional que nos indicará la posición de cada palabra en nuestra secuencia se podría calcular con una única función sinusoidal como esta. La idea es la misma que antes cuando trabajamos con 0 y 1, pero ahora todo esto se resume en esta fórmula de aquí. En el paper original del transformer, si lo buscáis, esto en realidad es un poquito más complejo con una combinación de senos y cosenos, pero donde la intuición de lo que hemos visto hoy se mantiene. Quiero remarcar que todavía existen algunas consideraciones más que se han tenido en cuenta en el diseño de esta codificación posicional, pero que no vamos a entrar a ver en este vídeo porque ya implicaría entrar a estudiar algunas matemáticas un poquito más pesadas. Os voy a dejar abajo en la caja de descripción un enlace a un artículo donde está todo perfectamente detallado al completo y que podéis leer si queréis profundizar más en todo este tema. Pero para hoy me basta con que entendáis que es este token posicional con el cual podemos informar al transformer de cuál es el orden de los diferentes tokens, de por ejemplo las diferentes palabras que componen a nuestra frase, a nuestra secuencia y que esto es algo fundamental. Una formulación basada en senos y cosenos que permiten a la red neuronal de nuevo poder entender cuál es el orden de las palabras suministradas como input. Y esto considero yo que dentro del transformer es uno de los elementos clave. ¿Por qué? Porque una secuencia sin orden es caos y esta ordenación en los modelos recurrentes se conseguía como hemos visto a través de la dependencia de procesar una palabra, luego otra, luego otra, luego otra y así sucesivamente. Aquí está el problema. Lo que estamos viendo es que en una red neuronal recurrente existe una dependencia por la cual cada palabra tiene que esperar a que se hayan procesado las palabras anteriores para llegar a su procesamiento. Algo que difiere por completo a lo que ocurre en un transformer. Y ahí es donde el transformer gana la partida, porque al procesar todas las palabras al mismo tiempo este procesamiento se puede paralelizar. Podemos sacar rendimiento a nuestras GPUs, a nuestras TPUs, a nuestras no sé qué PUS. Podemos sacar el rendimiento de la alta paralelización de los procesadores modernos haciendo que estas arquitecturas sean capaces de procesar una mayor cantidad de datos, un mayor número de secuencias y ser mucho más eficientes en su entrenamiento. Creo que ahora ya entenderás mejor de dónde surgen estos enormes modelos del lenguaje como GPT-3 o Megatron Turing. Enormes modelos del lenguaje que tienen una gran cantidad de parámetros y que han sido entrenados pues con enormes datasets. Unos entrenamientos masivos que no hubieran sido posibles sin una arquitectura tan paralelizable como el transformer. Algo que con las redes recurrentes en 2015-2016 no era posible. Dejar atrás a las redes recurrentes para apostar por arquitecturas altamente paralelizables basadas en mecanismos de tensión pues implicaba entonces perder esta información posicional que venía por la dependencia secuencial del procesamiento. Algo que se ha solucionado a través de la codificación posicional que hemos visto hoy. Así ya sabes que la cosa va de ondas. La próxima vez que veas el diagrama de los transformers ya podrás entender qué significa este simbolito de aquí y que ha permitido que en el enorme potencial que se esconden dentro de los mecanismos de tensión de un transformer todavía pueda existir algo de orden. La verdad es que es maravilloso poder entender cuáles son las matemáticas que están tras tecnologías tan impresionantes como GPT-3 o modelos que pueden por ejemplo aprender a programar. Y sobre este último tema sobre inteligencias artificiales que aprenden a programar y cuáles son sus implicaciones es de lo que voy a estar hablando la semana que viene. Me han invitado a participar en el evento anual para desarrolladores que organiza el equipo de Samsung Dev Spain. El 18 de noviembre por la tarde voy a estar dando una ponencia sobre este tema junto a una agenda completísima donde se va a estar hablando de mucha tecnología. Si queréis ver estas charlas, la mía y la del resto de compañeros y todas las sorpresas que va a haber en este evento os informo de que los registros ya están abiertos, es gratuito y simplemente tenéis que acudir a la página de Samsung, os la dejo abajo en la caja de descripción y os apuntáis y listo. Vale, la tarde del 18 de noviembre tenemos una cita así que apuntadlo bien en el calendario y disfrutemos de la tecnología como hacemos siempre juntos. Si te ha gustado este vídeo recuerda que puedes apoyar la actividad de este canal a través de Patreon, es la forma más directa que tienes de apoyar todo el contenido que hago, sería el equivalente invitarme a una caña pero versión digital. Tenéis el enlace abajo en la cajita de descripción y es el apoyo de todos vosotros lo que permite que este canal pues siga estando vivo y siga trayendo contenidos como este. Vuestro apoyo se convierte en estos vídeos. Mola. Chicos, chicas, me despido, nos vemos con más inteligencia artificial la próxima semana. Podéis ver mientras este vídeo de aquí que está muy chulo, no sé de qué va, te lo recomendado YouTube a ti porque te conoces mejor que yo. Este vídeo te va a gustar mucho.
DOC0046|NLP e IA|Ya es una realidad, GPT-3 está disponible para todo el mundo. Sí, para todo el mundo, incluso para ti. Si quieres puedes empezar a utilizar hoy esta increíble inteligencia artificial. Para quien no lo recuerde, cuando hablamos de GPT-3 estamos hablando de un enorme modelo del lenguaje, una inteligencia artificial que está entrenada para que, bueno, pues para una secuencia de texto que el usuario puede suministrar, ésta aprenda a predecir cómo tiene que seguir completándolo. Este sistema se hizo muy popular en 2020 cuando OpenAI, la empresa que está detrás de esto, pues publicó sus resultados y bueno, pudimos ver que este enorme modelo, esta enorme red neuronal, pues por primera vez estaba demostrando un rendimiento superior, algo nunca visto antes, a la hora de generar lenguaje natural, el lenguaje con el que tú y yo nos comunicamos. Aquí en el canal y en internet habréis podido ver un montón de ejemplos de cómo se puede utilizar GPT-3, por ejemplo, para generar textos, para generar conversaciones realistas entre diferentes personajes que podemos especificar, para escribir artículos periodísticos, para redactar poesía e incluso para resolver un montón de tareas para la cual esta inteligencia artificial de partida no estaba entrenada. GPT-3 fue impresionante y desde el minuto cero esto ha sido una tecnología que ha estado, bueno, pues cerrada al público general puesto que solamente se podía acceder a través de una API privada. Pero ahora esto se ha acabado y es que OpenAI ya por fin, en un tweet que publicó esta semana, pues ha confirmado que el acceso a la API ya está disponible para todo el mundo. Ya cualquier persona se puede loguear a través de la página de OpenAI y puede acceder a los diferentes servicios que ofrecen a través de su API que podéis utilizar para vuestros proyectos o para directamente investigar cómo funciona esta inteligencia artificial. Sinceramente creo que estamos en un punto de inflexión muy interesante y es que abrir el acceso a esta tecnología va a permitir que mucha más gente pueda estar en contacto por primera vez con un modelo tan potente como este, que tiene sus claros y sus oscuros, que funciona muy bien a veces, pero también en otras ocasiones funciona muy mal. Hoy os voy a contar una serie de trucos, consejos e informaciones que creo que tenéis que manejar antes de empezar a trabajar con esta API. Quiero que este vídeo sirva para que si conoceis gente que le pueda interesar esta tecnología, pues lo compartáis y así todos empezar a marcar un poco este punto de inflexión que representa que una tecnología como esta esté tan disponible ahora para el público. ¿Cómo empezamos con esto? Pues como digo el acceso a GPT-3 es bastante sencillo, simplemente tenéis que loguearos en la web de OpenAI, completar una serie de datos personales. Por algún motivo creo que nos están dando acceso a ciertos países como Perú, no sé exactamente por qué, pero me imagino que no costará mucho sortear este tipo de barreras geográficas que a veces se colocan en este tipo de herramientas. Luego tendréis que seleccionar cuál es el motivo por el cual queréis utilizar esta API, parece que dependiendo del motivo que elijas podrás tener más acceso o menos acceso a las herramientas, pero algunos de ellos implica tener que comunicarte con el equipo e informarles de para qué vas a utilizar la API. Así que aquí cada uno elegir la propuesta que mejor os convenga. Y a partir de este momento se te abren las puertas. Tienes acceso a GPT-3. En este punto la forma más rápida de empezar a jugar con GPT-3 es a través del Playground. El Playground es este entorno que habréis visto muchas veces en mi canal de YouTube donde yo he interactuado con el sistema y es básicamente esta ventana que tenemos aquí donde podéis escribir cualquier texto como input, darle a control enter y que GPT-3 continúe con la generación. Por ejemplo, GPT-3 es un enorme modelo de lenguaje capaz de simular el lenguaje natural, ha sido desarrollado por OpenAI y cuenta con una enorme base de datos de más de 19 millones de entradas que se encuentra dentro de un repositorio público. Con esto hay muchas cosas que comentar sobre el funcionamiento de GPT-3. Quien haya visto los vídeos previos que he ido publicando en el canal durante el último año sabrá que este sistema no se tiene que basar en una realidad que sea rigurosa, sino que en este caso lo que va a hacer es crearte un texto que en la forma, no en el fondo, pues parezca coherente. Tú esto se lo puedes leer a cualquier persona y ella dirá vale, tiene sentido, pero tiene sentido desde un punto de vista de la forma, no del fondo. GPT-3 no tiene por qué darte datos correctos y esto es muy importante tenerlo en cuenta. Otro detalle interesante es que fijaos que yo me estoy comunicando con GPT-3 en español. Esto es un modelo de lenguaje que principalmente está entrenado con texto en inglés, pero eso no nos impide poder comunicarnos con él en español. Todavía no se tiene muy claro si el rendimiento de GPT-3 es mejor o peor en inglés que en español o en español que en inglés o si realmente su capacidad de generar lenguaje trasciende al propio idioma y luego puede comunicarse traduciendo, por ejemplo, del inglés que sería su lenguaje nativo al español y con esto podría generar un texto que fuera realmente bueno incluso en otros idiomas. Pero ya útil, podéis utilizarlo en español si así queréis y por lo general para mí los resultados que me ha proporcionado son bastante buenos incluso trabajando con mi propio idioma. Y aquí hay otro detalle, GPT-3 es muy bueno adaptándose al input que le hayas dado. Si el texto que le has proporcionado es rico en detalles, con frases complejas y bien construidas, el texto que te va a generar GPT-3 por lo general será un texto rico en detalle, con frases más complejas y bien construidas. Si por el contrario tu input se basa en un texto, en una conversación más simple y con poca profundidad, pues GPT-3 se va a comunicar así. Así que en muchas ocasiones si el resultado que obtienes no es tan bueno, pues preocupate en ver si el input que le estás proporcionando realmente se asemeja a lo que quieres que genere. Una conversación de ascensor como esta te va a generar una conversación de más... Wow, no me esperaba esta respuesta. Porque ya tenemos aquí a GPT-3 que es un robot que habla nuestra lengua, que tiene una inteligencia artificial que habla nuestro idioma y que además ha venido desde España, desde Barcelona. Bueno, no me esperaba este tipo de respuesta, ¿vale? Pero por lo general suelen ser... Vale, oh mira, GPT-4. Bueno, lo que está claro es que os lo vais a pasar bastante bien con esto. Si por el contrario le planteáis una conversación un poquito más desarrollada, pues ahí podemos obtener respuestas también más interesantes. Desde luego la definición de la inteligencia artificial es un tema amplio y complejo. Podríamos decir que la inteligencia artificial es el conjunto de sistemas de orden superior y resolver problemas, bla bla bla. ¿Veis la diferencia? El input, el contexto en este caso es fundamental. Si nos fijamos en la derecha tenemos un menú donde podemos configurar además el comportamiento que queremos que tenga GPT-3. Tenemos parámetros como la temperatura que nos permitiría, por ejemplo, pues tener respuestas más originales y creativas, un poco fuera de la distribución normal de respuestas que podría dar este sistema o la longitud de las respuestas, cuántos tokens queremos que se generen o otros criterios por los cuales podemos configurar un poco el resultado que queremos obtener. De todos ellos, el que creo que es más interesante explicar es este de aquí, que es el engine. Realmente GPT-3 no es una única inteligencia artificial, sino es una familia de modelos. Tenemos dos familias de modelos y para cada uno de ellos tenemos diferentes tamaños de GPT-3. Para cada grupo tenemos ordenados de mayor a menor diferentes tamaños de modelos, donde los mayores serán más potentes, te darán mejores respuestas a tus problemas, serán más lentos también a la hora de generar sus resultados y también más caros. Porque sí, esto cuesta dinero. Ahora vamos a ver ese punto. Modelos como Hada serán más baratos, más rápidos y estarán destinados, por ejemplo, a resolver tareas más sencillas. Sin embargo, de este menú lo que me gustaría comentar es la diferencia entre los modelos base de los modelos instrag. Esto es algo que no he comentado en el canal de YouTube puesto que es una incorporación de los últimos meses, pero es muy interesante. En el modelo habitual de GPT-3 muchos sabréis que lo importante es cómo defines el contexto del problema que quieres que resuelva. En este caso podéis ver que si yo por ejemplo quiero crear un sistema que pueda responder automáticamente e-mails en base a una serie de topics que yo puedo definir, lo que tendría que hacer sería primero mostrarle ejemplos a GPT-3 de lo que quiero que haga. En este caso podría ser una serie, un listado de topics y redactar un e-mail que se parezca a lo que yo quiero que este sistema genere. Esto me permitiría que por ejemplo ahora yo pueda redactar una serie de topics y darle a generar y que GPT-3 pueda continuar completando la tarea que yo le he especificado. No estoy contento con la última campaña, no se han cumplido los términos del contrato, tendréis noticias de los abogados de mi agencia, estoy disponible para encontrar una solución amistosa y ahora yo podría coger aquí, siguiendo el formato de antes, poner e-mail dos puntos y que GPT-3 intenta ver si resuelve esta tarea, me genere un e-mail. Buenas tardes, no estoy contento con las campañas pasadas y por ello informo que no estoy dispuesto a continuar con las mismas condiciones. No se han cumplido los términos de nuestro contrato y por ello informo que os enviaré noticias de mi agencia. Estoy disponible para encontrar una solución amistosa, pero eso es algo que no depende de mí. Un saludo, Carlos Santana Vella. Vemos que más o menos GPT-3 ha podido hacer esa tarea que nosotros le hemos pedido, pero siempre ha apoyado en cómo nosotros hemos planteado el contexto anterior. Algo que además nos obliga a jugar bastante con el input. A lo mejor este formato con el que yo he planteado este problema no es el correcto y habría alguna otra forma diferente o a lo mejor hace falta más ejemplo de emails para llegar a una solución que a mí me satisfaga. Pero esto se ha vuelto mucho más sencillo con estas nuevas versiones de GPT-3 que son los modos Instruct, que están todavía en beta, y donde la cosa cambia por completo. En este caso los modos Instruct están pensados para que tú puedas plantearle lo que quieres que haga el modelo directamente como una instrucción, como una orden y no tanto basándote en ejemplo de cómo quieres que sea ese problema. En este caso lo único que le voy a pasar como input es la instrucción de que quiero que redacte un email basándose en los siguientes topics. Y vamos a ver qué tal desempeña su trabajo, a ver si sabe que tiene que escribir un email. En este caso, pues mira, podemos ver que sí, lo ha hecho en inglés. Le voy a pedir aquí ahora que lo haga en español. Redáctalo en español. Y vemos como el sistema empieza a obedecer a las instrucciones que nosotros le damos como input. En este caso el email es mucho más sencillo, yo creo que para esta tarea pues sí tendría sentido darle un email de ejemplo. Pero podéis ver cómo podéis trabajar con esta versión más orientada a plantear cada problema como una serie de instrucciones más que una serie de ejemplos. Antes en GPT-3 si tú querías hacer un diálogo pues tenías que empezar planteando el esquema de un diálogo, ahora con este sistema pues simplemente le tienes que dar la instrucción de quiero que desarrolles un diálogo filosófico que esté centrado en el origen del universo. Uno de los personajes será QuantumFracture, un importante youtuber español de física. Vamos a ver si funciona. QuantumFracture, ¿sabes que el origen del universo no es ni materia ni energía? ¿Qué es entonces? El origen del universo no tiene explicación. Nosotros lo llamamos el Big Bang y eso es todo. Bla bla bla. No sé si os dais cuenta de que ya todos tenéis acceso a esta tecnología y que podéis empezar a integrarlo por ejemplo con herramientas como Unity para crear videojuegos mucho más interesante donde puedas tener conversaciones con personajes que sean como estas y no sé, madre mía, se vienen cosas muy muy chulas. Y esta es la cosa, si queréis integrarlo en vuestros proyectos esto es muy sencillo. En este caso estamos trabajando a través del Playground que nos ofrece la página web de OpenAI pero fácilmente, por ejemplo, para esta configuración que tenemos aquí podemos irnos a este botón de aquí arriba y darle a View Code y ya nos van a mostrar directamente lo que sería el código necesario para, por ejemplo, en Python o cualquier otro tipo de alternativa. El código necesario para poder hacer la llamada a esta API y empezar a trabajar dentro del código con las respuestas que genere GPT-3. Si por ejemplo quisiera conectarme desde WorldCollab, pues primero tendría que instalar la librería de OpenAI. Una vez ya lo tenemos instalado pues simplemente sería conectarnos a la API de OpenAI, lo único que tendríamos que configurar aquí sería nuestra API Key, nuestra clave para conectarnos a la API, algo que es secreto y evidentemente no tenemos que compartir. Esto lo vamos a encontrar en la web de OpenAI en la documentación, aquí tendríamos pues para copiarla, nos la llevaríamos al código y una vez ya lo tengamos todo configurado pues es hacer peticiones a la API con la configuración que nosotros queramos. Podemos trabajar desde el Playground, buscar ahí la configuración que mejor nos convenga y luego copiar el código que ellos nos proporcionan. En este caso si lo ejecuto pues podemos ver que para la instrucción que habíamos puesto antes en el Engine de DaVinci Instruct Beta pues tenemos la respuesta, en este caso pues el debate en este discurso. Con esto así de sencillo podéis empezar a integrar esto en vuestro código. ¿Significa esto que ahora podemos utilizar esta tecnología en nuestros proyectos sin ninguna limitación? Sí, pero bueno, limitación sí hay y una de ellas es bastante importante y es el precio. Si os habéis loqueado en la página web de OpenAI y ya estáis jugando con todo este sistema tranquilos, no os van a cobrar porque OpenAI amablemente nos permite gastar hasta 18 dólares que ellos nos proporcionan de manera gratuita para probar el sistema y ver que realmente funciona y que nos interesa. Pero una vez que se agoten estos 18 dólares pues evidentemente tocará pagar por este servicio y esto es normal. Pensad que para que un sistema de inteligencia artificial como gbt3 funcione en una API como esta, pues la infraestructura necesaria tiene que ser enorme y tiene un coste bastante alto puesto que este sistema está desplegado en varias GPUs al mismo tiempo y ya os digo yo que esto por mucho Microsoft Azure que haya por detrás no es barato. ¿Cuánto cuesta? Pues depende del modelo que quieras utilizar. Los modelos más rápidos y menos potentes como el Ada o el Babbage pues tienen un precio de 0,0008 dólares por mil tokens y los más caros, el gbt3 más potente, el DaVinci, pues tiene un precio de 0,06 dólares por mil tokens. ¿Y a qué se refieren con estos tokens? Pues bueno, si hubiera visto mi vídeo sobre cómo funcionan estos sistemas de procesamiento de lenguaje natural lo sabrías, hay uno que se llama explícitamente de texto a tokens, pero tranquilo, te lo explico. Al final estos sistemas se basan en transformers que se nutren de secuencias de palabras que son dados como input y van a generar otra secuencia de palabras. Esta secuencia de palabras las podemos subdividir por tokens. Estos tokens podrían ser una frase entera, podría ser una palabra o en el caso de gbt3 podrían ser trozos de palabras. En este caso la gente de OpenAI ha compartido una herramienta muy interesante que es esto de aquí, el tokenizador, para poder comprobar cuántos tokens conforman a tu input y así poderte hacer una idea de cuánto te podría costar. Aquí podría poner pues Hola soy Carlos este tokenizador y aquí abajo podríamos ver que esto se compone de 13 tokens donde la palabra está dividida de esta manera. Hola soy Carlos y estoy probando este tokenizador. Así que ya sabéis, depende del modelo que queráis utilizar y de cuántos tokens conformen a tu input te costará más o menos. Un poco la conversión que podríamos tener en la cabeza es la que nos comentan por aquí. Puedes pensar en los tokens como trozos de palabras donde mil tokens son más o menos 750 palabras en inglés. Este párrafo por ejemplo pues serían 35 tokens y para que os hagáis una idea pues os voy a enseñar un poco cuál ha sido el consumo que yo he hecho durante la última hora en la grabación de este vídeo. Todos los ejemplos que habéis visto pues cuántos tokens ha consumido y cuántos dólares me han costado. En este caso podemos ver en esta gráfica de aquí que esto ha sido un total de más o menos 7000 tokens donde parte de ellos la mayoría ha ido por input y parte de ello lo que está morado ha sido la completación que ha hecho GPT 3. Tanto el input como lo que te completa GPT 3 ese número de tokens son los que se te van a cobrar y en este caso esto suma un total de 0 42 dólares lo cual realmente es un precio bastante asequible para poder al menos a nivel individual hacer proyectos interesantes. La cosa se vuelve más complicada si esto después lo conviertes tú en un producto donde estás haciendo uso de esta API y lo pones al servicio de muchos usuarios pues por ejemplo pensemos un videojuego donde cualquier persona pueda interactuar con los personajes y por tanto todos estén consumiendo de tu API. Ahí es donde vas a tener que tener un control real de los gastos que puede suponer estar utilizando GPT 3. Pero esa no es la única limitación que vas a tener si quieres hacer una aplicación que se va a convertir en algo real que la gente pueda utilizar antes vas a tener que pasar por la revisión de la gente de OpenAI y es que la apertura de la API evidentemente lleva ligado una serie de controles muy importantes ya que a OpenAI le ha preocupado desde el primer momento que no haya un mal uso de esta tecnología y por tanto vas a tener que pasar por la supervisión del equipo detrás de todo esto. Es por eso que os recomiendo muchísimo que si no queréis ser baneados de la API de GPT 3 dediquéis un tiempo importante a leer la documentación sobre cuáles son las políticas de lanzar una aplicación al mercado de cómo lo tenéis que utilizar de cómo podéis compartir resultados a través de internet todo esto está bien detallado en su página web y yo digo OpenAI se está tomando esto muy en serio y de haber abierto esta herramienta al público pues tiene unas fuertes medidas de control que todos deberíamos de respetar para que esto funcione como tiene que funcionar. Por ejemplo OpenAI estará bloqueando aplicaciones que se usen para generar tuits automáticos o post de Instagram chatbot que no estén limitados. Aplicaciones que se utilizan por ejemplo para generar artículos masivos que bueno se quieran colocar en post donde se quiera mejorar el SEO de una forma automática. Es decir cualquier mal uso que se puede imaginar para esta aplicación OpenAI va a estar persiguiendo así que de nuevo dedícale tiempo a leer la documentación. Y mi último consejo es que dediquéis un tiempo a ver todos los ejemplos que os proporcionan en la web de OpenAI. Hay un montón de ejemplos de uso de cómo podéis utilizar esto para ser traductores para ser sistemas de lenguaje natural para hacer clasificación para convertir una película a un emoji. Montón de ejemplos super interesantes y originales y creativos que os van a permitir entender mejor cómo funciona este sistema. Lo bonito que ha tenido GPT-3 desde el comienzo es que ha sido una enorme inteligencia artificial que entre todos hemos ido explorando cuáles son sus capacidades a través de jugar con el input y ahora todo el mundo tiene la posibilidad de acceder a esto e integrarlo en sus proyectos. Creo que estamos entrando en una fase muy interesante donde por primera vez tenemos un acceso masivo a una herramienta tan potente como GPT-3 y ahora es vuestro turno. Quiero que desarrolléis cosas que aprovechéis este sistema pues para potenciar vuestras ideas vuestros productos y que si hacéis algo interesante pues que quiero que lo compartáis y que lo pongáis en redes sociales, me etiquetéis y yo así le echaré un vistazo y lo compartiré también para que el resto de las personas lo veáis. Este vídeo pretende ser además un impulso a toda esta ola que OpenAI está motivando así que si conocéis gente que pueda desconocer de esta tecnología y que podría aplicarla de forma muy interesante pues os invito a compartir este vídeo en vuestras comunidades desarrolladores, grupos de whatsapp o amigos que conozcáis de la Facultad de Informática. Yo soy DotSesue, ya sabéis que aquí hemos hablado de Inteligencia Artificial durante mucho tiempo y hemos hablado de estos sistemas. Tenéis un montón de vídeos para entender cómo funciona por detrás, de hecho el vídeo del domingo pasado era parte de la explicación que nos permite entender de dónde surge el éxito de sistemas como GPT-3, por qué los Transformers han posibilitado tener tecnologías como esta en muy poquito tiempo. Sé que es un vídeo que por las estadísticas mucha gente no ha visto y en parte es porque YouTube no lo ha notificado así que os aviso que si queréis entender mejor cómo funcionan los Transformers lo tenéis disponible en mi canal, podéis cliccarme por aquí justo ya que estamos terminando y con esto podéis verlo. Chicos, chicas, esto es la revolución de la IA, está no empezando sino continuando y aquí en DotSesue os la seguiré comentando. Muchas gracias y nos vemos en el próximo vídeo.
DOC0047|NLP e IA|Este vídeo está patrocinado por la Universitat Politécnica de València. Imagínate que eres un ingeniero al que se le pide que para un texto dado de un tweet, por ejemplo, pues tienes que reconocer si ese texto tiene un sentimiento positivo o un sentimiento negativo. Esto es un problema de análisis de sentimiento y es un problema que se puede reducir al final a un problema de clasificación. Clasificame para esta secuencia de palabras si el sentimiento identificado es positivo o negativo. ¿Cómo lo resolverías? Pues si tu respuesta es que siendo un texto de Twitter siempre va a ser negativo, jaja, alabo tu humor, pero fuera de mi clase, ¿vale? Y es que te lo pregunto en serio, ¿cómo lo resolverías? Pues mira, si estuviéramos a comienzos de siglo allá por los dos miles, seguramente esto lo resolveríamos a través del análisis de la polaridad de las palabras. Buscaríamos un vocabulario de buenas palabras, otro de malas palabras, y asignando puntuaciones a cada palabra de un texto, pues acabaríamos con una puntuación total del sentimiento de la frase. ¿Lo ves? Es una solución bastante sencilla, pero no es óptima, ya que frases como esta compra es muy recomendable, si quieres tirar el dinero, podría apuntar una frase positiva cuando en realidad no lo es. Y rápidamente lo vamos a ver en código. Mira, en este caso, pues yo puedo coger una frase, procesarla con esta librería y con esto podemos extraer rápidamente cuál es su sentimiento. Podemos ver que una frase como me encanta comer en este restaurante, la ejecutamos, pues tiene un sentimiento positivo. Y si esto lo cambio por odio comer en este restaurante y lo ejecutamos, pues esto se analiza con sentimiento negativo. Pero claro, este análisis se queda insuficiente cuando usamos la ironía, por ejemplo, y para frases como me encantaría ir a este restaurante si quiero tirar el dinero, pues esta librería podemos ver que lo va a analizar también con sentimiento positivo. Hay forma de hacer este análisis más avanzado, teniendo en cuenta, por ejemplo, el contexto de las palabras. Pero ¿para qué? Si ya estábamos en 2012 y el boom de las redes neuronales acababa de comenzar. Y es que con la llegada del deep learning y las redes neuronales allá por 2012, la cosa se volvió más sofisticada. Ahora podríamos usar redes neuronales especializadas para el análisis de texto, como las redes neuronales recurrentes, para que éstas fueran aprendiendo cuál era la relación entre una secuencia de palabras dada como input y su sentimiento, dado como output. Esto era aprendizaje supervisado. Y claro, lo que nos obligaba era tener un dataset específico para este tipo de problema, es decir, un montón de ejemplos de tweets donde un humano se hubiera sentado a marcar manualmente si el sentimiento de ese texto era positivo o negativo. Ya sabéis, el típico esquema de entrenamiento supervisado, pero cambio de planes. Y si quisiéramos ahora que el sentimiento no se clasifique de positivo o negativo, sino con una review de 1 a 5 estrellas. Ah, vale, bueno, pues esto te obliga de nuevo a sentar a tus humanos y pagarles, en el mejor de los casos, para que vuelvan a hacer este proceso de etiquetado, pero en este caso valorando cada texto de 1 a 5 estrellas. Un dataset que ahora utilizaremos para hacer de nuevo nuestro entrenamiento. Y aquí tenemos algo importante que quiero que entendáis, y es que por aquel entonces, entrenar a una red neuronal para una tarea diferente nos exigía tener un dataset concreto, da igual si esa tarea se parecía o no se parecía a lo que habíamos entrenado previamente. Una nueva tarea era un nuevo dataset y un nuevo entrenamiento. Algo que en 2017 empezaría a cambiar. Y es que en el campo de la visión por ordenador, que ya llevaba unos cuantos años de ventaja al campo del procesamiento del lenguaje natural, ya se conocían estrategias para poder reducir la cantidad de datos necesarios para entrenar a un sistema. Una buena estrategia era la de tomar como punto de partida a un modelo que ya hubiera sido entrenado con anterioridad. Un modelo preentrenado que ya supiera reconocer los patrones que conforman a coches, mascotas, vehículos, paisajes, un montón de imágenes que ahora podríamos aprovechar en nuestro propio beneficio para reentrenarlo. Eso sí, utilizando una menor cantidad de datos que le dieran ese ajuste final al modelo para que este pudiera hacer bien la tarea que nosotros quisiéramos. Y tiene sentido, ¿no? Al final es mucho más fácil aprender a reconocer patrones en una radiografía cuando sabes ver que tener que aprender a ver desde cero. Pues esta misma idea, que sabíamos que funcionaba bien en el ámbito de la visión por ordenador, fue la que se quiso llevar al procesamiento del lenguaje. A partir de 2017, y coincidiendo con la llegada de los transformers, que eran potentísimos modelos capaces de procesar secuencias de texto y comprender el lenguaje mejor que cualquier otra red neuronal anterior, pues empezamos a entrenar modelos de inteligencia artificial para aprender a hacer exactamente eso, a entender el lenguaje. Así, tal cual, toma tarea difícil. ¿Cómo podemos hacer que una inteligencia artificial pueda aprender el lenguaje? Bueno, pues de forma similar a cómo lo hacemos nosotros en el colegio cuando queremos aprender un lenguaje, y es sacando el workbook y practicando. Por ejemplo, para algunos modelos se cogían frases y se enmascaraban algunas palabras, y el objetivo de la inteligencia artificial era aprender a rellenar dichos huecos, encontrar la palabra más probable. U otros modelos, pues estaban especializados en tomar una frase y aprender a cómo finalizarla. Ejercicios sencillos, pero claro que repetidos millones y millones de veces durante días y días de entrenamiento, pues conseguían que estos transformers acabaran por aprender cuál era la estructura de nuestro lenguaje. Estábamos entrenando los primeros modelos del lenguaje basados en transformers. Y el cambio de paradigma aquí es importantísimo, porque si recordáis, antes, para nuestra tarea, pues necesitábamos de muchos humanos esclavizados que fueran etiquetando todo nuestro dataset para poder hacer así el entrenamiento. Pero fijaos que ahora nuestro esfuerzo se reduce a prácticamente cero. Fíjate que para estas actividades que hemos fijado, lo único que necesitamos es coger frases de internet y de forma automática ir borrando palabras o separar entre lo que sería el input y la siguiente palabra predecir. Todo esto gratis. El cambio de paradigma aquí es que ya no necesitábamos dedicar tantísimos recursos para etiquetar manualmente a nuestros datos. Ahora podíamos establecer algún tipo de proceso donde automáticamente estas etiquetas fueran generadas, además sobre cualquier tipo de texto que quisiéramos. Estábamos pasando del aprendizaje supervisado al aprendizaje autosupervisado. Y como conseguir datos etiquetados ya no era un problema, la tendencia a partir de entonces fue la de ir aumentando los dataset de entrenamientos en órdenes de magnitud. Claro, ya no estábamos limitados por el etiquetado manual. Y así vio comienzo la era de los enormes modelos del lenguaje. Modelos entrenados con grandes corpus de textos tomados de todo internet para que de forma general aprendieran a entender cómo nos comunicamos, el significado de las frases, la estructura de nuestro lenguaje, todo. Y donde además se hizo habitual que estas grandes organizaciones que entrenaban a estos enormes modelos, pues gastando computación, infraestructura, talento en investigación, pues acabaran compartiéndolos gratis. Modelos gratuitos que tanto tú como yo podemos descargar y empezar a utilizar. Y aquí toca hacer reconocimiento de la comunidad de Hackingface, que se ha convertido en un portal donde se están recopilando todos estos grandes modelos que las grandes compañías están liberando, y que los han integrado todos en un repositorio donde fácilmente puede buscarlo, probarlos e incluso descargarlos y utilizarlos a través de su librería Transformers. Se puede ver aquí en su documentación cómo tienen un montón de modelos disponibles, de los más famosos de los últimos años tienen Ver, tienen Clip, Diverta, Distilbert, Image, GPT, T5, bueno, un montón de modelos que podéis utilizar automáticamente. ¿Cómo? Pues instalando su librería Transformers vamos a poder configurar rápidamente un pipeline donde se va a configurar todo para tokenizar nuestras secuencias, para descargar los modelos todo automáticamente, en este caso configurado para hacer análisis de sentimiento. Vamos a ejecutarlo y veremos cómo rápidamente la librería va a empezar a descargar por defecto un modelo de tipo Distilbert, y una vez se descarga pues podemos ver que cada una de estas frases directamente pasa a ser analizado. Está súper bien, sentimiento positivo, espero que no sea tan pesado como tener que programar todo desde cero negativo y negativo también la última. Fijaos que aquí estamos utilizando un modelo pre-entrenado, yo no lo estoy entrenando, sino que me estoy aprovechando del entrenamiento que otra organización ha hecho previamente para esta tarea de análisis de sentimientos, y de hecho si quisiera sacarle más rendimiento a este modelo pues podría aprovechar algún dataset etiquetado y darle un reentrenamiento, hacer fine tuning, para poder así adaptar mejor el modelo a la naturaleza de mi datos, algo que notaría más rendimiento y sin necesidad de tener que utilizar tanto datos como si fuéramos a hacer un entrenamiento desde cero. Mola. Y parecería que la historia se acaba aquí, ¿verdad? Ya hemos entrenado nuestro sistema analizador de sentimientos con muy muy buen rendimiento, además hemos reducido la cantidad de datos a utilizar, ¿qué más pedir? Bueno, llegamos a 2020 y aquí se desata la locura. Por aquel entonces los transformers estaban demostrando un rendimiento sobresaliente al analizar texto, y además permitían paralelizar mejor su entrenamiento sobre GPUs y TPUs, haciendo factible entrenarlos cada vez con más datos. Y como acabamos de ver, contar cada vez con más datos pues ya no era un problema, el límite era el cielo. Bueno, Internet. Todo el volumen de texto recogido de nuestras interacciones en webs, redes sociales, enciclopedias, todo era susceptible de ser utilizado como datos para un entrenamiento masivo. Y una empresa quiso intentarlo. Una empresa que ya había estado probando previamente a entrenar a sus modelos generativos pre-entrenados basados en transformers, o dicho más breve, a sus GPTs. Así, OpenAI en 2020, tras una primera y segunda versión, se le va la cabeza y entrena a GPT-3, el modelo del lenguaje más grande entrenado hasta aquel momento, con 175.000 millones de parámetros y 117 veces más grande que GPT-2. Y lo que sucedió después, pues no te sorprenderá, porque lo hemos comentado muchas veces aquí en el canal. GPT-3 se convirtió en un modelo muy bueno a la hora de generar lenguaje humano. Por primera vez contábamos con una inteligencia artificial que podía imitar la forma en la que nosotros nos comunicábamos. Pero además de esto, cosas interesantes sucedieron. Y es que, como hemos dicho antes, a GPT-3 y a este tipo de modelos se les entrenó con una tarea sencilla de encontrar una palabra o autocompletar texto. Nada más. Pero lo que se comprobó es que a estos modelos, cuando los escalabas a tamaños tan, tan grandes, pues en su tarea de aprender a autocompletar texto, también aprendían otras cosas. Tú podías, por ejemplo, escribir una frase como esta y luego añadirle esta frase en inglés sería y darla a autocompletar. Y de repente, GPT-3 te iba a demostrar que sabía traducir de español al inglés. Pero ojo, nunca había sido entrenado para esto. Y lo mismo sucedía si lo hacíamos en chino o en python. Ya muchos sabéis a través de este canal que estos enormes modelos del lenguaje son los que hacen funcionar a modelos como Copilot Codex o Alpha Code. O también podía actuar en forma de chatbot si tú le ponías el formato adecuado a modo de diálogo. Esto era una locura, porque, repito, en ningún momento se le había entrenado explícitamente para resolver ninguna de estas tareas. Su única tarea era autocompletar. Y así en muy poquitos años pues habíamos pasado a entrenar desde cero a modelos para cada una de las tareas que quisiéramos, a acabar con modelos que directamente y por motos propios habían aprendido todas estas tareas automáticamente. Lo único que habíamos tenido que hacer era escalar el tamaño del modelo. Ahora sí, esta era la revolución de los enormes modelos del lenguaje. ¿Y el analizador de sentimientos qué? Pues aquí viene el ejemplo más loco que mejor explica toda esta evolución que ha vivido el campo del deep learning en los últimos años, porque vamos a intentar aprovecharnos de estas dinámicas que parece que ha aprendido GPT-3 para ver si podemos resolver nuestro problema. ¿Cómo lo haría? Pues en este caso lo que haría sería crear algún tipo de dinámica con este enorme modelo del lenguaje para que entienda que lo que quiero hacer es analizar el sentimiento de estas frases. Por ejemplo, le podría poner un caso inicial donde sabríamos que esta primera frase está súper bien esta librería pues tiene un sentimiento positivo y ahora lo que vamos a hacer es dejar que GPT-3 pues haga la tarea que también sabe hacer que es autocompletar texto. En este caso para la frase espero que esto no sea tan pesado como tener que programar todo desde cero. Vamos a ver qué hace. Sentimiento y nos genera automáticamente entiende que tiene que ser un emoji que es un sentimiento pues neutro medio tristón y si por ejemplo ahora le pongo la frase de testo tener que hacer esto, pues va a entender que el sentimiento de esta frase es negativo y esto mola muchísimo porque en ningún momento a GPT-3 le estamos entrenando para que haga análisis de sentimientos simplemente es un enorme modelo del lenguaje que hemos escalado en su tarea de autocompletar texto que acaba pues aprendiendo a hacer un análisis de sentimiento y si le ponemos sentimientos diferentes pues por ejemplo tengo miedo de tener que hacer esto sentimiento ahí está nos sale el miedo o por ejemplo si le cambiamos la dinámica del problema no a tener que hacer ahora clasificación de una a cinco estrellas no pues vamos a probar a ver si ahora lo entiende de texto tener que hacer esto nos marca una estrella efectivamente vamos a probar con una frase positiva esto podría ser una frase de 3 4 estrellas máximo vamos a darle a ejecutar ahí estaría 4 estrellas hacer esto a ver sentimiento pues 12 estrellas y en realidad esto es tan versátil como vosotros queráis podéis aprovechar de estos enormes modelos del lenguaje para hacer clasificación de frases entre harry potter y el señor de los anillos pues tenemos estas frases de aquí y con esto pues gpt 3 debería saber que 10 puntos para greefindo eres de harry potter vamos a ver si lo entiende efectivamente y con esto pues ya podríamos probar con cualquier frase que queramos no pues cuenta con mi hacha del señor de los anillos podemos coger otra vez por aquí la comarca está en peligro también debería ser el señor de los anillos dovi es libre pues debería ser harry potter y así veis como rápidamente hemos creado un clasificador esto lo podéis probar y es un ejemplo de uso de gpt 3 que es uno de los enormes modelos del lenguaje que poco a poco van saliendo si queréis saber más de cómo poder utilizarlo pues ya sabéis que aquí en el canal tenemos un vídeo dedicado a esto y ojo este vídeo no pretende decirte que si quieres crear un analizador de sentimientos lo tengas que hacer de esta forma y es que lo correcto para garantizarnos una buena robustez y calidad de las predicciones pues sería coger a nuestros grandes modelos y reentrenarlos con datos etiquetados como hemos comentado anteriormente no el objetivo de este vídeo es otro si hoy te he contado esta breve historia que creo que ilustra muy bien los cambios tan bestiales que se han vivido en el mundo del deep learning en los últimos años es porque creo que explica muy bien también el futuro que está por venir y comprendiendo esto pues ya estás listo para entender todo lo que viene después qué pasaría por ejemplo si cogemos a estos enormes modelos del lenguaje y seguimos haciéndolos más grandes en el próximo vídeo entre otras muchas cosas os estaré hablando de palm que es un modelo creado por google que cuatriplica en tamaño al modelo gpt 3 un modelo que de nuevo escala se vuelve mucho más grande y que de él emergen nuevas propiedades que nos han vuelto a sorprender a todos de todo esto y de muchas otras cosas súper interesantes de estos enormes modelos del lenguaje seguiremos hablando en el próximo vídeo chicos chicas muchas gracias como siempre y nos vemos con más inteligencia artificial tecnología y ciencia aquí en dot csv
DOC0048|NLP e IA|Meta lo acaba de hacer. Acaban de sacar la segunda versión de su modelo Llama. Han sacado Llama 2, en un movimiento que rompe por completo todo el mercado de los modelos de generación de texto. Cogiendo la puerta que poco a poco durante los últimos años, OpenEJ ha ido cerrando y abriéndola de una patada por completo. Y además haciendo esto de la mano de Microsoft, quien ahora parece ver un aliado nuevo para la batalla de las Cías en Meta. ¿Por qué y quién se beneficia de todo esto? Y bueno, ¿cómo salí ganando vosotros en esta lucha de grandes empresas? Hoy vamos a estar comentando por qué la salida de Llama 2 es toda una revolución en el campo de la inteligencia artificial. Y hablando de grandes modelos del lenguaje y de todo el impacto que va a tener esta tecnología en el futuro inmediato, tengo que hablar de este máster de aquí, el máster ejecutivo en inteligencia artificial del IEA, el Instituto de Inteligencia Artificial, que ya hemos comentado aquí en el canal veces anteriores y que llega con una sexta edición, una edición que va a comenzar en el mes de octubre, que podéis matricular ya. Las plazas son limitadas, así que tenéis que ser rápidos. Y este es un máster que está muy bien orientado si queréis aprender sobre todo a esta revolución de la IEA desde un enfoque no técnico, vale, no hace falta ningún conocimiento previo para poder matricular, porque este máster está orientado sobre todo a aprovechar en proyectos reales, a saber cómo utilizar todo el potencial de esta tecnología en vuestros proyectos, a conocer las arquitecturas más importantes que están saliendo ahora y todo esto de la mano de gente muy experta, gente que sabe muy bien trabajar con toda esta tecnología como Andrés Torrubia, quien ha estado en el canal un montón de veces y que ya conocéis. Y luego también pues una serie de expertos que van a estar participando y que son gente protagonista de toda esta revolución que está sucediendo ahora. Tenemos gente como Cristóbal Valenzuela, CEO de Runway, ya sabéis qué empresa es. Tenemos gente de Hanging Face, tenemos gente de Twitter, tenemos gente de todo tipo de todas las especialidades, pues abogados, empresarios, inversores, toda la gente que pueda estar relacionado con toda esta revolución que está ocurriendo, todo aunado en este máster. Y bueno, luego a todos ellos, pues yo me sumo a la charla inaugural. Voy a estar dando esta ponencia de salida y ahí también tendremos oportunidad de conocernos. El máster es online, dura seis meses, tenéis la posibilidad de matricularos ya. Y además si lo hacéis, pues tenéis un código de descuento de 300 euros con el código DOTCCV300. Aprovechad la oportunidad porque este máster merece la pena. Para entender bien por qué Llamados es tan importante, primero tenéis que entender cuál ha sido el desarrollo, la historia, el culebrón que ha ido ocurriendo en los últimos meses. Estamos en febrero de 2023, hace unos meses, y ChadGPT ya lleva un tiempillo en nuestras vidas. Y mientras que OpenAI nos tiene impresionados por las capacidades de su modelo, hay gente en internet que se empieza a hacer la siguiente pregunta. ¿Podría la comunidad open source crear un modelo como ChadGPT pero que esté abierto y disponible para que todo el mundo lo use como quiera? Y la pregunta era legítima, ya que la moral de la comunidad estaba bastante arriba después de lo que había ocurrido en el año 22. Recordemos que cuando en abril de 2022 aparecieron los primeros modelos comerciales de generación de imágenes como Dali2, creíamos que 1. Estos modelos solo los podían hacer las grandes empresas como OpenAI o Google, y 2. Que bueno, de tener un modelo así disponible aún así sería imposible de utilizar y ejecutarlo en nuestro hardware habitual. En agosto de 2022, una empresa, Stability AI, movió ficha e hizo la inversión de entrenar a StableDiffusion, y lo compartió para uso y disfrute de la comunidad y el resto es historia. Entonces, con esa perspectiva ya en febrero de este año, la comunidad online estaba muy motivada para repetir la hazaña que se había logrado con StableDiffusion y Dali2, pero en este caso con ChadGPT. Y de ahí surgieron muchas iniciativas como la de Open Assistant que, como muchos recordaréis, estuvimos apoyando aquí desde este canal. Pero, claro, si StableDiffusion nació fue porque una empresa, Stability AI, decidió pagar la fiesta y hacer la inversión de pagar toda la computación necesaria. Y si ahora queremos tener a nuestro ChadGPT, pues alguien tenía que hacer esa inversión. Y aquí es donde aparece Meta y le pone la mano en el hombro a la comunidad y le dice, Tranquila, aquí tienes a tu modelo, aquí tienes a Yama. Bueno, en realidad el mensaje de Meta no fue tan épico y de hecho la historia que hay detrás de esto es bastante rocambolesca. En el caso de Yama, cuando Meta lo presenta, realmente lo que dicen es, por seguridad no vamos a liberar este modelo en internet, sino que solo lo compartiremos con aquellos investigadores que se registran en este formulario y de forma súper segura mandaremos el archivo por aquí. Una semana. Una semana tardó en filtrarse el modelo de Yama en internet. Y para darle más comedia al asunto, la forma en la que se filtró fue bastante de coña. Lo que pasó es que en el repositorio de GitHub, oficial de Meta, donde se compartía información del modelo, un usuario hizo un pull request donde daba una alternativa a eso de rellenar el formulario de seguridad para descargar de los servidores de Meta. Lo que ofrecía era un enlace a un torrent donde podías descargarte el modelo, tú y cualquier persona, según decía, para ahorrar consumo de ancho de banda. Internet es maravilloso. Y hay quien discute si realmente el modelo se filtró o se filtró de forma intencionada por Meta. No lo sabemos. Pero lo que sí sabemos es que Yama ya estaba en internet y ahora era turno de la comunidad open source. Y a todo esto a lo mejor te estarás preguntando, Carlos, ¿qué es Yama? Pues Yama es un enorme modelo del lenguaje, el LLM por sus siglas en inglés. Una inteligencia artificial entrenada con muchísimo texto y cuya tarea es aprender a predecir cuál es el siguiente trozo de palabra. Es decir, aprender el lenguaje y aprender a escribir. Y ojo, Yama no es chat GPT. Yama sería equivalente a un modelo como GPT-2, como GPT-3, como Palm de Google. Sería un modelo del lenguaje cuya única tarea es aprender a predecir la siguiente palabra. Lo bueno es que no es un chat GPT, pero para construir un chat GPT, tener acceso a este tipo de modelos es algo fundamental. Ya que luego, como vimos en este vídeo de aquí, una vez tienes a uno de estos modelos capaces de generar lenguaje y escribir correctamente, entrenarlo un poco más para que cumpla este rol de chatbot amigable que cumple instrucciones y que se rige dentro del marco de lo correcto y lo moral, pues no es tan complicado. GPT-3 o Yama serían estos modelos base, estos modelos fundacionales capaces de generar lenguaje, pero que todavía no serían ese chatbot funcional como chat GPT. Y aquí es donde a mediados de marzo aparecen modelos como Alpaca y Vicuña. Los primeros modelos que basados en Yama se empiezan a reentrenar para que cumplan instrucciones y actúen como el chatbot que a todos nos gustaría tener. Y si te das cuenta, todo esto estaba ocurriendo en el mes de marzo, al mismo tiempo que OpenAI estaba dando otro salto de gigante con la salida de GPT-4, y empresas como Anthropic pues estaban presentando a su modelo Claude. Modelos muy impresionantes, pero privados, que alejaban aún más el objetivo a conseguir. Y aunque la comunidad estaba consiguiendo tecnología impresionante para trabajar, pues Yama, Alpaca, Vicuña, si te das cuenta todavía estos modelos dependían de que grandes organizaciones como Meta, como Stanford, que tenían recursos computacionales suficientes, pues hicieron este preentrenamiento o reentrenamiento a posteriori. Seguíamos dependiendo de la computación para poder avanzar, algo que cambiaría con la llegada de Lora. Lora es una técnica que quizás usó en uno de los vídeos de Stable Diffusion, y es que se trata de una técnica que ganó bastante popularidad el año pasado al permitir coger un modelo como Stable Diffusion y reentrenarlo, hacerle fine tuning, a un costo computacional mucho más reducido que de la forma tradicional. Cuando tú tienes una red neuronada y quieres reentrenarla actualizando sus parámetros, tener que actualizar los millones y millones de parámetros que constituyen a un modelo como Yama puede llevar mucho tiempo y dinero. Y aquí la técnica de Lora lo que consigue es un reentrenamiento semejante, pero dedicando la computación solo actualizará un número muy inferior de parámetros. ¿Y qué ganamos con esto? Pues tiempo de reentrenamiento mucho más bajos y a menor costo. Estamos hablando que si quisieras hacer un fine tuning de un modelo como GPT-3 que tiene 175 mil millones de parámetros, Lora te permitiría hacer algo similar solamente actualizando 17 millones de parámetros, lo que sería una reducción de 10 mil veces. Esto, evidentemente, Lora Mola se merece una hola, un tsunami, e, e, no, eso no entra. Y no solo esto, sino que la agilidad de la comunidad Open Source también permitió que en semanas se pudiera implementar avances que a otras empresas le habían llevado meses. La integración con modelos de visión para implementar la famosa multimodalidad que todavía estamos esperando, el uso de herramientas semejante a los plugins de ChatGPT, ventanas de contexto más grandes o optimizaciones que permitían ejecutar estos modelos en tu propio ordenador o en un hardware más limitado como el de un móvil. Y tú ahora estarás pensando, pero Carlos, ¿esto no son enormes modelos de lenguaje? ¿Esto no son modelos con miles de millones de parámetros que ocupan muchísimo espacio en la memoria de la GPU y que por tanto no podríamos ejecutar en una tostadora? El poder ejecutar estos modelos en nuestro ordenador se está consiguiendo, y esto gracias a que la comunidad Open Source optimiza muy bien. Y aquí otra de las claves de los últimos meses están en las técnicas de cuantización. Esta es una técnica que te permite tomar los parámetros de tu red neuronal, que como sabéis son números decimales que ocupan un cierto espacio en tamaño de bits en memoria, y cambiarlo a otro tipo de dato que ocupe menos espacio, pues a lo mejor una cuarta parte menos. Así, a costa de algo de precisión y de rendimiento del modelo, podemos conseguir mejoras sustanciales en cuanto a ocupación de memoria, reduciendo fácilmente en 4 o en 8 el tamaño original del modelo, y permitiendo que cada vez más GPUs puedan ejecutar. Así, si os dais cuenta, el margen de separación entre los modelos Open Source y los modelos privados, como el chat GPT o VAR, en cuestión de meses se ha ido cerrando radicalmente, algo que quedó patente en este artículo de aquí, titulado No Tenemos Ventaja Competitiva y Tampoco OpenAI la Tiene, una carta presuntamente filtrada por un trabajador de Google, donde se reconocía que ni Google ni OpenAI iban a ser capaces de sostener en el tiempo la ventaja competitiva frente a la comunidad Open Source, principalmente motivada por la salida de llama en febrero de este año. Porque pensadlo, al ritmo al que se está moviendo la comunidad Open Source, no sería una locura que el año que viene tengamos un modelo como GPT-4 pero de libre acceso. Y en ese momento, imaginad que sois una empresa, que tenéis que decidir si queréis contratar los servicios de OpenAI, de Google, para mandar vuestros datos a una empresa de terceros, donde utilizar a uno de estos modelos privados. ¿Preferiríais eso o utilizar un modelo que podéis ejecutar en vuestro hardware, en vuestro ordenador, en vuestra empresa, cumpliendo la privacidad de vuestros datos y de vuestros clientes? Pues la pregunta no es tan fácil de responder. Y es que llama tiene un problema, y es que las empresas no podrían utilizarla porque la licencia no permitía uso comercial de este modelo. Por lo tanto, puesto que la licencia con la que se había liberado a llama no permitía el uso comercial, no tenemos ni llama, ni alpaca, ni vicuña, ni nada. Este era el gran problema de llama. Y por suerte, en los últimos meses, en toda esta explosión de enormes modelos de lenguaje, hemos tenido muy buenas alternativas con licencias que sí permitían su uso comercial, como el modelo MPT de 30.000 millones de parámetros o el modelo Falcon de 40.000 millones. Modelos que en rendimientos se acercaban a lo que ofrecía llama, pero que todavía ninguno conseguía superar. Y yo creo que ahora todos entendéis la importancia de lo que ha sucedido esta semana. La llama quería ser libre. Y Meta así lo entendió. Con la salida de la segunda versión de llama, a unos pocos meses de la primera versión, lo que Meta ha regalado a la comunidad de Deep Learning es un modelo más potente que la primera versión, y ahora sí disponible para uso comercial. Aún así, hay varias incógnitas que resolver, y la más importante, la que más me ha descolocado es ¿qué pinta Microsoft en todo esto? ¿Y por qué de repente empresas que parecían que estaban compitiendo aparecen aliadas para anunciar a este modelo? Recordemos que Microsoft es el principal aliado comercial de OpenAI, creadores de chat GPT. La primera cosa que quiero empezar con es OpenAI. Simplemente, Microsoft ama OpenAI. Y uno de los principales beneficios que ellos sacaban de esta alianza comercial era poder integrar mucha de la tecnología de OpenAI en sus productos. Por ejemplo, BinChat, Windows Copilot, Microsoft 365. Entonces, si esto es así, ¿qué sentido tiene ahora estar aliándose con la competencia quien está proponiendo además romper el mercado por completo al liberar una alternativa Open Source? Pues analizando la posteriori diría que es un movimiento bastante inteligente. La ventaja de liberar tu tecnología es que ahora tienes a toda la comunidad online trabajando sobre ella, optimizándola, mejorándola, a un ritmo que ninguna otra empresa ni laboratorio de inteligencia artificial puede igualar. Mejoras en tu tecnología que luego tú como empresa, como empresa, te vas a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento tú como empresa te puede beneficiar al integrar esta tecnología en tus productos y servicios. Aquí Meta se beneficiará cuando integren a Yama2 en su Instagram, en su WhatsApp, en su Facebook y de la misma forma su aliado comercial Microsoft también se beneficiará de ello cuando lo integran en Windows, en Office y en todos sus servicios. Por tanto, tú y tus socios comerciales se podrán beneficiar de los avances en esta tecnología. Pero Carlos, no se supone que si el modelo es de uso comercial, pues cualquier empresa se podría beneficiar de los avances que se produzcan en Yama2. Y la respuesta es que sí, pero con un asterisco. Y es que aquí Meta ha incluido una cláusula muy curiosa en su licencia, donde si eres una empresa con más de 700 millones de usuarios activos, ahí sí que tienes que pedirle permiso a Meta para poder utilizarlo. Una cláusula que evidentemente está colocada para limitar el acceso comercial al modelo de sus grandes competidores. Y hablando del modelo, recuperemos el objetivo original. Es mejor que el chat GPT por respuesta corta, es comparable a chat GPT 3 y no llega al nivel de chat GPT 4. La nueva versión de Yama viene en tres tamaños medido por su número de parámetros, donde el modelo mayor será el que mejor rinda y el menor el que más rápido y menos requerimiento de hardware necesitará y que posiblemente veamos integrado en muchos dispositivos móviles a lo largo del año. Además, en esta ocasión no solo están compartiendo el modelo del lenguaje base Yama2, sino también un modelo reentrenado, como era Vicuña, para actuar como un chatbot y así tener una variante ya más orientada a ser como chat GPT. El modelo tiene una ventana de contexto de 4000 tokens, lo cual lo hace equivalente al modelo GPT 3.5. Y como se puede ver en esta tabla, algunas de las evaluaciones que se ha hecho de la inteligencia de este modelo nos muestra que su nivel está a la altura de la versión gratuita de chat GPT, la versión 3. Pero hay una pega y quiero que lo veáis bien en esta tabla. Fijaos bien en la diferencia que existe en la evaluación de human evals. Esta evaluación lo que mide es la capacidad del modelo de poder generar código que sea funcional, de poder programar. Y aquí se puede ver lo que sería una de las grandes carencias de este modelo, y es que parece que no lo han entrenado con el objetivo de generar código en mente. Algo que es extraño, porque una cosa que se ha comprobado en los últimos meses es que estos enormes modelos del lenguaje, cuando los entrenas con código de programación, no sólo mejoran evidentemente en sus capacidades de programar, sino que también mejoran en sus capacidades de razonamiento lógico y resolución de problemas a través del lenguaje natural. Es decir, lo vuelve más inteligente y por algún motivo meta descartado esto. Y creo que esto ilustra bien por qué creo que OpenAI pues por ahora puede estar tranquila y es que todavía no existe un modelo ni privado ni de libre acceso que ponga entre las cuerdas a GPT4, que es el rival a batir. Recordemos que la punta de lanza y quien ha detonado toda esta revolución ha sido OpenAI a través de chat GPT, quienes son los que han puesto algo innovador sobre la mesa. Y me imagino que Microsoft esto no lo va a olvidar tan rápido. Entonces sí, OpenAI creo que puede estar tranquila, pero no se deberían de relajar. Y es que algo que ha cambiado es la narrativa. OpenAI lleva desde hace unos años, desde la salida de GPT2, pues metiendo esta idea en la cabeza de que estos modelos no se deberían de liberar tan a la ligera por motivos de seguridad. Un discurso que les ha permitido ir cerrando poco a poco esa puerta que estaba muy abierta en el mundo del deep learning de compartir modelos, compartir paper y que ha instaurado un secretismo comercial muy raro en los últimos años. Pues ha llegado a meta y ha dicho mira Google, OpenAI, yo es que no vengo aquí a pelearme por a ver quién es el modelo más potente. Yo creo que esto debería estar en abierto, rompo el mercado, llego con mi modelo, lo hago open source, saco Sam, saco Dino, saco todo lo que tengo y pues ha cambiado el discurso. Quién sabe si esto ahora motivará a OpenAI a seguir compartiendo modelos en abierto. Y por último, qué esperar de todo esto? Qué va a pasar ahora? Por qué es tan revolucionario? Pues lo que podéis esperar ahora es una explosión de chatbots, de servicios conversacionales, de optimizaciones en el modelo de llama que van a llegar desde ya. Seguramente tengáis curiosidad por saber si el modelo cabe en vuestra GPU, si podéis ejecutarlo en vuestro móvil y todas estas cosas. Pero mi consejo es que esperéis un poco a que la comunidad trabaje. Esto se va a mover muy, muy rápido. Ahora vamos a ver en cuestión de días cómo organizaciones van a empezar a reentrenar a sus modelos basados en llama dos y los van a compartir. Veremos gente que se anime a optimizar y a cuantizar el modelo para que quepa mejor en memoria. Veremos personalizaciones y fine tunings entrenadas con Lora. Y bueno, cuánto ha pasado? Dos, tres días. Pues ya hay gente en Twitter que está compartiendo que han conseguido ampliar la ventana de contexto a 8000 tokens con técnicas que ya se conocían. Es decir, todo se va a mover muy, muy rápido. Aún así, si queréis testear el modelo, hay un montón de opciones y os voy a dejar abajo en la caja de descripción un par de enlaces para que podáis echarle un ojo. Dicho esto, quiero que te deis cuenta de que estamos viviendo tiempos excepcionales, donde si por una parte ya la generación de imágenes estaba explotando con la llegada de StableDiffusion, ahora la llegada de llama dos introduce un nuevo tsunami, una nueva ola y una nueva corriente que vamos a estar viendo cómo se desarrolla en los próximos meses. Chicos, chicas, ya sabéis que toda la actualidad y todo el conocimiento que os puedo brindar sobre inteligencia artificial lo tenéis aquí en mi canal de YouTube dot Csv. Tendremos más en el próximo vídeo. Muchas gracias y hasta la próxima.
DOC0049|NLP e IA|Fue hace medio año cuando OpenAI con Githat Copilot y luego Codex demostró que, bueno, pues que la inteligencia artificial podía aprender a programar. Ellos tienen el primer golpe sobre la mesa a la hora de presentar un sistema de inteligencia artificial que por primera vez demostraba un muy buen rendimiento a la hora de generar código de programación que cumplía con aquellas demandas que el usuario había descrito. Esto, como digo, fue el primer paso hacia inteligencias artificiales que estaban aprendiendo a programar. Y mucho de vosotros que habéis trabajado mano a mano con Codex me lo habéis comentado. Esta se ha convertido en una herramienta que verdaderamente agiliza vuestro flujo de trabajo a la hora de programar autocompletando de manera inteligente aquellas partes de código que por lo general íbamos a copiar a esta cover flow. Ahora, Codex no es aún una superinteligencia artificial a la que tú le puedas pedir oye, générame el código de este videojuego de plataformas o générame un código que resuelva este problema. Aún no. Codex está entrenado para resolver tareas que son relativamente sencillas y que puedan ser resueltas en pocas líneas de código. Y no pasa nada, es normal. Al final esta no deja de ser una de las primeras versiones que tenemos de este tipo de tecnologías. Y ahora es cuando yo diría algo así como será cuestión de años y no meses hasta que un sistema como Codex evolucione hasta convertirse en algo mucho más potente. Y no me equivocaría. Porque esto es justamente lo que ha pasado. Y es que pocos meses después de la gran presentación de Codex el otro gran laboratorio de inteligencia artificial, DeepMind, ha entrado también al juego de las inteligencias artificiales que aprenden a programar. Y lo han hecho con mucha fuerza. De la compañía que se propuso dominar el juego de mesa Go y acabó ganando al campeón del mundo con AlphaGo. La compañía que se propuso enseñar a una inteligencia artificial a jugar al StarCraft y lo consiguió con AlphaStar. El laboratorio que se propuso resolver uno de los problemas científicos más relevantes de la época. El plegado de proteínas. Y acabó revolucionando el campo de la biología estructural con su sistema AlphaFold. Ahora entra de lleno en la automatización de la programación con su nuevo sistema AlphaCode. Me encanta el jueguito este que se traen con los nombres de llamar a todo AlphaFold, AlphaStar, AlphaGo, AlphaCode. AlphaCode no es solamente un sistema de inteligencia artificial que sepa generar código que sea funcional, que podamos ejecutar como pasaba con Codex. No. Sino que se trata de la primera inteligencia artificial que es capaz de programar soluciones para problemas de competiciones de programación. Y esto es muy potente. Para el que no haya participado nunca en una competición de este tipo, de lo que estamos hablando es de una competición donde se te va a presentar una serie de problemas computacionales complejos, en el que tienes que presentar ya no solo un nivel de conocimiento del lenguaje de programación que vayas a utilizar, sino también un conocimiento profundo en estructuras de datos, en algoritmos, formas de resolver estos problemas, y también una cierta inteligencia y creatividad a la hora de desarrollar tu programa. Puesto que este ya no solamente tiene que funcionar correctamente, sino que además tiene que hacerlo sin abusar de los recursos computacionales de tiempo y memoria. Vamos, que por lo general no vale desarrollar un algoritmo que resuelva ese problema por fuerza bruta. De hecho, para ilustrarlo mejor, déjame que te enseñe uno de estos problemas, uno de estos desafíos que AlphaCode ha sabido resolver. Backspace. El enunciado dice así. Imagínate que te doy dos cadenas de textos como estas, S y T. Ahora imagínate que tú puedes escribir cada carácter de esta palabra uno a uno, pero como alternativa a pulsar la tecla correcta, también puedes pulsar cuando tú quieras la tecla Backspace, la tecla de borrar. Eso significa que en el momento en el que tú decidas, en vez de escribir un carácter, pues puedes tomar la decisión de borrar el último que has escrito. Por ejemplo, una opción podría ser escribir todos los caracteres de la palabra y punto. O otra opción podría ser escribir los primeros dos caracteres, luego pulsar borrar, por lo cual perdemos uno, luego pulsar otros dos caracteres y luego borrar de nuevo, acabando con este texto de aquí. Y así podrías tener un montón de combinaciones donde escribir caracteres y borrar que te van a llevar a diferentes palabras resultantes. ¿Y cuál es tu tarea? Pues crear un algoritmo que te diga si hay alguna forma en la que con este mecanismo de escritura tú podrías obtener la palabra T. Como se puede ver, no es una tarea sencilla. Ya solamente leer el enunciado te va a llevar una serie de minutos hasta tener un conocimiento pleno de exactamente qué es lo que te están pidiendo. Y claro, implementarlo pues ya ni te digo. De hecho, voy a intentarlo. Y fue en este momento cuando Carlos se dio cuenta de que en realidad no había mirado nada de la teoría necesaria para afrontar este tipo de competiciones. Un mínimo de conocimiento de cómo era esto de la programación dinámica o recursividad o cosas así que uno ve en la carrera, pero que luego si no lo utilizas se acaba olvidando. Aquí podemos ver cómo se dedica a hacer lo que mejor sabe. Carabatos en el paint. No, aquí en realidad es un momento donde me paro un poco a pensar realmente cómo plantear el problema. Aunque parece que estoy ahí dedicando mucho tiempo al dibujito. En realidad estoy pensando y el dibujo me sirve un poco para plantear la solución. Y opto por hacer un programa que haga este árbol de operaciones. Al final Backspace te va generando diferentes ramificaciones según la decisión que tomes si es darle a Backspace o no. Y eso lo intento afrontar con recursividad. Estoy creando un método que va a tomar dos strings. Toma también el texto que se va generando y también un índice que indica la posición del carácter. Y donde se ejecutan ambas opciones, la opción de pulsar el Backspace y pulsar el carácter. Esto al final se va repitiendo de forma recursiva y en el momento en el que encuentras que el texto que va generando es equivalente al texto T que te han dado como input, pues te daremos como buena la solución. Y en caso de no encontrarlo o algún caso particular, pues diremos que no, que no la hemos encontrado, devolvemos falso y ya está. Y para mi sorpresa, pues ha funcionado. Los casos de prueba han funcionado correctamente. Así que ya con toda la ilusión del mundo voy a mandar mi solución y me encuentro un primer error. Esto en realidad es un error que me sucede porque tampoco me he parado a investigar la plataforma ni cómo tiene que ser el input de la información. Y lo que hago es ponerme a buscar pues algún ejemplo de código que me pueda servir para ver cómo exactamente se ven los ficheros o cómo se tiene que dar el input. Y bueno, cuando lo encuentro, entonces sí hago la modificación del código y con toda la ilusión del mundo, nuevamente lo mando a la plataforma. Ahora sí parece que funciona, que pasa el primer test, pasa el segundo. Y con el tercero me sale este error de aquí. Time Exit, me he comido los recursos de tiempo, ya lo he comentado antes. Aquí no solamente hay que hacer una solución que funcione, sino también una solución que no se coma los recursos de tiempo y memoria. En este punto ya me empiezo a dar cuenta de que se me acaba el tiempo y que además seguramente esto hay que resolverlo, pues no con recursividad, sino con algún otro tipo de técnica que debería de haber estudiado antes. Y que Alpha Code ha sabido encontrar y con este pensamiento tan doloroso, pues asumo mi derrota y me impongo como castigo a hacer un futuro video explicando cuál es la solución correcta. En cualquier caso, os animo a que vosotros también lo probéis para que así podáis valorar mucho más lo que hace esta inteligencia artificial. Visto así, creo que sabrás valorar mucho más qué es lo que hace Alpha Code, ya que este sistema lo que propone es que tú le vas a poder pasar el enunciado del problema con sus descripciones, con su explicación, con sus ejemplos de input-output, y como resultado pues te va a generar un código que tú vas a poder ejecutar y vas a poder evaluar qué será funcional y qué resolverá este problema. ¿Qué opináis? Vuelvo a lanzar la pregunta que lancé en el video de hace unos meses. ¿Podrá una inteligencia artificial desarrollar el trabajo de un programador? Seguramente la opinión de algunos de vosotros habrá cambiado de hace unos meses al día de hoy. Pero bueno, no nos vamos a adelantar. Primero tenemos que entender cómo funciona este sistema. Y es que el funcionamiento de esta inteligencia artificial también es interesante. Al final estos nuevos sistemas, generadores de códigos, tanto Alpha Code como el antiguo Codex, se están basando en la potencia de estos enormes modelos del lenguaje que venimos entrenando desde hace unos años. En concreto, enormes modelos del lenguaje que se están nutriendo de gigantescos repositorios de código y a través de los cuales pueden aprender exactamente a eso, a poder programar a través de generar línea tras línea de código. Pero en este caso este sistema va más allá. Vamos a verlo. Lo primero que han hecho ha sido tomar un enorme transformer, un tipo muy especial de red neuronal, una arquitectura de deep learning que ya vosotros tenéis que conocer porque hemos hablado de ella en este vídeo de aquí y en este vídeo de aquí. Y lo han entrenado con un dataset de más de 700 gigas de código público de GitHub. Esto sería pues coger a un chaval que no sabe nada y de repente enseñarle un montón de ejemplos de código y un montón de documentación y que él solo pues aprenda a encontrar los patrones en la secuencia de texto que le permita aprender a expresarse con dichos lenguajes de programación. Y esto ya sería similar en funcionamiento a lo que hace Codex. Esto es una especie de GPT-3 pero que ha sido entrenado sobre un enorme repositorio de código. Este sistema ya sabría expresarse en diferentes lenguajes de programación y generar código que resuelva tareas sencillas. Por tanto, ahora que sabe programar, el siguiente paso será prepararlo para las competiciones de programación. Para esto DeepMind ha creado un dataset propio con muchos ejemplos de problemas de competiciones que incluye el enunciado del problema, soluciones en diferentes lenguajes, ejemplos de pruebas que permiten evaluar si una solución es correcta o no e incluso metadatos sobre los problemas. Cuál es su dificultad, con qué tipo de algoritmo se podría resolver. Ojito con esto último porque luego no va a ser útil. La buena noticia es que este dataset, llamado Code Contest, se ha puesto disponible al público. Cualquiera puede acceder a él. Convirtiéndose por tanto en un recurso muy interesante para que otras organizaciones puedan crear sistemas similares. Este dataset se ha reentrenado a la inteligencia artificial, se ha hecho un proceso de fine tuning, acabando así con un sistema que ya no solo sabe programar, sino que también entiende cuáles son las dinámicas de las competiciones de programación. Que es capaz de tomar un enunciado descrito con lenguaje natural y con todas las complejidades que pueda tener, entenderlo, saber qué tiene que hacer y generar un código que implementa una solución para este problema. Bueno, un código no exactamente. Y es que el sistema de DeepMind no crea una respuesta correcta. Ni cientos, ni miles, sino millones de posibles propuestas de código. Y aquí es donde la máquina realmente empieza a demostrar habilidades con las que el humano no cuenta. Un humano en una competición pues no es capaz de imaginarse ni cientos ni miles de soluciones. Con suerte, yo llego a imaginarme una y hasta demasiadas me parecen. Este proceso de generar múltiples soluciones es uno de los puntos claves que diferencian alfacode. La estrategia para generar tantas soluciones diferentes a un único problema se explica de varias formas. Una de ellas es que se le pide al sistema que el 50% de las soluciones estén programadas en Python. Y la otra mitad en C++. Así ya te está garantizando una gran variedad de implementaciones diferentes. Pero además de esto, aquí es donde entran juegos los metadatos que mencionamos antes. ¿Recuerdas que te dije que para cada problema se contaba con una serie de metadatos que te identificaban ¿el tipo de dificultad de un problema o el tipo de algoritmo necesario para resolverlo? Bien, estos datos como es obvio pues no están disponibles para el modelo en el momento de la competición. No tiene sentido que tú le digas a la inteligencia artificial oye, tienes que resolver este problema pero que es muy sencillo y se hace a través de programación dinámica. Eso sería trampas. Pero en el entrenamiento no. Tú en la fase de entrenamiento puedes decirle a la IA oye, para este problema esta es su solución y este es un problema de dificultad media y esta solución está basado en fuerza bruta. Y así lo que puedes conseguir una vez la IA está entrenada es a través de estos metadatos puedes condicionar el tipo de solución que la IA te va a generar. Por ejemplo, para el problema de backspace que hemos visto antes pues tú le puedes decir a la IA oye, este problema es de dificultad media y se resuelve con fuerza bruta y te va a generar un tipo de solución. Y ahora le puedes decir no, no, este problema es sencillo y se resuelve con programación dinámica y te va a generar una solución diferente. Así es como modificando aleatoriamente estos metadatos pues se puede conseguir una generación mucho más diversa de tipos de soluciones que podrían resolver a este problema. Así es como han conseguido que Alpha Code genere este enorme pool de soluciones. Y no creáis que esto es trampa, eh? Que estamos resolviendo este problema a fuerza bruta generando un millón de soluciones. Alpha Code no manda a los evaluadores pues un millón de soluciones que ellos tienen que verificar a ver si hay alguna que esté correcta. No, para actuar en igualdad de condiciones pues la IA tendrá que encontrar una estrategia para a partir de todas estas soluciones de aquí encontrar únicamente aquellas 10 que sean mejores. Esto requiere por tanto encontrar un sistema de filtrado y selección que no es trivial. Y de hecho la solución que han encontrado me parece bastante ingeniosa. La primera estrategia que han establecido es que bueno, pues si el enunciado del problema nos plantea casos de ejemplo pues vamos a ver cuál de todos estos programas lo ejecuta correctamente. Si algún programa fallara esta prueba, ese programa evidentemente está mal. Y con esta estrategia Alpha Code consigue descartar de media un 99% de soluciones. Es decir, la mayoría del código está mal. Y claro, esto es un debate muy interesante porque si consideramos solamente esta parte del sistema de aquí pues no podemos decir que realmente sabe programar. La mayoría de código que genera es incorrecto. Pero claro, Alpha Code no es solamente esto. Alpha Code es esto más la generación de múltiples soluciones más su filtrado. Alpha Code es todo esto. Y si lo tomamos como un sistema al que le damos un enunciado y nos genera una serie de soluciones correctas, pues oye, funciona bien. Pero mi experiencia me dice que habrá mucha gente que considere que esto no es un sistema que sepa programar. Yo en mi caso la duda es que me despierta este proceso de filtrado es qué tan robusto será el sistema si en el enunciado, por ejemplo, no apareciera ningún ejemplo de prueba. Y está claro que nosotros como humanos pues realmente sí nos apoyamos en este tipo de ejemplos para entender mejor qué tenemos que hacer. Pero en realidad podríamos llegar a entender el enunciado y a generar un código que realmente fuera funcional. Y en el caso de Alpha Code pues no lo sé. En cualquier caso, la estrategia de filtrado funciona y para mí el sistema como un todo pues sigue pareciéndome bastante inteligente. Pero nos falta todavía un paso. Ya hemos filtrado este enorme número de soluciones y ahora nos hemos quedado solo con miles de ellas. ¿Ahora qué? Pues el último punto del sistema es clasterizar. Encontrar cúmulos de código, y esto es interesante, que sean similares semánticamente. Es decir, códigos que independientemente si están escritos de formas diferentes, la lógica que implementen sea la misma. Por ejemplo, este código de aquí y este de aquí pues no se parecen en nada. Pero tienes que saber qué hacen lo mismo. ¿Y cómo lo puedo saber? Pues mira, si para un mismo input generan un output que es similar es que su comportamiento interno pues también tiene que ser similar. Por tanto, cada claster representará soluciones que son funcionalmente diferentes. Y ahora de cada uno de ellos, de mayor a menor, se ha ido seleccionando a cada una de las soluciones candidatas que presentaremos como output final. Estas serán las diez soluciones para nuestro problema. Y así es como Alpha Code funciona y cómo ha conseguido posicionarse más o menos a mitad de tabla en una competición real contra programadores humanos. Y aunque se está hablando de que si en este tipo de competiciones, en la parte de la cola de la tabla, pues realmente lo que nos encontramos son estudiantes de programación que no tienen el nivel suficiente, sinceramente me da igual. El hecho de tener un sistema como Alpha Code que es capaz de resolver algunos de los problemas que hemos podido ver en este video es verdaderamente impresionante y es un nuevo y gran paso en este camino que ya se abrió con Codex el verano del año pasado. Hoy estamos aquí, pero si DeepMind ha demostrado algo durante los últimos años es que cuando se proponen solucionar un problema tardan muy pocas iteraciones en conseguirlo. Y el problema que se han propuesto es la automatización de la programación. Y esta es solo la iteración número uno. Y ya os oigo gritar en los comentarios que si es Kainet, que si la IA nos va a comer, que si es Kainet, que si es Kainet, sois unos pesados. Pero sobre esto hay algo curioso que os voy a comentar. Pero antes déjame recordarte que si este contenido te ha gustado, si quieres apoyar este proyecto, que este conocimiento sobre tecnología de vanguardia esté aquí en YouTube, pues lo puedes apoyar a través de Patreon. Patreon es una plataforma donde vosotros podéis apoyar de forma activa que este contenido exista. Podéis hacer una aportación mensual, ya sea la cantidad que sea, si queréis apoyarlo, si queréis formar parte de esta revolución de la inteligencia artificial, pues abajo en la caja de descripción tenéis un enlace, podéis entrar ahí, podéis registraros rápidamente y con eso podéis apoyar a este canal. Y ahora vamos con lo de Skainet. Y es que hay una curiosidad sobre este paper que me ha llamado mucho la atención. Siempre está esta broma de Skainet, siempre me la ponéis en los comentarios, desde el vídeo número uno me lo habéis puesto. Pero me ha llamado la atención encontrarme un párrafo que hace referencia a algo en esta línea. En esta pre-publicación a la hora de identificar los diferentes riesgos que podrían estar asociados a un sistema como este, DeepMind hace una muy buena identificación de muchos de los ricos que podemos encontrar, pero acaban con este párrafo de aquí, donde nos hablan de riesgos avanzados de inteligencia artificial. Exactamente lo que dicen es lo siguiente. En el largo plazo la generación de código nos podría llevar a riesgos avanzados en inteligencia artificial. Las capacidades de programación nos podrían llevar a sistemas que recursivamente puedan escribir y mejorarse a sí mismos. Rápidamente llevándonos a sistemas más y más avanzados.
DOC0050|NLP e IA|Si me preguntaron cuál ha sido el avance científico más relevante del año 2020, podría elegir entre varias cosas, la verdad que ha sido un año bastante interesante y donde el campo de la inteligencia artificial también nos ha dejado con grandes titulares. De todos ellos, hoy elijo uno. Unos resultados que son bastante recientes del mes pasado y que no sólo consolidan a la inteligencia artificial como una herramienta muy importante a nivel científico, sino que también podría ser una tecnología revolucionaria dentro del campo de la biología, al haberse resuelto uno de los grandes problemas dentro de este campo. En el vídeo de hoy hablaremos de alfafol 2 y el problema del plegamiento de las proteínas. Y si vamos a hablar de biomedicina, de proteínas y demás cosas bonitas, pues para eso tengo a la invitada perfecta, a Sandra Hortonoves, que la voy a invocar... ¿Ahora es cuando entra? No voy a cortar nada de esto. No, por favor, corta eso. Pero si no se me ve... Está moviendo el cable de alguna manera. ¿Sabes qué? Todo esto va a salir. Todo esto va a ser raín. Por favor, por favor. Por favor. Sandra Hortonoves, del canal La Hyperactina, donde se dedica a hablar de proteínas, biomedicina y cosas de estas que no conozco. Hoy Sandra ha venido aquí a ayudarnos a comprender mejor toda la naturaleza de este problema del plegamiento de proteínas que soluciona alfafol, ¿vale? Y esto lo vamos a hacer en dos vídeos, en dos colaboraciones que tenéis disponibles. Uno es este vídeo, donde vais a entender más cómo funciona alfafol. Y otro básicamente es mi vídeo, en el cual vamos a hablar un poco más largo y tendido de las proteínas, del plegamiento y de cómo alfafol nos va a ayudar mucho en biomedicina. Así que vamos a empezar primero con la parte biomedica del asunto. Las proteínas, moléculas indispensables para la vida, se tratan de una secuencia de unas moléculas más pequeñas llamadas aminoácidos. Pero más allá de ser una simple cadena de aminoácidos, las proteínas son las moléculas más complejas y sofisticadas que se conocen y eso es debido en parte a su plegamiento. Cada tipo de proteína, ya sea una hemoglobina o un anticuerpo, tiene una estructura tridimensional única que viene marcada por esa secuencia de aminoácidos. Es decir, según los aminoácidos que la formen, esa proteína se va a plegar de una forma u otra. No solo eso, sino que si una proteína está mal plegada, va a ser por una parte incapaz de realizar esa función, lo cual puede ser letal para la célula, pero es que además puede dar lugar a enfermedades tan conocidas como el Alzheimer o el Parkinson. De todos modos, tenéis este tema mucho más ampliado y detallado en mi canal, en el que le damos más caña a esta parte biomédica del asunto. Pero aquí vamos a hablar de inteligencia artificial. Efectivamente, aquí nos vamos a centrar más en entender cómo funciona AlphaFold. Y ya está la historia. Si quieres incentivar a mucha gente a participar en la resolución de este problema, ¿cómo podrías conseguirlo? Pues, por ejemplo, planteando una competición. Una competición donde se les pidiera a los participantes que para una secuencia de aminoácidos dada, que pertenece a una proteína cuya estructura es desconocida por ellos, generen métodos de modelado que puedan predecir cuál podría ser esta estructura. El resultado generado se comparará con el resultado real, obtenido con otro tipo de técnicas mucho más costosas que ya mencionaremos luego, y finalmente se evaluará quién ha obtenido el mejor resultado. Vamos, una competición típica. Esta competición es la Critical Assessment of Protein Structure Prediction o CASP. Y se viene realizando cada dos años desde 1994. Esta competición no sería noticia en este canal, sino fuera porque una empresa tan poco conocida como es DeepMind, no siendo un laboratorio que se dedica al problema del plegado de proteínas, ni siendo este su campo de estudio, se metieron a participar utilizando todas estas herramientas de deep learning que conocemos y bueno, pues quedaron primeros. Cada una de estas barras representa la puntuación total obtenida por cada equipo participante y podemos ver que en 2018, participando, repito de nuevo, por primera vez, DeepMind con su sistema AlphaFold quedó en primera posición, mostrando que efectivamente hacer uso de modelos de deep learning a la escala a la que lo hace DeepMind es una alternativa viable. Y esto ocurrió en 2018, pero dos años después, en 2020, esta competición se ha vuelto a repetir y como era de esperar, había muchos ojos puestos en AlphaFold y en cómo habría evolucionado. Y en efecto, los resultados publicados mostraron cómo AlphaFold 2, a nueva versión, consiguió una puntuación impresionante y que dejaba clara la gran diferencia que existía entre los resultados de DeepMind y el resto de participantes. Como es lógico, los resultados dejaron flipando a la comunidad científica y en palabras de los organizadores, AlphaFold 2 habría resultado satisfactoriamente un desafío que permanecía abierto desde hacía 50 años cuando se planteó por primera vez. Carlos, tú que eres tan listo. A ver, ¿cómo lo han hecho? ¿Cómo funciona AlphaFold? Pues esto en realidad no es un problema sencillo, porque si lo pensamos aquí, lo que estamos haciendo es intentar predecir a partir de una secuencia unidimensional de datos, la secuencia de aminoácidos, cuál es la estructura tridimensional de la proteína, cuál es la posición que ocupa cada aminoácido en todo este espacio tridimensional, sacar toda su estructura. Y para llegar a esa estructura tenemos que fijarnos en las propiedades químicas que tienen los aminoácidos de esa secuencia de entrada. Más que nada para entender las dinámicas que hacen que poco a poco esa secuencia se vaya plegando hasta llegar a la estructura final. Si, por ejemplo, decidieramos ignorar todas estas dinámicas y directamente que la proteína se plegara de manera aleatoria, existiría un espacio de búsqueda donde para una proteína estándar de tamaño normal podrían existir tantas configuraciones que evaluar cada una de ellas requeriría más tiempo que la edad del universo. Una de estas frases que DeepMind disfruta tanto. Y claro, siendo esto así, pues es evidente que tenemos que atender a cómo funcionan estas dinámicas para poder comprender cómo se pliega realmente una proteína. Todas estas propiedades que ya hemos explicado en el vídeo del canal de Sandra. Y sí, cuando he dicho que tenemos que atender nosotros a cuáles son estas dinámicas no me refiero a nosotros, me refiero a un algoritmo de inteligencia artificial que aprenda a descubrir cuáles son estos patrones. Pequeño disclaimer, a ver, si bien los resultados de Alpha Fold 2 ya los conocemos los de la competición, hay que decir que todavía no se ha publicado ningún paper. Al igual que ocurrió con la competición de 2018 que salieron a finales de ese año y hasta principios de 2020 no tuvimos el paper en nature. Pero eso nos impide que podamos profundizar en este paper de la primera versión para ganar intuición de su funcionamiento y combinarlo con lo poco que sabemos para ver cómo ha evolucionado Alpha Fold 2. Y cuando lo hacemos descubrimos algo interesante y es que DeepMind se ha empeñado en convertir este problema de convertir una secuencia unidimensional a una estructura tridimensional en un problema diferente, en un problema de convertir una imagen a otra imagen. Veréis, como ya hemos visto muchas veces en el canal, muchas de las herramientas de inteligencia artificial que hemos desarrollado los últimos años se aplican sobre imágenes, encontrándonos uso donde con redes neuronales podemos tomar una imagen de entrada y convertirla a otra imagen de salida diferente, tomar una imagen y generar un mapa de segmentación, tomar una imagen y modificar su estilo, tomar una imagen y borrar un elemento. Todo eso es posible. Y son tan potentes estas redes que trabajan con imágenes que es habitual ver cómo en ocasiones hay problemas donde nuestros datos sin ser imágenes se adaptan para que lo sean y así podamos utilizar estas redes. Por ejemplo, es normal ver proyectos donde trabajando con la onda de audio, que es una secuencia, ésta la conviertan a su espectrograma, una imagen, para así continuar usando estas redes. ¿Eso de que cuando tienes un martillo todo te parece un clavo? Pues eso. Y en el caso de la primera versión de Alpha Fold la idea es similar. Lo que han hecho ha sido tomar esta secuencia de aminoácidos y de alguna manera convertirla a una imagen para que después un algoritmo de inteligencia artificial se encargue de traducirla a otra imagen que represente la estructura de la proteína. Vamos a centrarnos en los datos de entrada. Vamos a fijarnos qué información podemos extraer de nuestra secuencia de aminoácidos y que podemos suministrar a Alpha Fold. Para este tipo de problemas partimos de la secuencia de aminoácidos y una forma de extraer información de ella es a través de una técnica llamada M.C.A. o Multiple Sequence Aligned, que consiste en alinear y comparar muchísimas secuencias de aminoácidos, de proteínas de distintos seres vivos que tienen una relación evolutiva y por tanto su secuencia de aminoácidos es parecida. Al hacer esto es posible que veamos que en algunos casos se produzca el siguiente patrón. Cuando un aminoácido de la secuencia cambia debido a una mutación hay otro de esa secuencia que también cambia. Es decir, podemos ver cómo hay aminoácidos que de alguna forma están conectados y eso es así porque esos aminoácidos interaccionan dentro de la proteína. Como esa interacción es necesaria para mantener la estructura de la proteína, Si un aminoácido cambia debido a una mutación, el otro tiene que cambiar por nálides. Esos dos aminoácidos coevolucionan en el tiempo. De esta forma comparando muchas secuencias y viendo esos aminoácidos que coevolucionan podemos conocer qué interacciones se producen dentro de esa estructura tridimensional de la proteína que nos pueden ayudar a predecir cómo se va a plegar. Esta información de cómo interactúan dos aminoácidos que ocupan una posición diferente en la secuencia la podemos estudiar con diferentes estadísticos como la correlación y con esto podemos ir completando una matriz de datos que nos indique cómo interactúa cada aminoácido de la secuencia consigo mismo conformando una matriz de datos que se va a parecer a una imagen. Toda esta información queremos configurando en diferentes matrices de datos será la que le suministraremos como entrada a Alphafold. Y ahora, cito textualmente del paper, utilizamos redes neuronales cuya estructura son similares a las usadas para problemas de reconocimiento de imágenes. Las redes neuronales... venga, es tu parte que te gusta. ¡Convolucionales! Las redes neuronales... Lo que se busca es que la red aprenda a generar una imagen como esta. ¿Qué es? Pues lo creas o no, esta información representa la estructura tridimensional de la proteína. Fíjate bien en el resultado que genera Alphafold. Esto es un distograma y lo que viene a representar es la predicción que ha hecho la red de la distancia a la que se puede encontrar cada aminoácido de la secuencia dada como entrada. Mira, para que lo entiendas bien, imagínate que nuestra secuencia estuviera reducida a solo 5 aminoácidos y quisiéramos representar en esta matriz la distancia a la que se van a encontrar cada uno de ellos en el espacio tridimensional. El resultado de las distancias lo anotaremos en la siguiente matriz, donde cada celda va a representar cuál es la distancia de cada aminoácido de la proteína con el resto de los elementos. Evidentemente, cada aminoácido consigo mismo estará a una distancia de 0, que es el mismo elemento, así que en la matriz siempre encontraremos en la diagonal que la distancia es 0. Es normal que los aminoácidos cercanos en la secuencia se encuentren también a una distancia cercana en el espacio tridimensional, de ahí que aparezca esta franja amarilla de aquí representando su cercanía. Y luego, si sucede que nuestra proteína se ha plegado por la interacción entre dos partes de la cadena, veremos que también su cercanía quedará representada en la matriz. ¿Lo ves? Y es esta matriz, esta imagen, lo que la red neuronal aprenderá a predecir. Así es como han convertido el problema del plegado de proteínas en un problema de convertir una imagen a otra. Y ojo, esta es la parte de inteligencia artificial. Con esto ya tenemos obtenida cuál es la matriz de distribución de las distancias de cada uno de los aminoácidos dentro de la proteína plegada. Claro, pero esto no es la proteína en 3D. El siguiente paso que han hecho ha sido el de utilizar un algoritmo que conocemos aquí en el canal, el descenso del gradiente, para poco a poco ir optimizando la estructura de la proteína para que ésta se vaya plegando, optimizando los ángulos de torsión, como hacemos con los parámetros de una red neuronal, para que poco a poco la estructura de la proteína vaya haciendo que las distancias entre aminoácidos se parezca a la predicha por la red neuronal. Tras optimizar, el resultado será la estructura tridimensional de la proteína. AlphaFault habrá terminado su trabajo y la proteína estará plegada. Vale, pero ¿qué datos han utilizado para entrenarlo? Lo que ha hecho la gente de Bitmine ha sido tomar muchas de las estructuras de proteínas conocidas que se encuentran en el Protein Data Bank y que se han obtenido utilizando métodos tradicionales más justosos. Y han entrenado al sistema de AlphaFault para que aprendiese a reconocer los patrones que hacen que se pliegue de una forma u otra, obteniendo resultados muy precisos. Para medir la precisión de las predicciones de este plegamiento, la métrica que se utilizó es el llamado GDT o Global Distance Test. El GDT iría del 0 al 100 e indicaría el porcentaje de residuos de aminoácidos que se encontraría en la posición correcta, dentro de un margen de error. Y para que os hagáis una idea de una puntuación de unos 90 GDT aproximadamente, se consideraría un resultado comparable a los métodos actuales, que serían la cristalogracida de rayos X o la resonancia magnética nuclear. Pues de los resultados de este año, AlphaFault obtiene ni más ni menos que una puntuación de 92,4 GDT. Increíble. ¿Qué significa esto? Pues que realmente AlphaFault ha conseguido unos resultados increíbles, ya sea comparándolos con los resultados del segundo mejor equipo en esta competición, o en términos generales comparándolos con los métodos científicos actuales. Y donde la calidad de los resultados nos permite empezar a soñar en utilizar estos métodos computacionales como alternativa a los métodos más clásicos, que son más costosos y requieren de mucho más tiempo. Sobre esto, lo que tenemos que entender de AlphaFault 2 es que la evolución que ha sufrido ha sido la evolución tradicional que hemos visto en el campo del deep learning, en donde los algoritmos basados en mecanismos de atención, como por ejemplo los transformers, han ido ganando popularidad por su potencia. Sabemos que dentro de las tripas de esta versión de AlphaFault 2, existen elementos comunes a los que podemos encontrar en una arquitectura como el transformer, que encontramos en muchos de los grandes modelos del lenguaje actuales como GPT-3. Aquí dejamos que sea el propio modelo el que pueda prestar atención a los diferentes elementos de la secuencia dada como entrada, y también a todas las múltiples secuencias alineadas que hemos mencionado previamente. Utilizando toda esta información, AlphaFault consigue sobresalir en su tarea. Y esto es impresionante, porque si lo pensamos fríamente, lo que ha ocurrido aquí es que un grupo de ingenieros que realmente son expertos en deep learning, han venido con su tecnología a una competición de gente de biología, y han centrado su atención literalmente en resolver este problema concreto. Han sabido adaptar las necesidades de este problema a la tecnología actual que contamos, a los bloques fundamentales que utilizamos en el deep learning, y con ello han conseguido un resultado meteórico. Y con esto me pregunto, ¿cuál es el límite? Siempre que contemos con los datos haciendo uso de una metodología de deep learning, ¿podremos alcanzar resultados como esto? ¿Podemos resolver cualquier reto que la ciencia nos plantee? Pues en concreto en el campo de la biomedicina, conocer la estructura de las proteínas nos puede ayudarnos a entender mejor cómo funciona nuestro organismo, sino también a conocer que hay otras enfermedades que son debidas a mal plegamiento de las proteínas como el Alzheimer, o por ejemplo ayudarnos con el desarrollo de fármacos. Aunque para entender esto y para hablar de esto más largo y tendido tenéis nuestro vídeo, bueno es mi vídeo, está en mi canal, nuestro vídeo, pero ya no queréis que vayáis al otro vídeo del otro canal a verlo. Pero sí que es nuestro vídeo, ahora queda como una bola atrás. O sea, es nuestro vídeo en mi canal, pero es nuestro vídeo. Lo que está claro es que Alpha Fold 2 ha marcado un antes y un después que mucha gente está comparando con lo ocurrido a principios de décadas con ImageNet. Ya lo he comentado en varias ocasiones en el canal, pero la revolución del deep learning que estamos viviendo en esta última década realmente tiene su punto de partida en los resultados tan impresionantes que consiguió un equipo al utilizar esta tecnología por primera vez en la competición de ImageNet. Fue al alcanzar estos impresionantes resultados cuando gran parte de la industria y la academia se vio interesada por esta tecnología, dando paso a todos los avances que hemos visto en visión por ordenador, en modelos generativos, en conducción autónoma, AlphaGo, Transformers y hoy Alpha Fold 2. Muchos comparan ese momento con lo que podría ocurrir ahora en el campo de la biología, siendo la victoria de Alpha Fold 2 un motivo para traer mucho interés en esta tecnología y empezar a aplicarla de manera sistemática para desarrollar nuevos métodos que puedan acelerar la investigación en el plegado de proteínas. Un avance que demuestra cómo campos tan distintos como la inteligencia artificial y la biomedicina al final tienen tanto que aportarse la una a la otra. ¡Pum! Pues ya hemos terminado. ¡Mucho mal de daño! Ya hemos terminado el vídeo y... ¿Has sacado el guión? Pues no, porque todavía queda más contenido. Tenéis otro vídeo entero en el canal de Sandra donde estamos hablando de todo este proteín folding, vamos a explicarlo en más profundidad. Y vamos a hablar de las proteínas y de cómo se pliegan y hacen robotitos y toda la relación que tiene con la informática que nos toca. Ya, ¿no? De hecho... De hecho, voy a aprovechar para enseñar... ¿Por dónde vas? ¡Mirá lo que haces! ¡No se lo he leído! El libro de Sandra Ortonoves, ¿qué puede salir mal? Un libro donde explica todo esto que también vamos a estar hablando en tu vídeo y muchas más cosas sobre biomedicina. Sí, es como que hay varios niveles, o sea, para entender bien todo lo que es este tema plegamiento, proteínas, Alpha Fold, inteligencia artificial y todo esto... ...susto. Tenéis que ver ambos vídeos porque ambos se complementan súper bien y te dan información de cada uno de los campos bastante completa. Pero si queréis saber más de biomedicina y os estáis iniciando en el campo o estéis estudiando una carrera similar o biomedicina tal cual, podéis leer este libro que está, creo, bastante completo. Está mal que lo diga yo, pero... Está fatal. Pero es un muy buen regalo para Navidades ahora que están llegando las Navidades. Y nada más chicos, hasta aquí el vídeo de hoy. Es que no vamos a seguir alargando porque es que la cosa tampoco da para más. Así que ya sabéis, más inteligencia artificial no en el próximo vídeo sino en el canal de Sandra. ¿Algo más que decir? ¿Tú últimas palabras? Un besito. Pues sí. Adiós. ¡Ue! ¡Posad! ...reinvumante, si prestamos atención... ¿Qué haces? No. Joder, es que te voy a editar, yo voy a entrar tú y tú. No, entro... Te voy a hundir la carrera. Pero que lo puedes hacer. No, no, no. A ver, estos son... Vale, vale. Intimidades de los youtubers que cuando graban a cámara. Sandra... Hoy... ¿Qué situación? No puedo. No te jodas por favor. Y esta es la historia. Sí. No. Primera vez. DeepMind con su sistema. DeepMind. DeepMind. DeepMind. Aunque los resultados... Vamos. Cierra la... Y ahí me ven. Venga. Pues en concreto en el mundo. No, el mundo es el queso. Venga.
DOC0051|NLP e IA|Hablemos de Sam, pero no es de Sam, sino es de Sam. Segment Anything Model, las tijeras más potentes de la inteligencia artificial. Un nuevo regalo que nos ha facilitado el laboratorio de meta en forma de modelo open source, que todos nosotros podemos empezar a descargar y utilizar en nuestros proyectos. Una ya que busca impulsar una idea muy potente en el mundo de la visión por computador, de forma similar a como los modelos GPT han revolucionado por completo el mundo del procesamiento del lenguaje natural. Y un modelo que para muchos que lo han probado ya, pues está suponiendo un avance frente a los modelos anteriores con los que habían trabajado. ¿Por qué? ¿Por qué es tan importante un modelo que solo segmenta, que solo recorta imágenes? ¿Y qué supondrá este modelo para el futuro de la visión por computador, de la robótica, de la realidad aumentada que tanto promueven desde meta? Hablemos de Sam. Pero antes de avanzar, quiero agradecer al patrocinador de este vídeo, a Hostinger, por apoyar este contenido. Hostinger es uno de los mejores servicios de hosting y dominio disponible actualmente, donde además te van a ofrecer un montón de herramientas que te van a simplificar el proceso de crear tu página web, algunas de ellas integrando incluso inteligencia artificial. Con los servicios que te ofrece Hostinger podrás abrir tu página web a internet sin dificultad en pocos minutos, desde registrar con ellos tu propio dominio hasta configurar paso a paso cómo quieres que se vea tu página web final. En este caso voy a crear un nuevo sitio web, que será un porfolio personal, y aquí tengo la opción de elegir alguna de sus plantillas predefinidas de calidad y listas para ser personalizadas. Esta por ejemplo está muy chula, tiene un estilo que me gusta bastante, pero aún así he visto por aquí que también tienen un creador de webs con IA, así que lo vamos a probar. Y poniendo unos pocos datos y una descripción de a qué me dedico, automáticamente se va a crear una web con todos los textos e imágenes listos y con un diseño que si quiero puedo modificar y personalizar. El editor además te permite fácilmente cambiar imágenes, arrastrar elementos, muy fácil todo, e incluso tiene algunas herramientas más avanzadas como esta que me ha gustado, donde usando IA te va a estimar el mapa de atención de a dónde mirará la mayoría de usuarios cuando vean tu página web. Muy muy chulo. Tenéis todas estas funcionalidades a muy buen precio usando mi código de descuento. Esta es una oferta por tiempo limitado, utilizando mi código .csv vais a tener acceso a un descuento exclusivo. Tenéis los enlaces y el código de descuento abajo en la caja de descripción y de nuevo muchas gracias a Hostinger por patrocinar este vídeo. Sam es el Segment Anything Model, o modelo que segmenta cualquier cosa. Un modelo de segmentación capaz de... segmentar cualquier cosa. Buen naming. La tarea de segmentación en Deep Learning existe desde hace años y es una tarea en visión por computador donde embete tomar una imagen como input y intentar predecir qué hay en toda esa imagen. Por ejemplo, en esta imagen hay un perro o aquí hay un gato, pues aquí la predicción de la clase se va a hacer a nivel de píxeles. Es decir, la red neuronal va a predecir, hey, en este píxel hay un trozo de perro y aquí también y aquí también. Y en estos píxeles de aquí, de la misma imagen, pues hay un trozo de gato, gato, gato, gato y así hasta completar la predicción de todos los píxeles de la imagen. Cuando has hecho esto, lo que obtienes como resultado es un mapa de segmentación. Un mapa donde vas a tener identificado los diferentes elementos que hay en tu imagen. Y cuando tienes la masa de píxeles ya identificada y localizada, pues esto te puede servir para múltiples tareas. Pues por ejemplo, para borrar el fondo de una imagen y así sacar un nuevo sticker para WhatsApp o usar esa máscara para hacer un inpainting y borrar justo ese objeto. En generación de imágenes también hemos visto lo útil que son los mapas de segmentación en proyectos como Gauguin, donde a partir de un mapa de segmentación se obtenía una imagen realista. En audio, por ejemplo, la segmentación de imágenes se ha utilizado sobre los espectrogramas de audio para eliminar ruidos molestos que pueden estar afectando a tu grabación. O en medicina, desde hace ya unos años, esto se viene utilizando para poder segmentar y diferenciar muchos de aquellos patrones que por no estar acostumbrado el ojo humano no puede detectar tan fácilmente. Así que sí, segmentar imágenes bien es una tarea muy útil para un montón de tareas. Y ahora llega Meta y nos dice, hey, pues con esta tecnología esto lo podéis hacer mucho, mucho mejor. ¿Y por qué mucho, mucho mejor? Bueno, pues esta es la idea con la que quiero que os quedéis en este vídeo, y es que lo interesante del segment anything model no es tanto el segment, sino el anything. Y es que este modelo es capaz de segmentar cualquier cosa, cualquier cosa que se le ponga por delante. Aquí hay una diferencia fundamental con los ejemplos de modelos que hemos comentado antes, donde tú a priori ya le estás indicando a la IA qué tiene que segmentar. Por ejemplo, tú la entrenas para segmentar perros o segmentar gatos, entrenándolas solo para esa tarea como un problema de aprendizaje supervisado. Sin embargo, con Sam la cosa es diferente y es que este modelo es capaz de segmentar cualquier objeto que sea segmentable. Es decir, si yo, por ejemplo, te enseño esta imagen y te pido que identifiques algún patrón interesante, alguna forma u objeto, no vas a encontrar nada. Pero si, por ejemplo, te enseño una imagen como esta, aunque nunca haya visto este objeto, sí sabrás identificar que este es un objeto diferente a este y que sus partes también diferenciables son esta y esta. Y esto lo sabes porque eres capaz de entender diferencias entre patrones, texturas, la continuidad de las formas. Eres capaz de generalizar la información del mundo que has visto con tus ojos. Y esto es lo que ahora inteligencias artificiales como Sam también pueden hacer. Y es impresionante. Esta capacidad de la IA de poder segmentar bien un objeto que nunca ha visto antes durante su entrenamiento es lo que en deep learning llamamos zero-shot learning. La capacidad de una IA de poder resolver una tarea a pesar de, bueno, para ese caso concreto no haber recibido ningún ejemplo de entrenamiento. Algo que nosotros los humanos solemos hacer con mucha frecuencia. Y ahora Sam, pues siguiendo la misma estrategia de entrenar un modelo a lo bestia con una gran cantidad de datos, pues nos ofrece un nuevo modelo fundacional capaz de generalizar la noción de objeto. Siendo capaz de desplegar sus habilidades en contextos que nunca ha visto durante su entrenamiento. Siendo capaz, por ejemplo, de poder segmentar la forma de un teléfono pintado con un estilo concreto aun cuando nunca ha sido entrenado para hacer esta tarea de segmentación en este estilo de cuadros. Ah, que no me crees. Vamos a probarlo. Os voy a dejar abajo el enlace a la página web para que vosotros también lo probéis de Sam, que es esta página de aquí donde está toda la información que estamos comentando y donde también podréis encontrar una demo para probar toda esta herramienta. Cuando entramos vamos a ver un mensaje como este que nos indicará que este es un proyecto de investigación, que ninguna imagen que subamos se va a quedar almacenada, lo cual está muy bien. Aceptamos los términos y con eso podemos ver todas las fotografías que ellos ponen a nuestra disposición para probar la herramienta. De todas ellas me gustan mucho los ejemplos con imágenes como esta, dibujos o arte, donde los objetos no son objetos tan cotidianos, puesto que están representados artísticamente, no son fotografías reales y eso me llama mucho la atención cómo Sam consiga aún así detectar los diferentes objetos y hacer la segmentación correctamente. Podemos marcar aquí, bueno, por los diferentes elementos que hay en el dibujo, podemos marcar. Podríamos ver el sol, lo tendríamos aquí, podríamos ir a las diferentes partes que está identificando, un poco por el estilo, por la forma o podríamos darle aquí a Everything para detectar todo de golpe y ver qué elementos diferencia. Aquí estaría haciendo el análisis y saca una segmentación de las diferentes flores, del suelo, de las diferentes partes de los muñecos, etc. Entonces lo que vamos a hacer es subir una imagen, vamos a hacer un par de ejemplos. Por ejemplo, aquí estoy subiendo una imagen de un robot que he hecho con midjourney y si os fijáis ha habido una primera etapa donde lo que ha hecho Sam ha sido analizar toda mi imagen. Está analizando todos los elementos, está creando un vector de embedding que va a almacenar la información visual de lo que ha observado y a partir de ahí luego con ese vector de embedding es con lo que se va a generar todo el tema de la segmentación. Entonces me gusta mucho esta imagen porque he intentado generar una imagen de un robot que está medio oculto por el humo, por una nube para ver si Sam consigue identificar toda la figura del propio robot. Si yo me voy moviendo por la escena vemos que uno de los elementos que identifica es el suelo, aquí no lo estaría marcando, nos está marcando también un poco toda la parte yo entiendo que de humo y si nos ponemos encima el robot, fijaos cómo consigue sacar una máscara perfecta donde incluso las patas, sería esta parte de aquí, justamente esta parte. Pues como las patas está consiguiendo detectarla a través del humo, algo que no es tan sencillo para un algoritmo de este tipo. Además la forma de operar de Sam pues le permite no solo hacer segmentación del objeto completo, sino que si te vas moviendo pues puedes ver cómo también tenemos la cabeza por separado y podríamos movernos también al ojo, a las diferentes partes del ojo, es decir, podríamos segmentarlo todo. De hecho, aquí hay una herramienta que es multi más que te permite seleccionar un objeto y que Sam pueda sacar diferentes partes de lo que está seleccionando, no te predice diferentes máscaras para un punto que haya seleccionado. Podríamos, por ejemplo, marcarle un ojo y aquí veríamos cómo nos saca pues la pieza del ojo, el ojo y la cabeza por completo. Y esta es otra foto con la que quería probar, que es una foto con mayor complejidad, con mayor nivel de detalle. Es en la frutería más antigua de Madrid, por lo que he podido leer y esta persona se llama Félix y nos va a ayudar ahora pues para hacer un poco de segmentación sobre esta imagen. Hay mucho detalle, así que lo que voy a probaritarle a la función Everything, que va a sacar una segmentación de muchos objetos que haya en la escena. Lo que va a hacer es trazar una malla de diferentes puntos y luego va a seleccionar aquellos que crea que son puntos de interés. No creo que saque todos los objetos por separado, porque la resolución del algoritmo no es tan alta como la que hemos podido ver en el paper con más de 500 máscaras, pero vemos que consigue un resultado bastante bueno delimitando muchos de los objetos de interés de esta escena. Vemos que cada tomate en este sexto está delimitado. Creo que todas las cestas aquí estarían perfectamente delimitadas. Esta se ha quedado fuera. Vemos que diferentes cestas de frutas, pues también están marcadas con algunas frutas más diferenciadas, no sé si porque el color sea distinto. Y vemos también que nuestro amigo Félix aquí también ha sido perfectamente segmentado en este proceso. Fijaos además que si nos vamos moviendo con el cursor, pues Sam va actuando y va intentando segmentar aquel punto donde marcamos y podemos llegar a algunas de las máscaras que antes no se han seleccionado con la función de Everything, porque como hemos dicho, Everything realmente no cubre todo, sino que traza una malla de puntos y donde cae, cae. Pero fijaos que aquí, donde antes teníamos seleccionado todos los elementos, si nos movemos un poquito, pues podemos llegar incluso a diferenciar las diferentes frutas cuando vamos pasando el cursor por encima. Luego además tenéis la opción de marcar un objeto con una bounding box. En vez de hacerlo con un punto de interés, pues podríamos decirle, mira, aquí en esta caja que te estoy marcando, hay un objeto de interés, intenta adivinar cuál es. Y entonces te consigue predecir perfectamente que el objeto, el concepto que cae dentro de este cuadrado, pues tendría que ser nuestro amigo Félix. Y con esto pues ya tendríamos las presentaciones hechas. Félix ya conoces a Sam, Sam ya conoces a Félix y vosotros pues ya conocéis esta herramienta. Y la pregunta es, ¿por qué es tan importante un modelo como Sam? Pues hay tres motivos. Primero, porque es muy versátil. Versátil porque Meta ha diseñado su arquitectura pensando en aceptar diferentes tipos de prompts, que van a permitir al usuario interactuar de distinta forma con el modelo. Por ejemplo, como hemos visto, podemos marcar puntos que marquen qué objeto queremos segmentar y qué parte no queremos segmentar, para diferenciar a nuestro objeto de otros elementos. Pero también este modelo acepta el uso de bounding boxes que delimiten la región de la imagen donde tiene que encontrar un objeto de interés y con la misma dinámica también podríamos utilizar máscaras. Ahora, de todos los inputs, el más interesante, aunque es algo experimental que solo muestran en el paper, es el uso de un prompt de texto donde tú puedes indicar una descripción donde digas quiero que se segmente la rueda del coche y boom, automáticamente la IA lo segmenta. Y esto último, aunque es algo experimental que están haciendo con el modelo clip, me parece una de las funcionalidades más interesantes porque podría servir de puente con modelos como GPT-4, donde tú puedas dejar a la IA, GPT-4, que ya sabemos que puede operar bastante bien con este tipo de herramientas, que elija qué elemento segmentar de una imagen para completar una tarea mayor. Y seguramente no tardaremos mucho en ver implementaciones de este tipo. ¿Por qué? Pues por el segundo motivo por el que Sam es un modelo tan interesante. Y es que es un modelo open source. Y aquí es imposible no hacer la semejanza de este modelo con whisper de OpenAI. Son modelos completamente diferentes. Uno sirve para transcripción de audio, este para segmentar imágenes, pero en ambos casos representan modelos que vienen a dar una solución a un problema clásico de deep learning, transcripción de audio, segmentación de imágenes, problemas muy típicos donde, bueno, ya existía una gran variedad de modelos anteriores que eran capaces de hacer esto. Pero de repente aparecen, llegan, son open source y barren por completo cualquier investigación y cualquier empresa que pudiera ser competencia. Ya lo sabéis que whisper fue un enorme regalo para la comunidad y ahora Sam llega también a nuestras manos para que cualquiera de vosotros pueda empezar a jugar con él, a usarlo y a construir. Tendréis abajo en la caja de descripción el enlace al repositorio de GitHub, donde podréis encontrar el modelo y todas las instrucciones para que empecéis a jugar con él y ver que también funciona, aunque esto ya os lo digo yo, funciona muy bien. Y aquí el tercer punto. Sam es importante por esto mismo, porque funciona muy bien. En el paper podemos encontrar algunos ejemplos bastante impresionantes, donde la hayas capaz de identificar más de 500 máscaras de objetos, cada uno de ellos segmentado correctamente. Y esto evidentemente Meta lo enseña para mostrar músculo, pero aquí lo que me parece interesante no es tanto el músculo, sino el cerebro, la utilidad real que un sistema de este tipo podría tener. Y por eso me alegra mucho encontrarme en redes ejemplos de profesionales que están indicando el salto cualitativo que en sus primeros usos les está suponiendo esta herramienta. Y pensad en lo que hemos dicho antes, las capacidades SiruShot van a permitir a Sam poder hacer bien su labor incluso sobre datos que no haya visto previamente, datos como imágenes satelitales tal y como podemos ver aquí o aplicados a datos capturados por un sonar. Pensad que en muchos trabajos este proceso que estamos viendo suele ser un proceso de etiquetado manual, donde el usuario tiene que estar con su ratón, clicando en las imágenes y segmentándolas, o en el mejor de los casos asistido por un modelo que ha tenido que ser entrenado específicamente para esa tarea, algo que no siempre es posible o no siempre funciona bien. Pero ahora y repito de nuevo, Sam recién salido de la caja es capaz de desplegar sus habilidades sin necesidad de entrenarlo para ello. Y bueno, si no funcionara, podrías utilizarlo de base para hacer un fine tuning con tu conjunto de datos. Es lo bueno de que este modelo sea open source. Y en este punto quiero que te des cuenta de algo muy interesante que está sucediendo y es que cuando entendemos que en el mundo del deep learning dos de los factores fundamentales para entrenar a IAS más potentes son, bueno, la escala de los modelos, ok, pero también el volumen de datos que utilizamos, pues se están dando una serie de avances que creo que pueden influir muchísimo en esto. Recientemente un paper demostró que utilizando modelos como chat GPT, pues se puede automatizar el proceso de etiquetados de un dataset con una calidad superior a lo que antes se conseguía utilizando a grandes equipos humanos y a una fracción del coste. También con tecnologías como whisper de transcripción de audio hemos conseguido desbloquear todo ese texto que estaba atrapado detrás de las ondas de audios, podcast, vídeos y que ahora podemos sumar también al dataset de entrenamiento. Y ahora sumando la tecnología SAM a este equipo, pues tenemos una nueva herramienta que es potentísima, haciendo a SAM un modelo que no es solo valioso por sí mismo, sino por lo que va a facilitar el entrenamiento de futuros modelos de visión por computador. Y esto, como decía antes, es lo que le interesa a Meta. ¿Por qué? Bueno, porque ellos son Meta de Metaverse, son el Metaverse. Y una de sus divisiones más potentes en la empresa es todo lo que tiene que ver con su hardware de realidad virtual y realidad aumentada. Hardware que se beneficia enormemente de todos los avances que se van dando en el campo de la visión por computador. Y poca broma con esto porque creo que Meta está muy bien posicionada en la próxima revolución que se va a vivir de todo el hardware de realidad aumentada a VR combinado con todo lo que estamos viviendo ahora en inteligencia artificial. Y de hecho ellos mismos han mostrado algunos ejemplos del uso de SAM para captura de vídeo en primera persona, donde el punto central al que se orienta la visión del usuario pues queda segmentada por este modelo, dándole una visión clara y estructurada de lo que el usuario está viendo. ¿Entendéis ahora cómo van encajando todas las piezas? Estamos en un punto donde la realidad empieza a aproximarse de nuevo a la ciencia ficción. Y a partir de aquí, con el ritmo de desarrollo que lleva la IA, pues a ver dónde acaba todo esto. Estamos viviendo tiempos interesantes. Y si te gusta todo esto, si quieres estar al día con la revolución de la inteligencia artificial, si te gusta este contenido, como explico, como te enseño todo lo que está pasando, pues si no lo has hecho todavía, suscríbete a este canal que cada semana estamos hablando con nuevos vídeos de toda la actualidad, también a través de directos. Y si no lo has hecho y quieres apoyar este proyecto y agradecer todo lo que estamos divulgando aquí en abierto para todo YouTube, pues puedes hacerlo a través de Patreon, que es una plataforma donde con una pequeña aportación pues podéis agradecer todo este contenido que os traigo aquí al canal. Nuevamente agradecer a Hostinger por haber patrocinado este vídeo. Tenéis abajo el código de descuento en la caja de descripción y por aquí dos vídeos que son muy interesantes y que deberíais de echarle un ojo. Chicos, chicas, nos vemos en el próximo vídeo. Adiós.
DOC0052|NLP e IA|2022 será recordado como el año de Stable Diffusion, de Dali2, de increíbles modelos generadores de texto como Palm o generadores de código como Alpha Code y sin embargo charlando el mes pasado con Andrés Torrubia, él me comentaba que lo más interesante que había visto este año era una inteligencia artificial que venía del laboratorio OpenAI, una IA llamada Whisper. ¿Qué es para ti de lo que ha salido este año lo más impresionante? Pues curiosamente, fíjate, curiosamente hasta ahora Whisper yo creo, ¿sabes por qué? Curioso, eh? Por lo que me impresiona Whisper es que Whisper funciona, es como para mí Whisper, si fuera del coche autónomo sería el primer self-driving del dictado, ¿sabes? El primero que se parece a una persona. Bueno, pero para que entiendas tú primero qué es esto de Whisper te voy a pedir que hagas el siguiente ejercicio, te voy a reproducir un audio en inglés y tu tarea es transcribir cada una de las palabras que estés escuchando. ¿Estás listo? 3, 2, 1. ¿Has entendido algo? Ya, yo tampoco. Pues a oídos de esta inteligencia artificial esta es la transcripción perfecta que ha conseguido. ¿Y qué tal tu coreano? Bueno, pues para Whisper tampoco es problema y también puede transcribir este audio en perfecto inglés. Y bueno, también me entienda a mí. Esto que está viendo en pantalla ahora es el speech to text que consigue Whisper cuando le paso la pista de audio que estás escuchando. Fíjate bien, no sólo consigue una transcripción casi perfecta, entendiendo incluso palabras concretas como Whisper o speech to text, sino que también es capaz de generar puntos, comas y otros signos de puntuación que a otros muchos modelos comerciales de reconocimiento del habla pues se le suela atragantar. Y esto es muy interesante. Bueno, no esto, sino Whisper. Whisper en general tiene muchas cosas interesantes. Y la primera cosa interesante es el contexto en el que esta herramienta aparece. Tras un año de increíbles logros por parte del laboratorio de inteligencia artificial de OpenAI, de repente de la nada surge una iniciativa colaborativa como Stability.ai que en septiembre toma por bandera el hacer open source, muchas de las tecnologías que OpenAI por su parte pues ha decidido guardarse para sí y compartir sólo bajo servicios de pago. Para mí esto tampoco es un problema, puesto que al final OpenAI como empresa pues tiene que pagar sus facturas y al menos nos está dando una forma de acceder a estas potentes inteligencias artificiales. Aprende Google. Pero claro, llega un muchachito nuevo a la ciudad y empieza a regalar caramelos a los niños y de repente el chico popular pues empieza a haber desplazado. Y en ese preciso momento llega a OpenAI de la nada y nos regala a Whisper para beneficio de todos. Porque sí amigos, esto es open source. Que sé que os encanta escuchar estas palabras. Al final del vídeo voy a enseñar un mini tutorial para que veáis qué sencillo es utilizar esta herramienta y también os voy a compartir un notebook para que sea súper sencillo para vosotros. Y esto es lo que hace a Whisper una herramienta súper interesante, pero no es la única cosa. Y aquí es donde viene una de las cosas que más ha llamado mi atención y es que Whisper no es un complejo sistema que hayan diseñado para procesar audio como nunca antes había hecho o un sistema súper complejo con un montón de módulos de procesamiento. No. Whisper es esto de aquí, una red neuronal de tipo transformer de las de 2017. No tiene ningún cambio, ninguna novedad, es una arquitectura que ya todos nosotros conocemos. Entonces si esto es así, ¿por qué no existía ya una tecnología como Whisper? Pues la clave que hace a Whisper algo tan potente está en los datos y en cómo han estructurado su entrenamiento. Para entrenarlo, OpenAI ha utilizado ni más ni menos que 680 mil horas de audio con su correspondiente texto. Una brutalidad. Y es que si hacéis el cálculo 680 mil horas y empezaras a reproducirlas ahora, acabarías de escucharla dentro de 77 años. Te podrías asegurar que en algún momento en el cielo verías surcar al cometa Halley. Pero es que además una cosa muy interesante es que estos audios vienen en múltiples idiomas, permitiéndonos poder entrenar a un modelo que es multilinguaje, que puede entendernos si le hablamos en español, en inglés, en coreano, da igual. Pero la cosa no se queda solo ahí. Y es que Whisper además de ser un sistema multilinguaje también es un sistema multitarea. Esta es una tendencia que como ya vimos en el vídeo sobre gato, en el mundo del deep learning cada vez es más frecuente. No entrenar a la inteligencia artificial para una única tarea, sino entrenarla para varias diferentes. Haciendo así que su aprendizaje sea mucho más sólido y robusto. Como hemos visto, Whisper puede tomar audios en inglés y transcribirlos al inglés, o audio en coreano y transcribirlo al coreano. Pero el mismo modelo también puede identificar qué lenguaje se está hablando, o actuar como un detector de voz para clasificar cuando en un trozo de audio se está escuchando no a una persona. O también la tarea que más interesante me parece de todas, que tú le puedas hablar a Whisper en cualquier idioma y que él te lo transcriba automáticamente al inglés. Y en este caso no sabría deciros por qué, pero para mí esta me parece una funcionalidad fascinante. Parece que tampoco nos ofrece nada nuevo, no? Al final tú puedes coger el texto que genera cualquier transcriptor de texto en tu idioma y pasarlo por un traductor. Pero en este caso me parece fascinante el ver cómo algo tan sencillo como un único modelo de deep learning te permite poder hablarle en cualquier idioma y que te genere el texto en inglés sin tener que combinar ningún tipo de herramientas. Es súper sencillo. Y lo de los datos que hemos comentado antes también es súper interesante. Porque mi primera intuición aquí es que OpenAI, pues en la búsqueda de un dataset masivo de estas 680.000 horas de audio que tuviera una transcripción de texto para poder hacer este aprendizaje supervisado, pues posiblemente había acudido a una de las mayores fuentes que podemos encontrar en internet, que es YouTube. Al final ya sabéis que todos los vídeos de YouTube tienen generados subtítulos automáticamente. Pues no, justamente en esto OpenAI hace mucho hincapié en su paper para explicarnos que han hecho un proceso de filtrado para eliminar del dataset cualquier aparición de texto generado por sistemas automáticos de reconocimiento del habla. ¿Por qué? Pues justamente para evitar que Whisper aprendiera también aquellos defectos, aquellos vicios que los otros sistemas automáticos también pudieran tener. Dicho esto, ahora que estamos hablando de Whisper y de YouTube, hay una teoría que quiero contaros que me parece muy interesante, no es nada que esté confirmado, pero que podría explicar la razón de existir de esta herramienta y que podría tener cierta relación con un futuro GPT-4. Esta es una idea que escuché en el canal del Dr. Alan Thompson y que dice que en un futuro próximo donde GPT-4 pues empezara a entrenar, Whisper podría ofrecer al sistema una enorme fuente de datos con la que sistemas anteriores no habían contado. Pensemos que un sistema como GPT-3 se ha entrenado con un montón de artículos de Wikipedia, de libros, de foros, de conversaciones de internet, pero nunca ha podido acceder a toda esa fuente hablada que puede estar en bases de datos como YouTube. Una herramienta como Whisper podría ser utilizada para barrer por completo a YouTube, transcribir muchos de sus audios y obtener de bloquear una nueva fuente de datos que antes no habría sido posible utilizar para entrenar a un futuro modelo del lenguaje. Este es el enorme valor que tiene una herramienta como Whisper y que creo que hace tan interesante esta tecnología. No, no resuelve una tarea que sea espectacular como generar imágenes o generar vídeo, pero resuelve una tarea muy útil y casi la resuelve hasta la perfección. Ojo, digo casi, no es perfecta. A veces algunas palabras se equivocan evidentemente y no cubre todos los lenguajes que existen en el planeta Tierra y bueno, por buscar alguna limitación frente a otras herramientas comerciales pues tampoco funciona en tiempo real. Todavía procesar el audio dependiendo de la longitud, pues te puede llevar unos cuantos segundos, a veces algún minuto, pero es una herramienta sólida, es madura, es útil y además open source, permitiendo que ahora cualquiera pueda acceder a una herramienta profesional de transcripción y traducción de texto mejor que cualquier alternativa gratis. ¿Qué? ¿Ah, que también vosotros queréis acceder a esta herramienta? Bueno, venga va, os preparo un tutorial facilito para que todos podáis utilizarlo, vamos a hacerlo en Google Collab. Pero antes y aprovechando que estamos hablando de programación, de desarrollo, de innovación, dejadme que os recuerde que quedan muy poquitos días para que se celebre el Samsung Dev Day, que es el evento tecnológico que celebra cada año la comunidad de Samsung Dev Spain, que es la comunidad oficial de Samsung para desarrolladores españoles. Este será un evento gratuito que no os podéis perder. Si estáis en Madrid podéis asistir presencialmente el día 16 de noviembre en el claustro de los Gerónimos del Museo del Prado y si no, pues podéis conectaros online a través de su streaming. Pero eso sí, hay que registrarse. Yo tuve la suerte el año pasado de poder participar con una ponencia sobre generación de código con inteligencia artificial y la experiencia fue genial. Así que ya lo veis, será un evento cargado de charlas geniales hablando de tecnología, de innovación, de aplicaciones y además va a estar presentado por MiDooDev que seguramente muchos de vosotros le conozcáis, así que no os lo podéis perder. Os voy a dejar abajo en la cajita de descripción un enlace a la página web de Samsung Dev Spain donde vais a encontrar toda la información respecto a la agenda donde registraros y un montón de recursos más. Nos vemos el 16 de noviembre. Pues vamos a ver cómo podemos utilizar Whisper nosotros en nuestro propio código. Para esto vamos a utilizar Google Collab, ya sabéis que Google aquí nos está cediendo una máquina virtual gratuita que podemos utilizar y vamos a verificar siempre que tengamos activado el tipo de entorno con aceleración por hardware GPU. Vamos a darle aquí, vamos a dar a guardar y ahora el primer paso será instalar a Whisper. Para ello vamos a usar estos dos comandos de aquí. Instalar, esto lo podéis encontrar en el propio repositorio de github de Whisper os voy a dejar abajo en la cajita de descripción estos comandos, le damos a ejecutar y dejamos que se instale. Una vez instalado vamos a subir algún audio que queramos transcribir. Yo en este caso voy a probar con la canción de Rosalía de Chicken Teriyaki, vamos a colocarla para acá, la arrastramos y ahora el siguiente paso pues vamos a coger aquí y vamos a poner el comando necesario para poder ejecutarlo. Vamos a darle aquí a song.mp3, se llama el archivo que hemos subido, vale, song.mp3. La tarea va a ser transcribir el tamaño del modelo, hay diferentes tamaños según si quieres más velocidad a la hora de hacer la inferencia o si quieres más precisión en los resultados. Yo por lo general trabajo con el modelo medium que es el que me da buenos resultados, hay modelos mayores, hay modelos menores, probad y en este caso pues simplemente donde vamos a colocar el archivo de salida. Ejecutamos y ya está, ya está, no hay que hacer nada más, vale, ya estamos utilizando Whisper. La primera vez tardará un poco porque tiene que descargar el modelo pero a partir de este momento podéis utilizar este sistema para transcribir cualquier audio que queráis, mola. Vale, vemos que en este caso ha detectado que el idioma es español, ha hecho la inferencia automática porque no le hemos dicho que vamos a transcribir del español, lo podéis hacer si queréis y cuando ya está ejecutada esta celda pues podemos venirnos para acá. Vemos que se ha generado la carpeta audio transcription y aquí tenemos las diferentes opciones, podemos abrir el song.txt y aquí le abrimos el archivo vemos que pues tenemos toda la canción perfectamente transcrita que en este caso siendo la rosalía pues tiene más mérito. Si en vez de querer hacer la transcripción quisierais hacer la traducción, es decir convertir vuestra voz, vuestro audio al inglés, pues lo único que tenéis que hacer es cambiar aquí la tarea por translate y en este caso Whisper trabajará para traducir aquello que ha transcrito. En este caso si os dais cuenta el comando que hemos utilizado ha sido el de consola pero a lo mejor queréis utilizar Whisper dentro de vuestro código entonces también tenéis la opción de trabajar con la propia librería de Whisper, es simplemente esta línea de código de aquí, lo importamos, cargamos el modelo que queramos, aquí pues yo cargaría el modelo medium que es el que como digo funciona mejor para mi caso y con el modelo cargado luego aquí llamamos a model.transcribe, vamos a poner aquí song.mp3, le damos a ejecutar y en cuestión de unos segundos pues ya tendremos de nuevo nuestra transcripción. Y aquí lo tenemos, la rosalía rosa sin tarjeta se la mando a tu gata, te la tengo con roleta, no hizo falta serenata. Pues ok. Igualmente para hacer la vida más fácil he preparado un notebook que podéis utilizar, está abajo en la cajita de descripción donde tenéis ya todo el código listo para empezar a trabajar, simplemente tenéis que entrar comprobar que está la gpu activada, le damos a este botón de aquí para instalar pues todo lo necesario, aquí elegimos la tarea que queremos hacer pues si es transcribir a cualquier idioma o traducir al inglés y le damos a ejecutar. En este caso la celda está preparada para que en el momento en el que empieces a ejecutarla está grabando ahora mismo tu micrófono, es decir ahora mismo estaríamos generando un archivo de audio que luego vamos a utilizar para transcribir con whisper, esto es por si queréis hacer una transcripción en tiempo real de cualquier clase o cualquier cosa que necesitéis. Vamos a darle a parar, le damos a este botón y en un momento tenemos el resultado de lo que hemos dicho. Igualmente luego abajo os añado los dos comandos necesarios para poder transcribir o traducir el audio que vosotros subáis. Por último también tenéis que saber que si queréis algo más sencillo pues hay página webs donde podéis probar este sistema pues subiendo vuestros propios audios o grabando desde el micrófono. Y esto sería 2022 se está ganando la verdad que un año espectacular en cuanto al número de juguetes neuronales que están llegando a nuestras manos para construir un montón de herramientas y para poder toquetearlos. Ahora os toca a vosotros qué podéis hacer con esto pues podéis construir un montón de cosas súper interesantes podéis conectar por ejemplo whisper con stable diffusion para que a viva voz tú le puedas pedir que te genere un cuadro o podéis por ejemplo coger todas vuestras clases en la universidad o todas las reuniones de trabajo, transcribirlas, crear un enorme banco de transcripciones y luego con la API de GPT-3 hacer un chatbot que te permita consultar hacer preguntas y respuestas sobre toda esa fuente de información. Por ejemplo algo que yo quiero hacer es coger pues todos los vídeos de mi canal de youtube y transcribirlo generar subtítulos de buena calidad tanto en español como en inglés y poder hacer estadísticas y consultas de cuántas veces he dicho por ejemplo la palabra machine learning. Hay un montón de aplicaciones que podéis empezar a construir que podéis empezar a crear combinando todas estas tecnologías. También hay un perro ladrando de fondo que me estaba molestando bastante. Bueno lo que decía que podéis crear un montón de cosas y hay mucho por hacer. Desde aquí desde este canal vamos a seguir haciendo experimentos con esta tecnología voy a seguir trayendo nuevas herramientas así que si no lo has hecho todavía suscríbete, dale a la campanita para que te lleguen siempre las notificaciones de que hay vídeo nuevo y si quieres apoyar todo este contenido ya sabéis que podéis hacerlo a través de patreon abajo en la cajita descripción. Tenéis un par de vídeos por aquí que son súper interesantes, no sé cuáles son pero son súper interesantes de echarle un ojo y nos vemos con más inteligencia artificial chicos chicas en el próximo vídeo.
DOC0053|Computer vision e IA|Si eres una persona interesada en la inteligencia artificial o acostumbras a ver mi canal, sabrás que gran parte de la revolución que se ha vivido en los últimos años dentro del deep learning se lo debemos en sus inicios a los avances del campo de la visión por ordenador, y en concreto al desarrollo de un tipo de red neuronal muy importante que está especializada para trabajar con imágenes, las famosísimas redes neuronales convolucionales. La importancia de estas redes viene de su capacidad de poder descifrar los patrones más complejos en enormes datasets de imágenes, dotando de ojos a máquinas que tanto pueden observar rostros de personas como radiografías de pacientes como los peatones que se cruzan ante un coche autónomo. Hemos conseguido que las máquinas puedan percibir el mundo que les rodea. Una tecnología tan importante que, ahora que lo pienso, creo que nunca ha dedicado un vídeo entero a explicar sus fundamentos. A ver, no es cierto esto. En muchas ocasiones en el canal hemos hablado de redes generativas, autoencoders, modelos pics to pics y más cosas, que al final implicitamente están utilizando los principios tras este tipo de redes. Y es por eso que no solamente quiero hablaros de que son las redes neuronales convolucionales, sino que quiero ir más allá y por eso os adelanto de que este vídeo es la primera parte de una trilogía donde no solo vamos a estar viendo los fundamentos detrás de estas redes, sino que también vamos a ver una serie de técnicas de optimización que nos van a permitir entender mejor su funcionamiento. Y no solo eso, sino que os va a encantar, por fin lo sé, va a ver, y a notebooks de toda la teoría. Es decir, vamos a ir programando todos los conceptos que vayamos viendo en esta serie de vídeos. Así que nada, mi consejo es que si no estáis suscritos, os suscribáis, por supuesto. Y si estos vídeos ya están publicados, que echéis un vistazo a la caja de descripción abajo que estarán todos puestos en modo de playlist. Bien, ya estamos todos en el tren del hype. Perfecto, porque antes de empezar quiero hablaros sobre el patrocinador de este vídeo. Se trata de la nueva edición del Master Ejecutivo sobre Inteligencia Artificial en castellano que viene de la mano del IIA, del Instituto de Inteligencia Artificial. Se trata de un máster que está diseñado, está orientado para personas, para profesionales, directivos, inversores que realmente quieran buscar extraer un valor a toda esta tecnología de Inteligencia Artificial que no paramos de ver aquí en el canal y que lo quieran hacer sin cometer los típicos errores que se dan en la industria. No busca ser un máster muy técnico en la parte de matemáticas ni programación, sino que lo que quiere es daros una visión realista de lo que es la Inteligencia Artificial y cómo se puede implementar lejos de sensacionalismos. El poder liderar un proyecto de IA, el poder tomar decisiones estratégicas y el poder ir orientando vuestro perfil profesional hacia una tecnología que todos sabemos que tiene un gran potencial de cambio, pero que no mucha gente sabe llevarlo a la práctica. Además contaréis con una plantilla de profesores que tienen una cosa muy importante y es experiencia real. Destaco el perfil de, por ejemplo, Andrés Torrubia, que si estás metido un poco en el mundo del Machine Learning, os sonará. No sólo por ser una persona que tiene una amplia experiencia dentro de plataformas como Kaggle, sino por también su podcast Software 2.0, donde entrevista a gente muy del ámbito no sólo académico, sino también industrial. Es decir, un perfil de profesores que es todo un lujo. El máster evidentemente, dada la situación actual es 100% online, lo cual es una ventaja para muchos de vosotros y se realizará de enero hasta junio de 2021. Si os interesa, os recomiendo que os matriculeis ya, porque además de que las clases son limitadas, el precio de la matrícula va a estar reducido hasta el 1 de diciembre. Y no sólo eso, sino que también por ser espectadores de Doge.csv, contáis con un código de descuento del 10%. Tenéis toda la información aquí abajo también en la descripción. Así que nada, aprovechad esta oportunidad si queréis mejorar vuestros perfiles profesionales. Echad un ojo a la página web del máster y ahora sí, comenzamos. Para el vídeo de hoy necesitamos entender bien los principios básicos que hay detrás del proceso de visión, no el artificial, sino el humano. Si yo, por ejemplo, te muestro esta imagen de aquí, rápidamente sabría decirme qué es lo que estamos viendo, ¿no? Una cara. Esto es porque tú seguramente has escaneado la imagen y ha detectado la presencia de algunos elementos que tú ya sabes, por tu aprendizaje, que conforman a un rostro. Aquí hay ojos, aquí hay una boca, aquí hay una nariz… genial. Pero ¿por qué sabes que esto es un ojo? Bueno, porque ha detectado una serie de elementos que conforman normalmente a un ojo. Una pupila negra, líneas que son pestañas, superficies blancas… y todo esto también lo has sabido reconocer porque eres capaz de detectar patrones circulares, cambios de contrastes, texturas y así sucesivamente. Al final, si reproducimos los pasos que realiza tu córtex visual, con lo que nos encontraremos es con un procesamiento en cascada, donde primero se identifican aquellos elementos básicos y generales y donde en posteriores capas esto se combina para generar patrones cada vez más complejos. Toda esta neurociencia, que bien explicada podéis encontrar en el libro de mi amigo Ignacio Crespo, fue la fuente de inspiración para investigadores como Jean Lecant, quien en 1989 introdujo el primer diseño de una red neuronal convolucional para la detección de números escritos en cheque pancarios. ¿Y qué es una red neuronal convolucional? Pues un tipo de red neuronal cuyo diseño ha sido pensado para sacar partido a algo muy evidente que encontramos en una imagen, su estructura espacial. Es decir, con una red neuronal típica, clásica, vanilla, la de toda la vida, una red neuronal multicapa, lo que estaríamos haciendo es introducir el valor de cada pixel como si una variable independiente se tratara, como si fuera un vector plano. En ningún momento le estaríamos dando la importancia que tiene su posición dentro de la imagen, significando esto que lo mismo daría a meter imágenes con esta ordenación, que con esta, que con esta otra. La red neuronal multicapa solo vería un vector plano de pixeles, pero no. Nosotros sabemos que en realidad esta independencia de pixeles no es habitual en una imagen. Normalmente el valor de un pixel va a estar muy ligado al de sus pixeles vecinos, tanto en su ancho como en el alto. Y esto es lo que hace que surjan estructuras, formas y patrones que analizadas correctamente nos pueden beneficiar a la hora de entender que estamos viendo. Y con esta idea surgen las redes neuronales convolucionales. Una red neuronal convolucional es un tipo de red neuronal que se caracteriza por aplicar un tipo de capa donde se realiza una operación matemática conocida como convolución. Una convolución aplicada sobre una imagen no es más que una operación que jugando con los valores de los pixeles es capaz de producir una imagen como esta. Ay, perdón. Concretamente cada pixel nuevo que vayamos a generar se calculará colocando una matriz de números que llamaremos filtro o kernel sobre la imagen original y donde multiplicaremos y sumaremos los valores de cada pixel vecino para obtener así el nuevo valor de aquí. Si desplazamos el filtro realizando esta operación por toda nuestra imagen, lo que obtendremos como resultado será esta nueva imagen de aquí. Dependerá por tanto de cómo configuremos los parámetros de nuestro filtro que podremos obtener un resultado u otro. Por ejemplo, un valor central de 1 y el resto de los vecinos a 0 nos hará una copia exacta de la imagen original. Vamos, que no habremos hecho nada. Si por ejemplo multiplicamos por 1 el valor de cada pixel incluyendo a los vecinos y dividimos entre el número total de ellos entre 9, lo que estaremos calculando será la media de los colores de los pixeles. En conjunto nos dará como resultado un desenfoque en la imagen original. Método que se utiliza para implementar los efectos que vemos hoy en día en muchos de los software de edición que utilizamos. Y fíjate bien en esto. Si por ejemplo configuramos los valores del filtro para que estos pixeles de aquí multipliquen en negativo y estos de aquí en positivo, lo que conseguiremos es que cuando el filtro se sitúe en una zona donde los colores son iguales aquí y aquí, ambos términos en positivo y negativo se cancelarán y que para valores contrarios su activación sume más. Escaneando con este filtro toda nuestra imagen, ¿qué sucederá? Es que habremos configurado un filtro que se activará cuando encuentre diferencias de contraste y por tanto que sirva para detectar bordes verticales. ¿Lo ves? Con esto lo que hay que entender es que esta operación de convolución sobre una imagen puede detectar cosas diferentes según cuáles sean los valores del filtro que definamos. ¿Y cuál vamos a definir? Pues no vamos a ser nosotros, sino que estos son los valores que la red neuronal irá aprendiendo poco a poco para poder hacer mejor su tarea. El trabajo de estos filtros para detectar patrones es el principal trabajo de la red neuronal convolucional. A cada una de las imágenes generadas se le conoce como un mapa de características, ya que actúa como un mapa donde se nos indica en qué parte de la imagen se ha detectado la característica buscada por dicho filtro. Cada píxel blanco será una activación que nos indique que el elemento buscado estaba ahí. Por tanto, entra una imagen, aplica una serie de convoluciones y genera un conjunto de mapas de características. ¿Es eso todo? Pues podríamos decir que sí. La cosa es que al final, el potencial de este tipo de redes se encuentra en que esta operación se va a realizar secuencialmente, donde el output de una de las capas se va a convertir en el input de la siguiente. Fíjate por ejemplo cómo si de entrada tenemos una imagen normal a color, con sus tres canales RGB, esto se podría interpretar como que en realidad tenemos tres mapas de características diferentes donde se han detectado elementos en rojo, en verde y en azul. ¿Lo ves? Tenemos tres mapas de características donde si ahora realizamos la operación de convolución con vamos a decir 16 filtros, podremos detectar 16 cosas diferentes que notarán como resultado 16 mapas de características. 16 imágenes que ahora pasarán a ser el input de la siguiente capa convolucional. Pregunta. Perfecto, sí, ok. Podemos utilizar filtros de tamaño 3x3 o 7x7 píxeles para ir escaneando poco a poco nuestra imagen a la búsqueda de patrones. ¿Pero es esto suficiente? ¿Es posible que un filtro tan pequeño pueda detectar patrones tan complejos como por ejemplo un ojo, la rueda de un coche o el comportamiento errático ante el miedo al fracaso de una persona que vive en una sociedad que ni se plantea ni busca un objetivo común a seguir? ¿Es posible que un filtro detecte todo esto? No lo sé, dímelo tú. Ah, has preguntado. Vale, te respondo yo. La respuesta la encontramos en la sucesión de capas, en el deep del deep learning, en la profundidad de la arquitectura de nuestra red. Lo que sucede es que la operación de convolución cada vez se va a ir haciendo más potente. Fíjate, ahora donde antes teníamos una región de 9 píxeles, nuestro filtro lo ha convertido en un único píxel de información. Y por tanto, si volvemos a aplicar ahora una convolución sobre estos mapas de características, en realidad estaremos accediendo a cada vez más información espacial de la imagen original. Esto es algo que además podemos incentivar reduciendo cada cierto tiempo la resolución de nuestros mapas de características. Ya Carlos, pero es que al final te estás flipando mucho con esto de las convoluciones ya que no dejan de ser unas operaciones que sí, que te pueden detectar cambios de contraste, de texturas, superficies planas, pero tampoco pueden hacer mucho más, ¿verdad? Pues sí y no. A ver, sí es cierto que solamente pueden hacer eso, pero piensa que aquí estamos haciendo detecciones sobre las detecciones recibidas de las capas anteriores. Es decir, al final la operación de convolución por sí sola tampoco puede detectar cosas muy complejas más que bordes y patrones muy simples. Pero volver a hacer estas detecciones sobre las detecciones anteriores nos permite componer cada vez patrones más complejos. Por ejemplo, imagínate un filtro que pudiera detectar la presencia de un elemento en la región de los píxeles vecinos en estas cuatro esquinas. Un filtro como este. No parece gran cosa, ¿verdad? Pero imagínate que esto se aplicara ahora sobre los mapas de características de haber detectado en las capas anteriores bordes con diferentes inclinaciones como estas. Esto, si lo componemos, podría darnos como resultado una forma geométrica sencilla parecida a un rombo. Y esto, a lo mejor sumado con el resultado de otro mapa de características así, nos podría empezar a generar un patrón sencillo de un ojo. Es por eso que el diseño de una arquitectura convolucional normalmente se representa en artículos y papers como una especie de embudo donde la imagen inicial se va comprimiendo espacialmente, es decir, su resolución va disminuyendo, al mismo tiempo que su grosor va aumentando. Es decir, el número de mapas de características que vamos detectando va en aumento. ¿Y dónde acaba todo esto? Pues bueno, cuando tu imagen ya pasa por todo el embudo convolucional, ya llegaremos a un punto donde habremos detectado todos los patrones necesarios. Contaremos con muchos mapas de características que ahora sí podemos meter como input independientes dentro de una red neuronal multicapa que acabará por tomar la decisión de qué es lo que hay en esa imagen. Y esto ahora sí son los fundamentos de una red neuronal convolucional. Un tipo de arquitectura de diseño elegante y que realmente si os fijáis cuenta con una serie de operaciones que son bastante sencillas y donde el único aprendizaje, el único entrenamiento que se realiza es el de aprender los filtros. Es súper interesante ver cómo la red aprende automáticamente esta idea jerárquica donde Los primeros filtros son filtros muy básicos y donde luego por composición se van encontrando filtros cada vez más abstractos. Una arquitectura cuyo diseño ha estado presente en toda la revolución que hemos vivido en el campo del deep learning y que nos ha permitido contar con todos los avances que disfrutamos hoy en día. Quedan una serie de preguntas. Cómo podemos estar tan seguros de esto? Cómo sabemos realmente que la red está aprendiendo jerárquicamente? Cómo sabemos qué patrones está aprendiendo? Acaso es posible abrirla y poder acceder a sus entrañas? La respuesta es que sí. Y el cómo lo sabréis en el próximo vídeo. Por fin tengo ya el vídeo de redes neuronales convolucionales. Era un vídeo que llevaba mucho tiempo queriendo hacer porque es algo fundamental en todo lo que vamos viendo en el canal y espero que os haya servido, que os haya servido para entenderlo mejor, para aprender si no lo conocíais, para que lo tengáis como recurso para ponerlo en clase. Y en cualquier caso, si tiene un valor para vosotros, recordaros que está disponible el patrón del canal para que apoyéis todo el trabajo que desinteresadamente hago aquí en el canal para todos vosotros, para que tengáis toda esta información gratis en YouTube. También os recuerdo que tenéis abajo en la descripción toda la información del máster que os he comentado al principio por si queréis echarle un vistazo y nada más. Nos vemos con más inteligencia artificial en el próximo vídeo.
DOC0054|Computer vision e IA|Este vídeo está patrocinado por la Universitat Politécnica de València. Llevo unas cuantas semanas con una idea en la cabeza y es que creo que el campo de la inteligencia artificial se está acelerando, que las cosas están yendo bastante más rápido de lo que pensábamos y que el deep learning ha tomado un camino muy prometedor con esta línea de modelos generalistas que son capaces de hacer un montón de cosas que creo que en los próximos años nos van a traer un montón de sorpresas y aplicaciones increíbles. En ese sentido GATO, uno de los últimos trabajos del laboratorio DeepMind, viene a hacer una buena demostración de ello y es que GATO consigue hacer lo que ya hacía DQN o Mew0 en trabajos anteriores, aprender a jugar con un rendimiento superior al del humano a diferentes juegos de la consola Atari. Pero también GATO va a poder generar texto realista como haz un modelo tipo GPT-3 y va a permitirte poder chatear con él o hacer descripciones de una imagen dada como input. Pero no solo eso y es que GATO también puede aprender a generar los movimientos que tiene que realizar un brazo robótico real para resolver por ejemplo la tarea de apilar cubos. Una misma red entrenada, GATO, puede aprender a hacer todo esto. Y esto es importante porque mira, cuando hace un par de años DeepMind presentó el sistema Mew0 que era capaz de adaptarse a diferentes juegos de Atari y aprender a jugar muy bien a ellos, pues lo sorprendente de aquel sistema era que realmente habían diseñado una arquitectura de Deep Learning que era capaz de adaptarse a diferentes tipos de problemas. Tú tenías que entrenarla para cada juego y ésta pues te daba un muy buen rendimiento jugando a ellos. Mola. Pero de lo que estamos hablando hoy es algo mucho más impresionante porque sí, estamos hablando de una misma arquitectura pero entrenada una única vez. Entrenada para poder adaptarse a cada uno de estos juegos y no solo eso sino todas las otras tareas que hemos descrito anteriormente. Sí, aprender a jugar a juegos pero también a chatear, a describirte imágenes, a mover brazos robóticos. Una misma red neuronal entrenada para gobernarlas a todas las tareas. Este trabajo va en la línea de lo que ha demostrado modelos como GPT-3 en el campo del procesamiento del lenguaje natural. ¿Por qué diseñar y entrenar a un montón de modelos independientes para que hagan tareas muy concretas? Como por ejemplo aprender a resumir un párrafo de un texto o aprender a traducir del español al inglés. Si en realidad lo que podemos hacer es coger a uno de estos modelos, escalarlo a lo bestia, darle un montón de datos y entrenarlo para una tarea mucho más genérica como aprender el lenguaje. Cuando a GPT-3 se le entrena para aprender a predecir cuál es el siguiente token, cuál es la siguiente palabra dada una secuencia anterior pues aprende muy bien su tarea. ¿Puede autocompletar texto? Sí. Pero es que además esta tarea de autocompletar es lo suficientemente versátil para que el modelo pueda realizar otras tantas tareas para las que nunca fue entrenado. Y es que al final esta tarea de autocompletar texto es lo suficientemente versátil para poder adaptarla a otro tipo de tareas. Por ejemplo, para un texto muy largo yo podría escribir en una frase un resumen de este texto sería y darle autocompletar. Y GPT-3 me haría un resumen. Pues esta misma idea es la que GATO quiere explorar, pero en este caso ampliando el abanico de tareas que este sistema puede resolver. Y es que el truco es el siguiente, mira. Cuando digo que esto está muy inspirado en cómo funcionan los enormes modelos del lenguaje basados en transformers que aprenden muy bien a modelizar secuencias de datos es porque aquí la arquitectura de GATO es eso, un transformer. En este caso un transformer de 1200 millones de parámetros. Algo que no es tan grande a lo que ya estamos acostumbrados últimamente. Y es que su tamaño sería semejante a lo que sería un modelo GPT-2. Claro, la cosa es, si al final aquí el transformer lo que hace es modelizar muy bien secuencias de token, como por ejemplo texto, ¿cómo podemos adaptarlo a este problema que tenemos, donde tenemos un montón de fuentes de datos diferentes? Pues no pasa nada, si Mao manova la montaña, la montaña se descompone y se convierte en secuencias de tokens. Por ejemplo, texto, ya sabemos que esto es fácilmente convertible a una secuencia de vectores. Imágenes, no pasa nada, las descomponemos en parches de 16x16 píxeles y las suministramos como secuencias. Y por ejemplo para los juegos de Atari, pues tampoco pasa nada, las acciones que el usuario hace en el mando también se van a registrar como una secuencia de botones pulsados. O si queremos enseñar a un brazo robótico a moverse, pues los ejes de giro del brazo robótico serán dados como una secuencia de entrada. Todos los datos de distinta naturaleza adaptados a las necesidades del transformer. Y así funcionaría Gato. Por ejemplo, en el caso del juego de Atari, pues los datos que tenemos son, por un lado, la observación de la pantalla, el estado del juego, y por otro lado, las acciones que hacen que el juego evolucione, los botones que pulsas con el controlador. Pues lo que hace Gato es tomar como input esta secuencia de acciones y observaciones de la pantalla, codificadas como hemos visto, y de esta secuencia aprenderá a hacer predicciones de cuál es la siguiente acción que el jugador tendría que ejecutar. Esta acción será llevada al entorno de simulación donde se ejecute el juego, lo que generará nuevas observaciones que volveremos a codificar para así poder repetir el proceso generando más y más acciones. Así con estas estrategias como Gato puede adaptarse al contexto de los juegos de Atari, pudiendo aprender estrategias que, como digo, le da un rendimiento superior al del humano. Pero claro, es que la misma red entrenada también es capaz de generar texto, también es capaz de generar movimientos de brazos robóticos, también es capaz de describirte imágenes... Es una locura. Esto es lo interesante de este trabajo, y es que entra de lleno en una de las tendencias más interesantes dentro del campo del deep learning de los últimos años. La multimodalidad. El dejar que las redes neuronales puedan aprender de un montón de datos de diferente naturaleza, imágenes, texto, audio, y que con esto está demostrado pues puedan tener un aprendizaje mucho más rico de la información que le estamos suministrando. Por ejemplo, en 2021, cuando nuestros sistemas de visión por ordenador dejamos de entrenarlos únicamente con imágenes, empezamos a integrar también información de lenguaje natural, pues esto funcionaba mejor. Ahora lo que está haciendo DeepMind es ampliar la fuente de datos y el número de tareas a resolver, dejando que un sistema lo pueda aprender todo. ¿Y no se parece esta idea de multimodalidad y aprendizaje multitarea a cómo funciona nuestro cerebro? Este trabajo es interesante no por lo que pueda ser ahora mismo. Pues el sistema a veces funciona muy bien, a veces funciona bien, a veces regular y a veces mal. Como por ejemplo en la generación de chats o descripción de imágenes. Preguntamos, explícame los principios de los agujeros negros. Y Gato responde, creo que estás intentando que te responda una pregunta sencilla, ¿qué es un agujero negro? Sí, efectivamente comienza. Bueno, pues los agujeros negros usualmente ocurren cuando una estrella muere e implosiona y hay una reacción en cadena. Esto típicamente causado por la expansión de los núcleos de helio, vale, bueno, no está mal. O por ejemplo, ¿cuál es la capital de Francia? Marsella. ¿Por qué es Marsella famosa? Por la música jazz. Ok, se inventa los hechos en esta conversación, algo que también le suele pasar a GPT-3. ¿Cuál es la capital de Francia? París. Bueno, en este caso dicen que el rendimiento es muy malo. La verdad que estas conversaciones son bastante decentes, mejores incluso que GPT-2, que era un modelo incluso de mayor tamaño que Gato. Pero bueno, es cierto que estas son conversaciones que ellos han seleccionado específicamente por el buen rendimiento del sistema. Por lo general suelen ser muy superficiales y en muchas ocasiones con datos que son erróneos. Algo similar sucede también con la descripción de imágenes. Podemos ver aquí, en este caso sí seleccionadas aleatoriamente, pues que son descripciones relativamente buenas. Pero que en algunas ocasiones se equivocan. Un hombre con barba que mantiene un plato de comida, bueno, más o menos. Un grupo de personas junto a un caballo, más o menos los conceptos están. Un par de personas que están en el océano, bueno esto sería equivocado, pero vemos que las siguientes descripciones si serían correctas. Un surfero que está tomando una ola en el océano. Un jugador de béisbol que está golpeando una bola. La verdad que es impresionante que el mismo sistema que ha aprendido a jugar los juegos de Atari, repito, también sea capaz de hacer esto. Es algo que en mi cabeza estoy todavía intentando encajarlo. Pero claro, ¿funciona? Pues sí, pero no es tan impresionante si lo empezamos a analizar para cada una de las tareas en específico. Contamos a día de hoy con sistemas de deep learning mucho más potentes que consiguen rendimientos superiores para muchas de estas tareas que estamos viendo. Pero es que la cosa es que Gato no tiene por qué sobresalir en todas estas tareas. No está diseñado con ese plan. La idea tras este modelo, como he dicho antes, es similar a la de GPT-3. Vamos a entrenar a un gran modelo con una tarea lo suficientemente genérica como para poder adaptarla a otras múltiples tareas. Y en este caso el rendimiento que pueda tener no nos va a importar tanto. Porque la cosa tras estos modelos y donde reside su enorme potencial, es en que nos va a servir de base para luego nosotros poder reentrenarlos a la tarea que queramos. Eso sí, utilizando una menor cantidad de datos. Muchísimos menos datos de lo que necesitaríamos si lo fuéramos a entrenar de cero. Y otra cosa más, lo que estamos viendo hoy es el primer paso de lo que está por venir. Porque al final este modelo no deja de ser semejante en tamaño a un modelo tipo GPT-2. Pero que no nos tiene que haber duda de que en futuros años veremos versiones mucho más grandes y potentes de este sistema. Si Gato todavía no ha evolucionado a Tigres, en parte por la motivación de aplicar este proyecto a la robótica real. Si el modelo fuera mayor, los tiempos de inferencia aumentarían. Y esto con la capacidad de computo que tenemos a día de hoy sería inviable para poder ejecutarlo en tiempo real. Y aquí nos encontramos en 2022. La verdad que es increíble lo rápido que está avanzando el campo de la inteligencia artificial. Yo llevo unas semanas desde la salida de Dali2, que viendo todos los avances que están surgiendo, pues Dali2, Palm, Flamingo y un montón de cosas que quiero traeros aquí al canal y que quiero contaros y explicaros. Porque es verdaderamente impresionante. Pues creo que he visto actualizado mi perspectiva respecto a lo que la inteligencia artificial podrá conseguir en los próximos años. No sé si realmente estoy deslumbrado por lo que Dali2 nos ha mostrado. O si es que realmente estoy empezando a entender los cambios de paradigma que están ocurriendo en el mundo del deep learning. Y también el potencial que se esconden tras nuevos modelos que están surgiendo. Pero de verdad creo que estamos en un camino mucho más interesante de lo que estábamos hace un año. Pensad que, bueno, si visitáis vídeos de 2019, 2020, cuando yo hablaba de redes como la de AlphaStar, que conseguía jugar al StarCraft de forma súper eficiente. Fijaos que esos modelos eran modelos que tenían una gran complejidad. Tenían un montón de módulos de procesamiento para procesar cada uno de los tipos de información que se integraban en aquel sistema. Y al final todo aquello era para resolver una única tarea, que era jugar muy bien al StarCraft. Es decir, para resolver una única tarea compleja necesitábamos de un montón de sistemas y módulos de procesamiento diferentes. Y ahora quiero que lo comparemos con lo que tenemos hoy en 2022. Que para resolver una gran variedad de tareas que son complejas en sí misma, ahora el único cerebro que estamos entrenando es esto de aquí. La versatilidad de estas propuestas me hacen pensar que en un futuro cercano nos será extraño tener robots físicos reales con los que podamos verdaderamente interactuar y que podamos entrevistarles y preguntarles sobre oye, ¿qué estás viendo? ¿Qué estás observando a través de tus sensores, de tus cámaras? ¿Cómo estás moviendo tus articulaciones? ¿Cómo te estás desenvolviendo en el mundo que te rodea? ¿Cuál ha sido la estrategia inteligente que has desarrollado para resolver inteligentemente esta tarea que yo te he asignado? Esto es fascinante. Lo repito, estáis tratando con una versión de 2CV que ahora mismo está muy hypeado y que tiene las expectativas muy por las nubes. Así que tomadlo como tal. Pero quiero decir que si hace un año o dos años me hubieras preguntado si algún día llegaríamos a alcanzar este concepto de la Inteligencia Artificial General, la AGI, yo te hubiera dicho que no, que seguramente eso era un concepto muy alejado de lo que estamos haciendo a día de hoy. Una ciencia ficción, una quimera del campo del deep learning que no llegaríamos a alcanzar. Y hoy lo que te diría es que a la vista de los resultados que se van obteniendo y teniendo en cuenta que todavía quedan muchos obstáculos por resolver y que todavía el objetivo está muy lejos, creo que hemos encontrado el camino correcto para movernos en la dirección adecuada. Y mi sensación es que este camino que estamos recorriendo, que dio comienzo hace 10 años con la revolución del deep learning y las redes neuronales, pues creo que está entrando en una nueva etapa. Llamémosle deep learning 2.0 como sea, pero es una etapa donde conceptos como los modelos base o la multimodalidad o nuevos modelos que están surgiendo, pues nos están redefiniendo las expectativas que podemos tener sobre lo que la Inteligencia Artificial será capaz en los próximos años. Creo que nos aproximamos hacia una década que es fascinante. Y ya sea si estoy equivocado completamente porque hoy estoy hypeado hasta las nubes o si verdaderamente tengo razón, en los próximos vídeos que están por venir al canal quiero explicaros cómo funcionan todos estos factores que creo que están redefiniendo lo que es el campo del deep learning. Si os gusta este contenido, mis recomendaciones es que os suscribáis al canal y activéis las notificaciones para recibir siempre alertas cuando suba vídeo. Y si queréis apoyarlo pues ya sabéis que lo podéis hacer siempre a través de Patreon. Si queréis apoyar la divulgación en el campo de Inteligencia Artificial, si queréis apoyar toda esta revolución y que todo el mundo lo pueda conocer y vosotros seguir aprendiendo de todo esto, pues la mejor forma de hacerlo es a través de Patreon y os dejo abajo un enlace para que podáis echarle un vistazo y hacer vuestra aportación. Chicos, chicas, muchas gracias como siempre y nos vemos en el próximo vídeo. Como siempre, el futuro que está por llegar lo conocerás antes aquí en DotsCSUV.
DOC0055|Computer vision e IA|Hace cosas de un año el panorama de la creación de contenido con inteligencia artificial vivió su gran tsunami con la salida de Stable Diffusion, el primer modelo de generación de imágenes a partir de texto que se liberaba en abierto para que cualquiera, para que tú, para que yo, lo podíamos descargar y utilizar en nuestro PC sin límites y sin control. Fue en agosto de 2022 cuando se inició la revolución de la creación de contenido con inteligencia artificial. Y ahora, cuando casi se cumple un año de la salida, Stability AI, la principal impulsora de todos estos modelos, ha dado un golpe sobre la mesa AU para publicar Stable Diffusion XL. De nuevo, un modelo open source que todos podemos descargar y utilizar en nuestros ordenadores y que ahora sí presenta una mejora sustancial en cuanto a la calidad de imágenes que genera, quedándose muy cerquita de otros modelos privados como Mid Journey, que creo que lo van a tener bastante complicados para no ser arrollados por el tsunami del open source. Así que en este vídeo vamos a responder las grandes preguntas. ¿Es Stable Diffusion XL una mejora tan importante respecto a los modelos anteriores? Y en ese caso, ¿cuáles son las mejoras? ¿Es solo calidad o hay algo más? Y bueno, la pregunta fundamental, ¿cómo podéis vosotros utilizar este modelo de forma gratuita y sin necesidad de un ordenador superpotente? Las respuestas a todas estas preguntas las tendréis en este vídeo para que vosotros también podáis surfear este tsunami que ya está aquí. Y crearás imágenes espectaculares que querrás mostrar al mundo en tu propia página web personal, en tu propio proyecto. ¿Cómo lo haces? Pues a través de Hostinger, que con todo su servicio va a ser muy sencillo crear en minutos, guiado paso a paso, tu propia página web. Ya que Hostinger pone a tu servicio un creador de sitio web superintuitivo donde podrás elegir entre una gran variedad de plantillas de calidad que te gusten. Pues si estás buscando algo profesional y moderno, esta sería perfecta. Y luego podrás ir modificando cada elemento para darle tu toque personal, simplemente clicando y arrastrando elementos. ¿Eh? Como dices, que en el siglo XXI esto es trabajo para una inteligencia artificial. Pues ojito que en su creador de webs también hay funcionalidades de inteligencia artificial para generación de texto, para la generación de los logos de la webs e incluso modelos predictivos que te dibujarán un mapa de atención de qué partes será más interesante para tus usuarios, para darlo en cuenta en el diseño. Y luego también te fostean la página web, que también está muy bien. Si queréis probar todas estas funcionalidades a muy buen precio, os dejo este código de descuento que tendréis abajo también en la cajita de descripción para que lo probéis. Aprovechadlo y ahora vamos a generar unas cuantas imágenes. Pues la primera respuesta es obvia, más calidad de imagen. Sacadito de la caja, podréis comprobar cómo los primeros prompts que probéis con el modelo XL os darán resultados muy superiores a las versiones anteriores de Stable Diffusion. Rápidamente podréis ver cómo los prompts que probabais en la versión 1.4, 1.5 y 2.1 llevados al modelo actual se convierten en imágenes mucho más espectaculares. Pues vamos a probar. Uno zoopanda, vestido de Gandalf. Fotografía de un patito de goma gigante en mitad de la ciudad de Madrid. Pintura al óleo de un robot metido en un tarro de cristal al estilo de Van Gogh. O por ejemplo mi prompt favorito, un pangolin surfeando una ola. Como veis estos son resultados que se acercan bastante a lo que otros modelos comerciales y privados, como Dalidos en sus últimas actualizaciones o Mid Journey, pueden ofrecer. Con la ventaja de que en este caso Stable Diffusion, amigos, es open source. Y en este caso la mejora en calidad de imagen no solo viene por la parte visual, sino también en la resolución, siendo en este caso las imágenes generadas imágenes de 1024x1024, lo cual aporta mucho más detalle y calidad a las imágenes generadas. Respecto a esto también hay mejoras que son más sutiles, pero que aportan muchísimo al resultado final, que se han introducido en la fase de entrenamiento. Pues por ejemplo durante el entrenamiento se ha tenido en cuenta el centrar correctamente el motivo importante de cada imagen utilizada para conseguir así que Stable Diffusion no genere resultados que estén mal centrados, ya que con los modelos anteriores era habitual ver resultados con partes importantes que caían fuera de la imagen y que ahora con el modelo XL pues ya no es un problema que ocurra en la mayoría de casos. Pero las mejoras en calidad no son las únicas mejoras que trae este modelo, y es que Stable Diffusion XL es más inteligente. Al final lo interesante de estos modelos no es que solo generan imágenes visualmente bonitas, sino también que tengan cierta lógica. Si yo le pido un pangolin surfeando lo que quiero es un pangolin subido en una tabla en el mar, y no otra cosa diferente. Y aquí sí se ha podido comprobar que Stable Diffusion XL responde mucho mejor a los prompts que escribimos, entiende mejor qué es lo que se le pide y esto añade mucho más control sobre esta herramienta. Y esto es algo difícil de comprobar hasta que pongamos a jugar con el modelo y veamos qué también responde, pero sí es cierto que ya se ha demostrado que Stable Diffusion XL es lo suficientemente versátil como para ejecutar numerosos estilos diferentes. Y hay gente que ha creado catálogos enteros probando diferentes estilos y técnicas con el modelo XL, y los resultados que se observan son muy prometedores. Además sus mejoras en inteligencia también vienen demostradas por sus mejoras en capacidades como saber entender mejor cómo distribuir los elementos en una imagen según se lo pidamos nosotros, pues quiero que aparezca un cubo rojo sobre un cubo azul. Esto lo puede hacer mejor. Mejoras también a la hora de entender la cardinalidad, pues quiero tres gatitos en una imagen y que no te genere ni dos ni cuatro. O mejoras también a la hora de generar texto que sea legible, que se pueda entender. Algo que no funciona siempre al 100%, todavía hay letras que se repiten, palabras que no salen correctamente, pero ciertamente este es el mejor modelo que lo ejecuta, tanto si lo comparamos con los modelos open source como con los modelos privados. Así que sí, StableDiffusion XL es más y es mejor. Pero ¿por qué XL? Pues es XL porque se trata de un modelo más grande, con tres veces más parámetros que el modelo de StableDiffusion original. De hecho, el modelo ni siquiera es un único modelo, sino que en realidad son dos. Por un lado tenemos el modelo base que se va a encargar de hacer una primera propuesta de generación, que podemos observar como una imagen final. Y luego hay un modelo refinador, un refiner, que a modo de image to image, pues va a tomar el resultado del modelo base y va a depurarlo añadiendo más detalle fino y haciendo que la imagen se vea con más calidad. Esto de aquí sería el antes de aplicar el refiner y esto sería el después. Y claro, teniendo no uno sino dos modelos y además diciendo que son más grandes, pues ya me veo a todos vosotros preguntando, pero Carlos, ¿esto cabe en mi GPU? Porque ya sabemos que es marca de la casa del equipo de Stability AI, pues todos los modelos que van liberando se puedan ejecutar en hardware convencional como el que tú tienes en tu ordenador. Pero este es un requisito que es complicado de mantener cuando al mismo tiempo se está intentando competir en calidad contra modelos como Mid Journey. Y aquí el equipo de Stability ha hecho un trabajo impresionante. Han comentado que este modelo podrá ejecutarlo si cuentas con una GPU que tenga 8 gigas de VRAM, que debería ser suficiente para ejecutar al menos el modelo base y que seguramente de esa cifra luego la comunidad Opensource cuando lo vaya optimizando pues consiga bajar esto a valores de 6 gigas o 4 gigas a costa de que el modelo vaya más lento. Entonces, ¿qué opciones tenéis si queréis probarlo? Pues tenéis varias. Como el modelo Opensource, pues ya muchos servicios lo han integrado en sus herramientas y páginas como ClipDrop o como Playground ya ofrecen, pues si te registras el poder utilizar el modelo un poquito de forma gratuita. Simplemente preocuparos de tener seleccionado el modelo de StableDiffusion XL, configurad lo que necesitéis y escribid vuestro prompt y testead que os devuelve el modelo. Esto está muy bien para sacar unas primeras impresiones de que también rinde StableDiffusion XL. Ahora, si queréis tener un control total, pues la mejor opción es tenerlo ejecutado en vuestro ordenador. Si contáis con el hardware suficiente para poder ejecutarlo, os recomiendo que echéis un vistazo a este tutorial de aquí, donde os va a guiar paso por paso sobre cómo instalarlo en vuestros equipos. Y ahora, para los que no contéis con el hardware suficiente en vuestros ordenadores, os voy a enseñar cómo podéis instalarlo y utilizarlo de forma gratuita a través de Google Collab. Vamos a estar trabajando con este Google Collab Jack. Ya sabéis que Google Collab nos ofrece hardware gratuito, no estamos trabajando con la GPU de vuestro ordenador, estamos trabajando con la GPU que Google nos ofrece de forma gratuita. Y lo primero que tenemos que verificar es que en entorno de ejecución, cambiar tipo de entorno de ejecución, aquí esté marcado Acelerador por Hardware GPU. Con eso ya sabemos que estamos trabajando con la GPU como corresponde. Vamos a clicar aquí, le vamos a decir ejecutar de todos modos. Vamos a esperar a que se ejecute esta celda de aquí. Cuando se haya ejecutado, luego vamos a ejecutar esta otra celda de acá. Este proceso puede tardar 1 o 2 minutos mientras se va instalando todo. Poco a poco se va instalando y ya estamos por la segunda celda. Y vamos a esperar a que aquí en el texto que está apareciendo pues nos aparezca el enlace que estamos buscando. Vemos que todo está funcionando sin errores y vamos a buscar en el texto pues esta cajita de aquí donde hay un enlace que nos va a llevar justo a la interfaz que vamos a estar utilizando. Y esta interfaz es la de StableSwarm UI, que es la interfaz que ha sacado el equipo de Stability junto a StableDiffusion XL, que todavía está en alfa, todavía está en una versión bastante prematura, pero que promete bastante. Porque, sobre todo, te permite, si tienes en tu ordenador, pues varias GPUs, poder coordinar todas ellas para ejecutar StableDiffusion, lo cual está bastante bien. Lo primero que tenemos que hacer aquí es hacer la instalación de esta interfaz. Pues vamos a aceptar la licencia, elegimos el color de interfaz que nos guste, en este caso oscuro. Vamos a elegir que lo vamos a ejecutar solo en este PC, en este caso en la máquina de Google Collab. Vamos a elegir la forma en la que se va a ejecutar StableDiffusion, en este caso con la interfaz de Config UI, bastante popular últimamente. Y aquí importante tenemos que seleccionar los modelos que queremos. Podéis elegir el modelo 1.5 de StableDiffusion, el del año pasado, el 2.1. Bueno, podéis activarlo si queréis hacer comparaciones. Pero en este caso lo que nos importa es el StableDiffusion XL, el base y el refriger. Con esto ya aquí nos dice que esta es nuestra configuración. Si estamos de acuerdo con ello, le damos a Instalar. Y claro, como estamos trabajando en Google Collab, pues cada vez que cerrais Internet, a la suerte vais a tener que repetir este proceso de instalación. ¿Por qué? Porque la máquina que Google os está dando se va a eliminar una vez terminéis de trabajar con ella. Con lo cual, esta instalación de la interfaz de StableDiffusion la tendréis que repetir en cada caso. No sucedería así si estuvierais trabajando en vuestro equipo en local, pero es lo que hay. Al final es hardware gratuito. No nos vamos a quejar. Y una vez instalado, ya tenéis aquí la interfaz. Esta es una interfaz visual similar al Automatic 1111, quien haya trabajado con él. También tenéis por aquí el editor basado en nodos, que está ganando bastante popularidad de Confi. Pero en este caso vamos a centrarnos en esto. Lo que tenemos que ver es que los modelos están bien instalados. Para eso vamos a venir aquí abajo a la pestaña de modelos. Vamos a clicar estos dos puntitos de aquí para que se nos abra el enlace donde está Official StableDiffusion y aquí deberían aparecer los modelos que hayamos instalado. En este caso, el StableDiffusion 1.0 y el StableDiffusion Refine. Como quiero trabajar con el StableDiffusion base para generar imágenes, vamos a clicarlo. Cuando lo clico vemos que aquí está seleccionado y vemos que automáticamente aquí se ha reconfigurado todo para que la resolución ya esté en 1024x1024. Podéis venir aquí si queréis cambiar el aspect ratio por la proporción de alto y ancho de la imagen. Podéis seleccionar diferentes aspect ratios, en este caso lo dejo 1x1 para que salgan imágenes cuadradas. Y ya solamente con esto podéis venir aquí al prompt y podéis escribir lo que sé. Vale, pues por ejemplo, un perro vestido de pirata bajo el agua. Fotografía para que saque aquí una fotografía. Y con esto vamos a darle a Generar Imagen y vamos a ver si todo funciona. Oye, Carlos, que me está tardando muchísimo. Bueno, es que la primera vez que le damos a ejecutar no solo se está generando la imagen, sino que podéis leer aquí que se está cargando el modelo, con lo cual va a tardar un poquito más, pero luego ya veréis que la media de generación va a estar entre 20-30 segundos, lo cual puede parar. Una opción que es gratuita no está nada mal. Podéis utilizar Stable Diffusion, el modelo más avanzado, el XL, generando imágenes cada 30 segundos. Bastante bien. Y lo vemos, lo vemos, lo vemos. ¡Boom! Primera imagen generada con Stable Diffusion XL. Y bastante, bastante bien. Fijaos que la imagen tiene muy buena calidad. Se cumple todo lo que le hemos pedido. Aquí hay un símbolo un poco raro de pirata. Y además, muy interesante también por debajo de la imagen, va a ir integrado en ella todos los metadatos sobre cómo la hemos generado, lo cual va a venir muy bien a la hora de intercambiar imágenes a través de internet. Fijaos además que el resultado está bastante bien siendo solo el modelo base el que ha entrado en juego. Aquí no hemos utilizado el refinador todavía. Esto es el resultado que te genera de Stable Diffusion base. Y ya con ello podéis generar imágenes bastante espectaculares. Aún así, si queréis utilizar el modelo refiner, ¿cómo lo vamos a hacer? Pues vamos a venirnos aquí abajo, vamos a activar el refiner y vamos a abrir este menú para seleccionar el modelo refiner. Vamos a activar esto aquí. Y aquí debería de aparecernos el modelo refiner. Yo me estoy encontrando que esta interfaz, todavía al estar en alfa, tiene bastantes bugs. Uno de ellos es este, que el listado de modelos a veces falla. Entonces el truco aquí es simplemente darle a F5, actualizamos. Y tras actualizar, si no ha fallado nada, fijaos que aquí ya sí que nos aparece el Stable Diffusion refiner. Entonces, si tenemos el modelo base seleccionado y tenemos el modelo refiner en el refiner model, vamos a darle a generar imagen de nuevo y vamos a ver qué sale. Y en este caso, pues obtenemos una nueva imagen de un perro vestido de pirata bajo el agua. Bastante espectacular. Hay algún error por aquí, pero vamos a ver aquí abajo los metadatos que todo está correcto. Y efectivamente podéis ver que esta imagen lo ha generado el modelo base, pero que luego el refiner model ha sido el Stable Diffusion refiner. Es decir, aquí están todos los modelos actuando como debería, todo ejecutado de nuevo en Google Collab. Y también ojo cuidado que hay gente que se equivoca y acaba activando el modelo refiner como modelo base, es decir, generan imágenes directamente con el modelo refiner y eso genera unos resultados. Vamos a decir curiosos. Quiero que lo veáis porque es bastante, bastante curioso. Vamos a darle a generar imagen ahora con el modelo refiner seleccionado y a ver qué pasa. Vale, tenemos la imagen, se ve la cabeza del perro bien. Qué está pasando? Ojo, cuidado que esta foto ya es un poco más rara. Y al final esto es lo que el refiner nos puede ofrecer. El refiner al final es un modelo que está entrenado para generar pues detalle fino, detalle más marcado, más rico en las diferentes partes de una imagen, pero no está pensado para generarte una imagen con una estructura global coherente. De eso ya se ocupa el base model, es decir, el base model hace como un primer boceto estructural de toda la imagen y luego el refiner refina. Entonces, si tú utilizas el refiner como modelo base, lo que va a pasar es que te va a generar una imagen. Sí, con los patrones de lo que le has pedido. Pero donde no hay ninguna estructura global y eso genera pues cosas bastante curiosas, bastante amorfa, donde se repiten muchos patrones de aquello que le has pedido tú como input. Así que ya sabéis que vamos a tener siempre el modelo base bien marcado. Y por aquí os recomiendo ajustar cosillas como el número de steps. Podemos subirlo un poquito más a 30, que eso está visto que es el punto dulce donde vamos a conseguir una buena cantidad de detalles. Podéis manejar el aspect ratio o si queréis tener más control, pues podéis utilizar una imagen de inicialización que te va a permitir pues yo que sé si tienes tu capacidad de dibujo nivel paint, pues puedes venirte con este dibujo, te lo llevas para la aplicación. Le damos aquí image, puedes cargar este archivo, cargamos aquí la imagen y con esto pues ya simplemente sería ajustar estos parámetros donde y image creativity te marca cuánto quieres que influencie tu imagen al resultado final. Si lo acercamos más a un valor de uno, significa que le estamos dando más posibilidad de ser creativo, estable diffusion, es decir, tu imagen no va a influir tanto si lo bajamos más cercano a cero, pues es todo lo contrario. Ola, tu imagen va a tener mucha más fuerza en el resultado final. Aquí recomiendo que si lo que ha generado en el paint es una especie de boceto donde tú estás simplemente ubicando los elementos, pues le pongas un valor alto de creatividad y con esto pues simplemente voy a ajustar aquí el aspect ratio porque el dibujo tiene esta proporción 16,9 y vamos a poner aquí arriba pues algún prompt que sea interesante, por ejemplo. Vale, en este caso puesto, pues frutas explotando, high quality estudio, fotógrafo y también podemos marcar algún negativo prompts que ya sabéis que esto te permite pedirle al modelo que no quieres que aparezca y esto suele servirnos para orientarlo a resultados más visualmente espectaculares. Por ejemplo, en este caso no quiero que sea digital art porque estoy buscando una fotografía, no quiero que sea ilustración, no quiero que sea low quality, no quiero que tenga artefactos JPEG. Es decir, aquí ponéis un listado de aquello que no queréis que aparezca. Esto lo podéis buscar en internet en un montón de de ejemplos de negativo prompts. De hecho, voy a dejar abajo la caja de descripción un ejemplo y vamos a ver qué sale de aquí. Y aquí tendríamos un ejemplo de fruta explotando, utilizando la imagen de inicialización que le hemos especificado. Vamos, por ejemplo, a cambiarlo. Vamos a pedirle que en vez de esto sea un pixel art a ver qué sale de aquí. Y ahí lo tendríamos, no? Pues el pixel art que le hemos pedido con nuestra imagen de inicialización, con nuestros prompts positivos, negativos, todo esto funcionando correctamente. Seguimos. Y qué esperar de un modelo con este potencial liberado en internet? Pues la respuesta es muy sencilla. Solo tenemos que mirar atrás al último año para ver todo el trabajo que ha desarrollado la comunidad open source a un ritmo frenético. Lo que podemos esperar es una comunidad entera trabajando por mejorar esta tecnología, optimizándola, integrándola con otras herramientas, creando mejores interfaces, creando más funcionalidades. Podemos esperar funcionalidades que están llegando ya como outpaintings customizaciones con Lora, modelos como control net que permitirán controlar mucho mejor que se genera con este modelo. El poder crear con mucha más calidad nuestros avatares más realistas con técnicas como DreamBoot o el poder aplicar estilos a partir de una imagen como vimos en este vídeo con style. Todo esto y mucho más va a llegar. Y esto sucede porque si os dais cuenta, si miramos atrás a los últimos meses, muchos de los trabajos más impresionantes que empresas como Google, como Microsoft han ido sacando en trabajo donde mostraban sus resultados, pero no notaban acceso a las herramientas, pues ellos lo lograban porque tenían acceso en sus empresas a modelos, generadores de imágenes de calidad y ahora la comunidad open source también tiene a un modelo de estas características. Esto va a suponer un impacto directo en las funcionalidades que estos modelos van a desplegar en los próximos meses y no solo en la generación de imágenes. Hemos visto trabajo de generación de modelos 3D que se basan en estos modelos de generación de imágenes para poder funcionar. Y ahora con StableDiffusion XL, pues la posibilidad de tener a uno de estos modelos open source se hace cada vez más real, algo similar a lo que ocurrirá con los modelos de generación de vídeo que ahora, al menos en el ámbito privado, están demostrando unos resultados que son verdaderamente impresionantes. Si a todo esto le sumamos la publicación de llama 2 en los modelos del lenguaje open source que estuvimos comentando en el vídeo de la semana pasada, está claro que estamos viviendo un momento que no se ha vivido con anterioridad, una época durada para el open source y la inteligencia artificial. Un enorme potencial donde ahora la comunidad, donde ahora vosotros podéis empezar a explorar e investigar, a jugar con todos estos modelos para sacarle todo su rendimiento y empezar a construir pues todas las herramientas que seguramente serán protagonistas en un futuro no tan lejano. Chicos, chicas, con este vídeo y con el de la semana pasada, creo que hacemos un cierre de arco argumental muy interesante, donde vamos a hacer coincidir también el final de la temporada de vídeos hasta septiembre. En agosto voy a tomarme un mes de descanso donde si hay novedades, pues nos veremos a través de algún directo puntual. Pero creo que es el momento perfecto para cerrar una temporada, un arco argumental que ha estado protagonizado por una revolución de la IA generativa 2022-2023, donde la actualidad nos ha tenido arrollado semanas tras semanas y donde ciertamente en este canal le hemos dedicado mucho hueco a vídeos como este donde explicamos las novedades. Pero sí noto que me ha faltado pues hablar más de fundamentos, hacer cosas más prácticas. Y sé que muchos de vosotros, pues también ese es el contenido que estáis esperando. Entonces para septiembre vamos a abrir una nueva temporada donde sí vamos a seguir hablando de actualidad, donde voy a intentar mantener un ritmo de publicación de vídeos más frecuente y donde también dedicaremos tiempo a entender los fundamentos de todo esto, de cómo funciona toda esta tecnología y donde vamos a estar practicando con ella también, pues de la forma que sé que a vosotros os gusta. Chicos, chicas, muchas gracias por estar aquí, por seguir apoyando el canal, por apoyar en Patreon los que sois Patreons de este contenido y lo hacéis posible. Muchas gracias a todos y nos vemos en septiembre con más Inteligencia Artificial.
DOC0056|Computer vision e IA|Meta lo ha vuelto a hacer. A ver, la cosa es la siguiente. Quiero grabar un vídeo, pero la voz no me acompaña, no... No tengo voz, no puedo grabar dos horas de metraje para el siguiente vídeo, así que os propongo un experimento. ¿Y si dejamos para este vídeo que tanto mi voz, como mi presencia, como todo yo, se encarga una inteligencia artificial? No te preocupes, tío, que a partir de aquí me encargo yo. Meta lo ha vuelto a hacer. Y es que hace unas semanas ya os hablé de cómo esta compañía nos sorprendía al liberar una tecnología como SAM, capaz de segmentar cualquier objeto de una imagen que se le pusiera por delante. Y ahora, unas semanas después, nos encontramos con nuevos trabajos que siguen haciendo avanzar el mundo de la inteligencia artificial y en concreto en el campo de la visión por computador, donde Meta está haciendo una aportación enorme liberando tecnologías como la que vamos a estar hablando hoy. Tecnología que bueno. Entre otras, muchas cosas nos va a permitir hacer cosas tan locas como sumar un audio con una imagen para obtener otra imagen correspondiente. O también usar prompts acústicos para generar imágenes con stable diffusion. Ya llegaremos a eso, mola bastante, ya verás. Hoy vamos a hablar de ImageBind. De la multimodalidad ya hablamos hace no mucho en este vídeo de aquí, y es la idea de no entrenar a la inteligencia artificial con un único tipo de dato, por ejemplo solo texto o solo imágenes, sino de combinar varios tipos de datos en el entrenamiento para conseguir aplicaciones más interesantes. El ejemplo más famoso de esto son las IAS generativas de imágenes, donde las capacidades multimodales te permiten expresarte con un texto. El prompt para generar una imagen visual. Y también hemos visto recientemente el caso inverso, donde a partir de una imagen dada como input podemos usarla para generar texto, como próximamente veremos en modelos como GPT-4. Pues bueno, Meta ha cogido esta idea de la multimodalidad y la ha llevado más allá. Y para que lo podáis entender bien, primero tenéis que entender un poquito mejor qué es esto de aquí. No que es un pangolin, no, sino cómo funciona internamente esta idea de la multimodalidad. Mirad, imaginad que tenemos una red neuronal entrenada para reconocer imágenes, solo imágenes, es decir, un único tipo de dato, unimodal. En su capacidad de entender el contenido visual de las imágenes, la red podrá codificar cada una de ellas en un vector o embedding. Y estos vectores, lo que representarán, son coordenadas en el espacio construido por la red, donde conceptos visualmente similares se encontrarán en un punto cercano del espacio. Y si, por ejemplo, codificas una imagen con un contenido totalmente diferente, pues su lugar en este espacio vectorial estará lejos de los puntos de los pangolines. Veis? Así podemos saber que la IA ha aprendido bien su tarea, sabe percibir que estas imágenes visualmente representan algo similar y que esta de aquí no. Bien, ha aprendido a diferenciar pangolines de coches. Ahora, y si quisiéramos ser multimodal, y si ahora queremos que la red percibiera como el mismo concepto, tanto las imágenes del pangolín como un texto en el que se menciona el pangolín. Es decir, lo que queremos ahora es que independientemente de lo que le demos a la IA sea una imagen o un texto, ésta sea capaz de procesarla y generar vectores de embeddings cercanos tanto para la imagen como el texto, puesto que al final son el mismo concepto, ¿no? Esta idea es la que en el paper se menciona como un espacio de embedding conjunto, una característica que como veremos luego nos ofrecerá un montón de posibilidades. Pero, ¿cómo conseguimos esto? ¿Cómo conseguimos explicarle a la red neuronal que esta imagen y este texto de alguna forma están relacionados? ¿Qué representan la misma cosa? Pues, esto no es tan complicado. ¿Sabéis la cantidad de imágenes en Internet que cuentan con títulos? Descripciones de lo que representan. Contexto alternativo que nos explican con texto lo que se puede ver en una imagen. Éste es el dataset perfecto de imágenes y texto ideal para nuestra tarea de entrenar a un sistema multimodal. Pero, ¿multimodal es sólo imágenes y texto? Pues no. Y aquí es donde entra el nuevo trabajo de meta, donde atentos han querido entrenar a la inteligencia artificial para conjuntamente ser capaces de codificar texto, audio, mapas de profundidad, imágenes térmicas y datos de unidades de medición inercial que nos da información sobre la velocidad, orientación y fuerzas de un objeto. Todo esto aprendido por un único modelo. En palabras de ellos, un espacio de embeddings para unirlos a todos. Claro, ¿cómo consigues esto? Antes hablábamos de que podíamos unir imágenes y texto porque naturalmente podemos encontrar en Internet pares de datos de cada tipo. Pero, ¿y si quisiéramos unir texto y audio? ¿Tenemos datos de esto? Bueno, sí. Ahora que contamos con otras IAs como Whisper que nos puede transcribir horas y horas de audio en texto, vale. Pero, ¿y mapas de profundidad con audio? ¿Encontrar pares de datos así es raro? ¿O mapas de profundidad con datos de mediciones inerciales? Aún más raro. Pues aquí es donde brillantemente meta ha tenido la siguiente idea, y es conectar todos estos tipos de datos usando las imágenes y vídeos como punto central de conexión. De ahí el nombre de este proyecto, ImageBind. Pensadlo bien. Hemos dicho que en Internet podemos encontrar muchos ejemplos de imágenes y textos conjuntos. ¡Perfecto! Ya tenemos texto. Además de imágenes, también existen numerosos datasets de imágenes reales emparejadas con sus mapas de profundidad. Y también datasets de imágenes reales con sus mapas de temperatura. ¡Perfecto! Además, a través de los millones y millones de horas de vídeos publicados en Internet, también tenemos un dataset enorme de fotogramas, imágenes asociadas con un audio. Ahí de nuevo tenemos otra conexión, y también si necesitamos relacionar datos de velocidad y aceleración con vídeos, por suerte meta cuenta con un dataset enorme de vídeos grabados en primera persona que cuenta también con estas medidas. Con lo que sí, es usando imágenes y vídeo, que meta ha conseguido emparejar varios datos de imágenes y conectar varios tipos de datos diferentes, y más interesante de forma transitiva, ha conseguido conectar tipos de datos que no suelen aparecer emparejados de forma natural, como por ejemplo audio e imágenes térmicas. ¡Mola, eh! Claro, la pregunta es, ¿y todo esto para qué? ¿De qué nos sirve ahora que una red neuronal pueda codificar tanto el sonido de un pangolín, como su texto escrito, como el sonido que hace, o su mapa de calor tomado por una cámara térmica? ¿Para qué todo esto? Pues aquí viene lo divertido. Contar con un espacio latente conjunto tiene unas cuantas cualidades que podemos aprovechar, y una de ellas sería lo que en inglés se conoce como Information Retrieval, y que en español podríamos traducir como búsqueda de información, y lo vais a entender muy bien. Si yo por ejemplo quiero hacer una búsqueda en una base de datos de imágenes similares a esta, con el sistema que hemos desarrollado antes, tengo el mecanismo perfecto. Si codifico mi imagen en un vector de embeddings, obtendré un punto en este espacio, donde por cercanía a otros puntos podré encontrar imágenes que sean parecidas. ¿Lo ves? Es una forma muy inteligente de poder hacer búsquedas de imágenes conceptualmente semejantes. Pero claro, con ImageBind ya no estamos hablando solo de imágenes, sino que en este espacio ahora estarán cerca aquellas imágenes, textos, audios, imágenes térmicas, que representen la misma cosa, dejándonos con un montón de posibilidades muy locas. Meta ha enseñado ejemplos donde consiguen encontrar imágenes a partir de un audio que les representa. Por ejemplo, mirad en este caso, como este audio devuelve imágenes de perros ladrando, pero además atentos al detalle de cómo posiblemente por el eco de la habitación donde se oye, las imágenes de vueltas son de perros en interior. Cierra los ojos, escucha este audio, y dime qué imagen te viene a la mente. Pues este ejercicio de conectar diferentes fuentes de información, en este caso audio con imágenes, es lo que ahora también puede hacer la inteligencia artificial. Y para el caso contrario también hay ejemplos donde para una imagen dada se puede obtener un audio que lo represente. Esto mola porque combinado con Sam podemos coger en una imagen o un vídeo y automáticamente estar seleccionando diferentes objetos para escuchar cómo suena. ¿Cómo sonaría este elemento de aquí? O este elemento de aquí. O este. Esto es algo que podría tener utilidad en videojuegos procedurales o para producción audiovisual donde quieras sonorizar lo que se ve en una imagen. Y pensad que esta estrategia la podemos aplicar usando cualquier tipo de pares de datos. Por ejemplo, una búsqueda con texto nos podría devolver el sonido y la imagen más cercana de lo que hemos escrito. ¿Veis qué interesante es esto? Pues si esto os gusta, esperad porque la cosa se puede volver aún más loca. Y es que cuando dejamos que una red neuronal construya y ordene este espacio matemático, lo que ocurre es que en él se conservan ciertas propiedades matemáticas. Al final, cada uno de estos puntos no deja de ser un vector. Vectores con los que podemos operar matemáticamente. Por ejemplo, tú puedes tener un vector que represente a una imagen en este espacio y también puedes tener un vector que represente a un sonido como este. Y con estos dos vectores tú puedes sumarlos, al final no dejan de ser vectores, para obtener un nuevo vector resultado. ¿Y qué crees que va a pasar aquí? ¿Qué crees que tipo de datos representará este nuevo vector? Pues esto es lo interesante. Y es algo que ya se sabía que ocurría con espacios vectoriales como estos bien construidos. Y es que si tú tomas, por ejemplo, el vector que representa a imágenes de pangolines y lo sumas con el vector que representa a imágenes de coches, el resultado de esta operación es otro vector, que apuntará a otra zona de este espacio donde encontraremos también un vector que podría representar conceptualmente la suma de los elementos anteriores. A lo mejor aquí es donde estarían localizadas las imágenes de coches en forma de pangolines, si es que eso existe. ¿Lo entendéis? Pues volviendo al ejemplo de antes, sí, este álgebra de vectores también se cumple, pudiendo sumar una imagen de una playa con el audio de un perro para obtener como resultado un vector que estará cerca de una imagen de un perro en una playa. ¿Veis qué loco es todo esto? En su web, Meta ha enseñado más resultados de este tipo, donde se puede comprobar que efectivamente la IA está comprendiendo los conceptos con los que está trabajando y los ha ordenado correctamente en su representación interna. Como podrás imaginar, este espacio vectorial que hemos armado no solo sirve para buscar información ya existente en nuestro dataset, como hemos hecho antes, sino que también estos vectores los podemos usar conjuntamente con otros modelos de inteligencia artificial, como las IAs generadoras de imágenes. Hasta ahora, muchas de estas redes se condicionan en vectores que representan información textual dado por un prompt para luego convertirlas en imágenes, pero imaginad ahora, poder condicionar lo que genera la red a un prompt de audio por ejemplo. Y esto es lo que ha querido explorar Meta aquí, el concepto de multimodalidad llevado más allá, y de nuevo este es un modelo que han puesto a disposición de la comunidad, y que han puesto a experimentar con cosas como usar prompts de audio para stable diffusion. ¿Y cuál es el límite de todo esto? Pues no lo sé. Pero lo que comenta Meta en este trabajo es que visionan un futuro cerebro artificial multimodal capaz de procesar toda esta información y nuevas fuentes más como el tacto, el habla, olores o señales de resonancia magnética. O bueno, eso dicen. Espero que este vídeo os haya servido para entender mucho mejor los fundamentos de mucho de lo nuevo que va saliendo en inteligencia artificial y si queréis apoyar todo este contenido, ya sabéis que podéis hacerlo a través de mi Patreon con una aportación mensual. Chicos, chicas, muchas gracias por verme y nos vemos en el
DOC0057|Computer vision e IA|Hola a todos y bienvenidos a un nuevo video en el canal, en este video te explicaré de qué se trata la visión computacional, por lo tanto empecemos con el video. La fantasía de que una máquina es capaz de simular el sistema visual humano es antigua. Hemos recorrido un largo camino desde que aparecieron los primeros trabajos universitarios en la década de 1960, como lo demuestra la llegada de los sistemas modernos trivialmente integrados en las aplicaciones móviles. Hoy en día la visión por computador es uno de los subcampos más importantes de la Inteligencia Artificial y Machine Learning, dada su amplia variedad de aplicaciones y su tremendo potencial. Su objetivo es replicar las poderosas capacidades de la visión humana. ¿Qué es la visión por computador? La visión computacional es el subcampo de la Inteligencia Artificial que intenta imitar las capacidades de la visión humana, y por visión humana no nos referimos solo a los ojos o a la capacidad de ver imágenes. No es tan trivial como simplemente tomar una foto con el teléfono. El propósito no es imitar solo la vista, sino imitar la percepción, la capacidad de los humanos de dar sentido a lo que ven. La visión por computador se centra en la creación de sistemas digitales que pueden procesar, analizar y dar sentido a los datos visuales, imágenes o videos, de la misma manera que los humanos. El concepto de visión computacional se basa en enseñar a los computadores a procesar una imagen a nivel de pixel y a entenderla. Técnicamente, las máquinas intentan recuperar la información visual, manejarla e interpretar los resultados a través de algoritmos de software especiales. ¿Cómo funciona la visión computacional? La tecnología de visión computacional tiende a imitar la forma en que funciona el cerebro humano. Pero ¿cómo resuelve nuestro cerebro el reconocimiento visual de objetos? Una de las hipótesis populares afirma que nuestro cerebro depende de patrones para decodificar objetos individuales. Este concepto se utiliza para crear sistemas de visión por computador. Los algoritmos de visión computacional que usamos hoy en día se basan en el reconocimiento de patrones. Entrenamos a los computadores en una gran cantidad de datos visuales. Los computadores procesan imágenes, etiquetan los objetos en ellos y encuentran patrones en esos objetos. Por ejemplo, si enviamos un millón de imágenes de flores, la computadora las analizará, identificará patrones que son similares a todas las flores y, al final de este proceso, creará un modelo flor. Como resultado, la computadora será capaz de detectar con precisión si una imagen en particular es una flor cada vez que les enviamos imágenes. Los métodos y técnicas de deep learning han transformado profundamente la visión computacional junto con otras áreas de la Inteligencia Artificial, hasta tal punto que para muchas tareas su uso se considera estándar. En particular, las redes neuronales convolucionales han logrado resultados más allá del estado de la técnica utilizando técnicas tradicionales de visión por computadora. Estos cuatro pasos esbozan un enfoque general para construir un modelo de visión por computadora utilizando redes neuronales convolucionales. Crear un conjunto de datos compuesto de imágenes anotadas o utilizar uno ya existente. Las anotaciones pueden ser la categoría de la imagen para un problema de clasificación, pares de cajas delimitadoras y clases para un problema de detección de objetos o una segmentación de píxeles para cada objeto de interés presente en una imagen. Para problemas de segmentación de instancia, extraer de cada imagen las características pertinentes a la tarea en cuestión. Este es un punto clave en el modelado del problema. Por ejemplo, los rasgos utilizados para reconocer rostros, rasgos basados en criterios faciales, obviamente no son los mismos que los utilizados para reconocer atracciones turísticas u órganos Entrenar un modelo de deep learning basado en los rasgos aislados. Entrenar significa alimentar el modelo de machine learning con muchas imágenes y aprenderá basándose en esos rasgos, cómo resolver la tarea en cuestión. Evaluar el modelo utilizando imágenes que no se utilizaron en la fase de entrenamiento. Al hacerlo, la precisión del modelo de entrenamiento puede ser probada. La estrategia es muy básica, pero sirve bien al propósito. Este enfoque, conocido como aprendizaje supervisado, requiere un conjunto de datos que abarque el fenómeno que el modelo tiende para aprender. Tareas típicas de la visión computacional. La visión por computadora se basa en un extenso conjunto de tareas diversas combinadas para lograr aplicaciones altamente sofisticadas. Las tareas más frecuentes son el reconocimiento de imágenes y de video, que básicamente consisten en determinar los diferentes objetos que contiene una imagen. Clasificación de imágenes. Probablemente una de las tareas más conocidas en la visión computacional es la clasificación de imágenes. Permite clasificar una imagen TADAP como perteneciente a una de un conjunto de categorías predefinidas. Tomemos un simple ejemplo binario. Queremos categorizar las imágenes según el contenido, una atracción turística o no. Supongamos que se construye un clasificador para este propósito y se proporciona una imagen. El clasificador responderá que la imagen pertenece al grupo de imágenes que contiene atracciones turísticas. Esto no significa que haya reconocido necesariamente el lugar, sino más bien que ha visto previamente fotos del sitio y que se le ha dicho que esas imágenes contienen una atracción turística. Una versión más ambiciosa del clasificador podría tener más de dos categorías. Por ejemplo, podría haber una categoría para cada tipo específico de atracción turística que queramos conocer. En tal escenario, las respuestas por entrada de imagen podrían ser múltiples. Detección de objetos. La detección de objetos es similar a la clasificación de imágenes, excepto que con la detección de objetos la imagen puede contener muchos objetos que se localizan y clasifican. En este patrón de código de detección de objetos, un modelo está entrenado no solo para clasificar imágenes de Coca-Cola, sino también para localizar cada botella dentro de la imagen. En la siguiente se ve que reconocieron tres categorías diferentes de productos de Coca-Cola. Además de devolver las etiquetas y la confianza, proporciona coordenadas que permite a la aplicación dibujar cajas delimitadoras alrededor de cada objeto identificado. La detección de objetos se utiliza en una amplia variedad de casos de uso, en los que lo interesante no es una única clasificación que describa toda la imagen, sino que pueden ser muchos objetos de diferentes categorías en la imagen. Dando a una aplicación la capacidad de identificar, localizar y contar los objetos, incrementando las posibles aplicaciones de este tipo de visión computacional. Identificación de objetos La identificación de objetos es ligeramente diferente de la detección de objetos, aunque a menudo se utilizan técnicas similares para lograr ambas. En este caso, dado un conjunto específico, el objetivo es encontrar instancias de dicho objeto en imágenes. No se trata de clasificar una imagen como vimos anteriormente, sino de determinar si el objeto aparece o no en una imagen, y si aparece, especificar el lugar o los lugares donde aparece. Un ejemplo puede ser la búsqueda de imágenes que contenga el logotipo de una empresa determinada. Otro ejemplo es la supervisión de imágenes en tiempo real de cámaras de seguridad para rostros de una persona específica. Seguimiento de objetos de videos Cuando se utiliza la detección de objetos en los videos, a menudo se quiere seguir el rastro de los objetos de un fotograma a otro. La detección inicial de objetos se puede hacer extrayendo un fotograma del video y detectando los objetos en el fotograma. Además de contar los objetos y localizar cada uno de ellos, el seguimiento de los objetos a medida que se desplazan de un fotograma a otro añade otra dimensión a lo que puede hacer. Casos de uso comercial Las empresas utilizan cada vez más las aplicaciones de visión computacional para responder a las preguntas comerciales o para mejorar sus productos. Probablemente ya forman parte de su vida cotidiana, sin que te des cuenta. A continuación se presentan algunos casos de uso popular. Organización de contenidos Los sistemas de visión computacional ya nos ayudan a organizar nuestro contenido. Apple Photos y Google Photos son un excelente ejemplo. Estas aplicaciones tienen acceso a nuestras colecciones de fotos y añaden automáticamente etiquetas a las fotos, y nos permiten navegar por una colección de fotografías más estructuradas. Estas aplicaciones crean una vista curada de tus mejores momentos para ti. Motores de búsqueda visual La tecnología de búsqueda visual se puso a disposición del público con la aparición de Google Images en 2001. Un motor de búsqueda visual es capaz de recuperar imágenes que cumplen con ciertos criterios de contenido. La búsqueda de palabras clave es un caso de uso común, pero a veces podemos presentar una imagen de origen y solicitar que se encuentren imágenes similares. Reconocimiento facial La tecnología de reconocimiento facial se utiliza para hacer coincidir las fotos de los rostros de las personas con sus identidades. Esta tecnología está integrada en los principales productos que usamos a diario. Por ejemplo, Facebook está usando la visión computarizada para identificar a las personas en las fotos. El reconocimiento facial es una tecnología crucial para la autentificación biométrica. Muchos dispositivos móviles disponibles en el mercado hoy en día permiten a los usuarios desbloquear los dispositivos mostrando sus caras. Para el reconocimiento facial se utiliza una cámara frontal. Los dispositivos móviles procesan esta imagen y, basándose en el análisis, pueden decir si la persona que tiene el dispositivo está autorizada en él. Lo importante de esta tecnología es que funciona realmente rápido. Realidad aumentada La visión computacional es un elemento central de las aplicaciones de realidad aumentada. Esta tecnología ayuda a estas aplicaciones a detectar objetos físicos, tanto superficies como objetos individuales dentro de un espacio físico determinado en tiempo real, y a utilizar esta información para colocar objetos virtuales dentro del entorno físico. Automóviles autoconductores La visión computacional permite a los autos dar sentido a su entorno. Un vehículo inteligente tiene unas cuantas cámaras que capturan videos desde diferentes ángulos y los envías como señal de entrada al software de visión computacional. El sistema procesa el video en tiempo real y detecta objetos como marcas en la carretera, objetos cercanos al auto como peatones u otros autos, semáforos, entre otros. Uno de los ejemplos más notables de las aplicaciones de esta tecnología es el piloto automático en los automóviles Tesla. Salud La información de las imágenes es un elemento clave para el diagnóstico en medicina porque representa el 90% de todos los datos médicos. Muchos diagnósticos en la salud se basan en el procesamiento de imágenes, rayos X, resonancia magnética y mamografía, solo por nombrar algunos. Y la segmentación de las imágenes demostró la eficacia durante el análisis de las exploraciones médicas. Por ejemplo, los algoritmos de visión computacional pueden detectar la retinopatía diabética, la causa de ceguera de más rápido crecimiento. La visión por computador puede procesar imágenes de la parte posterior del ojo y clasificarlas según la presencia y la gravedad de la información. Agricultura Muchas organizaciones agrícolas emplean la visión computacional para vigilar la cosecha y resolver los problemas agrícolas comunes como la aparición de malas hierbas o la deficiencia de nutrientes. Estos sistemas procesan imágenes de satélites, aviones no tripulados o aviones e intentan detectar los problemas en una fase temprana, lo que ayuda a evitar pérdidas financieras innecesarias. La visión computacional es un tema popular en los artículos sobre nuevas tecnologías. Un enfoque diferente de la utilización de los datos es lo que hace que esta tecnología Tremendas cantidades de datos que creamos diariamente se utilizan en realidad para nuestro beneficio. Los datos pueden enseñar a los computadores a ver y comprender los objetos. Esta tecnología también demuestra un importante paso hacia la creación de una inteligencia artificial que será tan sofisticada como la de los humanos. Con esto finalizamos la explicación. Si quieres aprender más sobre inteligencia artificial puedes visitar nuestra página web AprendeIA en donde encontrarás más información, como también ebooks y cursos que pueden ayudarte en tu aprendizaje. El link lo encuentras en la cajita de información debajo de este video. Y por supuesto te dejo la pregunta del video ¿Cuáles de las siguientes afirmaciones crees tú que sea cierta? Opción 1 La visión computacional es un subconjunto de Machine Learning. Opción 2 Los métodos y técnicas de deep learning han transformado profundamente la visión computacional. Opción 3 Las tareas más frecuentes de visión computacional son el reconocimiento de imágenes y video. Deja en los comentarios cuál crees que es la respuesta correcta, puede ser una o más las respuestas correctas. También te recomiendo que nos sigas en nuestras otras redes sociales en donde publicamos mucha más información de la que tenemos publicada acá. Todos los links se encuentran en la cajita de información de este video. No te olvides de suscribirte al canal ya que semanalmente encontrarás un nuevo video sobre algún tema relacionado a la Inteligencia Artificial, solamente tienes que presionar el botón rojo debajo de este video. Nos vemos en el siguiente video. Chao.
DOC0058|Computer vision e IA|¿Te has preguntado alguna vez cómo la Inteligencia Artificial puede crear contenido nuevo y original o cómo se pueden hacer pronósticos precisos de eventos futuros? Bueno, eso es gracias a la Inteligencia Artificial generativa y la Inteligencia Artificial predictiva. Aunque ambas formas de Inteligencia Artificial utilicen diferentes técnicas, algoritmos y metodología, es cierto que ambas pueden generar resultados útiles que pueden aprovecharse para tomar decisiones informadas y mejorar la eficiencia en diversos sectores. Por esta razón, exploraremos estos dos principales tipos de Inteligencia Artificial y analizaremos las diferencias entre ambas. Hola, soy Litty González de Aprendida y en el vídeo de hoy hablaremos sobre la Inteligencia Artificial predictiva y la Inteligencia Artificial generativa. Por lo tanto, comencemos. Comencemos definiendo la Inteligencia Artificial predictiva. ¿Te has preguntado cómo se hacen las diferentes predicciones en el mundo tecnológico? La respuesta es la Inteligencia Artificial predictiva. La Inteligencia Artificial predictiva es un campo emocionante que se enfoca en hacer pronósticos precisos sobre eventos futuros y todo comienza con los datos. La Inteligencia Artificial predictiva utiliza datos actuales y antiguos para entrenar un modelo predictivo, que es una especie de cerebro que aprende de los datos y los utiliza para hacer predicción. Pero aquí está todo el truco. Cuanto más datos tenga el modelo, más preciso será la predicción. Así que si queremos hacer predicciones precisas, necesitamos muchos datos. Ahora, si te preguntas cómo funciona todo esto, es gracias a los algoritmos que se utilizan en la Inteligencia Artificial predictiva. Estos algoritmos son como una especie del cerebro artificial que ayudan los modelos a reconocer patrones y relaciones complejas en los datos. Así que cuando hacemos una predicción, podemos estar seguros que es muy precisa. Este tipo de Inteligencia Artificial se puede utilizar en cualquier parte. Por ejemplo, en las finanzas, la Inteligencia Artificial puede ayudar a predecir los precios en las acciones e identificar oportunidades de inversión. En la medicina puede analizar datos médicos para predecir la progresión de enfermedades o los resultados de los pacientes. En el marketing puede analizar el comportamiento de los consumidores para hacer recomendaciones personalizadas y publicidad específica. Veamos ahora de quién se trata la Inteligencia Artificial generativa. ¿Te imaginas tener un ordenador que puede crear su propia música, arte, inclusive historias? Pues eso es exactamente lo que hace la Inteligencia Artificial generativa. La Inteligencia Artificial generativa es un campo que se enfoca en la creación de sistemas informáticos capaces de generar contenidos nuevos y originales. El corazón de la Inteligencia Artificial generativa se encuentra en un modelo que aprende los patrones y estructuras subyacentes en un conjunto de datos determinados. Una vez que el modelo ha aprendido estos patrones, puede crear nuevos ejemplos que comparten características similares con los datos originales. Este tipo de Inteligencia Artificial puede ser utilizado en una amplia gama de sectores creativos, como por ejemplo en el campo del arte, en donde la Inteligencia Artificial generativa puede crear obras de arte únicas, visualmente impresionantes, incluso puede imitar el estilo de artistas famosos. En la música, la Inteligencia Artificial generativa puede componer melodías y armonías originales aprendiendo de una amplia colección de piezas musicales. Y en la escritura, la Inteligencia Artificial generativa puede crear historias coherentes y atractivas o incluso ayudar a los autores a desarrollar nuevas ideas argumentales. Y para que sepas, SHAP-GPT está dentro de la categoría de la Inteligencia Artificial generativa. Veamos ahora las diferencias entre la Inteligencia Artificial predictiva y la generativa. La Inteligencia Artificial generativa y predictiva son dos tipos diferentes de Inteligencia Artificial con objetivos y enfoques distintos. Veamos esta diferencia desde varios puntos de vista. Comencemos con los objetivos. La Inteligencia Artificial generativa trata de crear cosas nuevas y originales. Es como un artista que crea arte inspirándose en lo que ya existe, aprende patrones y estructuras de datos y luego genera nuevo contenido similar. La Inteligencia Artificial predictiva intenta predecir el futuro con las tendencias. Es como un adivino que mira las cartas o líneas de la mano para hacer predicciones, analiza datos del pasado para identificar patrones y luego usa esos patrones para hacer predicciones. Veamos ahora las diferencias en el uso de datos. La Inteligencia Artificial generativa no necesita datos estiguetados o resultados conocidos para ser entrenado. Aprende de la distribución de probabilidad de los datos que le das y luego genera nuevos datos en lo que aprendió. Usa técnicas como las redes neuronales generativas. Por su parte, la Inteligencia Artificial predictiva sí requiere datos etiquetados con resultados conocidos para ser entrenado. Usa técnicas de regresión, análisis de series temporales o algo en edensición para identificar patrones y luego hacer predicciones. Veamos ahora las diferencias desde el punto de vista de técnicas y algoritmos. La Inteligencia Artificial generativa utiliza técnicas como las redes neuronales adversarias GAN o los autocodificadores variacionales para poder crear contenido nuevo. Mientras tanto, la Inteligencia Artificial predictiva utiliza técnicas como el análisis de regresión, análisis de series temporales o modelos de aprendizaje automático como árboles de decisión, bosques aleatorios o redes neuronales. Finalmente, veamos la diferencia desde el punto de vista de los resultados. El resultado de la Inteligencia Artificial generativa es contenido nuevo que no existía originalmente pero se parece a los datos de entrenamiento. Por su parte, en el resultado de la Inteligencia Artificial predictiva son pronósticos y previsiones sobre eventos o resultados futuros basándose en los patrones de datos pasados. Como puedes ver, ambos tipos de Inteligencia Artificial son realmente potentes. Con una pueden realizar predicciones precisas sobre eventos futuros basándose en lo ocurrido en el pasado, por lo que puede ser utilizado por las empresas que buscan realizar operaciones de forma más eficiente y eficaz. Mientras que el otro tipo de Inteligencia Artificial se foca en crear contenido nuevo y original a partir de los datos de entrenamiento. Y justamente esto es lo que está recibiendo mucha atención por parte de los usuarios por sus increíbles resultados. Pero este es solo el comienzo. La Inteligencia Artificial está proliferando a un ritmo asombroso y estos dos tipos lidera el grupo de casos de uso de alto valor. ¿Te imaginas como la Inteligencia Artificial generativa y la Inteligencia Artificial predictiva podrían ayudar a las empresas a tomar decisiones más informadas y eficientes en el futuro? Las posibilidades son infinitas. ¿Te gustaría empezar con la Inteligencia Artificial? Estoy trabajando en dos talleres teóricos y prácticos que serán dictados completamente en vivo que de repente te puedan interesar. Uno está enfocado a explicar sobre la Inteligencia Artificial predictivo en donde aprenderás a cómo tener predicciones utilizando datos históricos, todo esto sin escribir una línea del código. Y el segundo está enfocado en la Inteligencia Artificial generativa y más específica en la generación de texto. Acá aprenderás a cómo sacarle todo el provecho a los modelos de lenguaje como SHA-GPT. Si quieres más información te dejo los enlaces de ambos cursos en la cajita de información debajo de este vídeo y en los comentarios. Con esto finalizamos este vídeo. Recuerda que si te gustó este vídeo dale manita arriba y suscríbete al canal y activa la campanita para que no te pierdas de ningún contenido que estaré publicando sobre Inteligencia Artificial. Espero verte en un próximo vídeo. Chao.
DOC0059|Big Data|Muy buenas a todos, mi nombre es Abraham Requena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una multinacional española. Bueno, aquí me gustaría haceros una pequeña introducción acerca del mundo del Big Data. El Big Data es un término que escuchamos a diario y que, bueno, creo que su definición aún no está muy definida. Entonces vamos a intentar definir el Big Data. Mira, aquí os muestro una primera definición que a mí me gusta mucho, que dice que el Big Data es como el sexo de los jóvenes, que todo el mundo habla de él, nadie sabe realmente cómo se hace, todo el mundo piensa que el resto lo está haciendo, por lo cual todo el mundo dice que lo está haciendo y efectivamente es un poco así, es un término que todo el mundo quiere hacer, pero bueno, creo que aún está por definir. Si vamos a una definición un poco más teórica, la Wikipedia nos define el Big Data como un término que hace referencia a una cantidad de datos tal que supera la capacidad del software convencional para ser capturados, administrados y procesados en un tiempo razonable y el volumen de los datos masivos va a crecer constantemente. Esta definición me gusta bastante más y es cierto, el Big Data a mí me gusta definir el Big Data como la capacidad de tratar grandes volúmenes de datos en un tiempo aceptable. Bueno, en Big Data vamos a tener lo que se conoce como las cuatro V. El Big Data tiene la V del volumen, tiene la V de la variedad, tiene la V de la velocidad y tiene la V del valor y normalmente también se suele añadir una quinta V que es la velocidad. Veamos que cada uno de ellos. Bueno, el volumen es eso, es que en Big Data vamos a tratar con volúmenes de datos muy grandes, vamos a tratar con datos del orden del terabyte o del petabyte y esos datos van a ser muy variados, es decir, vamos a trabajar con imágenes, con ficheros secuenciales, con ficheros de texto, etcétera. Otra V del Big Data era la velocidad y bueno, como su definición dice, el Big Data consiste en tratar grandes volúmenes en un tiempo aceptable, es decir, con una velocidad adecuada. Y por último, conseguir valor, porque tú en Big Data vas a tener unos datos en bruto, unos datos crudos, a los cuales haciendo una serie de transformaciones vamos a ser capaces de conseguir información. Esa información va a tener un valor útil para la empresa. Y bueno, la quinta V es la veracidad, nosotros podemos desde unos datos brutos conseguir una información, pero bueno, ¿cómo deberá es esa información? Pues ahí es donde entra la quinta V. Bueno, definiciones aparte, lo que no podemos negar es que la información está creciendo constantemente a día de hoy. En este gráfico podemos ver cómo cada 60 segundos se están escribiendo más de 98 mil tweets o se están creando más de 695 mil estados de Facebook o se están escribiendo 168 millones de correos, es decir, estamos generando casi 2 terabytes de datos por segundo. Toda esta información se puede utilizar. Toda esta información cruda debemos de ser capaces de tratarla para conseguir una información, ¿vale? Entonces es aquí donde aparece Apache Hadoop. Con Apache Hadoop lo que vamos a ser capaces de procesar esa cantidad de datos tan grande de una manera distribuida en distintos nodos que formarán nuestro clúster de máquinas en un tiempo digno, de manera que seamos capaces de convertir, que seamos capaces de convertir esos datos en bruto en información. Pues bueno, esta es una pequeña introducción al Big Data y durante el curso veremos cómo podemos implementar nosotros mismos Big Data.
DOC0060|Big Data|Muy buenas, mi nombre es Abraham Requena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una empresa multinacional española. Bueno, durante este vídeo vamos a hacer una pequeña introducción a los fundamentos de Hadoop. Nos podemos preguntar ¿qué es Hadoop? Bueno, pues Hadoop no es más que un framework que nos permite el procesamiento distribuido de grandes cantidades de datos, usando modelos de programación simple sobre un cláster de máquinas. ¿Cuáles son las características básicas que posee Apache Hadoop? Bueno, en primer lugar, su procesamiento distribuido. La idea de Hadoop es poder distribuir los datos, poder paralelizar el tratamiento de los datos de forma que cada nodo de nuestro cláster de máquinas procese una parte de los datos. De esta manera, estaremos ganando velocidad. Luego, es un proceso eficiente porque consigue procesar los datos en poco tiempo. Es un framework económico porque se puede escalar fácilmente de manera horizontal. Si se nos queda pequeño nuestro sistema, lo que tenemos que hacer únicamente es añadir un nuevo nodo que lo añadiremos a nuestro cláster de máquinas y ya no tendremos problemas. ¿Qué más? Bueno, es un sistema tolerante a fallos porque usa la alta disponibilidad y además usa la replicación. Los datos suelen estar replicados con replicación 3 en el HDFS, que es el sistema de almacenamiento de Hadoop. De forma de que si un nodo cae tendremos el dato en el resto de nodos. Y por último y no menos importante es que es un proyecto open source, es un proyecto de código abierto. ¿Cuál es la arquitectura básica que sigue Hadoop? En Hadoop se distinguen cuatro módulos principales. El Common Utilities, el YARN, el HDFS y los procesos MapReduce. El Common Utilities lo forman todos los JAR y todas las librerías que son necesarias para ejecutar Hadoop. El YARN es el gestor de recursos de Hadoop. Como hemos dicho, Hadoop es un sistema distribuido en distintas máquinas, por lo cual debe de haber un gestor de recursos que vaya gestionando esto durante en todas las máquinas. Luego tenemos el HDFS, este es el sistema de archivo distribuido de Hadoop. Este sistema de archivo está instalado en cada uno de los nodos de nuestras máquinas y los procesos MapReduce de arriba siempre se apoyarán en el HDFS para poder coger los datos paralizados, para poder paralizar el procesamiento. Y por último, como hemos dicho, los procesos MapReduce. Esto es una manera de implementar el software que lo que conseguimos es paralizar de nuevo los datos. Siempre vamos al mismo fin, para la paralización de los datos. Entonces tendremos que crear unos procesos Map y unos procesos Reduce de forma que primero iremos agrupando los datos y luego haremos cálculo con esos grupos de datos. Todo esto lo veremos durante el curso. Y como hemos dicho, la configuración habitual de Hadoop es tenerlo en un clúster de máquinas, de forma que tengamos una máquina maestra y tengamos N máquinas esclavas. La idea es que la máquina maestra gestionará todas las tareas y las enviará a las máquinas esclavas. Esas máquinas esclavas van a realizar todos los procesamientos de los datos y finalmente terminarán informando de nuevo a la máquina maestra. Bueno, ¿qué opciones tenemos para trabajar con Hadoop? Bueno, pues tenemos tres opciones. La primera de ellas es Microsoft Azure. Microsoft Azure es un servicio que nos proporciona Microsoft y que nos permite tener nuestras máquinas en la nube. Entonces tendríamos nuestras máquinas en la nube y pagaríamos según la cantidad de máquinas que tengamos y las características de las máquinas. Es un sistema bastante interesante. Bueno, luego tenemos Hortonworks. Hortonworks es la última distribución que ha salido acerca de Hadoop. Que bueno, con Hortonworks nos podremos descargar una máquina virtual que abrimos con un cliente de virtualización y tendremos todo nuestro sistema Hadoop instalado con todos los servicios. Y por último, que es la que utilizaremos en el curso, tendremos Cloudera. Cloudera fue la primera distribución acerca de Hadoop y igualmente nos permite descargar una máquina virtual con la cual podremos gestionar todo. Descargamos una máquina virtual que abrimos con un cliente de virtualización y ahí ya tenemos todo Hadoop instalado y también tenemos la Cloudera Manager que es un asistente para gestionar todo el clúster de máquinas. Y bueno, esto es todo. Y nada, os animo a continuar con el curso porque creo que va a ser un curso bastante interesante en el que vamos a poder implementar Big Data y crear procesos de MapReduce.
DOC0061|Big Data|Muy buenas, mi nombre es Abraham Raquena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una multinacional española. Durante el curso también vamos a hablar acerca de cuándo usar Hadoop y cuándo no usar Hadoop. Aquí hablamos un poco acerca de ello. ¿Cuándo debemos de usar Hadoop? Bueno, pues Hadoop está pensado para cuando debemos de procesar ficheros de texto muy, muy, muy grandes. Cuando hablo de muy grandes, estoy hablando del orden del terabyte o del orden del petabyte. ¿Cuándo más? Bueno, cuando nosotros tenemos una previsión de crecimiento de nuestros datos. Puede ocurrir que nuestro proyecto inicialmente contenga pocos datos, a lo mejor del orden del giga, estamos generando datos continuamente de forma que tengamos, por ejemplo, un crecimiento exponencial de los datos. Entonces, en este caso también es bastante útil utilizar a pase Hadoop porque llegará el momento en que necesitaremos dividir nuestros datos para conseguir velocidad. Además también porque Hadoop nos va a permitir un escalado muy fácil. Ya veremos que simplemente añadiendo nuevos nodos a nuestro cláster de máquina podremos aumentar el rendimiento de nuestro sistema. Bueno, también cuando tengamos tipos de datos variados, es decir, en Hadoop, en el HDFS, que es el sistema de almacenamiento de Hadoop, se pueden almacenar cualquier tipo de datos, ya sean imágenes, sean ficheros secuenciales, sea texto, ficheros más grandes, ficheros más pequeños, etcétera. Ya veremos cómo almacena todos estos ficheros el HDFS y cómo lo va dividiendo en bloques, que será lo que se irá almacenando. Y por supuesto debemos de asegurarnos que tenemos la capacidad de paralelizar nuestros datos. Al fin y al cabo Apache Hadoop lo que consiste es de tener grandes ficheros los cuales iremos dividiendo en pequeños grupos que se tratarán de manera paralela y con eso es con lo que ganaremos la velocidad. Por ello debemos de asegurarnos que podemos paralelizar nuestros datos. Bien, vayamos al polo opuesto. ¿Cuándo no debemos utilizar Apache Hadoop? Bueno, pues también veremos durante el curso que Apache Hadoop no es útil cuando necesitamos un análisis de tiempo, un análisis de datos en tiempo real. Hadoop es capaz de tratar muchos datos en un tiempo aceptable, pero tenemos que tener en cuenta que Hadoop trabaja en disco y el disco no es tan rápido, por lo cual puede ser que tengamos procesos más reduced que nos tarden horas o incluso días. Por lo cual si necesitamos el análisis en tiempo real, Hadoop no va a ser una buena solución. Durante el curso vamos a ver cómo Apache Spark nos puede solucionar este problema, ya que Apache Spark trabaja en memoria, lo que nos da una latencia mucho más baja y nos permite el procesamiento de datos en tiempo real. Vale, Apache Hadoop tampoco está pensado para cuando tengamos un sistema de datos, un sistema con una base de datos relacional, es decir, cuando tengamos un modelo de nuestros datos muy complejo, en el que necesitemos usar join, filter, uniones, etc. Si nuestro proyecto necesita de eso, Apache Hadoop en principio no va a ser una buena lección. ¿Cuál es la solución que se le puede dar? Bueno, durante el curso vamos a ver una herramienta que se conoce como Apache Hive y que lo que nos permite es lanzar consultas SQL sobre el HDFS. De esta manera tendremos nuestros ficheros alojados en el HDFS y luego tendremos con Apache Hive que crearles como una pequeña estructura de esos datos para poder ser consultados con un lenguaje muy similar a SQL que se denomina HiveQL. Bien, tampoco es útil Apache Hadoop cuando queramos modificar nuestros datos. ¿Por qué? Porque el HDFS utiliza la política de Great ones, rare many, es decir, escribimos una vez el fichero y lo podemos leer muchísimas veces, pero un fichero una vez que está escrito en el HDFS no puede ser modificado. Lo único que podemos realizar es añadir contenido al final del fichero o eliminarlo. ¿Qué ocurre si tenemos que modificar el fichero? Pues tendríamos que revisar un poco todo nuestro proceso porque deberíamos de coger el contenido que hay en el fichero, borrar ese documento y crear un documento nuevo con todo lo que había más lo que queramos añadir. Pero vamos, no es un proceso trivial y no podemos añadir en un punto aleatorio de un fichero del HDFS. Y por supuesto cuando no se pueda paralelizar nuestro trabajo. Como ya veremos, una de las características principales de Hadoop es la paralelización, es la capacidad de poder distribuir los datos a lo largo de los nodos de nuestro cluster. Por lo cual si nuestros datos tienen que ser tratados de una manera completamente secuencial y no se pueden paralelizar, Apache Hadoop no nos va a dar ninguna ventaja a la hora de tratar dichos datos. Y por último también vamos a ver durante el curso algunas malas prácticas, algunas prácticas que no debemos usar cuando implementamos Big Data con Apache Hadoop. Alguna de ellas es tener cientos de ficheros pequeños en el HDFS. Ya veremos que el HDFS está pensado para para para alojar ficheros muy muy grandes del orden del terabyte o del petabyte y almacenará esos ficheros en bloques de 128 megas. Por lo cual si tendremos, si tenemos muchos muchos ficheros pequeños con poco tamaño, esto no es útil en el HDFS. Otra mala práctica es tener muchos procesos map con poca duración. Ya veremos lo que son los procesos map reduce, pero la parte del map es la parte donde vamos a paralelizar el trabajo, donde vamos a dividir los datos. Entonces si tenemos demasiados procesos map, es decir, hemos paralelizado más de la cuenta y eso tampoco es útil porque todo lo que conlleva la ejecución del map no nos ayudaría, estaríamos perdiendo tiempo en ese punto. Luego otra mala práctica es tener demasiados pocos reviews para ficheros muy grandes. Si tenemos ficheros de 2 gigas y sólo tenemos 2 reviews, cada reviews estaría comiendo un giga completo de datos. Son muchos millones de líneas, por lo que la idea es poder paralelizar mucho más los datos. Si tenemos 10 máquinas, pues tendremos cada máquina con una parte del fichero. Y por último otra mala práctica es generar muchas salidas y pequeñas de los reviews. Ya veremos cómo en los procesos map reduce finalmente vamos a generar unos ficheros de salidas que serán ya nuestra información útil, que tendrá la V del valor que comentábamos del big data. Entonces si esos reviews generan muchas salidas y muy pequeñas, también estaríamos perdiendo tiempo, porque los reviews necesitan un tiempo para crear los ficheros y para escribir en el HDFS. La idea es tener menos ficheros de más tamaño. Y bueno, aquí os he mostrado un poco cuándo debemos usar cadub y cuando no debemos usar cadub. Ahora es el momento de suscribiros al curso y comenzar a ver la herramienta.
DOC0062|Big Data|Cuando queremos hacer algún tipo de análisis de datos, sea hallar algún estadístico, hacer alguna visualización, algún filtrado o entrenar un modelo de Machine Learning, usualmente cargaremos nuestros datos desde un archivo almacenado en nuestra computadora hacia un DataFrame de Pandas, y aquí gracias a la magia de Pandas vamos a poder hacer el análisis que querramos. Esta magia y rapidez de Pandas se debe a que los DataFrames se cargan en la memoria RAM de sus computadoras una sola vez. Luego de esta carga inicial, cualquier análisis o operación ejecutada sobre el DataFrame se hace con los datos ya cargados en la memoria RAM. Lo interesante es que las memorias RAM son muy rápidas al momento de operar sobre datos que allí se encuentran, pero ¿qué pasa cuando mis datos no entran en mi memoria RAM? Teniendo en cuenta que hoy en día la mayoría de computadoras personales tienen entre 8 y 16 GB de RAM, esto podría suceder muy fácilmente. En este punto vamos a obtener un error si queremos cargar todo en un DataFrame de Pandas y nos tocará implementar una solución en donde nuestro código lea y opere sobre nuestros datos por partes. Estas múltiples lecturas al disco duro son lo que va a matar la eficiencia y rapidez en nuestro proceso, ya que en comparación a la memoria RAM, leer datos del disco duro es de 10 a 100 veces más lento dependiendo de la calidad del disco. Sin embargo, ¿qué pasa si tenemos más de una máquina a nuestra disposición? Por cada máquina tendríamos una memoria RAM adicional que podríamos utilizar para que este procesamiento siga siendo rápido, y por sobre todo que soporte grandes cantidades de datos. Con esta premisa nacen algunos problemas, ¿cómo dividimos los datos en las memorias RAM de varias máquinas? ¿Cómo controlamos qué datos están en cuál máquina? ¿Cómo unimos los datos cuando queremos hacer algún análisis como una agrupación? Con estos problemas nace una solución, Apache Spark. Spark es un framework para procesar grandes cantidades de datos, cuya idea principal es distribuir los datos en las memorias RAM de varias máquinas y que cada máquina se encargue del procesamiento de la parte de los datos que está en ella. A este conjunto de computadoras se le denomina clusters, y a cada computador en el cluster se le llama nodo. Por cada cluster existe un nodo maestro o master node y varios nodos trabajadores o worker nodes. El nodo maestro se encarga de recibir las tareas que tienen que ejecutarse y dividir y repartir estas tareas a los nodos trabajadores, los cuales van a ejecutar estas tareas en paralelo sobre su parte de los datos utilizando el paradigma de programación map reduce, que en pocas palabras es un divide y conquista para grandes cantidades de datos. Y en este punto las tareas pueden ser, por ejemplo, cargar los datos, hacer algún filtrado, alguna agrupación, una somatoria o incluso entrenar un modelo de machine learning. Cuando los datos se cargan, estos se distribuyen, idealmente de manera uniforme a lo largo de las memorias RAM de los nodos workers. Y el nodo maestro conoce hasta cierto punto cómo se encuentran hechas estas divisiones de los datos, de modo que el nodo maestro pueda en algunos casos saber a qué nodos trabajadores asignar ciertas tareas. Por ejemplo, si el usuario nos pide mostrar los cinco primeros registros de un dataset, en este ejemplo solo el nodo trabajador 1 y 2 van a recibir tareas, evitando el trabajo innecesario de los otros nodos. Los resultados de cada tarea son retornados por cada worker directamente a la aplicación, donde existe un ente que se encarga de unirlos y presentarlos al usuario. La magia es que para nosotros, como usuario final, todo esto funciona detrás de cámaras. Nosotros nunca nos enteramos de cómo están distribuidos los datos ni de lo que está haciendo cada nodo. Nosotros solamente manejamos nuestros datos a un alto nivel como si estuvieramos manejando un data frame de pandas. Podemos cargar datos directamente de un archivo CSB, Excel o JSON y Spark se va a encargar de dividir los datos de manera eficiente a lo largo de los worker nodes. Luego podremos hacer consultas y análisis en los datos utilizando una sintaxis muy parecida a consultas SQL. Con Spark también podemos entrenar modelos de machine learning con la librería MLib y ejecutar algoritmos de análisis de grafos con la librería GraphX, librerías que trabajan encima de Spark y hacen uso de su procesamiento distribuido para hacer estas operaciones de forma eficiente. Spark es rápido gracias a este procesamiento paralelo y distribuido de los datos y el hecho de que los datos a procesar casi siempre estarán en memoria RAM, con lo que la velocidad de tareas de procesamiento de grandes cantidades de datos se puede incrementar en órdenes de magnitudes comparado con frameworks más convencionales como Hadoop. Pero ojo, esto no sucede por arte de magia, tendremos que ser eficientes también a la hora de escribir código para poder sacar el mayor provecho de Spark. Cabe aclarar que Spark no es un lenguaje de programación ni una librería, es un framework, un marco de trabajo sobre el cual trabajamos. Esto quiere decir que lo podemos utilizar en muchos lenguajes de programación, incluyendo Python, gracias a la librería de PySpark, siempre y cuando tengamos un clóster de computadoras con Spark instalado y configurado. Lo cual puede llegar a ser muy complejo. Para nuestra suerte existen algunos proveedores de servicios en la nube como AWS que nos dan herramientas que con un par de clicks ya tenemos configurado un clóster de máquinas para ejecutar nuestro código de Spark sobre nuestros datos. Spark también implementa un formato de archivos optimizado para la rápida lectura y guardado, denominado Parquet, así como también implementa diversas estrategias para hacer joins o manejar transformaciones de datos cuyo resultado ya no entra en la RAM. Pero todo eso es un tema para otro video.
DOC0063|Big Data|es fastidiado escalar microservicios verdad? no todo eso que nos cuentan es luego tan fácil y desacoplar las aplicaciones desde el principio tampoco es una tarea sencilla de hecho hay muchos casos de uso en los que te seguirá surgiendo la duda de puedo realmente hacer un desacople total o realmente voy a necesitar alguna funcionalidad que esté centralizada bueno en todo eso hay algo que te puede ayudar y que puede colaborar en que tu aplicación y tu aplicación creada con microservicios sea mucho más flexible es Kafka y te lo voy a contar ahora Kafka es un proyecto de la fundación apache y básicamente con pocas palabras y permitirme que no sea del todo exacto pero para ello lo que estoy buscando es que se entienda mejor Kafka es simplemente un sistema que te permite hacer un stream de eventos y poder comunicar cosas que suceden en un microservicio productor a otros microservicios consumidores con ciertas particularidades si os acordáis de la temporada anterior cuando os empecé a hablar de cómo desacoplar tus programas con microservicios hacía mucho énfasis en una herramienta o en un protocolo que es mqtd y que a mí me parece que es muy muy interesante para aprender de cara a luego implementar Kafka los dos utilizan un sistema de publicación suscripción y ese sistema de publicación suscripción lo que permite es que haya microservicios que comunican información y se olvidan comunican a una red de esa información y se olvidan mientras que hay otros microservicios que a diferentes velocidades pueden recibir esa información y hacer algo con ella en el caso de mqtd eso no es del todo así en el caso de mqtd puedes es factible es posible que algunos mensajes se pierdan sin embargo en Kafka no tienes ese problema en Kafka los tópicos son persistidos durante un determinado tiempo obviamente no van a persistirse para siempre pero sí que es verdad que los microservicios consumidores pueden leer a diferentes velocidades lo cual es muy interesante técnicamente Kafka tiene tres conceptos básicos el tópico la partición y la replicación los tópicos de Kafka están organizados como si fuera un feed de noticias que van fluyendo de forma continua el productor o las aplicaciones productoras escriben datos en estos tópicos para que los consumidores aplicaciones consumidoras puedan recibir la información de forma organizada la información está organizada dentro de lo que se denominan registros o records y están dentro del cluster esperando a ser accedidos por parte de los consumidores Kafka retiene en sus ficheros de log que realmente es donde finalmente está la información almacenada toda la información que ha sido publicada durante un determinado periodo de retención la posición de lectura de un determinado consumidor en el log es importante porque desde ese punto de lectura hacia las nuevas escrituras existe lo que se denomina un offset ese offset es un elemento que se puede ir avanzando bien de forma automática o bien de forma individualizada o bajo selección para poder acceder a un determinado registro de información las particiones son secuencias que no se pueden no se pueden cambiar y que están divididas están organizadas de tal forma que diferentes consumidores puedan acceder a la información de Kafka con la intención de que puedan leer a diferentes velocidades por otra parte Kafka y un determinado tópico puede tener muchas particiones múltiples particiones esto permite que puedan ser accedidos los tópicos en paralelo por diferentes consumidores y que cada consumidor lea y haga lo que necesite a su propia velocidad finalmente y en relación con el concepto de replicación de Kafka es necesario mencionar que la replicación sucede al nivel de la partición la replicación al final no es ni más ni menos que una redundancia sobre la información que ya tiene Kafka por así decirlo cada partición o réplica tiene un servidor que actúa como líder y tiene otro servidor que actúa como seguidores eso tiene más que ver con lo que os decía al principio de que Kafka necesita a su equipo y como service discovery pues para poder poner en comunicación a diferentes instancias de Kafka el líder de una determinada réplica es el que se encarga de todas las solicitudes de lectura y escritura mientras que los demás seguidores lo único que hacen es replicar la información que hace el líder o que o que escribe o lee el líder si el líder falla cualquiera de los seguidores pues puede tomar la función de líder y cuando el líder vuelve pues la carga se vuelve a distribuir muchos proyectos a día de hoy sobre todo en entorno cloud están desacoplados mediante sistemas de colas o sistemas de mensajería o simplemente por mencionar algo Kafka no es un requisito imprescindible yo he hecho proyectos en los que en lugar de utilizar Kafka me he valido de mqtt que es un protocolo que no es igual que Kafka pero que su sistema de publicación y suscripción sí que es similar por lo tanto cuando necesitas empezar a entrenar a un equipo para que pueda empezar a utilizar Kafka quizá es más sencillo el principio que utilicen mqtt y que se adapten a lo que es el funcionamiento de publicación suscripción que directamente soltar a alguien Kafka que quizá con los conceptos de partición tópico y replicación pues igual se pierden un poco en cualquier caso desacoplar microservicios no es tan complicado como parece pero sí es verdad que necesitas un elemento más y tienes que tenerlo presente así que nada con este vídeo espero haberte ayudado en lo que es Kafka déjame por favor abajo los mensajes si quieres que haga alguna aclaración y si no pues seguiré profundizando en todos estos temas de arquitectura así que nada muchas gracias y nos vemos chau
DOC0064|Big Data|¿Qué es Kubernetes? Kubernetes viene de la palabra griega kibernitis, que significa gobernador, timonel o capital de un barco. Kubernetes es un sistema de orquestación de contenedores de código abierto, aunque quizá esta definición sea un poco abstracta, así que la analizaremos parte por parte. En primer lugar, es necesario saber qué es un contenedor. En un sistema operativo, un proceso se define como el conjunto formado por las instrucciones de un programa, su estado de ejecución y sus recursos asociados. A través del sistema operativo, el proceso tiene acceso a parte de los recursos físicos disponibles ofrecidos por el hardware. Normalmente, los procesos corriendo en un sistema operativo tienen visibilidad los unos sobre los otros. Un contenedor se define como un mecanismo por el cual un proceso o un conjunto de procesos que corren en un sistema operativo quedan aislados del resto del proceso. Además, cada contenedor contiene los ficheros y dependencias necesarias para que estos procesos se puedan ejecutar correctamente. Por eso, al empleo de contenedores se le conoce comúnmente como virtualización de nivel de sistema operativo. ¿Pero cuál es la diferencia entre un contenedor y una máquina virtual? En el modelo de virtualización tradicional, el hipervisor actúa como elemento intermedio entre las máquinas virtuales y el hardware del servidor físico. Cada máquina virtual contiene un sistema operativo, junto con los binarios, librerías y aplicaciones que ejecutan. El contenido de una máquina virtual puede empaquetarse en imágenes para facilitar su distribución. Sin embargo, en el caso de la virtualización de nivel de sistema operativo, solo se emplea un sistema operativo sobre el que corren los contenedores. Cada contenedor tiene acceso al núcleo del sistema operativo y utiliza los componentes comunes del mismo. Así, los contenedores son mucho más ligeros que las máquinas virtuales, pues el sistema operativo no va empaquetado en el contenedor y contiene exclusivamente las aplicaciones, las librerías y los binarios necesarios para su ejecución. Para que estos contenedores puedan ejecutarse sobre el sistema operativo, es necesaria una capa intermedia, el container runtime o container engine, que se encarga de ejecutar los contenedores. Existen varias implementaciones. Docker, Cryo o Rocket son algunas de las más utilizadas. Al igual que las máquinas virtuales, los contenedores se generan desde imágenes. Así, una vez generada una imagen, puede distribuirse fácilmente y ejecutarse en cualquier máquina con un container runtime. Cabe puntualizar que las imágenes de contenedores se almacenan en un tipo de repositorio específico, conocido como registro de imágenes. El container runtime descarga las imágenes a ejecutar desde estos registros. Ahora llega la gran pregunta. ¿Para qué necesitamos orquestación si ya tenemos un container runtime que se encarga de los contenedores? La respuesta es sencilla. En entornos productivos, en los que es necesario garantizar alta disponibilidad, se utilizan múltiples nodos o máquinas, cada una con su propio container runtime. En esta situación, surgen una serie de interrogantes. ¿Cómo garantizamos que los contenedores de un nodo pueden comunicarse con los de los demás? ¿Cómo sabe cada nodo qué contenedores corren en los demás nodos? ¿Cómo se decide el nodo en el que se va a ejecutar un contenedor específico? ¿Cómo garantizo que un contenedor se reinicie si falla? ¿Y si un nodo se queda sin espacio, cómo garantizo que los contenedores se ejecuten en otro nodo disponible? La respuesta a todas estas preguntas y muchas más es la misma. Kubernetes Kubernetes permite agregar grupos de nodos que ejecutan contenedores y gestionarlos de manera eficiente y sencilla. Además, proporciona capacidades ideales para aplicaciones cloud-native, como autoescalado, permitiendo escalar las aplicaciones y los nodos en función del tráfico, así como una arquitectura diseñada para proporcionar alta disponibilidad. Kubernetes Kubernetes Kubernetes A nivel de aplicación, Kubernetes proporciona servicios de balanceo de carga, service discovery, orquestación de almacenamiento, gestionando recursos a nivel de contenedores y monitorización del estado de salud de las aplicaciones en el cluster. Además, proporciona utilidades para automatizar los despliegues y rollbacks de las aplicaciones. Kubernetes Kubernetes Kubernetes Si quieres saber más acerca de Kubernetes, contenedores y cloud computing, no dudes en visitar cubesandclouds.com y spainclouds.com
DOC0065|Big Data|Este es un microprocesador muy básico, esto quiere decir que su potencia de procesamiento es bastante baja, no hay mucho que podamos hacer con ella. Sin embargo, este microprocesador básico también es muy barato, entonces, ¿qué pasaría si yo comprara muchos de ellos y combinar a su capacidad de procesamiento? Bueno, esta es esencialmente la idea detrás de un clúster, un grupo de computadores completos interconectados que operan en conjunto como un solo equipo de cómputo. Con computadores completos me refiero a un sistema independiente que puede procesar por sí mismo, no tiene que ser necesariamente una computadora con monitor, mouse y teclado, puede ser solamente la unidad de procesamiento. Antes de continuar, debo mencionar que cuando un computador se vuelve parte de un clúster se le conoce como un nodo, así que por lo que resta del video, así les voy a decir. El principal propósito de un clúster es manipular grandes cantidades de datos en tiempo récord a través de la computación paralela. Pongámoslo así, piensa en los bits del Fórmula 1, durante esta parada se llena el tanque, se cambian las llantas, se hacen reparaciones y ajustes. Técnicamente todas estas cosas las podría hacer un solo mecánico de manera secuencial, pero tomaría mucho tiempo, es por eso que al tener múltiples mecánicos, todos ocupándose de distintas tareas simultáneamente, se termina mucho más rápido. En esta analogía, cada mecánico es un computador y el auto representa el sistema. Pero cómo funciona? En términos generales es muy sencillo, cada uno de los nodos es conectado a una red de alta velocidad y a todos ellos se les instala la misma capa de software intermedio conocida como middleware, la red les permite conectarse entre ellos y el middleware comunicarse. Ahora todos representan un solo sistema. Existen cuatro grandes beneficios de utilizar un cluster, la escalabilidad absoluta, la escalabilidad incremental, la alta disponibilidad y la relación precio prestaciones. La escalabilidad absoluta nos dice que un cluster puede estar conformado por decenas de computadoras y ya que cada uno es su propio procesador, entre más nodos tengas, más incrementa la capacidad de procesamiento de todo el sistema. La escalabilidad incremental denota que siempre se puede seguir añadiendo nuevos nodos al cluster, de modo que puedes comenzar con un cluster modesto de solo unos cuantos equipos y a medida que tu capital incremente, integrar más. La disponibilidad se define como el periodo de tiempo continuo en que un servicio está disponible. Volviendo a la analogía del formula 1, que tal si uno de los mecánicos se nos desmaya y tal vez no desayuno? Bueno, si esto pasa, otro de los mecánicos puede fácilmente tomar su lugar y terminar la tarea que estaba realizando antes de desmayarse. Al final, las reparaciones se terminan con éxito y nadie es afectado por lo sucedido. Casi nadie. En este sentido, ya que cada nodo es autónomo, el fallo de uno de estos no representa el fallo de todo el sistema. Por último, la relación precio prestaciones nos asegura que juntando varios equipos básicos, se puede conseguir una potencia de cómputo igual o incluso mayor a la de un equipo avanzado, por mucho menos costo. Esta ventaja es, particularmente, la que más atrae a las empresas. Ahora hablemos de su configuración. Existen dos formas de clasificar la configuración de un cluster, por la forma en que se activan y por la forma en que comparten información. En la primera clasificación, existen dos configuraciones, la espera pasiva y el secundario activo. Digamos que tenemos dos servidores o nodos. En la espera pasiva, el nodo que opera como el servidor principal se encarga de realizar todas las tareas de procesamiento, mientras que el servidor secundario se encuentra inactivo. No es hasta que el servidor principal falla que el secundario recibe la señal para heredar sus funciones. Esencialmente es un servidor de repuesto. Por otro lado, en el secundario activo, el servidor secundario también realiza tareas de procesamiento de manera simultánea. Y si el principal falla, simplemente recibe la carga extra de tareas que le pertenecían al principal. En la segunda clasificación, existen tres configuraciones. La configuración servidores separados, en que cada servidor tiene sus propios discos de almacenamiento y si llegan a fallar, otro servidor debe copiar los datos de su disco para poder utilizarlos. La configuración Share Nothing, en que los servidores y los discos de memoria están divididos por volúmenes. Ciertos servidores están conectados a ciertos discos y entre ellos pueden compartir su información. La configuración Share Disks, en que todos los servidores comparten los mismos discos de memoria y por lo tanto todos tienen acceso a ellos.
DOC0066|Big Data|Muy buenas a todos y bienvenidos al nuevo curso de Open Webinars. Mi nombre es Ejelo Sada y vamos a ver en qué consiste un sistema distribuido. Lo primero de todo, ¿qué es un sistema distribuido? Es un conjunto de equipos independientes que actúan de una forma transparente como un único equipo. ¿Cuál sería el objetivo? Es descentralizar tanto el almacenamiento de la información como el procesamiento. Todo esto lo estamos viendo de cara a el actisil y cómo funciona. ¿Qué ventajas aporta? Pues mayor eficacia, mayor tolerancia a fallos porque al estar distribuida la información, en el caso de que se nos caiga un nodo, dicha información va a encontrarse en otros de replicada. Mayor velocidad a la hora de realizar un procesamiento, cuando se realiza una consulta, los procesamientos se dividen entre todos los nodos que forman el sistema distribuido, en vez de, por ejemplo, enviarlo a un único nodo y que él tenga que hacer todo el trabajo. Y escalabilidad, es decir, si mañana necesitamos más procesamiento o añadir más disco duro, en vez de que los equipos crezcan de forma vertical añadiendo más almacenamiento, más RAM o más CPU, se añaden equipos de forma horizontal, que se añadirían al cluster o al sistema distribuido. ¿Qué aporta una base de datos distribuida? Pues la información se almacenaría en diferentes sitios, en diferentes sitios de la red, físicamente, aunque lógicamente, de cara a un usuario al final, es una única base de datos. Cuando nosotros queramos acceder a la información, vamos a lanzar una consulta e internamente él ya sabrá dónde se ha almacenado cada uno de esos datos. Independencia con respecto al sistema operativo, es decir, si el servicio se puede instalar en sistema operativo Linux o Windows, da igual, la base de datos es de forma distribuida y es completamente transparente el sistema operativo para el usuario al final. La información se queda fragmentada y las réplicas aportan alta disponibilidad. A continuación vamos a ver un vídeo, una especie de animación, en la que vamos a ver cómo se distribuye en el act-tc esta información distribuida. Suponemos que disponemos de un cluster distribuido del act-tc que está compuesto por tres nodos. Entonces nosotros tenemos a través de una entrada de datos toda la información que vamos a obtener. El act-tc paquetiza la información y la llama, la denomina shard. Entonces lo que va a hacer es, este shard que va generando, lo va a ir distribuyendo entre los distintos nodos. Pues por ejemplo, el primer shard lo va a colocar aquí y para conseguir el alta disponibilidad que comentábamos antes, que es una de las ventajas de un sistema distribuido, vamos a establecer que las réplicas sean de esta forma. Cuando yo indexo la información se crea una réplica en un nodo distinto. Muy importante esto, nunca se deba crear en el mismo nodo, ya que si tengo el shard primario y la réplica en el mismo nodo y este se cae, perdería esa información. Por tanto las réplicas se van a ir creando en nodo distinto a lo cual le va entrando la información. Pues como se puede ver, van entrando los shards, los paquetes y se van generando réplicas en nodo distinto. ¿Qué ganamos con esto? Que si mañana por ejemplo ocurre un error y perdemos esta información, no pasa nada. ¿Por qué? Porque el 1 primario tiene una réplica en este nodo, el 2 réplica tiene la primaria aquí y efectivamente todos tienen una réplica o el primario en otro nodo distinto. Entonces, ¿qué es lo que haría en este caso las T-Shards? Se percata de que hemos perdido un nodo y hasta que este nodo se recupere, lo que va a hacer es sabiendo que en ese nodo estaba esa información, voy a replicarla pues si yo tenía aquí el 1 primario y ahora tengo aquí la réplica del 1, voy a poner el 1 primario aquí. De esa forma voy a ir cogiendo todos los shards que había perdido y voy a seguir cumpliendo que la réplica siempre sea 1. De esta forma todos los shards que estaban asignados a este nodo lo he recolocado en los otros nodos y así no perdemos la información. Es una de las ventajas y creo que es la mejor ventaja del sistema distribuido hablando de base de datos, no de procesamiento. Entonces, este conjunto de nodos lo que forma es lo que nosotros llamamos un cluster. El acti-sheet, cuando nosotros tenemos una instancia de acti-sheet instalada, un servicio en un nodo, en un equipo se denomina nodo y un conjunto de nodos del acti-sheet forman un cluster. Cada nodo se encarga de peticiones HTTP y dispone de una resapi para contestar las peticiones que se le vayan realizando. Los nodos conocen el estado del resto y pueden reenviar las peticiones de los clientes al nodo indicado. Entonces ahora vamos a entender mejor este concepto cuando veamos qué tipo de nodo existe. Disponemos de el masternode, que es uno de los fundamentales que se dedica a crear y a borrar los índices. Cuando nosotros hablamos de índices en el acti-sheet nos referimos a base de datos. Entonces nosotros el masternode será el que cree, el que borre los índices, gestiona los nodos conectados, toma decisiones de colocación de datos. Cuando nosotros queremos indexar información en el acti-sheet, esa información llega al master y será el master el que diga este share va hacia allí, este share va a este nodo y estos datos van a este nodo. A ver que las reparte. De la misma forma cuando realizamos la búsqueda le preguntaremos al master dónde realizar la búsqueda de un determinado índice, de una determinada base de datos y será el master el que nos diga dónde podemos consultar cada información. Los datanodes su objetivo es almacenar la información y también procesarla. Cuando nosotros realizamos una consulta no es simplemente devuelven esta información. Podemos hacerle consulta más compleja en el que hagamos por ejemplo agregación. Por ejemplo si tenemos clientes y lo que nos debe cliente, lo que nos debe nosotros vamos insertando esos datos, podemos consultarle que nos devuelva del cliente x todo lo que nos debe. Entonces tiene que realizar una suma. Es un ejemplo muy básico pero es un procesamiento que tiene que utilizar también los datanodes. Por lo tanto se utilizan tanto para almacenar información como para procesarla. Injustnodes ejecutan una especie de preprocesamiento. El preprocesamiento que hablábamos antes de loctas. Pues si no requerimos tener un loctas instalado en una máquina y el preprocesamiento muy básico podemos hacerlo con un Injustnode que también va en el mismo nodo que el Elasticseed y lo que hace es recoger la información, aplicarle una serie de campos con unos filtros y pasárselo al masternode para que pueda realizar el indexado de dichas notas. Y el último de los nodos que es el que más quería destacar es el nodo coordinador. Lo que hace es encaminar la petición. Es un nodo que se instala en el cliente como en este caso por ejemplo nosotros utilizaremos Kibana y Kibana cuando realice las peticiones al Elasticseed lo que haría si no utiliza un nodo coordinador es preguntarle al master. Oye, ¿dónde puedo buscar esta información? ¿Qué pasa? Que si nosotros tenemos un cluster sobrecargado porque el master se dedica a hacer muchas tareas podemos sobrecargar un master cluster. Entonces el nodo coordinador lo que hace es mantener una comunicación constante con el master actualizándose toda la información que el master sabe gracias al nodo coordinador. De esta forma si nosotros con Kibana en vez de conectarlo directamente al master se conecta a uno coordinador, el coordinador cuando haya que buscar cierta información no le va a preguntar al master ya que conoce lo mismo que conoce el master sino que ya sabe dónde hay que buscarlo. Es una forma de evitar sobrecargar más el cluster a través del master y poder hacer las consultas con la información que conoce el nodo coordinador. Además de saber dónde escribir y dónde leer puede ayudar a la parte de procesamiento de datos. Es decir, lo que sería un bus reduce que se ve mucho en el procesamiento escalado de data es cuando yo hago una consulta, como se puede ver aquí, se enviaría dicha consulta a los data nodes los cuales van a hacer la búsqueda y una especie de preprocesamiento pero claro yo quiero obtener un único resultado de una única consulta. Si tengo tres data nodes no quiero obtener tres resultados. Vale, entonces lo que se hace es que los data nodes realizan ese procesamiento que les vamos a llamar una especie de mapeo y ese preprocesamiento una vez han terminado se lo envían al nodo coordinador el cual hace una especie de mergue de unión de los resultados tenido de todos los nodos y es el que presenta el resultado final aquí van a de esta forma ganamos velocidad y nos sobrecargamos el clases y bueno con todo esto terminamos este tema si está interesado tenemos en Open Webinars un curso de ELK.
DOC0067|Big Data|Muy buenas a todos, bienvenidos a un nuevo curso de Open Webinas. Mi nombre es Sergio Lozada y vamos a explicar en qué consiste y qué es ELEKKA. ELEKKA es un stack compuesto por tres pilares fundamentales que son el actiSYL, loctas y kibana. Podemos resumirlo como que sería un stack de datos que tiene un almacenamiento distribuido en el actiSYL que sería la base de datos, procesamos la información con loctas antes de almacenarla, vemos las visualizaciones y los dashboard en kibana y podemos realizar consultas sobre dicha información en tiempo real. Pero antes de seguir, ¿qué problemas son los que intenta resolver loctas? El primer problema es la falta de consistencia. ¿Qué consiste la falta de consistencia? Tenemos muchos dispositivos con lox. Dentro de nuestros servidores, por ejemplo, tenemos distintos servicios funcionando y ahora cada servicio tiene un tipo de lox distinto. Entonces, llegan los administradores del sistema y los DevOps geníicamente o administradores web o desarrolladores que necesitan acceder a dichos lox para comprobarlo. Entonces hay gran dificultad ya que los formatos varían y es difícil de entender. Por ejemplo, podemos poner aquí un ejemplo de un lox, otro lox distinto, otro lox distinto, si os fijáis son completamente distinto, ¿vale? Dependiendo del servicio. Incluso hay lox que a lo mejor falla o faltan campos. Entonces, siguiente problema que nos podemos encontrar, el formato de tiempo. De cada lox puede venir con un formato de tiempo distinto. Pues en este viene la fecha, otro como time stamp, otro con la hora al finalizar. Decentralizado, ¿vale? Imaginarse que tenemos que administrar 50 servidores, los lox se encontrarían repartidos en todos esos servidores y cada servidor puede tener un tipo de lox. Entonces, haciendo cuenta dentro de un servidor puede haber diferentes rutas donde encontrar los lox. Un administrador del sistema puede, si tiene dos o tres servidores que administrar, puede acceder por ellos por SSH, hacer un cap de los lox, un grep para buscar algún error, pero si tienes 50 o 100 servidores, la verdad que el SSH y un grep ya no es escalable, ya no puede seguir. Y el último problema puede ser quizás la falta de conocimiento que no se implementan bien las políticas. Como por ejemplo, las personas que tienen que acceder a un lox no tienen permiso. Un lox se encuentra dentro de un servidor, una persona trabaja con un servicio en concreto, tiene que haber que le pasar el servicio, pero resulta que por políticas de empresas no se le permite acceder a dicha máquina. Puede ser uno de los casos. No tienen experiencia para entender una línea de lox. Todos sabemos que las líneas de lox, pues la verdad que hay los mejores lox que tienen 10 campos, pero hay otros que tienen 40. Entonces, cuando ves tal cantidad de información es complicado muchas veces saber este número que representa o esta cadena de texto que es lo que estás representando. Y también se desconoce dónde se encuentra dicha lox. Con servicios que los mejores usuarios finales que utilizan servicios pero que no han trabajado nunca con dichos lox, entran a la máquina, no saben dónde se encuentran los lox, cómo se actualizan, cómo se van repartiendo si se van borrando cada día. Entonces hay como una falta de conocimiento. ¿Qué busca realmente el acte civil? Pues coger toda esta información, procesarla, almacenarla de forma distribuida para poder escalar en Big Data y poder obtener buenos rendimientos con grandes cantidades de información y transformarlo en esto, en visualizaciones. Tener visualizaciones con las que nosotros podemos identificar anomalías, comportamientos, eventos, picos y de forma gráfica y visual. Es decir, lo que nosotros vamos a ver en esta diapositiva, en estas visualizaciones y dashboard no es muy complicado de ver en lo que sería un lox. Entonces, en modo resumen, ¿qué es lo que está realizando Eleka? Recoliza lox de eventos, de aplicaciones, procesa dicha información y la pone a disposición de las personas que lo necesitan, tiene módulos de seguridad, aunque va por SPAC, que ahora mismo es de pago, pero se puede hacer una especie de seguridad adelante y implementar para que cada usuario pueda acceder a la información que realmente solo pueda administrar, que no se cruce, que un usuario no acceda a información la cual no le pertenece. Formateamos los campos. Hemos visto que hay lox que siguen un formato, un RFC, un SysLog, pero puede haber lox que tengan un formato irregular, ya sea de aplicaciones nuestras propias o de algún servicio que no siga ningún formato, como puede ser un SysLog. Entonces, dichos lox son prácticamente una cadena de texto, una cadena de texto que es muy complicada de entender. Entonces, con loxTasks, que es la parte de preprocesamiento que veremos a continuación, nosotros trabajaremos dichos lox y en vez de tener una cadena de texto lo que tendremos son distintos campos, campos que después podremos utilizar para el tema de filtrado. Y presentamos dicha información en dichos campos después en visualizaciones, ¿vale? Es donde podremos realizar búsquedas, donde podemos realizar filtrados, agregaciones y ver la información mucho mejor que en un fichero de texto en módulos. ¿Qué componentes son los que forman el LK? Bueno, pues el primero de ellos sería el HTC, es una base de datos distribuida, distribuye la información entre todos los nodos, por tanto es tolerante a fallos y tiene alta disponibilidad. Al igual que distribuye la información, distribuye Cuando se realiza una consulta o una búsqueda y la información esa se encuentra distribuida, será cada nodo el que procesa dicha información y devuelva los resultados, más o menos por tanto podrá obtener mejor rendimiento. LoxTasks es esa parte de preprocesamiento antes de guardar la información en el HTC que hemos comentado, donde nosotros recojaremos un input, una entrada, trabajaremos los eventos y los sacaremos por una salida, antes de almacenarlo en las bases de datos. Y por último, Kibana es el más visual, es donde nosotros vamos a generar la visualización sobre la información, donde vamos a generar los dashboards. Bien, no solo son estos tres, es decir, esos tres son los pilares, pero no son los únicos módulos que tienen. Alrededor aparece DITS, que son una especie de shipper, una especie de recolector de información, que recogen información, pues ya sea de un fichero, log de unos datos, de eventos, métricas del sistema, como puede ser CPU, como puede ser RAM, hacen comprobaciones de qué servicios se encuentran activos, analizan a nivel de red los paquetes, ven el tiempo de respuesta entre ellos. Y Expac, que sería el módulo de pago, que ofrece algunos plugin extra, como puede ser el sistema de seguridad, monitorización de todo nuestro sistema, alerta, de uso de grafos para ver la relación entre los distintos nodos y los distintos vértices de nuestros datos. Y desde hace unas semanas, machine learning, que sería una forma de aprender no controlada. La propia máquina iría aprendiendo, basado en tiempo, unos comportamientos de red, por ejemplo, pues que por la mañana hay tal cantidad de tráfico en una empresa, en tal oficina y por la noche te cae, y él solo es capaz de aprender dicho comportamiento durante unas semanas y después todo lo que se sale de ese comportamiento te avisaría. Por último, a modo resumen, tenemos todas nuestras fuentes, esas fuentes envía información a Loctas, el cual tiene muchos plugins, tanto de entrada como de salida, entonces podemos meter gran cantidad de datos en Loctas, Loctas procesará dicha información antes de que se almacene la base de datos de las tesis y con Kibana montaremos visualizaciones que accedan a esa información y nosotros poder estamos utilizando. Y esto es un pequeño resumen de qué consiste Loctas.
DOC0068|Big Data|Pido que compartan con sus grupos de Discord, de WhatsApp, sus redes sociales haciendo un salto mortal hacia atrás. Mierda, no que me duele la espalda. Muy buenas digitales y bienvenidos al Videoblog de la Informática Corporativa. Hoy vamos a usar la Stream API o la funcionalidad esta que tiene Spark para trabajar con Streams. En realidad Spark tiene dos APIs para trabajar con Streams, lo que sería la Structured Data Stream y los DStreams. La diferencia entre cada uno de ellos vendría a ser que la Structured Data Streams es para trabajar básicamente con Streams usando DataFrames, que es lo que vimos en el primer vídeo, os lo dejo por aquí para que lo veáis, donde podremos trabajar con los datos usando pues esto, esta API de los DataFrames y la SQL y este tipo de cosas que nos lo hacen más fácil, pero es menos escalable y no podemos trabajar con realmente Big Data de verdad usando solamente DataFrames. Podemos trabajar con cantidad de datos, pero tiene un límite. Los DStreams en cambio nos permiten trabajar con Streaming usando el API de los RDD, lo que vimos en el segundo vídeo, os lo dejo por aquí, que esta API es muchísimo más escalable, procesa los datos entre varias máquinas, no como los DataFrames, que los procesa en una sola máquina y este tipo de cosas. En este vídeo nos centraremos en los DStreams, los Distributed Streams. Como veréis es una alternativa, en este canal hemos hablado de otras alternativas como por ejemplo Apache Kafka con su Stream API, que era super potente. Seguramente para muchas cosas será mucho mejor utilizar Apache Kafka y para otras usar Apache Spark, ya lo veremos. Es muy importante entender cómo funciona cada herramienta para escoger la mejor herramienta para cada caso, porque una mala herramienta te puede mandar atrás del proyecto entero. Por eso es tan importante el trabajo de los arquitectos y meter a la gente que sabe al principio de todo y no al final, porque muchas veces cuando ya tienes el marrón es cuando llamas al que sabe y luego es complicado de arreglar. Pero bueno, esas son otras historias. Así que, ya estoy dentro de la pantalla. Como veis aquí tengo mi blog www.sabercronado.com donde tenéis cantidad de artículos. Os recomiendo mucho que les echéis un buen vistazo, tenéis las redes sociales, mi currículum, mis servicios de formación, de consultoría, de desarrollo, etcétera, etcétera. Y la comunidad Patreon para los héroes que quieren apoyar de verdad el canal, que de verdad que los necesitamos. Cada vez hay más gente y esto tiene pinta de despegaría. También nos podéis apoyar puntualmente vía PayPal, por si no queréis estar involucrados dentro de la comunidad patria. Y ya está, lo que vamos a hacer hoy va a ser lo de siempre. A través del terminal arrancaremos primero el Apache Spark en modo stand-alone, es decir, que tendremos el master y un solo worker en la misma máquina. Para ello me meto en la carpeta de Spark, la carpeta S-DIN. Aquí hago start master. Esto nos abre un master. Perdón, el master lo tendremos también, nos abre un dashboard súper útil porque nos dice dónde está escuchando. Si tenéis problemas para seguir lo que estoy haciendo, sobre todo revisad los anteriores vídeos, súper potente. Y aquí lo que hago es startworker.sh y aquí le paso el master al que se tiene que conectar. Y aquí ya tenemos una instalación con su worker que es el que va a trabajar. Perfecto. Me meto en la carpeta BIN, que es donde tenemos Spark Submit para lanzar las aplicaciones. Pero primero vamos a crear nuestra aplicación. Para ello con Nano, que es mi editor favorito en modo texto, me creo una aplicación que va a ser SD de D-Streams o DS de D-Streams.py. Acordaros en estos vídeos estamos creando todas las aplicaciones usando Python, que es una gran herramienta que se utiliza muchísimo en ciencias de datos, Big Data, este tipo de cosas, pero que podemos hacerlo en Java, Scala o R también. Vamos a inicializar nuestra aplicación importando pues los paquetes, las librerías. En este caso será Spark Context y Streaming Context. Esto ya nos viene inicializado cuando estamos trabajando con la consola, pero no cuando estamos creando una aplicación. Lo siguiente va a ser inicializar el contexto. Primero creamos el contexto de Spark, donde le ponemos el nombre de la aplicación y el master y después inicializamos el contexto de Streaming. En este caso a partir del contexto de Spark y cada cuántos segundos se van a procesar los datos, porque esta API lo que hace es conectarse a un origen de datos, un socket, un fichero o lo que sea, va leyendo los datos que le van llegando por Streaming y cada x segundos hace todo el proceso de datos. Esta es la lógica de negocio y en este caso le estamos diciendo que cada 10 segundos procese los datos que le vayan llegando. Y aquí lo tendríamos. Siguiente paso, inicializar el socket. En este caso le decimos, metemos en la variable lines las líneas que irá leyendo y aquí lo que creamos es un socket text stream. Aquí podríamos crear un file text stream y otros orígenes de datos al que le pasamos el host y le pasamos también el puerto al que va a conectarse. O sea, importante, él no estará escuchando en este host y en este port, sino que va a conectarse. Por lo tanto tenemos que poner algo a funcionar en este puerto. Para ello vamos a usar netcat, que es una aplicación que viene normalmente en todas las distribuciones Linux y que es una especie como de navaja suiza para para trabajar con red. Nos permite crear servidores, incluso tomar el control remoto de otras máquinas, pero esto es otra historia. Hacemos netcat menos lk 90 90 y esto lo que nos abre es un socket, es un server que queda ahí escuchando para que se conecten otros. En este caso yo puedo poner aquí hola mundo y esto lo va a mandar al que se conecte. Ya tenemos nuestro servidor que va a ir mandando datos y aquí tenemos nuestro programa que se va a conectar a este servidor. Ahora vamos a hacer toda la lógica de procesar los datos. Esto lo haremos de esta manera, fijaros. Esta API, la de destreams, es calcada o la misma API de los RDDs. Acordaros que aquí lo que podemos hacer es aplicar un map reduce o cantidad de funciones. El otro día nos lo expliqué, pero mirad aquí en la documentación de Spark tenéis todas las transformaciones de los destreams y aquí también podéis encontrar las transformaciones de los RDDs y este tipo de cosas. Podemos aplicar un mapeo, un flat map, un filtro, repartition, union, count, reduce, este tipo de cosas. En este caso, al igual como hicimos en el anterior vídeo, cogemos el RDD y le aplicamos un flat map. En este caso no le aplicamos una función sino que le aplicamos una lambda. En el anterior vídeo hicimos las dos cosas, aplicamos una función y una lambda. Acordaros que en Python normalmente en la práctica siempre acabas usando funciones porque en las lambdas de Python no puedes poner más de una instrucción, y es complicado. En este caso hacemos un flat map y por cada línea que nos llegue lo que le haremos será un split para que nos transforme cada línea. Cada registro que es una línea va a ser un registro una palabra. Y después lo siguiente que hace es un mapeo para transformar cada palabra y le pone a cada palabra el peso de 1. Y finalmente aplicamos la función de reducción, esto es un map reduce de cajón y lo que hace es agrupar todas las palabras y hace por cada palabra que encuentra les hace la suma de 1 más 1 pues 2, 3, 4 y así va sumando las distintas palabras y esto lo hace en paralelo. El motivo de que lo hagamos usando es el modelo este de map reduce es porque queremos paralelizar el trabajo y que en lugar de hacerlo calcularlo linealmente que esto al final tiene el límite de la potencia de la máquina. Y finalmente cogemos el wordcount y le hacemos un pprint que básicamente lo que hace es imprimir los 10 primeros registros más que suficiente. Una vez hemos acabado con esto simplemente arrancamos el contexto de streaming y esperamos a que se termine. Y ya está, no tiene nada más le decimos guardar y vamos a ver si esto funciona hacemos un spark-submit master le pasamos el master que sea aquí voy a buscar cuál es la url del master copiar perfecto y le ponemos de ese punto pi que es nuestra aplicación le dimos enter y esto ahora perfecto fijaros esto se ha quedado a la espera y a los 15 segundos hará el primer procesado perfecto y ya ha procesado el hola mundo que yo le he pasado aquí aquí yo le ahora le puedo poner hola mundo hola mundo adiós mundo y él vale me ha pillado el primer hola mundo y luego me ha pillado el hola mundo adiós vale otra vez hola mundo hola mundo adiós mundo perfecto y ahora me va pues espero que a procesar esto tarda los 15 segundos yo le he metido dos olas tres mundos y un adiós que es lo que teníamos aquí uno y dos olas dos tres mundos y un adiós y esta es la funcionalidad de trabajar con datos en streaming utilizando spark como veis si nuestro procesado va a ser muy simple es muchísimo más práctico utilizar apache Kafka os dejo el vídeo por aquí de apache Kafka para que le echéis un vistazo si queréis procesar o simplemente os queréis ahorrar el hecho de tener que copiar los datos en cada una de las máquinas workers podéis usar esta esta esta API que realmente es útil y bueno y esto es todo vale espera espera espera ya estoy fuera fuera de la pantalla vale como comentaba esto es todo o esta es toda la lógica que hay detrás de apache spark en lo que respecta al tema de streaming en próximos vídeos le vamos a meter mucha caña a cosas súper interesante como es la API de machine learning o la API de graph x que nos permite trabajar con grafos y nos permite hacer cosas tan peligrosas como una vez tenemos un usuario por perdón si estamos procesando la navegación de los usuarios no y tenemos un usuario que ha comprado algo o que ha hecho clic en un anuncio podemos ver la navegación de este usuario compararla con la navegación de otros usuarios parecidos y así enseñarles el mismo anuncio o el mismo producto para que hagan clic en lo que hace pues facebook y todas las redes sociales grandes y estos serán futuros vídeos como veis hasta ahora nos hemos quedado en todo lo que sería ingeniería de datos vale para procesar datos sacar report este tipo de cosas y ahora empezaremos a trabajar con realmente con las APIs que les interesan a los científicos de datos a los que están ahí mirando a ver qué que petróleo pueden sacar así que está atentos a los próximos vídeos por cierto que la semana que viene no habrá vídeo el motivo estoy de vacaciones vale me voy unos días y me voy a olvidar de crear vídeos así que como siempre por favor manita arriba y comentar decirme que os parece que os gustaría ver vale y bueno suscribiros en la web hecho ya eso ya ni se debería de decir y sobre todo compartir por favor porque es como crecemos vale compartir con vuestros grupos de discord de en foro coches vale a ver si alguien me comparten foro coches vale y tengo ahí un nicho ahí de seguidores en whatsapp vale en las redes sociales en todas partes con vuestros compañeros de universidad con vuestros compañeros de trabajo a ver si hacemos crecer esto vale qué buena falta me hace muchísimas gracias y hasta la próxima
DOC0069|Big Data|No os quiero hacer un spoiler, pero es Python. Muy buenas digitales y bienvenidos al videoblog de la informática corporativa. Y hoy continuamos con el tema de Big Data, que sí, que hace muchos meses que os prometo más vídeos sobre Big Data y ciencia de datos. Pero es que me cuesta mucho encontrar la fórmula para hacer vídeos, porque yo siempre hago vídeos y artículos sobre mi día a día. Y mi día a día sobre estos temas, pues engloban muchas tecnologías, mucha planificación, mucha coordinación con mucha gente. Y cuesta mucho sintetizarlo todo en un solo vídeo que dure 20 minutos. Pero ahora creo que he encontrado la fórmula, con lo cual vamos a tirar de este hilo a ver qué tal. Hoy vamos a continuar un post, un artículo que hice la semana pasada, donde explicaba los comandos bash más usados o las mejores herramientas bash para trabajar con ficheros muy grandes. Ficheros de terabytes, por ejemplo. En este sentido, pues explicaba lo básico. Leer las líneas de un fichero, cortar ficheros, dividir ficheros, comprimir ficheros, etcétera, etcétera, etcétera, que es la operativa básica pues para trabajar con Big Data en bash. Al final de este artículo había un pequeño script en bash para procesar un fichero JSON y convertirlo a CSV. Y lo que vamos a hacer ahora va a ser un paso más. Vamos a crear scripts para procesar grandes ficheros de gigabytes y ver cuál es el mejor lenguaje para hacer este tipo de trabajos. En el artículo usábamos bash porque era de lo que iba el artículo y porque es súper práctico. Un script de bash prácticamente no tienes que instalar nada o cuatro cositas para empezar a trabajar con ello. A partir de aquí ya tendríamos que tener algún lenguaje para crear scripts. En este sentido, Python es uno de los lenguajes que se ha impuesto más en tema de ciencia de datos. No es para menos. Y es que tiene cantidad de opciones out of the box para trabajar con estructuras de datos, trabajo con ficheros. El rendimiento que tiene es bueno, etcétera. Y lo compararemos también con Java, que es el otro lenguaje que yo utilizo muchísimo para aplicaciones corporativas. De hecho, muchos proyectos para trabajar con tema de Big Data y ciencia de datos están desarrollados sobre Java. Por ejemplo, Hadoop, Spark, etcétera. Todo ello funciona sobre Java, con lo cual es un buen lenguaje para hacer este tipo de trabajos. Y los vamos a comparar los tres para ver cuál es la mejor solución. Todos tienen sus pros y todos tienen sus contras. No me voy a enrollar más. Nos vamos a la pantalla. Muy buenas, digitales. Pues aquí estamos en la pantalla. Y aquí está mi blog, donde, como siempre, os hago un poco de spam. Y os digo que aquí tenéis toda mi lista de servicios, por si creéis que os puedo ayudar en vuestras empresas. Tema consultoría, formación, desarrollo full stack. Lo que necesitéis, que este 2021 está siendo duro, ya os digo. Aparte de esto, lo que os comentaba, este es el artículo que tuvo bastante éxito, donde os enseño cómo, por ejemplo, ver los recursos que tenéis disponibles para trabajar. Superimportante. No empecéis a hacer procesos en Big Data sin saber si tenéis bastante memoria RAM, que no es que no tengáis mucho CPU y que todo va bien. Incluso espacio en disco, que también es lo suyo. Y os explico cómo leer ficheros, las cabeceras, filas e incluso jq, tal, tal, tal. Y aquí, al final, os puse un pequeño script para procesar un fichero de texto, donde cada línea era un registro en JSON. JSON, este tipo de ficheros, donde cada línea es un registro en JSON, es muy difícil, sobre todo en Big Data, porque nos permite guardar datos complejos. Si, por ejemplo, usásemos CSV y quisiésemos guardar facturas, es muy complicado en un solo registro tener los datos del cliente, las líneas de la factura, los productos y líneas de descuento, formas de pago, etcétera, etcétera. En cambio, en JSON es muy fácil de moldear. Por eso se utiliza tanto este formato en Big Data. Pues lo que vamos a hacer hoy va a ser, este mismo script lo vamos a implementar usando Bash, usando Java, usando Python y vamos a ver pros y contras de usar cada solución. Mirad, aquí tengo una carpeta y aquí tengo un fichero que si lo miramos, menos LH, ocupa 17 GB. En el proyecto que estoy trabajando ahora, estoy procesando 8 ficheros como este. ¿Vale? Porque no lo tienes en un solo fichero, porque si no sería muy complejo de procesar, así que lo divides en varios ficheros. Para que os hagáis una idea de la magnitud. Son casi 8 millones de registros super largos. Entonces, lo primero que vamos a hacer va a ser hacer una porción más pequeña de este fichero. ¿Vale? Porque esto al final es un vídeo de YouTube y todo va a funcionar a la primera o casi, pero lo habitual es que cuando estás haciendo un script para procesar algo, pues te falle varias veces. Y si cada prueba te lleva 20 minutos, pues vas a tardar mucho más a tener optimizado el script que si cada prueba te dura un minuto. Entonces, lo que haces primero es siempre procesar una parte más pequeña de los datos para ir más rápido a tener todo el proceso bien moldeado. ¿Cómo hago esto? Pues, por ejemplo, como yo me voy a coger los 1500 primeros registros. Para hacer esto hago hit menos 1500, que son los registros que quiero coger, del fichero 1.data y lo voy a meter en el fichero 1.short.data.json. Esto es Bash puro y duro. Si queréis que haga más vídeos de Bash, incluso algún curso que tengo ganas de hacerlo, por favor decímelo en los comentarios para saber si realmente hay gente interesada o no. Le doy al enter y ahora ya menos lh, ya tengo los dos ficheros, uno de 17 gigas y el otro con 153 megas, mucho más ligero de procesar. Para que os hagáis una idea, cada uno de estos registros tiene esta pinta. Si cojo uno de los registros a la azar, por ejemplo, el 15, pues hago hit menos 15, tail menos 1, del fichero 1.short.data. Es un fichero super largo, si le quiero dar formato jq, todo esto es lo que tiene nuestro registro. Registros con cantidad de información. Yo lo que quiero es convertir cada uno de estos registros en un fichero CSV con solamente tres campos. Porque es lo único que me interesa para cargarlo en la base de datos que me va a funcionar en mi aplicación. Entonces iremos primero al primer script, que es el script que tenemos en Bash. Este script tiene esta pinta, bash barra to CSV. Este es mi script en Bash, que de hecho es el mismo que tenemos en el artículo. Básicamente este script tiene dos parámetros. El parámetro 1, que es el fichero de entrada y el parámetro 2, que es el fichero de salida. Y yo, si quiero generarme o transformar este CSV, este JSON a CSV, lo que hago es bash barra to CSV. Y le paso el fichero de entrada, que es 1.short.taltaltalt, y le voy a llamar a la salida 1.bash.csv. Le doy al enter y perfecto. Ha tardado un montón. Lo que pasa es que he cortado para no estar aquí como 40 segundos. Hago ls-l y aquí tengo mi fichero bash, que si, para que os hagáis una idea, los tres primeros registros de 1.bash tienen esta pinta. Como veis, mucho más reducido. Pero hemos dicho que queríamos comparar, porque hasta ahora hemos visto que el script bash es práctico, porque es un fichero que le das permisos de ejecución y lo ejecutas. Es lo que tenemos aquí. Pero vamos a ver cómo de rápido es. Para esto vamos a usar el comando time. Time es un comando que tenemos en bash también, que cuando le pasas o cuando ejecutas un comando, te dice lo que ha tardado en ejecutarse. Entonces, lo único que voy a hacer va a ser, esto es mi script en bash, le pongo time. Perfecto. Y ya fijaros, me da el tiempo real. Ha tardado 26 segundos en ejecutarse, de los cuales en código de usuario ha estado 23 segundos y en código del core del kernel ha estado 3 segundos. A ver, la métrica que nos interesa es el tiempo real, que es lo que tarda desde que lo ejecutas hasta que se para el proceso, 26 segundos. Eso está bien, pero claro, en Big Data una diferencia o esto para procesar solamente 1500 registros, teniendo en cuenta que tenemos con más de 7 millones, casi 8 millones, pues puede ser un auténtico desastre. Quizás necesitamos una alternativa mejor. Y si probamos con Python, Python es un lenguaje que se utiliza muchísimo en ciencia de datos. De hecho, no os podéis dedicar a esto sin saber Python, es como ser cocinero y no saber usar una olla. Es un lenguaje interpretado, eso quiere decir que solamente que tengamos Python instalado en la máquina, escribimos un fichero y lo ejecutamos, casi es como un script bash. Y vamos a ver qué pinta tiene este script. En este caso tenemos el script que está en Python2CSU.py. Esta es la pinta que tiene nuestro script. De hecho, fijaros, importamos tres paquetes, el paquete CSV para exportar a CSV, el paquete JSON para leer datos en JSON y el SIS que lo vamos a usar para leer los parámetros que le pasamos por línea de comandos. Básicamente lo que hacemos es leer el fichero de entrada que es el que le pasamos en el primer parámetro, declaramos también el fichero de salida que es el que leemos en el segundo parámetro y en el fichero de salida empezamos grabando la cabecera del fichero y luego hacemos todo el recorrido. En Python no tenemos ni puntos y comas, ni corchetes, no es como JavaScript o Java, sino que es tabulado. Él entiende que la tabulación, todo lo que va tabulado es lo que va dentro del for. Y básicamente lo que hace es leer la línea, la traspasa a JSON y en la línea que queremos insertar le metemos los datos. Cuando ha terminado guardamos la línea y cuando ha hecho todas las líneas cerramos el fichero. Como veis Python es un lenguaje diseñado para que sea fácil de leer. Vamos a ver ahora qué tal se comporta. Pues aquí hacemos Python, de hecho voy a coger el mismo o el otro que tenía. Pongo el time directamente para que me de los datos de los tiempos que ha usado. En este caso voy a usar python.to.csv.py y voy a procesar el fichero uno short data JSON y la salida la voy a llamar uno python.to.csv. Le doy al enter y perdonad, hemos dicho que era interpretado, con lo cual tengo que poner Python y que me interprete el script. Le doy al enter y esto fijaros ha tardado tan solo dos segundos frente a los 26. Estamos hablando de que ha tardado menos de un 10% del tiempo. Aquí cuando tenemos casi 8 millones de registros se nota mucho. Y si hacemos el ls-lh vemos que tanto el de bash como el de python ocupan 50k solamente y la información es la misma. Pongo hit menos cinco registros del 1.py. Y aquí tenemos los datos. No habría problema. Perfecto. Ahora vamos a hacer la prueba con Java. Java es el lenguaje corporativo por excelencia. Ahora en las empresas están empezando a entrar otros lenguajes porque hoy en día ya no se hace todo con un solo lenguaje. Pero cuando todo se hacía con un solo lenguaje y tenías una aplicación monolítica donde todo estaba escrito en el mismo lenguaje, se ejecutaba en el mismo CREA, todo, todo, todo. Los reyes de la informática corporativa eran Java y C Sharp por el otro lado. En este sentido crear scripts en Java es bastante más complejo. Primero porque lo tienes que compilar. Y bueno, esto ya te obliga a que el proceso de pruebas y demás sea bastante más largo. Vamos a ver en Java. Además tenemos que instalar las dependencias y luego ver el script. En este sentido podemos hacer cat java build punto properties para tener las dependencias. En este caso utilizaremos JSON y OpenCSUV para procesar los datos y nuestro script queda de la manera siguiente. En este sentido tenemos el paquete en el que está esto, los imports de todo lo que vamos a usar y todo lo que es el proceso. Hacemos el escáner del fichero que le pasamos como primer parámetro, declaramos el fichero de salida también, guardamos la cabecera y tenemos el bucle donde procesa por cada línea. Pues esto, lee la línea, lo convierte a JSON y guardamos todas las columnas y guardamos la fila y cuando acaba hacemos el flush para que termine de guardar todos los datos. Java, si Python está diseñado para ser ágil y legible, o sea Java está diseñado para no tener errores como esto se tiene que compilar, pues si hay problemas de sintaxis y demás nos daremos cuenta no durante la ejecución sino en el momento de compilar, etcétera, etcétera, etcétera. Son lenguajes totalmente distintos. Y vamos a hacer exactamente lo mismo. En este caso pongo el time y para ejecutar esto, yo ya lo tengo compilado, para ir más rápido en este vídeo, hacemos java menos jar y le ponemos el jar donde está compilado esto que os acabo de enseñar, está en JSON.jar, primer parámetro el 1.short.data.json y lo vamos a meter en el fichero para continuar con el mismo formato 1.java.csv. Le doy al enter y vamos a ver cuánto tarda esto en procesar. Perfecto, mira, ha tardado 8 segundos, bastante más de lo que ha tardado Python, lo que podría parecer que Java es más lento que Python. Esto en realidad no es bien así. Ahora lo que tendríamos que hacer es hacer lo mismo, no con una porción pequeña del fichero sino con todo el fichero. Yo esto ya lo he hecho para que no se nos haga muy largo y las conclusiones están aquí. Fijaros, tenemos el script de Bash que para 1500 registros en mi prueba ha tardado 26, en la que hemos hecho antes no sé lo que ha tardado. El de Java en la prueba que he hecho antes ha tardado 7 segundos, más o menos lo que nos ha tardado ahora y el de Python 2,65 segundos, también más o menos lo que nos ha tardado ahora. Java es un 70% más rápido que Bash, brutal, o sea, Bash no es un buen lenguaje para procesar gran cantidad de datos y luego el tema está entre Java y Python. Realmente Python es más rápido y además nos permite crear scripts de una manera más cómoda, por lo que comentábamos porque es interpretado, no hay que compilar, etcétera, etcétera. Entonces el lenguaje para trabajar para este tipo de cosas es Python, sin lugar a dudas. Estoy pensando que quizás me faltaría aquí la comparativa con Go, a ver si realmente Go es mucho más rápido que Python. Entonces nos lo podríamos plantear, lo dejo para más adelante. Fijaros, a la hora de procesar gran cantidad de datos, cuando hacemos la prueba con los 17 GB, Bash ha tardado 2892 segundos, una burrada. Creo que tarda como 40 minutos, por eso no lo hago aquí delante en el vídeo. Java ha tardado 682 segundos, un 76% menos, fijaros que a más cantidad de trabajo más rápido ha sido Java. Y Python ha tardado 603 segundos, un 11% menos que Java. Fijaros en la diferencia, cuando hay pocos datos, Python es mucho más rápido, a medida que crece la cantidad de datos, Java le va ganando posiciones. Esto es por temas de cómo trabajan internamente los dos lenguajes, procesan los datos, etcétera. Incluso, seguramente yo podría haber optimizado un poquito más el script en Java. En cualquier caso, Python es ahora mismo el líder indiscutible, por las dos variables estas que os comento. Por un lado, facilidad de crear los scripts y por el otro lado, velocidad. Aunque Python fuese un poquitín más lento, el hecho de poder trabajar más ágilmente lo hace mejor candidato, atención, para crear scripts, para procesar datos grandes, ficheros grandes, incluso pequeños también. Creo que no me he dejado nada, espero que haya quedado claro cómo funciona todo esto. Mirad el artículo, porque está bien, os explica cosas como ver el número de líneas que hay en un fichero. Un fichero wc-l y el 1.short tiene 1500 líneas. Si nos miramos el de Java, csv tiene 1501, porque tiene la primera línea que es la cabecera, que les hemos puesto a todos. Y bueno, esto es todo, creo que no me he dejado nada. ¿Qué os ha parecido? Ha estado super interesante. Ahora, después de haberlo hecho, quizás me ha faltado el ejemplo con Go, a ver si realmente este lenguaje nos aporta muchísima más velocidad. Si me lo pedís mucho en los comentarios, os haré no un vídeo, sino un artículo con el mismo script que hemos probado en Python y en Java, usado en Go, a ver si nos aporta grandes beneficios o simplemente no aporta nada nuevo. Y tanto por tanto, pues ya está. Lo de siempre, suscribiros si no lo habéis hecho ya. Manita arriba, por favor, muchos comentarios y compartid, porque es la manera como estamos creciendo muchísimo. Estos vídeos ayudan a muchísima gente, la gente no para de decírmelo, que ha aprendido mucho, que ha encontrado trabajo, que tal, tal, tal, gracias a estos vídeos. Y es una pena que no estemos creciendo más, porque el estilo de YouTube es otro y nos penaliza muchísimo. Pero si le metemos ganas, si compartimos mucho, creceremos igualmente y demostraremos que los vídeos bien hechos y con calidad y con I más D detrás y tal, tal, tal, también crecen. Así que, venga, muchísimas gracias por compartir y hasta la próxima.
DOC0070|NLP e IA|tenemos a chat gpt, el chat de Bing y ahora también tenemos a Bard que es el modelo de inteligencia artificial desarrollado por google soy Elena, chica geek y en este vídeo te cuento todo lo que necesitas saber sobre él Bard es el modelo de inteligencia artificial conversacional creado por google en realidad lo lanzaron hace ya unos cuantos meses pero ahora es cuando más nos interesa porque acaban de lanzar su versión en español así en líneas generales es parecido a otros sistemas de inteligencia artificial que ya hemos visto aquí en otros vídeos en mi canal de youtube pero tiene ciertas características y peculiaridades así que te lo voy a enseñar con todo detalle para usar Bard necesitas una cuenta de google que puede ser la misma que usas para gmail o para drive o para youtube y tienes que entrar en esta página que es bard.google.com fíjate que cuando entras y se carga esta web ya te da unas cuantas ideas, unas cuantas sugerencias de para que puedes utilizar Bard, pues mira para escribir un mensaje para que felicites a tu amiga o para que te de consejos para evitar la procrastinación, etcétera esta página además también te avisa de algo importante y es que Bard es experimental y puede dar respuestas inexactas o inadecuadas, esto es muy importante que lo tengas en cuenta según lo que le vayas a preguntar pues hay que tomarse la respuesta un poquito con pinzas, recuerda que es una herramienta que está en desarrollo todavía, estamos asistiendo a su creación así que pues eso hay que perdonarle cualquier pequeña inexactitud que pueda tener en su respuesta y luego además si tienes dudas sobre qué es exactamente, cómo funciona y un punto importante cómo trata tus datos, que esto es una cosa que me soléis preguntar en los comentarios de los vídeos pues aquí tienes la sección de preguntas frecuentes que si haces click se te abre una pestaña nueva del navegador y aquí te explican exactamente qué es Bard, cómo funciona y sobre todo aquí en la parte de abajo pues cuáles son los términos del servicio, qué datos se recogen, cómo se usan, etcétera, así que si te preocupa este tema de los datos aquí tienes todo muy bien explicado, muy claro para que lo tengas todo claro así que nada cerramos esta pestaña, estamos aquí en la página principal de Bard y es tan fácil como hacer click en el botón de iniciar sesión aquí introduces tus datos de Google, tu dirección y tu contraseña, aceptas los términos de uso y ya puedes empezar a utilizar Bard y nada cuando inicias sesión entras en esta página que es la página principal de Bard, una vez que tienes tu usuario y contraseña si te das cuenta es muy parecida a la página de chatgpt, tienes aquí en el lado de la izquierda una barra lateral que por cierto se puede ocultar haciendo click en estas tres rayitas y aquí se va guardando tu historial de conversaciones, de preguntas que le hagas que además lo puedes renombrar si no me equivoco, si fíjate que al lado de cada título lo que haces tomar la primera pregunta que le haces, que en mi caso fue pues qué hora es en Tokio pero si tocas en estos tres puntitos tienes un menú que te permite fijar esa conversación, ponerle el nombre que tú quieras o directamente eliminarla si no la quieres conservar, como digo aquí se van guardando todas todas las conversaciones que tienes con Bard y cuando marcas cualquiera de ellas se te carga en la parte principal, aquí en el centro de la página es donde tienes la interfaz con la que interactúas con Bard, fíjate que aquí de nuevo nos vuelve a recordar que tiene limitaciones y que no siempre va a acertar para que lo tengas muy muy presente y muy en cuenta que no es un sistema perfecto y no hay que creerse a pies juntillas todo lo que te diga y nada pues te da una serie de sugerencias por si no sabes muy bien qué preguntarle por si te quedas en blanco en el momento que puedes preguntarle algo y si no pues nada aquí tienes el campo de texto en la parte de abajo pues eso para preguntarle lo que tú quieras desde lo más sencillo hasta lo más complicado no sé por ejemplo vamos a decirle cuánto dura un vuelo desde Madrid a Toronto, le preguntamos le damos a enter con esto ya ves que fíjate automáticamente se crea una conversación nueva que nos ha aparecido en la barra lateral de la izquierda y aquí tenemos la respuesta nos da una duración aproximada nos pone algunos ejemplos y fíjate que aquí abajo tenemos incluso enlaces podemos compartir la respuesta podemos buscarla directamente en google punto interesante tenemos otras versiones de la respuesta aquí arriba pues pueden ser más formales más informales de con estilos diferentes etcétera esto ya según pues por ejemplo donde tú quieras usar la respuesta y luego también la puedes calificar que esto también ayuda mucho al propio desarrollo de la herramienta a mi esta respuesta me ha parecido que está bien muy completa con ejemplos y demás así que mira le puedo poner un like incluso como los vídeos de youtube y si crees que no es buena que está que es incorrecta que tiene algún fallo lo que sea pues también lo puedes lo puedes comentar le puedes aquí poner decir que es una mala respuesta o incluso aquí abajo en los tres puntitos puedes decir que hay un problema legal con la respuesta imagínate que es alguna cosa de leyes tú sabes de ese tema y descubres que hay un error pues eso es importante reportarlo que más le puedes preguntar pues no sé cualquier cosa que se te ocurra lo clásico que todos le preguntamos a la inteligencia artificial que nos haga recetas con cosas vamos a decirle mira tengo en casa tengo pan tomates que más tengo que más tengo espera que piense a huevos también tengo y ensalada bueno lechuga y le voy a decir que me de unas cuantas ideas dame tres ideas y de ideas no de cenas rápidas por ejemplo no tiene que cocinar mucho ponemos cenas rápidas entre esperamos a que nos de la respuesta y yo creo que tratará un poquito más que antes porque tiene que pensar pero fíjate aquí ya tenemos sándwich tortilla ensalada pues un poco de todo también nos dan las cuantas ideas para aprovechar una cosa muy interesante también que hace bar son resúmenes le puedes copiar y pegar el texto por ejemplo de un artículo de la wikipedia y que te lo resuma para que sea más fácil de leer o que te haga un resumen de una película o de un libro mira por ejemplo un resumen el quijote el quijote en tres líneas a ver qué hace porque el quijote es bastante extenso a ver qué tal le queda resumen quijote en tres líneas mira tres líneas el personaje que se cree que es un caballo andante pues las aventuras tal y cual y jolín pues nos acaba de fastidiar el final del libro o sea que si no lo has leído lo siento pero aquí hay un pequeño spoiler pero bueno la verdad es que sí que está bastante bien hecho el resumen aquí tenemos otras versiones a ver cómo sería esta otra mira también lo resume en esta mejor porque mira dice que es una crítica a la sociedad española tal y cual y está también está es un poco un poquito más larga vale pues mira así que está muy bien porque hay diferentes versiones de la respuesta según donde tú la quieras utilizar y la verdad es que está bastante completo y nada como ves muy sencillito muy fácil de usar este es bar y puedes usarlo para un montón de cosas diferentes aparte de esto que te he enseñado con bar también tienes posibilidad de hacer otro tipo de actividades por ejemplo más tirando al lado artístico o creativo no sé le puedes pedir que te haga un poema o una idea de felicitación para una tarjeta de cumpleaños por ejemplo y también tiene una parte lúdica una parte divertida puedes jugar con bar a ciertos juegos lo que pasa que en estos dos puntos tanto en la parte artístico creativa como en la parte de juegos yo creo que flaquea un poco por ejemplo en los poemas estos días que lo he estado probando y me he pedido que me hiciera algunos poemas o que me diera ideas para una tarjeta de felicitación para una amiga y tal la verdad es que el resultado dejaba bastante que desear yo creo que yo sería capaz de hacer un poema mejor que bar y luego en la parte de juegos también hacía cosas muy raras por ejemplo tiene un juego de adivinar canciones que yo tengo la impresión de que siempre me decía que había acertado cuando por las pistas que me daba por la canción y la respuesta que yo le daba era evidente que no era esa canción que no era la correcta o también hay otro juego de adivinar películas que te va dando una serie de pistas y tú tienes que adivinar qué película es y una de las pistas que me daba era el propio título de la película con lo cual si ya me das el título pues no hay ningún juego en realidad otra cosa también que me ha pasado a veces con bar es que de repente en el medio de la conversación sin yo decirle nada y sin yo cambiar la configuración y nada me cambiaba el idioma y pasaba de español a inglés así por las buenas así que no sé hay esos puntos que falla un poquito todavía obviamente está en desarrollo como ya hemos comentado a lo largo del vídeo pero son cosas que por ejemplo en chat gpt no las veo y en cambio bar pues ahí sigue fallando la verdad es que así en líneas generales a mí me gusta más chat gpt que bar creo que chat gpt está más desarrollado afina más en las respuestas acierta más y si aunque bar tiene cosas chulas como por ejemplo lo de mencionar la fuente de sitios cuando te da respuestas o incluso darte diferentes versiones de una misma respuesta para que puedas elegir la que mejor se adapta a tus necesidades yo creo que chat gpt de momento tiene la delantera también es cierto que lo llevo usando más tiempo que bar bar solo lo he probado dos o tres días y bueno simplemente habrá que estar atentos porque desde luego tanto la una como la otra son herramientas como te digo que aún están en desarrollo aún se están creando por decirlo así y claro todavía tienen muchos puntos que mejorar las dos igualmente a mí me resulta súper curioso probarlas y te animo a que lo hagas a que te crees una cuenta y las pruebes y las pongas a prueba y les hagas preguntas complicadas difíciles a ver cómo cómo responde cada una de ellas porque estoy segura de que esta tecnología se va a desarrollar muchísimo y en los próximos años nos va a dar mucho que hablar y si no se te ocurre nada con que probarlas que esto a veces pasa no que tienes ahí la ventana del sistema de inteligencia artificial y no sabes qué preguntarle no te preocupes porque aquí te dejó un vídeo con unas cuantas ideas para que pongas a prueba ya sea a chat gpt o a bar y unos cuantos ejemplos de uso muy interesantes para que les pueda sacar todo el provecho como siempre muchas gracias por tu apoyo al canal y por ver este vídeo y nos vemos en el siguiente
DOC0071|NLP e IA|muchos habéis pedido un vídeo sobre este tema así que por fin lo tenéis aquí soy Elena chica geek y hoy vamos a hablar del famoso chat gpt bien lo primero de todo para poder entender qué es chat gpt y cómo funciona hay que explicar una pequeña diferencia tenemos por un lado chat gpt que es una interfaz diseñada en forma de chat que es lo que tú ves en tu dispositivo donde lo estés utilizando ya sea tu ordenador tu móvil etc y por otro lado tenemos gpt que es el modelo del lenguaje basado en inteligencia artificial y que es un poco el motor que alimenta el chat un poco el cerebro lo que le otorga todos los conocimientos y todas las funciones y posibilidades que permiten que este sistema pueda mantener una conversación contigo a través de esa interfaz diseñada en forma de chat este modelo de lenguaje ha sido creado por la empresa open eye cuenta con miles de millones de parámetros diferentes y además lo han entrenado con una cantidad enorme de datos y con todo esto lo que puede es pues darte respuesta a cualquier pregunta o consulta que le hagas bueno responder cualquier tipo de conversación y en general tener un diálogo contigo muy natural de forma que parece que realmente estás hablando con una persona de hecho eso es lo que más llama la atención cuando lo utilizas y también por supuesto la capacidad de respuesta que tiene que vamos que es capaz de contarte cualquier cosa y también la velocidad con la que lo hace esto es un poco la explicación general de que es gpt que es chat gpt y un poco cómo funciona este sistema este lenguaje este modelo del lenguaje basado en inteligencia artificial pero ahora bien seguro que te estás preguntando cómo se utiliza o cómo lo puedes probar así que es lo que vamos a ver en el siguiente punto lo bueno de chat gpt es que lo puedes probar de forma totalmente gratis al menos de momento hoy que estoy grabando este vídeo todavía sigue siendo gratis además no tienes que instalar nada lo único que necesitarás es tener un dispositivo con conexión a internet y utilizar tu navegador web y eso sí tendrás que crearte una cuenta de usuario en su página web oficial la web es chat.openeye.com que vamos a entrar aquí directamente mira chat.openeye.com entramos esperamos a que se cargue en el navegador y aquí la tenemos como ves súper sencillita dos botones el de login para entrar si ya tienes tu cuenta creada o si la quieres crear que es lo que vamos a hacer ahora para que veas cómo se hace le damos a sign up hacemos clic en este botón y lo único que hay que proporcionar es una dirección de correo electrónico que se metería en este campo y le darías a continuar y luego en el siguiente paso te pediría que definas una contraseña para tu cuenta de chat gpt si lo prefieres también puedes utilizar tus datos de cuenta que ya la tengas de microsoft o de google si tienes cualquiera de estas dos cuentas y quieres usarla para crear tu cuenta de chat gpt pues también lo puedes hacer un punto muy importante es que también tendrás que proporcionar un número de teléfono esto porque es pues porque openeye esta empresa te enviará un código a ese número de teléfono a través de un sms de un mensaje de texto para confirmar que eres tú como medida de seguridad para prevenir pues la creación de cuentas automáticas y demás hacen esto pues eso para asegurarse asegurarse de que eres una persona humana de que eres tú y que realmente estás creando esa cuenta y nada a partir de ese momento pues ya lo tienes y mira nada más iniciar sesión por primera vez esta sería la primera página que te encontrarías la primer lo primero que vas a ver con tu cuenta de chat gpt aquí te dice que bueno que puede ser que de información que es incorrecta que es normal porque es una herramienta que está en desarrollo le damos a siguiente aquí también dice que tengas cuidado con lo que compartes porque son datos que se pueden revisar porque están trabajando todavía en el desarrollo de este sistema y te aconseja que no compartas información sensible o privada esto por supuesto es totalmente lógico y es un consejo que yo también os doy le damos a siguiente y nada pues dice que está optimizado para diálogo y que no dudes en darle tu feedback sin dar respuesta te ha parecido por ejemplo especialmente útil o bien elaborada pues le puedes hacer un dedito arriba como en los vídeos de youtube para darles feedback a los desarrolladores y hacerles saber que esa parte está muy bien hecha y nada ya lo tenemos le damos a dan y este es el famoso chat gpt aquí también aparece en una serie de columnas que te dan pues ejemplos de que le puedes preguntar para que lo puedes usar aquí te explica un poco las capacidades del sistema cosas por ejemplo que me llaman mucho la atención es que es capaz de recordar lo que has dicho en conversaciones previas y también que admiter correcciones sobre cosas que hayas ido hablando con él en diferentes diálogos y por supuesto te avisa también de sus limitaciones te puede dar información que es incorrecta y sobre todo tener en cuenta que este sistema está informado o tiene datos de el mundo y de las noticias y demás hasta 2021 si le preguntas de cosas muy recientes que han pasado hace una semana o un mes pues probablemente no la sepa por cierto también un punto muy importante aunque tú veas toda esta interfaz en inglés y todo absolutamente escrito en inglés le puedes hablar en español sin ningún problema porque habla perfectamente de hecho mira le vamos a saludar le vamos a decir hola a ver qué nos dice como veis se utiliza en el campo de abajo le das a enter y simplemente con esto ya le mandas tu primer saludo y esperamos mira ya nos está respondiendo hola en que puedo ayudarte hoy así que vamos a ver algunas de las cosillas que puedes hacer con chat gpt ahora una vez que has creado tu cuenta has iniciado sesión pues eso te encuentras delante de la página oficial de chat gpt delante de la interfaz de chat y es lo que puedes utilizar para interactuar con este sistema y a partir de aquí tienes un montón de posibilidades te lo tienes que imaginar un poco pues como si fuera siri o alexa o el asistente de google pero bastante mejor que estos asistentes y eso sí en lugar de utilizarlo con la voz tiene que ser todo a través de palabras o frases escritas en el propio para empezar puedes pedirle cosas muy sencillitas más allá de preguntas sobre no sé geografía historia política o lo que sea le puedes preguntar por ejemplo que te de recetas de cocina que te de ideas para regalo para alguna persona o sugerencias de películas o libros que puedas disfrutar a continuación o incluso ideas para un viaje por ejemplo vamos a decirle muéstrame sugerencias para un fin de semana en roma le damos a enter y a partir de aquí mira pues nos va contando pues eso cosas sobre roma y también sitios para que podemos visitar durante ese fin de semana como ves lo va generando todo poco a poco a poco por ejemplo miran el día 1 nos recomienda coliseo fueron romanos que es lo más importante y así va respondiendo a tu pregunta por cierto mientras la va respondiendo fíjate que aquí a la izquierda tenemos un listado que son todos los chas que puedes tener con chat gpt se van guardando aquí en esta lista de la izquierda una lista que por cierto puedes eliminar en cualquier momento la puedes vaciar o eliminar cada chat individualmente y de esta forma también lo tienes un poquito más controlado de momento nos va a seguir recomendando cosas yo creo que para un día va a ser demasiado pero pero bueno como ves no le falta ningún tipo de información de detalle incluso te recomienda sitios para comer cosas a todo tipo de actividades vamos que lo tiene perfectamente controlado si no quieres esperar a que se complete toda la respuesta de hecho lo vamos a parar pues le das al botón stop generating y a que a partir de aquí ya le puedes preguntar cualquier otra cosa también lo puedes utilizar para hacer todo tipo de actividades divertidas o entretenidas y esto es especialmente interesante para hacerlo con niños puedes jugar a las adivinanzas adivinar un personaje o una película por ejemplo jugar al ahorcado también para adivinar palabras o incluso juegos tipo trivial con preguntas con diferentes opciones y tú tienes que decir la que crees que es correcta también puedes jugar a las historias colaborativas el propio chat gpt te propone una historia tú tienes que continuarla por tu cuenta y él también la sigue continuando un poco todo el hilo argumental por supuesto como ves son todo actividades basadas en texto por ejemplo le podemos proponer que nos diga una hay que tengo que hacer clic en el campo dime una adivinanza a ver si nos dice una y somos capaces de averiguar que es lo que nos está proponiendo mira claro aquí tienes una adivinanza sin ser ave tiene nido sin ser río tiene cauces siendo calle no es camino siendo cauce no es de agua que es pues no tengo ni idea así que le voy a decir que mira no lo sé no lo sé y se lo voy a preguntar que es porque ahora me quedado con la intriga la verdad la respuesta es la letra a vaya complicación la verdad pero bueno como ves tiene un montón de recursos para para ser un rato divertido con él y sobre todo como decía si lo pruebas con niños y otro uso también muy interesante es para cosas creativas además lo puedes incluso aplicar al trabajo yo por ejemplo estos días lo he estado utilizando para que me ayudara a crear textos más atractivos y más creativos para poner en redes sociales por ejemplo un pie de foto de instagram o un tweet para promocionar un vídeo etcétera también por ejemplo que te de ideas para hacer vídeos para escribir artículos cosas así o incluso mira cuentos para niños historias con una serie de parámetros que tú le indiques pues mira quiero una historia de un matrimonio que se va de viaje a roma pasar un fin de semana y les pasa alguna aventura cosas así es más también le puedes proponer que te escriba poemas vamos a decirle que nos escriba un poema sobre algún tema hacemos clic en el campo de chat y escribeme a ver un poema de cuatro versos y versos sobre el ipad por ejemplo a ver qué es capaz de sacar sobre esto en la palma de mi mano descansa el futuro ipad ventana el mundo de luz y cultura trazos de arte y letras en su pantalla aquí se ha quedado un poco atascado a no conectado sin barreras el mundo a una altura bueno no está mal para estar recién creado además por una inteligencia artificial así que como ves puede hacer prácticamente de todo estas son sólo algunas de las cosas que puedes hacer con chat gpt tal cual simplemente utilizando la interfaz de chat que tienes en su web oficial pero también tiene muchísimo potencial esta herramienta combinándola con otras utilidades online con las que puedes crear todo tipo de cosas te voy a mostrar por ejemplo algunas del algunos de los proyectos que he hecho en estos días que lo he estado probando por ejemplo una de las cosas que hice fue pedirle a chat gpt que me generase una copia del juego pong en html como yo no tengo ni idea de programación pues mira se lo pedí que debo tener el chat aquí guardado si fíjate ponga ponga en html code está aquí yo se lo pedí muéstrame el código html necesario para hacer un juego de tipo pong el tío me respondió me puso aquí el código me dijo cómo lo tenía que hacer donde tenía que copiar cada cosa etcétera así que simplemente me fui a una herramienta de programación online estaré aquí yo copié y pegué el código repito sin tener ni idea de programación y fui capaz de generar esto una copia del juego pong que funciona veis por sí misma perfectamente y sin tener ni idea de programación y sin tener que haber escrito ni una sola línea de código chat gpt me ayudó a hacer esto otro que hice fue este de aquí mira le dije también este otro chat que tengo aquí guardado volvemos hacia arriba del todo le dije que me hiciera un juego de tres en raya lo mismo me generó todos los códigos necesarios es más incluso este último se cortó porque la respuesta era demasiado larga le dije que no funcionaba me dijo que no había ningún problema que aquí tenía todo el código restante y lo único que hice fue pues lo que te he enseñado copiarlo y pegarlo era una herramienta de programación y aquí tienes mi juego de tres en raya de nuevo sin tener ni idea de programación lo pude programar yo misma sin ningún problema eso sí con ayuda de chat gpt y no sólo para programación para otra cosa que lo utilicé mira volvemos aquí es para crear peticiones para crear mira aquí para dali para esa herramienta también de inteligencia artificial que generase imágenes pues le pedí que me las hiciera en inglés porque esa herramienta trabaja en inglés y simplemente escogí una que me gustaba la copié la pegué en la herramienta y se me generaron directamente todas estas imágenes con propuestas de chat gpt así que era una herramienta de inteligencia artificial que es esta de aquí creando contenido para otra herramienta de inteligencia artificial así que como ves a partir de este momento que tú puedas usar chat gpt en combinación con otras utilidades aquí las posibilidades son todavía mayores lo que está claro es que esta tecnología de chat gpt tiene un potencial increíble y no sólo por sí misma en su propia herramienta sino especialmente en conexión o en colaboración con otras herramientas y otras tecnologías de hecho ya hay empresas que lo han integrado directamente en sus productos lo tienes ya por ejemplo en el buscador bing de microsoft o en la aplicación duolingo la que sirve para aprender idiomas ya tiene también un modo especial alimentado por la inteligencia artificial de chat gpt eso sí lo que hay que tener muy claro es que es una tecnología que todavía está en desarrollo prácticamente acaba de nacer está creciendo está aprendiendo se está desarrollando y por supuesto todavía comete errores hay cosas que no sabe vamos que no es infalible de hecho algunas veces que le he preguntado cosas que no tenía ni idea por ejemplo le pregunté por una chica que hiciera vídeos de tecnología en youtube y me habló de una tal marquesa que claramente estaba confundiendo con el youtuber marquis brownlee o por ejemplo le pregunté si conocía a chica geek y no me supo decir quién soy esto no me gustó nada de hecho cada vez que se lo pregunto me da un nombre diferente de una chica diferente o sea que realmente no tienen idea de quién soy así que como ves no es infalible pero lo que sí tengo claro es que es una tecnología que va a dar que hablar mucho en el futuro y que ahora mismo estamos asistiendo un poco pues a su nacimiento y a sus primeros pasos un poco pues como lo que pasaba a finales de los 90 cuando pudimos ver cómo empezaba google y nos dimos cuenta de que todo esto de internet el correo electrónico y demás tenía mucho potencial yo me siento un poco así con chat gpt estoy viendo que esta tecnología pues eso puede dar mucho mucho que hablar puede favorecer un montón de tecnologías propias pueden mejorar un montón de procesos actuales y que bueno sólo el futuro nos dejará muy claro hasta dónde es capaz de llegar de momento seguiremos experimentando y jugando con ella a ver cómo va creciendo cómo va aprendiendo y bueno pues un poco como hacíamos en su momento cuando salió siri o alexa o el asistente de google que por cierto si usas estos asistentes en tu móvil o en tu dispositivo en tu ordenador etcétera y te preocupa que puedan grabar lo que hablas con ellos aquí te dejo un vídeo donde te enseño cómo puedes evitar que esto suceda evitar que tus asistentes graben esas conversaciones y vete a saber qué hacen con esos datos después así que échale un vistazo porque es interesante como siempre muchas gracias por ver este vídeo y por tu apoyo y nos vemos en el siguiente
DOC0072|NLP e IA|Ha salido una nueva inteligencia artificial como ChasGPT llamada Cloud2 y esta es la primera inteligencia artificial que es capaz de plantarle cara e incluso superarle en muchos aspectos a GPT-4. Antes de empezar ¿Qué es Cloud2? Cloud2 es una IA de Anthropic, una empresa con sede en San Francisco que consiguió una financiación de más de 1500 millones de dólares. Pero bueno, no vamos a entrar en todos estos detalles, sino que vamos a pasar a lo que de verdad nos importa, a las 10 cosas que nadie habla de Cloud2 y que te interesan saber. Y esto para que entiendas que es mejor para ti, si GPT-4 o Cloud2. Vamos a empezar con el primer punto que para mi es uno de los mejores. Cloud2, además de tener todas las increíbles funcionalidades que te voy a enseñar en este vídeo, es 100% gratuita. A diferencia de los 20 dólares mensuales que cuesta ChasGPT en su versión Plus para acceder al GPT-4, Cloud2 es 100% gratuita con todas sus funcionalidades. Y ojo, no digo que no valga la pena pagar los 20 dólares al mes que vale GPT-4, pero es una cantidad que mucha gente no puede permitirse ya que realmente son 240 dólares al año. Pasamos al segundo punto que es que al entrar nos encontraremos con esta barra de aquí igual que en ChasGPT, pero con una diferencia. Este clip nos permitirá darle archivos ya sea de Excel, de PDF, de .txt y podrá interactuar Cloud con estos archivos. Y no solo podrá actuar con estos archivos, sino que es que además, Cloud tiene la capacidad de leer hasta 75.000 palabras, que es el equivalente a 100.000 tokens. Una completa locura, puesto que GPT-4 lo máximo que puede leer son 32.000 tokens, que equivalen a unas 24.000 palabras y ni siquiera en la versión que tenemos acceso tú y yo, en una versión con lista de espera que nadie tiene acceso prácticamente, pero la versión que tenemos acceso tú y yo de ChasGPT no puede leer ni 2.500 palabras. ChasGPT puede leer 75.000 para que os hagáis una idea de la locura que acaba de salir. Por ejemplo, podríamos ponerle, hola, y le ponemos el Principito, todo el libro entero, que para que lo veáis por dentro es este de aquí con sus 111 páginas y todo este texto que para nada es poco. Y aquí lo tendríamos un mega de archivo y le podríamos poner este prompt, por ejemplo, actúa como el escritor del Principito, hazme un resumen del libro en 10 puntos clave y dame las tres lecciones más importantes del libro. Le damos a enviar y ahora mismo tiene en un mismo prompt todo el libro del Principito junto a la orden. Y aquí lo tenemos, los 10 puntos clave del Principito, empezando por el principio del libro y acabando con que el piloto concluye TC y las tres lecciones más importantes que deja el libro. Lo esencial es invisible a los ojos, solo se ve en el corazón, hay que tomarse el tiempo para domesticar y crear vínculos verdaderos y debemos cuidar de lo que hemos domesticado y asumir esa responsabilidad. Vamos, que con este prompt larguísimo lo ha hecho perfecto. Y me podréis decir, pero si utilizan los plugins de GPT-4 también puedes darle documentos o con Chat PDF puedes dar hasta 50 hojas. Y sí, es cierto que puedes dar con Chat PDF hasta 50 hojas o que con los plugins de Chat GPT podrías dar también documentos, pero es que esta inteligencia artificial lo tiene No tienes que utilizar cosas externas a la propia inteligencia artificial, sino que ya está en la IA. Y recordar que para acceder a los plugins hay que pagar los 240 dólares anuales de Chat GPT+, mientras que esto es 100% gratuito. Para que veáis la comparación entre cuántas palabras se le puede dar a uno y a otro, aquí como os he dicho son 75.000, podemos ver que le hemos dado hasta un libro incluso. Y aquí tenemos 500 palabras, este texto son exactamente las 500 palabras, vamos a copiarlo y se lo vamos a dar a Chat GPT. Se lo ponemos una, dos, tres, cuatro y cinco veces, quedaría en un total de 2500 palabras y al dar a enviar nos diría que el mensaje que hemos enviado es demasiado largo. Por lo tanto Chat GPT no estaría aceptando ni 2500 palabras. Pasamos al punto número 3 y es que no sólo podemos darle 75.000 palabras, sino que también podemos darle hasta 5 archivos a la vez. Por lo que imagina que en el trabajo o en la universidad te han pedido que compares 5 documentos, pues podrías ponerlos todos en el mismo prompt. Por ejemplo podría poner, actúa como un experto en inteligencia artificial, dame los 5 puntos clave de cada uno de estos dos documentos sobre IA que ha juntado y adjunto dos documentos sobre investigaciones de IA. Le vamos a enviar... Y lo habría hecho perfecto. Aquí tenemos los 5 puntos clave de la investigación I y aquí los 5 puntos clave de la II. Además podemos ver que todos tienen sentido y que todos corresponden a los documentos. El punto número 4 es sobre seguir una conversación. Por ejemplo si ahora queremos seguir esta conversación de las investigaciones podríamos hacerlo hasta 75.000 palabras de contexto. Es decir que si ahora queremos continuar con esta conversación Cloud II se acordaría de en total 75.000 palabras, las anteriores más las nuevas. El punto número 5 es que Cloud II es mejor programando que GPT-4, al menos según la puntuación que han obtenido en la prueba Human Evolved. En esta prueba de programación GPT-4 obtuvo una puntuación de 67% mientras que Cloud II obtuvo una puntuación del 71,2%. Por lo tanto si quieres programar yo lo que te recomiendo es que utilices Cloud II, no solo porque ha obtenido una puntuación mucho más alta sino que tú mismo le vas a poder dar mucho más código y te da poder escribir mucho más código. Simplemente imagina que tienes un código de 50.000 palabras podrías dárselo a Cloud II mientras que a GPT-4 no le podrías dar ni un código de 2.500. Pasando al punto 6 Cloud II es más rápido escribiendo que chat gbt. Aquí podemos ver una comparativa de Cloud II con GPT-4. Esta página web es nad.dev y podemos comparar dosías. En este caso estamos comparando aquí Cloud II y aquí GPT-4. Mientras que Cloud II obtiene unos 100 caracteres por segunda aproximadamente, GPT-4 podemos ver que se queda en los 75, 70 más o menos. Así que aquí tendríamos otro punto a favor para Cloud. El punto número 7 son los datos actualizados que tiene Cloud II. Mientras que GPT-4 se limita a datos de 2021, no tienen ni idea de lo que ha pasado en 2022 ni en 2023, Cloud II sí que tiene conocimiento mucho más actualizado, tan actualizado como que llega hasta el 2023. Por lo tanto que si le preguntas cosas del 2022 o principios del 2023 te vas a ver contestar a la perfección. Aquí podemos ver el ejemplo claro, si le pedimos eventos memorables del 2022 nos dirá que sabe que hay una guerra en Ucrania, que sabe que se jugó el mundial de Qatar, las elecciones de Brasil, la muerte de la reina Isabel, el lanzamiento del telescopio James Webb o la compra de Twitter por Elon Musk. Vamos, que Cloud II está al día. A diferencia de chat gbt que si le preguntamos que si Elon Musk ha comprado Twitter nos dirá que no, que su conocimiento es hasta el 2021 y que Elon Musk no ha comprado Twitter. Pasamos al punto número 8 que aquí el ganador sí que es chat gbt. Si venimos aquí a ajustes, data controls y desactivamos esto de aquí, todos los datos que le vamos a dar a chat gbt no los va a utilizar para entrenar su modelo de lenguaje. Esta opción no la podemos ver en ninguna parte por Cloud ya que no la tienen disponible. Le digamos lo que digamos va a utilizarlo para entrenar su modelo de lenguaje. ¿Cuál es el punto negativo de esto? Pues que los datos que nosotros le vayamos a dar a Cloud II luego puede dárselos a otra persona a modo de respuesta. A diferencia de chat gbt cuando tenemos este modo activado, los datos que le damos se quedan entre él y nosotros. Al final si son conversaciones normales no pasa nada pero a la que le queremos dar unos datos un poco más sensibles sí que puede ser mejor dárselos a chat gbt. Pasamos al punto número 9 y también es un aspecto negativo de Cloud. Si os fijáis aquí estoy utilizando una VPN en América ¿Por qué lo estoy haciendo? Porque Cloud solo está disponible en el Reino Unido y en Estados Unidos. Por lo tanto yo que estoy en España si no utilizase la VPN no podría entrar a Cloud II pero bueno no os preocupéis porque ya habéis visto cómo he podido entrar y chat gbt con él utilizando la VPN. Esta VPN que estoy utilizando está integrada en el navegador Opera que es 100% gratuita y el navegador también es 100% gratuito. Simplemente tendremos que descargar el navegador y al darle aquí a VPN la encendemos y tendremos que poner a América ni Asia ni Europa. Una vez hecho eso al ir a cloud.ai ya podréis empezar a chatear con Cloud. Y el punto número 10 es que las plantillas de prompts del kit gbt sirven tanto como para chat gbt como para Cloud. Por ejemplo si vamos a crear el prompt perfecto desde cero vamos a la plantilla básica la gratuita que la tenéis en el primer enlace de la descripción. La copiamos y vamos a Cloud II aquí simplemente tendríamos que rellenar estos espacios y tendríamos un prompt optimizado 100% para que nos de los mejores resultados. Y como os digo esta plantilla sirve tanto como para Cloud II como para chat gbt. Si queréis acceder a ella está la versión gratis en la descripción que es un regalo que os hago y también está la versión de pago en el segundo enlace de la descripción. Espero que las disfrutéis muchísimo y vamos con la conclusión. ¿Cuál es mejor gbt4 o Cloud II? Obviamente no hay una respuesta de gbt4 es mejor o Cloud II es mejor sino que hay aspectos en los que cada uno es mejor que la otra. Por ejemplo si quieres tener conversaciones largas, quieres dar documentos largos o poner prompts muy largos evidentemente que Cloud II con las 75 mil palabras es mucho mejor que gbt4 con las menos de 2500. También han habido pruebas que indican que Cloud II programa mejor y tiene mayor capacidad para resolver problemas matemáticos. Y a nivel de razonar los dos con buenos prompts son muy buenos por lo tanto las dos son muy buenas opciones. El ver que Inteligencia Artificial razona mejor va a ser a base de tiempo y a base de probarlas muy detalladamente cada una. Por ahora como digo yo lo que voy a hacer y lo que os recomiendo es utilizar las dos Cloud II para textos muy largos y gbt4 para razonar aunque Cloud II también es buena razonando. Si quieres descubrir cómo van avanzando estas dos Inteligencias Artificiales y cuál acaba ganando la carrera y se posiciona como número uno, suscríbete al canal y activa la campanita porque os puedo asegurar que en cuanto haya una que supere la otra os lo voy a traer. Espero que os haya encantado el vídeo y adiós.
DOC0073|NLP e IA|La inteligencia artificial cada vez avanza más y más rápido. Antes alucinábamos con crear una imagen con un solo prompt y ahora podemos generar vídeos completos además de manera gratis e ilimitada. La comunidad con esto ya ha hecho cosas increíbles, han generado vídeos como animando memes, trailers de películas o muchos otros que son increíbles y que veremos a largo de este vídeo. Además también veremos las diferencias entre los dos generadores de imágenes más potentes hasta el momento como son Gen 2 de Runway y Pickalups. Gen 2 es como Mid Journey, es limitado y es de pago pero la calidad es muy pero que muy buena. Y luego tenemos a Pickalups que es como Blue Willow, es gratis y es ilimitado pero sacrificamos un poco de calidad. En este vídeo nos vamos a centrar en Pickalups ya que es gratis e ilimitado y vamos a empezar viendo todas las cosas increíbles que ha creado la comunidad. Por ejemplo aquí tenemos el corto de película que ha generado Abel Art. Básicamente es una película generada con Pickalups y con Gen 2 que como dice aquí es la historia de un hombre que una mañana levanta con la nevera vacía pero con la mente llena de memorias. Y durante la peli es todo el rato este hombre. Quiero que os fijéis en la calidad de todas estas imágenes. Abel Art lo que ha hecho ha sido primero generar todas estas imágenes en Mid Journey y luego pasarlas a ya sea Gen 2 o Pickalups. Sospecho que la mayoría ha sido creado con Gen 2 porque no vemos aquí la marca de agua y la calidad es muy buena. Después de ver ese corto de película generado con inteligencia artificial ahora vamos a ver cómo serán los comerciales de comida del futuro. Por ejemplo este es un comercial hecho con Pickalups, aquí sí que podemos ver la marca de agua aquí abajo y fijaros en la calidad de las imágenes. Las imágenes aquí también han sido generadas con Mid Journey y han sido pasadas a Pickalups para que se animen y para que le dé el movimiento que genera el vídeo. Este ha sido creado por Doorbrothers igual que este de aquí que sinceramente al verlo me quedé flipando y es de mis favoritos, ahora entendréis por qué. Aquí vemos un cuadro de Van Gogh el cual se cierra y comienza la magia. Esto también son imágenes que seguramente han sido generadas con Mid Journey, Leonardo o cualquier generador de imágenes y han sido pasadas a Pickalups para animarlas y mirar lo bien que está conseguido todo el estilo de Van Gogh, lo bien que se ve, sinceramente me parece una locura. Y todo esto ¿para qué ha sido utilizado? Pues como podemos ver aquí en esta botella para un comercial de Coca Cola. No es nada oficial evidentemente, lo ha hecho la comunidad, pero fijaros en que podrían hacerlo perfectamente. La comunidad de Coca Cola en el pasado ya hizo un comercial mezclando un poco de inteligencia artificial, no así evidentemente, pero esto nos da indicios de lo que veremos en un futuro en los anuncios. Anuncios completamente hechos con inteligencia artificial como estos de aquí pero con 100 veces más calidad. O también podemos ver a Caris AI animando el nuevo logo de Twitter, es decir de X con fuego. Este también ha sido con Pickalups y es que las posibilidades son infinitas, cualquier cosa que puedas imaginar la puedes hacer. Eso que estamos en el principio de los textos a vídeo, recordar cuando se crearon las primeras imágenes generadas con inteligencia artificial, aquello era algo parecido a esto, algo que está muy bien, es muy increíble a primera vista, pero que cuando pasa el tiempo y ves la diferencia te das cuenta de que aquello no tenía nada que ver con lo que hay ahora. También podemos ver este vídeo de StealthyTT que básicamente une un montón de inteligencias artificiales, que esto es algo que estamos viendo muchísimo también. Une Pickalups para crear la animación, es decir el vídeo, StableDiffusion para acoger la imagen original, es decir ha creado la imagen con StableDiffusion y la ha enviado a Pickalups, lo ha editado con Photoshop, supongo que con Photoshop AI, poniendo por ejemplo aquí el pájaro o dando los pequeños detalles, y ha creado la voz con 11labs y ha cogido sonidos de Pixabay. Todo esto lo ha unido en Filmora y este es el resultado. Yo la verdad es que lo encuentro una locura, me parece increíble que ya estamos en este punto, no por los usos que pueda tener StoneSeed que también los puede tener, sino por lo que puede llegar a convertirse una vez se vaya mejorando. Aquí tenemos a Amar Reishi enseñándonos cómo a generar una animación que no es tan fácil, pero que es algo que se puede hacer con el tiempo, y es algo que se puede hacer con el tiempo, y es algo que se puede hacer con el tiempo, y es algo que se puede hacer con el tiempo, Aquí tenemos a Amar Reishi enseñándonos cómo ha generado sus imágenes en Mid Journey, que he promo utilizado y las ha llevado a Pickalups para animarlas. Otra imagen que ha animado con Pickalups, pero la comunidad no solo se limita a coger imágenes de Mid Journey y animarlas en Pickalups, también por ejemplo animan memes, imágenes reales, por ejemplo, Scotty Wick ha creado este hilo de Twitter, donde anima memes, por ejemplo este conocido meme, podemos ver como lo ha animado poniendo llamas en movimiento en la casa y la niña en movimiento, o este famoso meme donde el chico gira la cabeza, en definitiva podríais animar cualquier meme que se os ocurra, pero ahora que ya hemos visto todas las cosas increíbles que ha creado la comunidad, ¿cómo puedes crear tú, tus propios vídeos que se te ocurran o que te haga ilusión crear? Para ello tendríamos que ir a la web pica.art y bajando daremos a adjoin beta, esto nos llevará a un enlace de Discord, aceptamos la invitación y aquí lo tendríamos, tendréis que ir al canal Generate One o cualquiera de los diez que hay, y para crear un vídeo tendríamos que poner barra create y aquí sería el prompt, también podemos poner la imagen asjuntándola aquí, por ejemplo podríamos poner esta imagen generada con Leonardo, y de prompt pondríamos hombre caminando hacia la tienda, le damos a enter y aquí tendríamos el vídeo que como podéis ver es exactamente lo que le hemos pedido, el hombre caminando hacia la tienda, bastante impresionante ¿verdad? Si os fijáis todos los vídeos son de tres segundos, si queremos algo más largo tendríamos que coger el último fotograma de este vídeo y volver a dárselo para generarlo o por ejemplo darle otra escena y juntarlas, sería crear más de un vídeo, estoy seguro que en un futuro implementarán la opción de generar vídeos más largos, pero por ahora todos los vídeos que se generan en pica.labs son de tres segundos, os estaréis preguntando y cómo puedo darle los mejores prompts para que me de los mejores resultados, la respuesta es muy sencilla, aquí tenéis una guía de prompts para generadores de vídeo, por ejemplo podemos poner barra GS y un número del 8 al 24, que este número es básicamente cuánto se parece el prompt al vídeo que va a salir, cuanto más alto sea el número, es decir si ponemos 24, el prompt será muy parecido a el vídeo que salga, en cambio si ponemos 8 el vídeo tendrá más creatividad y no será tan parecido al prompt que hemos puesto como al poner 24, ponemos barra nex y pondremos un texto, el texto que será, pues será lo que no queremos que haya en el vídeo, esto también está en la generación de imágenes y es básicamente el negativo prompt, luego tenemos guión AR y las dimensiones del vídeo, por ejemplo si pusiésemos guión AR 16 novenos, el vídeo saldría en 16 novenos, o también podríamos poner 916 11 45 o la dimensión que queráis, luego tenemos guión seed y el número de semilla, esto se utiliza para generar nuevos vídeos que se parezcan a otros anteriores, cogemos la semilla del vídeo anterior y la pegamos en el nuevo prompt, luego tenemos guión motion, cuánto movimiento quieres que haya en el vídeo, el número puede ir desde 0 hasta 4, entendiendo que 0 es nada de movimiento y 4 es mucho movimiento, vamos a crear un prompt con todos estos comandos para que se entienda mejor, por ejemplo podemos poner de prompt un hombre corriendo en una montaña, y empezamos con el primer comando barra GS 24, cuánto quiero que se parezca el texto al vídeo que se vaya a generar, luego vamos a poner el comando barra neg de negativo cloud de nube, es decir que no quiero que aparezcan nubes, luego vamos a poner barra AR con las dimensiones, vamos a poner por ejemplo de dimensiones 916, no estoy buscando ningún estilo de un vídeo generado anteriormente, por lo tanto no voy a poner ninguna semilla, y voy a poner barra motion 4 para que haya muchísimo movimiento, le damos a generar, y aquí tendríamos el vídeo que hemos generado con este prompt, podéis ver cómo hay muchísimo movimiento, cómo no hay nubes, cómo es un hombre corriendo en las montañas y cómo está en 916, así que si queréis crear prompts más precisos, seguir esta guía y lo conseguiréis. Ahora vamos a ir a chat gpd y vais a ver la magia de juntar muchas herramientas para crear un vídeo, por ejemplo vamos a decirle, dame 30 palabras clave relacionadas con la naturaleza, se claro y conciso y nos da 30 palabras clave, así que las copiamos y le vamos a decir, dame un guión para un vídeo corto de 200 palabras sobre lo increíble que es la naturaleza, utiliza estas palabras clave siempre y cuando tengan sentido en el guión, y le damos las 30 palabras clave, perfecto, ya nos ha dado el guión para el vídeo corto, con incluso los vídeos que podríamos crear, por ejemplo una toma aérea de una montaña y luego una vista panorámica de un río, imágenes de un océano y criaturas marinas nadando, y todo tiene relación, como por ejemplo con el vasto océano, con su majestuoso mundo marino, que son dos palabras clave de las 30 que le hemos dado, así que vamos a pedirle que nos de el texto del narrador junto, y aquí tenemos el texto con todas las palabras clave, lo vamos a copiar y vamos a ir a Ilevenlab, y aquí en segundos tenemos una narración generada hiperrealista, miradlo vosotros mismos. ¡Sencillamente impresionante! Y vamos a venir aquí a Leonardo AI para generar las imágenes gratuitamente, que luego transformaremos en los vídeos. Una vez dentro de Leonardo le daremos a AI Image Generation, y aquí pondré lo que nos dice Chadgbt, ponemos el prompt y ponemos las dimensiones, que será 16 novenos. Voy a generarlas con el modelo DreamShaper, por ejemplo, y le vamos a dar a generar. Aquí tendríamos la primera imagen, vamos a quedarnos con esta de aquí, la descargamos y vamos a hacer lo mismo con todas las escenas, así que lo hago y nos vemos enseguida. Tenemos todas las imágenes generadas con Leonardo, y que ahora vamos a animar para crear el vídeo. Para ello vamos a ir al Discord y vamos a poner The8, ponemos la primera imagen, y vamos a poner como prompt el sol iluminando los árboles en esta dimensión y con cuadro de movimiento. Aquí tenemos el vídeo y voy a enseñaros un truco para cuando queréis generar más de un vídeo, porque con la velocidad que va este chat, si no hacéis esto, seguro que perdéis los vídeos. Simplemente tendremos que dar clic derecho, crear hilo. Aquí ponemos un mensaje cualquiera y ya tendríamos un hilo creado. Ahora podríamos interactuar con el bot de Pica y crear nuevos vídeos sin tener que estar en este chat perdiendo nuestros vídeos o bombardeándonos con un montón de vídeos que crea la comunidad. Y vamos a hacer lo mismo con todas las imágenes que he generado. Ya tenemos todos los vídeos generados y descargados en el hilo, y ahora vamos a ir a CapCut para juntar el narrador con los vídeos. Podemos incluso utilizar este editor en línea con el navegador. Vamos a poner los vídeos que hemos generado, Mientras suba a CapCut quiero enseñaros los increíbles resultados que ha tenido PicaLab, como por ejemplo cómo juega con la luz en este bosque, con los animales, cómo juega con la luz de las nubes, el movimiento de los peces... La verdad es que me parece increíble lo que ha conseguido de las imágenes al convertirlas en vídeos. Si os fijáis, los vídeos duran 27 segundos y el audio 1 minuto 12, así que he cortado un poco el texto y vamos a volver a generarlo. Y gracias a CapCut ya tendríamos el vídeo completamente editado. Ahora simplemente tendríamos que darle a exportar, poner la resolución en 1080 y ponerle un nombre, por ejemplo vídeo inteligencia artificial. Esto lo dejamos tal y como está y le damos a exportar. Le damos a descargar y ya tendríamos nuestro vídeo 100% listo. Vamos a reproducir el mío. La conservación y la sostenibilidad son nuestro camino hacia la coexistencia armoniosa con la naturaleza. Y ahí estaría. Me ha encantado el resultado, pero lo que más me ha encantado ha sido el tiempo que hemos tardado en hacerlo, que es nada y menos. No hace mucho tiempo, si querías hacer un vídeo así, tendrías que pasarte horas y horas para hacer el guión, para narrarlo, para buscar los vídeos de stock, para unirlo todo en el editor y exportarlo. Y ahora como podéis ver, en minutos lo tenemos. Y para los que queréis aprender a hablar con ChatGPT, os presento el KitGPT. Es un kit de plantillas que podéis utilizar con ChatGPT, Cloud2 o cualquier similar. Y básicamente es esto de aquí. Una plantilla rellenable con la que puedes hacer cualquier prompt y que esté optimizado para Prom Engineering. Es decir, que una vez hayas rellenado estas plantillas, el prompt que hayas escrito sea mucho mejor al que se te podría ocurrir a ti de primeras. Esta es la plantilla básica, por ejemplo, si bajamos encontraríamos la plantilla de prompts avanzados y también podemos ver como al darle a cada uno de estos espacios nos llevará a cientos de opciones para rellenarlos. Por ejemplo, esos son los Actúa Como. También podemos ver 40 verbos, formatos y longitud, tonos de voz, audiencias, etc. Simplemente tendríamos que copiar esto, llevarlo a ChatGPT y pegarlo. Aquí simplemente rellenaríamos con texto estos huecos y crearíamos nuestro prompt perfecto. Pero bueno, el kit no solo tiene esto para llevarte de principiante a experto. También tiene otras cosas como, por ejemplo, la recopilación de los más de 150 mejores prompts o la recopilación de los más de 300 mejores prompts de marketing. Algunos incluso inspirados en Seth Godin, Gary V o muchas otras leyendas del marketing. También tiene una recopilación de más de 2000ías. Y bueno, no os hago más spoilers, tenéis el enlace en la descripción por si queréis acceder. Os aviso que es de pago, pero para los 10 primeros tendréis una sorpresa. Dicho esto, espero que os haya encantado el vídeo y adiós.
DOC0074|NLP e IA|El día de hoy te voy a mostrar algunas herramientas de Inteligencia Artificial tan sorprendentes que te van a volar el cerebro Bienvenido al futuro, ya es 2023 y la Inteligencia Artificial está revolucionando la forma en la que trabajamos y la forma en la que vivimos Seguramente ya has escuchado acerca del famosísimo Chat GPT, el chat bot inteligente del que todo mundo está hablando en estos momentos Desde que lo descubrí he estado sumergido dentro del mundo de la Inteligencia Artificial y he encontrado algunas herramientas extremadamente sorprendentes incluso más sorprendentes que Chat GPT Estas son algunas tecnologías que cambian el juego por completo, son capaces de hacer cosas como incrementar tu productividad aumentar tus ingresos, liberar tu potencial creativo y algunas incluso son capaces de clonarte, pero antes de comenzar quiero aclararte que no estoy patrocinado por ninguna de estas aplicaciones simplemente creo que son épicas y creo que podrán ayudarte lo único que te pido es que aplastes el botón de me gusta y si este vídeo te resulta valioso obviamente suscríbete al canal. La primera herramienta de Inteligencia Artificial que voy a mostrarte es bastante sencilla pero es muy efectiva por eso quiero mostrartela, todos sabemos que el tiempo es dinero y esa Inteligencia Artificial puede ahorrarte hasta un 40% de tu tiempo y además puede hacerte ver aún más inteligente, además es gratuita y cualquier persona puede utilizarla ya sea que tengas un trabajo o ya sea que tengas que administrar tu negocio. La primera herramienta es una extensión de Google Chrome llamada Compose AI te permite escribir más rápido especialmente cosas como coros electrónicos, es similar a Chat GPT y creo que utiliza esa misma tecnología pero con Chat GPT tienes que escribir tú el texto y con Compose AI está funcionando todo el tiempo dentro de tu navegador, así que vamos a ver cómo funciona, lo primero que haremos será entrar a la página de Compose.AI y descargar la extensión para Google Chrome, lo siguiente que haremos será dirigirnos a nuestro correo electrónico y abrir cualquier mail y supongamos que no tenemos ni tiempo ni tenemos ganas de escribir manualmente una respuesta, aquí Compose.AI te va a dar tres opciones de una respuesta por ejemplo decir que sí, decir que no o decir gracias y dependiendo cuál sea la que tú piques va a crear un correo mencionando la intención que tú tienes, un correo formal obviamente o por ejemplo podemos darle clic acá en el botón de Compose para pedirle que nos genere un correo con base en lo que queremos mencionar, entonces vamos a pedirle que responda responde que no podré el lunes y pregunta que si podemos reagendar para el viernes a la 1pm muestra mucho interés en la respuesta y de esta manera va a empezar a generarnos con Inteligencia Artificial múltiples opciones de correo electrónico de las cuales podemos elegir la que más nos guste por ejemplo está la red es con mucho el ofrecimiento de colaboración y informo que no podrá decir el lunes a nuestro encuentro quizá podamos regenerar para el viernes 1pm, espero su respuesta saludos y automáticamente nos genera una respuesta en formato correo electrónico el cual ya no tenemos que pensar desde cero y podemos simplemente elegir una de las opciones y darle en enviar otra gran cosa sobre esta herramienta es que empieza a aprender cómo escribes y empieza a utilizar tu estilo para los corros que te fabrica por ejemplo si tú utilizas la palabra saludos en lugar de gracias al final de tus correos va a empezar a aprenderlo y va a empezar a incluirlo dentro de los corros que te genere la segunda herramienta que voy a mostrarte es bastante aterradora pero está increíble porque puedes clonar tu propia voz utilizando tecnología de inteligencia artificial se llama Descript, Descript es un software de visión de audio y vídeo que es increíble por su propia cuenta pero tiene esa característica realmente genial llamada Overdub que es básicamente tecnología de clonación de voz ultra realista he estado jugando con esa función y estoy bastante impresionado para ser honestos es bastante aterrador porque es como un deep fake para tu voz entonces si cae en manos equivocadas quien sabe qué pueden hacer con esto Para usar esto deberás descargar la aplicación de Descript en tu computadora y si quieres utilizar la función de Overdub deberás proporcionarle entre 10 y 30 minutos de grabaciones de tu voz para que esa inteligencia artificial pueda analizarla y pueda después clonar tu voz la próxima herramienta que voy a mostrarte está épica y lo mejor de todo es que no solamente es una herramienta sino que es una suite entera de herramientas mágicas e inteligencia artificial con las que puedes hacer cosas sorprendentes he estado jugando con esta herramienta durante los últimos días y es sorprendente la cantidad de cosas que puedes hacer con ella se llaman runway y puedes obtener una cuenta 100% gratis para empezar a jugar con esto tienen muchísimas herramientas como generadores de retratos con inteligencia artificial generadores de animales con inteligencia artificial puedes eliminar los fondos de cualquier vídeo o imagen puedes convertir texto a imágenes pero con la que verdaderamente estoy sorprendido es con una función llamada inpainting básicamente es el siguiente nivel de vídeos de stock lo que puedes hacer con esta función es por ejemplo tomar este vídeo de un paracaidista pero supongamos que no nos gusta tener este paracaidista en el vídeo y que simplemente queremos tener el paisaje para poder disfrutarlo en vídeo lo que haremos será simplemente dibujar por encima de este paracaidista tampoco tiene que ser extremamente preciso y una vez que tengamos al paracaidista todo pintado vamos a soltar el cursor damos clic y va a eliminarlo por completo del vídeo utilizando inteligencia artificial lo cual es una locura vamos a poner vídeo desde el principio para que puedan verlo y como pueden ver es una locura eliminó por completo al paracaidista del vídeo eso es bastante difícil porque había nubes detrás no era un color sólido pero lo hace desaparecer y ahora desaparecido en todo el vídeo no he tenido que hacer nada con él más que rellenarlo pintarlo y la inteligencia artificial lo hace desaparecer y esto funciona con cualquier vídeo y con cualquier objeto que quieras quitar así que definitivamente vengan y prueben runway porque tiene muchísimas funciones que te pueden llegar a servir bastante si tienes una mala memoria como yo y literalmente olvidas lo que hiciste ayer entonces esta aplicación de inteligencia artificial es para ti hay una aplicación que literalmente recuerda todo lo que hayas visto dicho o escuchado es bastante revolucionario y es impresionante lo que esta empresa está logrando se llama rewind AI y lo promocionan como el motor de búsqueda para tu vida lo que hace es registrar todo lo que haces en tu computadora desde vídeos que ves páginas web que visitas notas que creas y cualquier conversación que tengas en discord no te preocupes puedes desactivar esto para el navegador incógnito de tu computadora todo se compila en una línea del tiempo en la que te puedes desplazar si quieres recordar lo que estabas mirando hace dos días por ejemplo además es genial porque en realidad transcribe cualquier vídeo que hayas visto lo que es realmente sorprendente por ejemplo si estás viendo este vídeo sobre inteligencia artificial en un par de semanas te acuerdas y piensas cuál era esa aplicación para borrar paracaidistas del cielo ahora puedes ir a rewind simplemente buscar borrar paracaidista y te mostrará cualquier información que hayas visto con la palabra borrar paracaidista incluido mi vídeo donde lo mencioné y te transcribirá todo lo que dije así que básicamente no necesitas tomar más notas ahora puedes simplemente buscarlo en rewind o por ejemplo si estás en una reunión de zoom en el trabajo y alguien dijo puedes conseguir esos informes y quieres encontrar de qué se trataba eso sobre los informes puedes simplemente buscar informes en rewind y te llevará de vuelta a eso o un artículo de noticias sobre criptomonedas escribiendo criptomonedas y te lleva de vuelta al artículo de noticias es algo bastante revolucionario en mi opinión en un inicio pensé que iba a consumir mucha memoria de mi computadora pero no lo hace ya que tiene una asombrosa compresión con inteligencia artificial de archivos entonces podrás tener años de tus recuerdos y no te consumiría tanto espacio el único inconveniente de esto es que solo funciona para max y en las versiones más recientes así que si no tienes mac no podrás usarlo pero hay otra alternativa llamada gd que es muy similar ya que registra todo lo que has estado viendo para que solo puedas buscarlo pero solo funciona dentro de un navegador mientras que rewind funciona en toda tu computadora ambas son excelentes opciones si tienes mala memoria como yo y quieres que la inteligencia artificial te ayude un poco antes de pasar a la siguiente permíteme hacerte una pregunta si te pidieran hablar en un escenario por 15 minutos y no tuvieras tiempo de prepararte lo harías bien la mayoría de las personas dirían que no pero con la siguiente aplicación puedes crear una presentación completa en segundos y sin ningún conocimiento previo se llama tone y es una herramienta de construcción de presentaciones o storytelling generadas por inteligencia artificial de nivel avanzado utiliza la misma tecnología que chat gpt y también detrás de dalí que es el creador de imágenes para que una presentación completa sobre cualquier cosa en segundos solo tienes que venir aquí y presionar crear y por ejemplo si quisieras crear una presentación sobre el arte generado con inteligencia artificial no tienes que escribir términos especiales solo escribe eso y comienza a generar una presentación completa desde cero es una presentación original no copiada de ningún lugar generada por inteligencia artificial desde cero es verdaderamente increíble porque desde que damos clic en crear en segundos nos crea una presentación completa nos pone un título atractivo para la audiencia nos dice acá los puntos que vamos a tocar durante la presentación aquí nos pone una introducción nos pone los usos del arte generado con inteligencia artificial las ventajas las desventajas aplicaciones para lo que podemos utilizar Arte con inteligencia artificial nos saca una conclusión y es verdaderamente increíble incluso le crea imágenes personalizadas cada 100% desde cero con inteligencia artificial obviamente no se pretende que la gente cree presentaciones sin preparación pero nos da un punto de partida excelente si por ejemplo realmente queremos hacer una presentación sobre el arte generado con inteligencia artificial podemos pedirla tome que nos la haga y simplemente cambiar las imágenes cambiar la tipografía editar un poco los textos pero ya nos genera una estructura para empezar y así no está simplemente mirando una pantalla en blanco y pensando tengo una presentación para la próxima semana y no tengo ni idea de por dónde empezar tome te da un muy buen punto de partida la siguiente herramienta se siente como algo que viene del futuro o sacado de una película de ciencia ficción porque nos permite crear personas con inteligencia artificial y podemos hacer que digan lo que nosotros queramos se trata de esta plataforma llamada d.i.d y hay varias maneras de utilizarla puedes subir una imagen y convertirla en un avatar hablante o puedes usar uno de los que ya tienen creados o incluso puedes utilizar su tecnología de inteligencia artificial para crear un avatar desde cero esto es muy útil si no quieres aparecer frente a la cámara pero quieres crear un vídeo con una persona hablando y la verdad es que es bastante realista incluso sus labios se mueven sincronizados con la voz está bastante cool así que les voy a mostrar cómo funciona primero nos vamos a su página de id.com le damos en free trial creamos una cuenta 100% gratis y una vez que ya tengas tu cuenta vas a darle clic en crear vídeo te va a aparecer esta página de aquí y vas a poder elegir entre uno de sus presentadores o avatares hablantes que ya tienen creados puedes crear uno desde cero con inteligencia artificial o puedes subir una imagen tuya y lo va a convertir en un avatar hablante una vez que elegimos nuestro avatar vamos a irnos a la parte de la derecha donde dice script vamos a escribir lo que queramos que diga por ejemplo en este vídeo hablaremos acerca de qué son las criptomonedas y cómo funcionan lo siguiente que haremos será elegir el idioma que queremos que hablemos vamos a ponerle español de méxico vamos a encontrarlo donde está aquí está vamos a elegir si queremos una voz femenina o una voz masculina y listo así de fácil le vamos a generar vídeo y va a generar un vídeo donde le va a dar vida a mi imagen y se va a mover los labios y va a empezar a hablar como si fuera yo elegimos la voz que queremos y listo simplemente le damos en generar vídeo y nos va a generar un vídeo con un avatar que habla y dice lo que nosotros queramos pero eso no es lo más sorprendente de todo como utiliza la misma tecnología que chat gpt podemos pedirle que autocomplete el texto por ejemplo le picamos acá en continuar el texto utilizado inteligencia artificial le damos clic y nos va a generar un texto acerca de lo que escribimos principalmente en mi caso como estábamos hablando de criptomonedas y cómo funcionan nos empieza a dar una explicación de qué son las criptomonedas y cómo es que funcionan es decir autocompletó el texto voy a ahorrar la mayoría para que no se tarde tanto vamos a darle generar vídeo y voy a mostrarles cuál fue el resultado es una verdadera locura está impresionante lo que esta plataforma puede hacer y quiero que vayan esta locura de que son las criptomonedas y cómo funcionan las criptomonedas son un tipo de moneda digital que combina el funcionamiento de una moneda convencional con la privacidad de una moneda digital es una verdadera locura obviamente si se ve algo robótico y no tan realista así que la mejor forma de utilizarlo para tu negocio o para crear un vídeo para youtube es mencionando que estás utilizando inteligencia artificial y usar esa excusa de por qué sonar robótico ahorita que está muy de moda la inteligencia artificial podrías crear un canal de youtube de por ejemplo noticias de inteligencia artificial nuevas aplicaciones de inteligencia artificial y que el presentador de los vídeos fuera un robot creado por inteligencia artificial en este caso a la gente le gustaría porque sería congruente con el contenido que estás creando y no se les haría raro que sonará robótico y no tan realista o si lo vas a usar para tu negocio podrás decir hola soy dan el asistente virtual de esta empresa y en este vídeo tutorial te mostraré cómo funciona nuestro software de email marketing con lo que es a que estés vendiendo la siguiente es una herramienta de inteligencia artificial que te puede hacer ganar muchísimo pero muchísimo dinero te permite cronarte y crear vídeos de venta personalizado se llama big human pero a diferencia de la herramienta anterior en big human son humanos reales que en verdad están hablando pero puedes hacer vídeos personalizados utilizando tecnología de inteligencia artificial es una locura así que voy a mostrarles cómo funciona lo primero que haremos será entrar a la página de big human vamos a crear una cuenta 100% gratis y ya que estamos en esta página de aquí puedes elegir entre clonarte a ti mismo o usar uno de sus templates obviamente mi recomendación es que utilices sus propios vídeos pero para este ejemplo y hacerlo más práctico utilizaremos uno de los templates por ejemplo vamos a elegir este de acá que es para mandar un mensaje personalizado para una persona que por ejemplo entró a nuestro e-commerce eligió productos para comprar pasó al checkout pero ya nunca los compró con esta tecnología lo que podemos hacer es mandarle un vídeo personalizado llamándolo por su propio nombre y diciéndole oye me di cuenta que dejaste tu carrito abandonado aquí te dejo un descuento del 10% para que te animes a comprar y de esta manera vamos a elevar muchísimo nuestra tasa de ventas o de conversiones vamos a ver un ejemplo le damos clic en esta le vamos a usar usar como template y creo que escuchen brevemente en audio vale No es que todavía haces parte de la compra pero procede a comprobarlo sé que el cliente se va a salir del barrio pero me gustaría que te interesa personalmente introducirte y ver si tienes alguna pregunta estamos más que nada básicamente es un vídeo invitándolo a que regresa a la página y que utilicen el descuento que le damos para recuperar su carrito abandonado y concluir su compra pero si se dan cuenta al inicio dice Hey name dice hey name porque la palabra name podemos cambiarla por cualquier nombre que queramos e incluso podemos conectarnos a nuestra base de datos para que automáticamente cree un vídeo con cada uno de los nombres por ejemplo en este caso vamos a utilizar blank spreadsheet y vamos a poner varios nombres por ejemplo Daniel vamos a poner Santiago y vamos a poner por ejemplo David y vamos a darle generar vídeos y nos va a generar un vídeo para cada uno de los nombres aquí está procesando esperamos un par de segundos y vamos a ver los vídeos y listo en segundos tenemos nuestro vídeo personalizado de venta vamos a escucharlo vamos a darle clic Hola Daniel, solo he oído que Y no es tan emocionante pero es un software de negocios impresionante y cuando te explique las posibilidades te va a volar el cerebro se llama browse AI y te permite extraer data de cualquier página web en internet no tienes que saber de programación no tienes que tener una API y no necesitas tener permiso para extraer los datos y estas son algunas de las cosas para las que puedes usar el video y no necesitas tener permiso para extraer los datos y estas son algunas de las cosas para las que puedes usar browse AI puedes usarlo para generar leads puedes conectarlo por ejemplo a linkedin y extraer leads de cualquier nicho en específico que a ti te interese puedes incluso extraer los datos directo a tu software de email marketing para mandarles un correo de ventas automatizado puedes usarlo para business intelligence conectar browse AI a las páginas web de tus competidores para extraer los datos de sus productos y sus precios almacenar los datos en un excel o donde tú quieras y así poder ver en tiempo real los precios que ellos manejan para tu ofrecer un precio más atractivo por ejemplo o puedes usarlo para finanzas inversiones conectarlo a coin market cap y extraer los datos del precio de las criptomonedas y de esta manera crear una especie de API y si por ejemplo tienes un blog acerca de criptomonedas una página web de noticias puedes colocar en tiempo real el precio en las criptomonedas o cualquier otro dato de coin market cap que quieras tener dentro de tu propia página en tiempo real hay una infinidad de posibilidades para los usos que le puedes dar a browse AI la novena y última herramienta es el famosísimo chat gpt una inteligencia artificial que le pides que haga cualquier cosa y te lo hace en segundos y aquí arriba te voy a dejar un vídeo que hice un par de días donde te muestro varias formas de cómo ganar dinero utilizando a chat gpt así que para un vídeo completo sobre chat gpt te lo voy a dejar acá arriba no te lo pierdas y si este vídeo te resultó valioso obviamente te pido que por favor te suscribas al canal actives campanita de notificaciones me dejes un buen like y déjame en los comentarios cuál de estas fue tu favorita yo soy dan fuentes y nos vemos en el próximo vídeo hasta luego
DOC0075|NLP e IA|Hasta este momento ya tenemos muchas opciones de modelos inteligentes para generar texto. Tenemos a GPT-3 y GPT-4, que se pueden utilizar desde ChatGPT. Tenemos Lambda, que se puede utilizar desde Google Bars, el competidor directo de ChatGPT. Y en el open source ya tenemos una lista mucho más grande como lo sería Lama, Koala, Vicuña y varios modelos abiertos que si bien no son muy usados, si están ayudando a impulsar más el avance de los Large Language Models. Pero como el generar texto e imágenes ya se está volviendo más común, ahora el siguiente paso que se está investigando es permitir que estos modelos hagan tareas más complejas. Es decir, que puedan crear archivos, ejecutar código, hacer búsquedas en internet, procesar archivos de imágenes y no por separado, sino más bien en conjunto. Es decir, que en lugar de que le pidas una porción de código y uno de estos modelos te responda con texto, mejor pídele que te crea un sitio completo y que él mismo se encarre de desplegarlo, testearlo y ejecutar todo el código, además de todos los archivos necesarios para que funcione. Por supuesto, para esto se tiene que dar acceso a tu sistema para poder hacer todo esto. Así que hay proyectos open source que te están permitiendo probar este tipo de tareas en los que podríamos encontrar a Hanging GPT, Auto GPT y Baby AGI, que les daré más información de estos en unos segundos. Y además de eso también están los frameworks que te permiten hacer todas estas tareas usando tu propio código, como sería el proyecto Land Chain. Ahora todo esto de hecho está haciendo que muchos entusiastas se estén empezando a preguntar Si desarrollando todo esto, estaremos llegando realmente a lo que sería una AGI de Artificial General Intelligence o una Inteligencia Artificial General. Verán, muchos investigadores durante muchos años han soñado con una AGI, ya que cuando se habla de inteligencia artificial siempre se piensa en una máquina capaz de hacer todas las tareas que hace un ser humano o incluso hacerlas mejor que un humano, teniendo la capacidad de aprender de todo. Y aunque no hay una definición muy precisa para lo que es una AGI ya que se ha investigado esto durante mucho tiempo, al parecer con todos estos pequeños avances relativamente, parece que estuvieramos viendo chispas de AGI. De hecho, este es el nombre de un paper de Microsoft llamado Sparf of Artificial General Intelligence, Early Experiments with GPT-4, en donde el estudio demuestra la capacidad de GPT-4 para lograr un rendimiento a nivel humano en tareas novedosas y difíciles en dominios que van desde las matemáticas, la codificación, hasta la visión, la medicina, el derecho y la psicología. Y concluye que podría verse razonablemente como una versión temprana, pero aún incompleta, de un sistema de Inteligencia Artificial General. Ya que el interés por desarrollar estos modelos y hacerlos cada vez mejores va en aumento. De hecho, si vemos un gráfico de la cantidad de investigaciones que se publican cada mes en Archive, un sitio que aloja las prepublicaciones de artículos científicos en distintos campos como la matemática, la física, las ciencias de la computación o la biología cuantitativa, estamos viendo que el sitio está alojando muchas más publicaciones desde el año 2021. Y actualmente con el impulso de modelos open source, esto solo está aumentando. Y no solo esto, sino que la capacidad de los parámetros que se están utilizando en los modelos actuales también se está incrementando exponencialmente. Y aunque el aumento de parámetros no nos hará llegar a niveles de AGI, sí se está empezando a ver avances notorios, gracias al impulso de modelos como GPT-4 que ahora pueden entrenarse a sí mismos. Eso fue revelado hace una semana en un paper llamado Reflection, en donde se habla de agentes inteligentes que pueden emular las autorreflexiones humanas para que puedan evaluar su propio rendimiento, similar a como lo hacen los humanos. Por ejemplo, usando la prueba HumanEval de OpenAI, que consiste en 164 problemas de Python que el modelo nunca ha visto, este respondió a 67% correctamente. Pero con la capacidad de autorreflexión ha llegado al 88%. Y no solo esto, sino que la comunidad no está esperando que OpenAI haga todo. Es por esto que modelos que están disponibles actualmente, además de GPT-4, están siendo combinados para que cada uno procese una tarea distinta y en conjunto pueden hacer tareas más complejas. Por ejemplo, este es Hagen GPT, un proyecto que usa GPT-4 como cerebro y que es capaz de delegar tareas a otros modelos abiertos que están alojados en Hagenface, un sitio donde se alojan proyectos AI open source. Y aunque tiende a ser muy básico estos tipos de ejemplos, esta es una prueba de concepto bastante interesante. Luego también tenemos a AutoGPT, que este es otro proyecto abierto, potencial por GPT-4, que puede darse prompts a sí mismo para completar de manera eficiente múltiples tareas, a través de textos encadenados y reflexión, pudiendo desde descargar archivos, escribir código y ejecutar scripts, que según su autor, este permite hacer desde debugging recursivamente hasta desarrollar proyectos completos. De hecho, este es el proyecto llamado la atención de Andrew Carpathy de OpenAI, llamándolo la siguiente frontera a alcanzar por estos modelos inteligentes. Luego también está RoboGPT, que es un proyecto similar y más simple a AutoGPT, que puede darles tareas desde consola e irá paso a paso mencionando lo que está haciendo, usando paquetes que convierte texto a voz, permitiendo que pueda solicitar código, lo guarde en un archivo, lo ejecute y vuelva a reescribirlo, hasta que cumpla con lo que le ha solicitado, aunque para proyectos bastante pequeños. Por ejemplo, aquí está obteniendo datos de los mejores lugares para nomadadigitales y al final lo guarde en un archivo CSV. Y finalmente también está BabyAGI, que es bastante similar a lo que ofrece AutoGPT y que también es un proyecto open source. Y no solo queda aquí, sino que también se están creando proyectos en sentido contrario, es decir, que también sirvan para acciones maliciosas, en donde aquí podríamos mencionar a ChaosGPT, que basado en AutoGPT tiene el objetivo de crear agentes y a maliciosos, usando GPT para entrenamiento, pero intentando evitar estos filtros amistosos que OpenAI siempre coloca a estos modelos. Ahora, como un ejemplo práctico, algo que podríamos ver a futuro. Está por ejemplo un proyecto personal por parte de un usuario de Twitter, en donde he mostrado cómo hablando con Siri puede pedirle con voz que crea una aplicación, dándole instrucciones de qué es lo que quiere, y que éste se conecte a una base de datos que pueda ejecutar códigos, se conecte a SuperBase, que es un servicio similar a Firebase, y finalmente que despliegue el proyecto en versed. Todo esto solo utilizando la voz. Y ahora, vamos a ver cómo se puede hacer con la aplicación. En este caso, vamos a usar la aplicación de la aplicación. Y vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Y ahora, vamos a ver cómo se puede hacer con la aplicación. Aquí va. Oh, sí, es muy rápido. Oh, esto va rápido. ¡Bum! Modo oscuro. Vamos a ver si está respondiendo. Ok, ahora está bien. Ok, ahora tenemos que probarlo. Ok, debemos poder submitirlo y esto debería funcionar. Profile creado sucesivamente. Ok, ok. Se muestra el profile. Vemos que tenemos una lista de todos los profiles. Vamos a clicar en eso. Ok, esto nos muestra los profiles. Ahora vamos a... Vamos a que esto esté respondiendo. ¡Sí, sí! Y si vamos a nuestra database, está en nuestra dv. Vamos a ver cómo se puede hacer con la aplicación. ¡Oh, sí! Ahora, todo esto es bastante interesante, pero con cada avance lo principal que se está probando es la automatización de creación de aplicaciones. Ahora, todo esto es bastante interesante, pero con cada avance lo principal que se está probando es la automatización de creación de aplicaciones. Es decir, que las aplicaciones se generen por sí mismas. Y esto está preocupando a muchos iniciantes en programación. Así que muchos están preguntando si vale la pena seguir aprendiendo programación o si debería haber un nuevo método de aprendizaje en todo caso. Pero esto lo trataré el día de mañana, ya que eso te va un poco más extenso. Así que nos vemos en el siguiente vídeo.
DOC0076|Mlops|En las etapas experimentales de desarrollo de un modelo de Machine Learning podemos implementar el prototipo simplemente en un computador personal, pero si queremos ya llevarlo a la etapa de desarrollo o a la etapa de producción, probablemente necesitamos o la colaboración de un grupo de trabajo, o tendremos que montar esta aplicación en la nube para darle acceso a diferentes usuarios. Pero esta aplicación muy probablemente no será definitiva, porque en este caso el Machine Learning es un proceso iterativo, los datos pueden cambiar, el modelo mismo puede cambiar, y por tanto cambiará el producto final. Así que durante todo este proceso empiezan a surgir varios inconvenientes, porque tenemos que garantizar que esta aplicación, que fue construida con ciertas especificaciones, pueda ser ejecutada por los colaboradores del proyecto, o desde un servidor, independientemente del hardware o del software que utilicen estos equipos remotos. Una alternativa a este problema es empaquetar la aplicación, usando algo que se conoce como un contenedor. Y en la actualidad Docker es una de las plataformas más usadas para esto. Pues si están interesados en conseguir empleo en el área del Machine Learning, definitivamente Docker es una de esas herramientas que deben saber manejar. Pues en este video veremos qué es Docker y para qué se requiere en el Machine Learning Engineering. Y al final también les voy a mostrar una guía sencilla para que puedan fácilmente incorporar Docker en su flujo de trabajo cuando sea necesario. Así que sin más preámbulos, comencemos. Partamos de un ejemplo hipotético para entender por qué estos contenedores de software resultan esenciales en el Machine Learning Engineering. Supongamos que estamos desarrollando una aplicación de Machine Learning inicialmente en nuestro entorno local, es decir, en nuestro computador. Instalamos todas las librerías requeridas, optimizamos el código para que aproveche la CPU y la GPU de nuestro computador, entrenamos el modelo y lo tenemos listo y funcionando a la perfección. Pero luego compartimos el desarrollo con otro colega o lo llevamos a otro computador o intentamos desplegarlo en la nube, usando entornos con una configuración similar. Pero resulta que no funciona. Comienzan a aparecer errores, conflictos con las versiones de las librerías o con el hardware y nos damos cuenta que realmente resulta muy muy complicado depurar todos estos errores y poner a funcionar el prototipo en un entorno diferente al nuestro. Incluso es posible que funcione, pero supongamos que en el equipo remoto de repente se modifica por ejemplo un driver y la aplicación o deja de funcionar o comienza a correr más lento de lo normal. Y el problema es aún más complejo si estamos en un entorno laboral, donde tenemos un equipo de desarrolladores y queremos hacer varias pruebas sobre el modelo en la etapa de desarrollo, antes de llevarlo a la etapa de producción y montarlo en un servidor para dar acceso a los usuarios. Pero ¿por qué tantos inconvenientes? Pues si analizamos por un momento la aplicación nos daremos cuenta que el código desarrollado es sólo la punta del iceberg, pero en realidad el problema es más complejo de lo que parece. Porque el código está montado sobre una API que puede ser por ejemplo PyTorch o TensorFlow que evoluciona constantemente, pero a su vez esta API depende de otras librerías que fueron desarrolladas de manera independiente como SciPy o NumPy o Python incluso y que también están evolucionando constantemente. Y para enredar aún más las cosas estas librerías están compiladas usando unas rutinas que son propias y específicas para la CPU local o incluso para la GPU o para los drivers de la GPU local. Así que cuando movemos el código del equipo local al de nuestros colaboradores o a un cluster en la nube, lo que estamos haciendo es introducir múltiples puntos de falla. Lo que quiere decir que es altamente probable que no exista coincidencia entre las librerías y dependencias usadas por el equipo local y el remoto. Para resolver todos estos inconvenientes necesitamos una herramienta que cumpla al menos estas tres condiciones. En primer lugar que nos dé resultados consistentes, es decir que nos permita ejecutar la misma aplicación de Machine Learning en diferentes computadores y obteniendo siempre los mismos resultados. En segundo lugar que nos dé portabilidad, es decir que nos permita empaquetar la aplicación y luego desplegarla por ejemplo en la nube en un servidor. Y en tercer lugar que se encargue en su totalidad del manejo de dependencias, es decir que podamos empaquetar la aplicación, pero también las librerías y todas las dependencias exactamente de la misma forma como funcionan en el computador local. Y acá es donde entra el rescate Docker, que es actualmente la plataforma más usada para la creación de contenedores de software. Hacemos una metáfora para entender lo que hace Docker. Cuando un barco de carga transporta mercancía lo hace usando varios contenedores. Cada contenedor tiene un producto diferente y esto se hace para evitar que diferentes productos de diferentes proveedores y con diferentes características se mezclen y se arme un desorden. Pues Docker hace lo mismo que estos contenedores del barco, solo que en lugar de productos tenemos el código y todas las dependencias asociadas a nuestra aplicación. En detalle lo que hace Docker es crear un contenedor que encapsula por completo no solo el código sino la totalidad de las dependencias, llegando hasta las librerías que interactúan con el hardware para acceder por ejemplo a los puertos de red o a recursos de CPU y GPU. Y con esto se logra estandarizar el entorno, porque se configura solo una vez y se puede distribuir todas las veces que sea necesario a diferentes hosts sin necesidad de reconfigurar una a una las librerías o el entorno del hardware. Y esta idea de encapsular la aplicación y todas sus dependencias en un contenedor resulta súper útil en el Machine Learning Engineering, porque en un proyecto el equipo puede usar Docker para encapsular diferentes módulos del proyecto, lo que facilita la distribución no solo a otros miembros del equipo sino que también reduce el tiempo de desarrollo y además en últimas facilita todo el proceso de despliegue y producción del modelo de Machine Learning. Y esto es mucho mejor que usar máquinas virtuales, pues por cada máquina virtual se requiere la instalación de un sistema operativo por cada aplicación que queramos ejecutar, mientras que con Docker cada aplicación estará en un contenedor, pero todos los contenedores estarán montados sobre un solo sistema operativo. Esto hace que un contenedor necesite menos recursos, sea más liviano, mucho más portátil y se pueda ejecutar más rápido que una máquina virtual. Bien, teniendo ya una idea detallada de lo que es Docker, veamos una guía que les permitirá fácilmente incorporarlo en sus proyectos de Machine Learning. La creación de un contenedor en Docker es un proceso de tres fases. Primero debemos crear un archivo Docker que contiene simplemente un listado de instrucciones para empaquetar la aplicación. Este archivo se almacena en el directorio del proyecto bajo el nombre Docker File. En segundo lugar está la imagen Docker, que es el paquete de software que contendrá el código, las librerías y todas las dependencias. Es lo que se distribuye a múltiples equipos remotos. Y en tercer lugar está precisamente el contenedor Docker. El contenedor es simplemente el resultado de ejecutar la imagen en un equipo remoto. Es decir que la imagen es una sola, pero puede haber múltiples contenedores si la ejecutamos en múltiples equipos remotos. Veamos en detalle este flujo de trabajo. Primero creamos el Docker File, que debe estar ubicado en la misma carpeta local del proyecto. Este archivo no tiene ninguna extensión y puede ser creado en un simple editor de texto. El formato usado es una instrucción en mayúscula seguida de una serie de argumentos. Usualmente la primera instrucción es FROM, con la cual le indicaremos a Docker la imagen base sobre la cual construiremos el contenedor. Por ejemplo, si nuestra aplicación está desarrollada localmente sobre una versión de Anaconda Python, entonces debemos especificar esto precisamente en el argumento. Otra instrucción muy usada es COPI, con la cual le indicaremos a Docker que tome todo el contenido de la carpeta local del proyecto y lo copie en el directorio que especificemos para el contenedor. Si por ejemplo nuestra aplicación tiene una interfaz a través del navegador, tendremos que habilitar un puerto de red. Para esto usamos la instrucción EXPOSE, seguida del número del puerto de red que queremos usar. Ahora debemos indicarle a Docker en qué directorio del contenedor se encontrará el archivo ejecutable de nuestra aplicación. Para esto usamos la instrucción WORKDIR, seguida de la ruta completa del directorio en el contenedor. Como nuestra aplicación muy probablemente requerirá varias librerías de Python, debemos crear un archivo de texto con estos requerimientos y luego desde Dockerfile incluir una instrucción para que estas librerías sean instaladas. Para eso usamos RUN, seguido del comando para instalar las librerías. Finalmente debemos especificar cuál comando se debe ejecutar al momento de correr la aplicación. Así que usamos CMD y como nuestra aplicación está construida en Python, el comando será simplemente Python, seguido del nombre del archivo que contiene el ejecutable de la aplicación. Y listo, con esto ya tenemos nuestro set de instrucciones, es decir el Dockerfile, con toda la información necesaria para construir primero la imagen y luego el contenedor de Docker. El siguiente paso es entonces construir la imagen, para lo cual tenemos que abrir el terminal de Docker, que viene ya incluido en la aplicación de Docker instalada en nuestro computador local. Nos movemos al directorio local en donde se encuentra nuestro Dockerfile y escribimos el comando Docker, seguido de la palabra clave build, del nombre que queremos darle a la imagen y de la ruta en donde se encuentra el Dockerfile. Y listo, ya tenemos creada la imagen en donde hemos encapsulado absolutamente todo, el código fuente, las librerías y todas las dependencias que se necesitan para poder correr la aplicación remotamente. Finalmente el último paso sería distribuir esta aplicación para ejecutarla remotamente, y aquí existen básicamente tres alternativas. La primera, que de hecho no es muy usada, consiste en crear un archivo comprimido de la imagen local, luego compartirlo con un usuario remoto y allí desempecarlo y luego ejecutarlo. La segunda es distribuirlo a través de Docker Hub, el servicio de Docker para compartir imágenes. Simplemente en este caso usamos Docker push y los usuarios remotos pueden descargar esta imagen con Docker pull y luego ejecutarla con Docker run. Y finalmente está la opción más usada en Machine Learning Engineering, que consiste en crear la imagen local y luego distribuirla y también ejecutarla en un servicio en la nube, como Google Kubernetes, Amazon Elastic Container o Azure Kubernetes. En este caso la imagen se distribuye nuevamente con Docker push, pero la ejecución depende de las particularidades de cada uno de estos servicios. En resumen, Docker es una plataforma que permite fácilmente encapsular y distribuir una aplicación de Machine Learning usando algo que se conoce como contenedor. Y este tipo de herramientas resulta muy útil en diferentes fases del proceso de desarrollo de un proyecto de Machine Learning, pero especialmente cuando queremos desplegarlo y llevarlo finalmente a producción. Así que si están interesados en desempeñarse profesionalmente como ingenieras o ingenieros de Machine Learning, definitivamente Docker es una de esas herramientas que considero deberían conocer y saber manejar. Como les mencioné hace un momento, comenzaremos a hablar de plataformas en la nube, como las ofrecidas por Google, Amazon o Microsoft, que son tal vez las más usadas en la actualidad para desplegar y llevar a producción modelos de Machine Learning. Por ahora esto es todo, los invito a continuar viendo otros videos del canal que les voy a compartir de este lado, les envío un saludo y nos vemos en el próximo video.
DOC0077|Mlops|SQL o SQL es un lenguaje de programación que tiene más de 40 años y que muchos en la actualidad consideran en desuso, pero a pesar de esto es después de Python el lenguaje de programación que sin duda alguna tiene que estar en la caja de herramientas de cualquiera que está interesado en trabajar en el machine learning o en la ciencia de datos en general, pues en este vídeo les voy a contar en detalle las razones por las cuales considero que SQL es fundamental en estas áreas, además vamos a ver qué es SQL en detalle y vamos a ver una guía completa para que fácilmente puedan comenzar a incorporarlo en sus proyectos, así que sin más preámbulos comencemos. El machine learning es la ciencia y el arte de programar computadores para que puedan aprender de los datos y en esta frase hay un elemento súper importante, los datos. Sobre los datos descansa toda la teoría y todos los desarrollos del machine learning, los datos y cómo interpretarlos y aprender de ellos han sido la razón de la evolución y de los logros impresionantes del machine learning durante sus más de 60 años de historia. Cualquier desarrollo que se haga en la academia o en la industria parte precisamente de los datos, así que resulta fundamental tener herramientas que permitan fácilmente acceder a esos datos y los tipos de datos usados en el machine learning vienen en dos sabores, los no estructurados como las imágenes, el audio y el vídeo y los estructurados, es decir los que vienen almacenados en formato tabular, que en el mundo real se encuentran en bases de datos y no en un único archivo como usualmente los podemos encontrar cuando estamos apenas iniciando en el machine learning. Y acá es donde entra SQL, el estándar usado en la mayor parte de las bases de datos corporativas a nivel mundial. De hecho el último reporte del 2020 hecho por Kaggle que resulta de una encuesta aplicada a más de 60 mil profesionales de 171 países y que trabajan en las áreas de machine learning y la ciencia de datos demuestra que después de Python SQL es el segundo lenguaje más usado en el mundo laboral en estas áreas. Así que a pesar de ser un lenguaje antiguo y de estar subestimado en la actualidad SQL resulta fundamental en el día a día para poder desarrollar proyectos y sistemas de machine learning. En unos momentos veremos en detalle las seis principales razones por las cuales considero que este lenguaje es fundamental en el machine learning engineering. Pero por ahora veamos en detalle qué es SQL. Pues en términos simples SQL es un lenguaje de programación para realizar consultas o queries, de ahí el nombre del lenguaje, en bases de datos relacionales. ¿Y qué es una base de datos relacional? Pues es una base de datos donde la información está almacenada en tablas pero esa información entre cada una de las tablas está interrelacionada, de allí el término relacional. Por ejemplo supongamos que estamos desarrollando un sistema de machine learning para un hospital que quiere hacer el seguimiento a varios de sus pacientes y la información está almacenada precisamente en una base de datos que contiene cuatro tablas, cada una con miles de registros que contienen información de los pacientes, los médicos, las citas médicas y los medicamentos suministrados. Aunque de entrada aparecen tablas independientes en realidad podemos ver que por ejemplo la tabla de pacientes se relaciona con la de citas médicas a través del ID del paciente y que a su vez la tabla de las citas médicas se relaciona con la de medicamentos a través del ID de la cita. Si queremos por ejemplo en un momento dado saber cuál médico trató a un paciente específico y qué medicamento le recetó, necesitamos relacionar la información entre estas cuatro tablas para poder extraer la información. Y esto es precisamente una base de datos relacional. Entonces como vemos la información no está almacenada en una sola tabla y por eso dependiendo de la consulta que queramos realizar necesitamos un método eficiente que nos permita relacionar la información entre diferentes tablas para poder extraer los datos que necesitamos. E-SQL fue creado precisamente para esto, para poder realizar este tipo de consultas fácilmente incluso si la base de datos es inmensa, lo que lo hace tremendamente útil para el caso particular del Machine Learning en donde precisamente desarrollamos modelos que muchas veces usan sets de datos con miles o millones de registros. Veamos entonces una guía de lo que considero son las formas de uso esenciales de SQL en el Machine Learning y en la ciencia de datos. La consulta más sencilla en SQL selecciona una columna de una única tabla, para ello escribimos inicialmente la palabra select y luego el nombre de la columna y en la segunda línea de la consulta escribimos la palabra from y luego especificamos el nombre de la tabla. Las palabras select y from se conocen como palabras clave y pueden ser escritas en mayúscula o minúscula, aunque usualmente se dejan en mayúscula para hacer el código más entendible. Usualmente trabajamos con sets de datos muy grandes así que muchas veces solo queremos extraer los registros que cumplan con ciertas condiciones, para esto podemos usar where, por ejemplo si en el caso anterior no queremos extraer la columna completa sino únicamente los registros de pacientes con edades mayores de 60 años, entonces escribimos la misma consulta anterior y agregamos una línea con la palabra where seguida de la condición. También podemos realizar agrupaciones y conteos, si por ejemplo queremos saber cuántos médicos tiene el hospital en cada especialidad, primero usamos select seguido de la columna de la tabla que nos interesa y agregamos la palabra clave count, luego con from especificamos en cuál tabla queremos realizar la consulta y finalmente usamos group by para especificar que queremos realizar la agrupación por especialidad. Si queremos por ejemplo saber únicamente las especialidades que tienen dos o más médicos podemos agregar la palabra clave having especificando esta condición. También podemos organizar los registros de forma ascendente o descendente usando como criterio columnas con datos numéricos o en formato de texto y las palabras clave order by y desk. Y como les contaba hace un momento lo más útil de SQL es que nos permite establecer relaciones entre varias tablas, por ejemplo si queremos saber cuál es el médico tratante de cada paciente podemos usar la palabra clave join para unir la información de dos o más tablas. Y estos son sólo algunos ejemplos básicos es decir como un nivel introductorio pero realmente existen muchos más comandos e infinidad de consultas que se podrían realizar y todo depende en últimas del tipo de datos que queramos extraer precisamente de esa base de datos pero más allá de eso realmente lo que pudimos ver en este ejemplo es que usar SQL es relativamente sencillo porque es un lenguaje de programación bastante intuitivo y si revisan las consultas que acabamos de realizar es relativamente fácil entender lo que estamos haciendo. Bien ya tenemos un panorama bastante detallado de qué es y cómo funciona SQL ahora si veamos las seis principales razones que considero por las cuales cualquier persona que esté interesada en trabajar en machine learning o ciencia de los datos debería aprender a usar este lenguaje. La primera y la más importante es porque cuando hablamos de datos estructurados generalmente SQL es el estándar utilizado en la mayoría de empresas así que si queremos un trabajo en machine learning o data science debemos ser capaces de realizar consultas y extraer información relevante de esas bases de datos. La segunda es porque usualmente estas bases de datos son inmensas con decenas o cientos de tablas y cada una con miles o incluso millones de registros. Pensar en almacenar esta información en una tabla de excel para luego consultarla es simplemente una opción que no resulta viable. La tercera razón está muy relacionada con la anterior y es que si sabemos manejar SQL podemos crear consultas muy específicas para extraer únicamente los datos que necesitamos para nuestro proyecto de machine learning. Además que en este proceso vamos a tener un contacto directo con los datos vamos a saber de dónde provienen cuál es su estructura y en últimas esto nos va a permitir un elemento fundamental de cualquier proyecto de machine learning que es precisamente entender los datos y esto nos facilitará las etapas posteriores del desarrollo por ejemplo para la depuración de los datos, la extracción de características, el análisis exploratorio e incluso la elección del modelo que mejor se ajuste a los datos. La cuarta razón es porque SQL se puede integrar fácilmente con Python o R así que fácilmente podemos usarla para extraer información de la base de datos y luego usar por ejemplo Python para realizar el procesamiento de estos datos e implementar el modelo. La quinta razón es porque a pesar de que tiene más de 40 años SQL sigue siendo usado como la base para implementar las nuevas generaciones de bases de datos en la nube como ocurre con los servicios ofrecidos por Google, Amazon o Microsoft. Y la sexta razón es porque simplemente el mercado lo está pidiendo. Si hacemos una búsqueda de los perfiles requeridos para la ingeniería del machine learning o la ciencia de datos veremos que en muchos de estos casos el manejo de SQL es un requisito fundamental. Bien con todo lo que les acabo de contar espero que puedan tener un panorama completo de la importancia de saber usar SQL si queremos desempeñarnos laboralmente bien sea en machine learning o la ciencia de datos. A pesar de que en los últimos años han surgido otras alternativas SQL sigue siendo el estándar de facto para interactuar con bases de datos y en mi opinión personal lo seguirá siendo por muchos años más. Además como lo vimos en los ejemplos anteriores aprender SQL es relativamente sencillo porque es un lenguaje muy intuitivo así que definitivamente si ya están trabajando o si están comenzando a formar su carrera como futuros ingenieros o ingenieras del machine learning definitivamente SQL es la herramienta que tienen que comenzar a aprender. Bien espero que les haya gustado este vídeo si es así no olviden darle un pulgar hacia arriba y compartirlo con sus amigos si creen que les puede gustar este contenido les envío un saludo y nos vemos en el próximo vídeo.
DOC0078|Mlops|Imaginen por un momento que como yo son fanáticos de la comida italiana y en particular de la pizza, así que deciden aprender a prepararla. Buscan la mejor receta disponible, comienzan a practicar y con el tiempo logran ser unos expertos preparando pizzas. Y les gusta tanto que se animan a montar una pizzería. Pero acá se dan cuenta de que una cosa era preparar la pizza en casa y otra muy diferente es comenzar a venderla. Se dan cuenta de que necesitan un mejor horno, más materia prima, alguien que tome los pedidos y que haga las entregas. Así que deben aprender varias cosas adicionales para lograr pasar de la pizza casera a un negocio de producir y vender pizzas. Pero bueno, se estarán preguntando qué tiene que ver esto de la pizza con el Machine Learning. Y tiene que ver mucho si queremos trabajar en una empresa resolviendo problemas usando precisamente el Machine Learning. Y es que si sabemos los fundamentos es como saber preparar la pizza en casa, pero llevar un modelo a producción es como precisamente montar una pizzería. Y acá hay una idea súper importante, debemos aprender a llevar el modelo a producción. Y el Machine Learning Engineering es precisamente esto, es tomar un modelo desarrollado usando esos fundamentos del Machine Learning, llevarlo a un usuario final y lograr que este modelo se pueda actualizar periódicamente para garantizar que tenga siempre el desempeño esperado. Y para lograr esto debemos tener unas habilidades y unas herramientas menos científicas y un poco más técnicas. De algunas de estas herramientas ya hablamos en videos anteriores como Git, SQL o incluso el uso de plataformas como Docker. Pero la cosa va más allá de esas herramientas. Pues lo que vamos a ver ahora es un panorama completo de lo que es el Machine Learning Engineering, es decir, cómo tomar ese modelo y cuáles son las etapas que se requieren para llevarlo a una fase de producción. Vamos a hablar particularmente del despliegue del modelo o el deployment, del model service o el servicio, de cómo se hace el monitoreo y en qué consiste el mantenimiento de ese modelo. De hecho, el ingeniero o ingeniera de Machine Learning es básicamente un híbrido entre el científico de Machine Learning, que conoce temas como el análisis exploratorio, el preprocesamiento de datos y el desarrollo de modelos, y un ingeniero de software convencional, que conoce cómo diseñar, probar, desplegar y hacer el mantenimiento del software. Pero existe una diferencia fundamental entre esta ingeniería de software convencional y lo que hace el Machine Learning Engineering, y es que en la primera el software usualmente es estático y solo ocasionalmente requiere algunas actualizaciones. Sin embargo, en el Machine Learning Engineering tenemos un modelo dinámico, porque una vez en producción los datos cambiarán y por tanto el modelo deberá ser reentrenado para ajustarse a esos nuevos datos, o de lo contrario sufrirá de algo que se conoce como degradación, pero de esto hablaremos en detalle en un momento. Así que un ingeniero o ingeniera de Machine Learning tiene que ser capaz de prevenir este comportamiento, o al menos de detectarlo y tomar los correctivos necesarios para evitar esa degradación del modelo. Para entender qué es esto de la degradación del modelo, necesitamos tener claro cuál es el ciclo de vida de un proyecto de Machine Learning Engineering, así que partamos de un ejemplo real. Veamos cómo funciona un sistema desarrollado por Airbnb, la plataforma online que permite contactar propietarios de viviendas en alquiler con huéspedes que necesitan este alojamiento. Esta plataforma es capaz de extraer información relevante a partir de las imágenes de los inmuebles subidas por los usuarios de la plataforma. Una de las metas de Airbnb es hacer que el mismo negocio sea más eficiente y que sus productos sean mucho más fáciles de usar, y uno de los inconvenientes es que muchas de las propiedades listadas en la plataforma tienen una descripción dada por los propietarios que no coincide con las características físicas del inmueble, y como existen miles de propiedades ofertadas a través de la plataforma, para Airbnb es imposible visitar personalmente cada propiedad para verificar esta información, así que lo que le interesa a Airbnb es verificar de la forma más eficiente posible que la descripción que hacen los usuarios coincide con lo que realmente se encuentra en el inmueble, y aunque de entrada parece que esta necesidad no tiene nada que ver con el Machine Learning, si la analizamos un poco más en detalle, veremos que de hecho podríamos desarrollar un sistema de visión artificial de detección de objetos que analice las imágenes subidas por los propietarios y detecte automáticamente los objetos de interés, los servicios, el equipamiento y el mobiliario disponible en el inmueble. Si volvemos al ciclo de vida del proyecto de Machine Learning, podemos decir que la primera fase es tener claro el problema o la necesidad de la empresa, en este caso Airbnb. Si el problema puede ser resuelto a través del Machine Learning, podemos pasar a la segunda fase de este ciclo de vida del proyecto, la delimitación del problema en términos propios del Machine Learning, es decir que debemos definir el tipo de modelo a usar, la entrada y la salida y sus criterios de desempeño. En el caso de Airbnb este modelo se encargará de detectar los diferentes objetos presentes en la imagen con un alto nivel de confianza, su entrada será una imagen y la salida será una serie de etiquetas, estufa, nevera, cocina, mesa, sofá, etc. Y este nivel de confianza se medirá, por ejemplo, con la precisión media-media. Las fases 3, 4, 5 y 6 son las etapas convencionales de un proyecto de Machine Learning. Recolectar y preparar los datos, de ser necesario realizar una extracción de características, entrenar y evaluar el modelo. Y en el Machine Learning académico o de investigación, o para los que estén hasta ahora comenzando en el mundo del Machine Learning, hasta acá llegaría esta fase del proceso. Pero si llegamos hasta acá no estaríamos dando respuesta al problema de negocio planteado, es decir, ya sabríamos preparar la pizza pero nos faltaría montar la pizzería. Es decir que ahora tenemos que llevar el modelo a producción para que llegue al usuario final, que en este caso es el sitio web de Airbnb. Y acá es donde entran los elementos adicionales del Machine Learning Engineering. Para montar nuestra pizzería se requieren cuatro fases adicionales, el despliegue del modelo, llevarlo a un servicio e incluir una fase de monitoreo y una de mantenimiento. El despliegue consiste simplemente en poner a disposición del usuario final lo implementado en las fases anteriores. Mientras que el servicio implica permitir que ese sistema acepte solicitudes del usuario y las lleve al modelo para generar una predicción. Y este despliegue que usualmente se realiza a través de servicios en la nube, viene esencialmente en tres diferentes sabores. La primera forma es llevando el modelo y todas sus dependencias a una máquina virtual, para lo cual se pueden usar librerías como Flask o FastAPI, de las que voy a hablarles en próximos videos. De hecho el sistema de Airbnb se implementa precisamente usando este esquema. La segunda alternativa consiste en usar contenedores de aplicaciones. Si no saben lo que es un contenedor, los invito a ver el video sobre Docker, en donde les explico en detalle cómo funciona esta plataforma. Usualmente se usa Docker para crear y distribuir este contenedor. Y luego un orquestador, que es como un director de orquesta, de ahí el nombre, que se encarga de coordinar en la nube la interacción de todos los elementos del sistema. Uno de los orquestadores más usados es precisamente Kubernetes, del cual también hablaremos más adelante en otro video. Y la tercera alternativa es hacer un despliegue sin servidor o serverless. En este caso el modelo y el código asociado para recibir datos y realizar la predicción se almacenan en un archivo .zip, que luego se sube a un servicio de la nube como Lambda, de Amazon Web Services, de Microsoft Azure y de Google Cloud. También en un próximo video hablaremos de este esquema de despliegue más en detalle. En este caso la gran ventaja con respecto a las dos formas de despliegue anteriores es que no se tienen que aprovisionar recursos como servidores o máquinas virtuales, solo se tiene que pagar por el tiempo de cómputo, es decir, al momento de realizar la predicción con el modelo. Bien, y después del despliegue y servicio del modelo están las fases de monitoreo y mantenimiento. Y la razón de esto es un elemento fundamental de cualquier proyecto de Machine Learning, los datos están cambiando constantemente. De hecho si volvemos al sistema de detección de objetos de Airbnb, podremos ver que el modelo inicial fue entrenado con un set de datos específico. Sin embargo, la plataforma está en continuo movimiento y los usuarios continuamente estarán subiendo imágenes de sus inmuebles, así que no existe garantía alguna de que el modelo siga funcionando correctamente para esas nuevas imágenes. De hecho, esto es una constante en prácticamente todos los sistemas de Machine Learning. El modelo se va degradando con el tiempo, lo que quiere decir que sus predicciones van desmejorando porque los datos que se introducen al modelo van cambiando periódicamente. Entonces la fase de monitoreo se encarga de verificar que el desempeño del modelo se mantiene, teniendo en cuenta que los datos están cambiando constantemente. Si hay degradación del modelo, entra a jugar la fase de mantenimiento, que se encarga de recolectar nuevos datos y reentrenar el modelo para desplegarlo nuevamente y retomar el ciclo de producción del sistema. Y si revisamos nuevamente todo el ciclo de vida de nuestro proyecto de Machine Learning, veremos que en realidad las fases no se ejecutan secuencialmente, sino que por el contrario se trata de un proceso cíclico, pues si los datos cambian es muy probable que en alguna etapa del proceso tengamos que regresar por ejemplo a recolectar más datos, para luego reentrenar el modelo, validarlo y llevarlo nuevamente a producción y este proceso se debe repetir periódicamente para evitar la degradación del sistema. Así que como hemos visto son muchos los elementos adicionales a la creación del modelo los que se requieren para poderlo posteriormente desplegar en una fase de producción. Y estas habilidades y herramientas son más de tipo ingenieril que combinadas con los fundamentos básicos del Machine Learning son lo que dan origen precisamente a lo que se conoce como el Machine Learning Engineering. En los próximos videos comenzaremos a ver más en detalle cada una de estas fases y hablaremos de cosas como por ejemplo los servicios en la nube y de cómo se hacen el despliegue, el servicio, el monitoreo y el mantenimiento de estos sistemas. Por el momento esto es todo, no olviden darle un pulgar hacia arriba de me gusta al video, suscribirse al canal, les envío un saludo y nos vemos en el próximo video.
DOC0079|Mlops|El despliegue de un modelo de Machine Learning consiste en llevarlo de la etapa de desarrollo a la etapa de producción para que esté a disposición de un usuario final y este aspecto ha cobrado mucha importancia en los últimos años para el desarrollo de aplicaciones a nivel industrial, así que en este vídeo nos enfocaremos en este despliegue y en particular veremos los aspectos a tener en cuenta cuando queremos llevar nuestro modelo a producción, así como las principales herramientas existentes en la actualidad. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Bits en donde encontrarán cursos de formación en ciencia de datos y machine learning y por una suscripción mensual de tan solo 10 dólares y ahora sí comencemos. Bien, comencemos viendo una definición más formal de lo que es el despliegue y de algunos conceptos asociados. El Machine Learning Engineering o Machine Learning Operations del cual hablamos en un vídeo anterior es un conjunto de prácticas que busca lograr el despliegue y mantenimiento de modelos de Machine Learning de manera confiable y eficiente y el despliegue es una parte central de este proceso que consiste en tomar el modelo entrenado y hacerlo accesible a un usuario final. Por ejemplo, en la etapa de desarrollo lo que hacemos usualmente es crear un primer prototipo del modelo y generalmente esto lo hacemos en un computador local a través de algunos scripts de python o usando notebooks de Jupiter o en la nube usando por ejemplo servicios como Google colap con el modelo ya entrenado y bien sea que se trate de un proyecto personal académico o que estemos trabajando para una empresa lo ideal es llevarlo a un entorno de producción para que sea accesible a un usuario final y esto es precisamente el despliegue. Y la elección del tipo de despliegue dependerá no solo del uso que queramos darle al modelo sino de varios requerimientos de diseño así que hablemos en detalle de estos requerimientos. El primer requerimiento a tener en cuenta es el tipo de predicción que queremos realizar que puede ser en tiempo real o por lotes. En tiempo real las predicciones son generadas y devueltas al usuario en el menor tiempo posible después de recibida la solicitud. Un ejemplo es Google Translate el usuario introduce el texto en el idioma original y casi de inmediato recibirá el texto traducido. Por su parte en las predicciones por lotes se procesa una gran cantidad de datos de entrada y el modelo genera las predicciones pero de forma asíncrona es decir que la predicción no es inmediata. Un ejemplo es el sistema de recomendación de Netflix que toma el historial de contenido visto por el usuario y aproximadamente cada cuatro horas genera nuevas sugerencias de contenido y otros dos requerimientos a tener en cuenta al momento de realizar el despliegue son la latencia y el rendimiento. La latencia es simplemente el tiempo de respuesta requerido desde que se envía la solicitud al modelo en producción hasta que se recibe la predicción y la idea es que esta latencia sea lo más pequeña posible máximo de unos pocos cientos de milisegundos. Por ejemplo si estamos usando un modelo de reconocimiento facial para permitir el ingreso a los trabajadores de una empresa queremos que las predicciones se generen lo más pronto posible y el rendimiento es el número de solicitudes por segundo que puede soportar el sistema donde está alojado el modelo y es un aspecto importante al hacer predicciones por lotes. Volviendo al ejemplo de Netflix que genera por cada usuario predicciones cada cuatro horas lo que buscaríamos sería un rendimiento alto teniendo en cuenta que el servicio tiene varios cientos de millones de usuarios. Y por último el tipo de despliegue también dependerá de la complejidad del modelo que queramos llevar a producción. No es lo mismo un modelo del tamaño de GPT-3 que un pequeño modelo como MobileNet. El primero requiere más capacidad de cómputo y de almacenamiento mientras que el segundo podría incluso correr en un dispositivo móvil. Bien teniendo claros estos requerimientos ahora sí podemos enfocarnos en las diferentes alternativas de despliegue que son esencialmente dos grandes grupos en la nube y lo que se conoce como on the edge. En la nube quiere decir que el cómputo requerido para las predicciones se realizarán servidores remotos alojados en la nube por lo cual los datos y las predicciones se transfieren a través de internet. Esta es la alternativa a usar cuando se requieren muchos recursos computacionales es decir cuando usamos modelos complejos o cuando la latencia no es un problema porque podemos aceptar retardos en el envío de los datos y la recepción de las predicciones o porque las predicciones se hacen por lotes. Al desplegar en la nube usualmente podemos acceder al modelo de dos maneras una es almacenando los datos de entrada al modelo en una base de datos y programando el sistema para que cada cierto tiempo tome esos datos y genere las predicciones. Las predicciones resultantes son también almacenadas en una base de datos y periódicamente son entregadas a la aplicación cliente encargada de llevarlas al usuario final. Otra forma muy usada es empaquetando nuestro modelo en una API que le permitirá recibir solicitudes hechas por el usuario final así como entregarle el resultado de la predicción. Esta última forma es usada cuando queremos hacer predicciones con baja latencia o cuando no son por lotes como por ejemplo el servicio de google translate y la otra forma de despliegue es la que se conoce como on the edge es decir que el modelo no va a estar alojado en la nube sino en el mismo dispositivo encargado de recibir las solicitudes que generalmente es un dispositivo con limitadas capacidades de cómputo como por ejemplo un teléfono móvil, una smartwatch o una tablet. En este caso la totalidad del cómputo se realiza en el dispositivo local on the edge así que esta alternativa se usa cuando no se requieren muchos recursos computacionales es decir cuando no se usan modelos muy complejos cuando se requieren bajos niveles de latencia como en los vehículos autónomos donde nos interesan tiempos de respuesta casi que de inmediato o cuando por temas de seguridad no resulta conveniente enviar información a través de internet como por ejemplo en los sistemas de reconocimiento biométrico. En este tipo de despliegue la predicción sólo puede ser en tiempo real ya que en dispositivos móviles tenemos recursos limitados que dificultan el procesamiento por lotes. Muy bien teniendo claros los tipos de despliegue que existen vamos a ver ahora un poco más en detalle las herramientas que se usan en cada caso aunque vale la pena aclarar que no es un listado exhaustivo sino más bien las que considero son las más usadas en la actualidad y además veremos estas herramientas organizadas por nivel de dificultad al momento de la implementación yendo de las más sencillas hasta las más complejas. En primer lugar están las herramientas de despliegue local que rigurosamente no permiten llevar el modelo a producción pero sí permiten desplegarlo en nuestros propios computadores es decir localmente sin llegar a un ala nube o a un dispositivo edge aunque pueden ser un primer paso para luego llevarlas a la etapa de producción. Dos de las más usadas son Flask y Fast API que son librerías de python que permiten empaquetar el modelo como una API y acceder a este localmente desde nuestro navegador de internet. La ventaja es que el modelo puede haber sido creado con cualquiera de las librerías usadas comúnmente en machine learning. También tenemos tensorflow serving y tors-serve que están optimizadas para tomar modelos entrenados con tensorflow o pytorch respectivamente y que con muy poca línea de código permiten desplegarlos localmente. Ambas permiten también hacer inferencias es decir predicciones en tiempo real o por lotes. Y finalmente si queremos hacer despliegue on the edge podemos usar tensorflow lite o pytorch mobile para desplegar modelos de tensorflow o pytorch. Luego tenemos las que he llamado las herramientas out of the box con las que podemos desplegar modelos como aplicativos web y sin necesidad de muchos recursos computacionales e incluso muchas de ellas son gratuitas. La más usada es tal vez streamlit que tiene un servicio gratuito en la nube para alojar nuestra aplicación y donde en unos pocos pasos podemos pasar de nuestro código en python a alojar el aplicativo en los servidores de streamlit. Este enfoque lo recomiendo para quienes estén interesados en desplegar en la nube proyectos personales o académicos de machine learning y que no necesiten predicción por lotes sobre todo porque la curva de aprendizaje es baja y en pocos minutos podemos tener el modelo disponible. La grandes ventajas es que no funcionan cuando queremos desplegar modelos complejos bien sea para hacer inferencia en tiempo real o por lotes. Esto debido a que por ser gratuitos tendremos nuestro modelo alojado en servidores con restricciones de memoria y de capacidad de cómputo. Y finalmente tenemos los servicios en la nube en los casos en los cuales se requiera mayor capacidad de cómputo o la posibilidad de inferencia tanto en tiempo real como por lotes así como la posibilidad de llevar nuestro modelo a múltiples usuarios finales. En la nube quiere decir que el cómputo de las predicciones se hace totalmente en servidores remotos donde los principales competidores son amazon web services google cloud o microsoft azure. Este enfoque es útil cuando tenemos modelos complejos que requieren muchos recursos computacionales o cuando la latencia no es un problema. Para este despliegue podemos incluso aprovechar las herramientas que habíamos usado para el despliegue local para así alojar estos aplicativos en cualquiera de estos servicios en la nube usando la misma lógica de empaquetamiento y acceso a través de solicitudes en la nube. La gran ventaja es que estos servicios permiten no sólo el despliegue del modelo sino la implementación de todo el ciclo del machine learning operations incluyendo el entrenamiento y el monitoreo. Las desventajas son una curva de aprendizaje muy alta y obviamente el costo de estos servicios. Y bien esto es todo por ahora déjenme abajo sus comentarios si quieren que en un próximo vídeo veamos en detalle cualquiera de estas herramientas que les he mencionado o un tutorial aplicado sobre estos métodos de despliegue y si aún no lo han hecho no olviden suscribirse al canal y darle un pulgar hacia arriba de me gusta a este vídeo y compartirlo con sus amigos y conocidos porque esto me va a ayudar un montón a llegar a más y más gente y seguir creando este tipo de contenidos. Así que les envío un saludo y nos vemos en el próximo vídeo.
DOC0080|Mlops|En el Machine Learning Operations se busca llevar un modelo de Machine Learning de la etapa de desarrollo a la etapa de producción. En la etapa de desarrollo lo que hacemos es entrenar y validar el modelo para luego llevarlo a la fase de producción donde lo desplegamos y lo ponemos a disposición del usuario final. Y es en esta etapa de despliegue donde pueden surgir inconvenientes que hagan que el modelo no tenga el desempeño esperado. Así que en este video veremos en qué consiste el monitoreo de un modelo de Machine Learning que nos permite observar continuamente el desempeño del modelo determinar por qué puede estar funcionando incorrectamente para luego tomar los correctivos necesarios. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Bits donde por una suscripción mensual de tan solo 10 dólares podrán acceder a cursos que les permitirán construir su carrera en ciencia de datos, Machine Learning e Inteligencia Artificial. Este mes estamos con el curso Introducción a la Ciencia de Datos en donde veremos un panorama general de esta disciplina así como una visión detallada de los conceptos y la ruta de aprendizaje requerida para incursionar en esta disciplina. Muy, pero muy recomendado para todos los que estén interesados en iniciarse en este campo. Y ahora sí, comencemos. En videos anteriores hemos hablado de qué es el Machine Learning Operations y en qué consiste el despliegue de un modelo que permite llevarlo de la etapa de desarrollo a la etapa de producción poniéndolo a disposición de un usuario final. Abajo en la descripción les dejo el enlace a estos dos videos. Pero esto es solo el comienzo de la historia porque el reto es lograr que el modelo siga haciendo buenas predicciones una vez entrenado y también después de haberlo desplegado. Para entender esto consideremos un ejemplo hipotético. Supongamos que desarrollamos un modelo para una tienda por departamentos con el fin de predecir cuántos productos de cada categoría se necesitarán cada semana de manera tal que los administradores de la tienda puedan tener con antelación la cantidad suficiente de productos en inventario. Entrenamos, validamos y desplegamos nuestro modelo y todo funciona a la perfección. Durante los primeros meses de uso del modelo se evidencia un incremento en las ventas pues siempre habrá una cantidad suficiente de productos en inventario para las diferentes categorías. Pero aproximadamente un año después los números comienzan a bajar. La demanda de algunos productos era mayor que la predicha por el modelo y para esas categorías no se tenían suficientes productos en la tienda y en otros casos ocurría lo contrario. Para algunos productos la demanda era menor que la predicha por el modelo así que se tenía un exceso de esos productos en inventario. En últimas este modelo que al comienzo funcionó a la perfección con el tiempo se fue degradando y en lugar de generar ingresos comenzó a generar pérdidas para la tienda. Y esto es precisamente un ejemplo de lo que generalmente sucede en la práctica. Desplegar el modelo no es la última fase del proceso porque a lo largo del tiempo puede comenzar a sufrir una degradación en su desempeño. Así que debemos monitorear continuamente su desempeño para detectar posibles fallos y tomar a tiempo los correctivos que sean necesarios. En esencia existen dos grandes grupos de situaciones que pueden explicar la degradación del desempeño los fallos de software y los fallos del modelo. Los fallos de software se deben, como su nombre lo indica, a elementos del software usado durante el despliegue que no funcionan de forma esperada. Por ejemplo pueden ser debidos a ciertas librerías o paquetes que no fueron instalados correctamente durante el despliegue o también debidos a fallos en la CPU o GPU de los servidores. Pero este tipo de fallos no es el que nos interesa, pues dependen de factores externos y podrían ser resueltos más por un ingeniero de software que por uno de machine learning. Los tipos de fallos que nos interesan son los relacionados directamente con el machine learning, que hacen que el modelo no genere buenas predicciones. Estos fallos son más difíciles de detectar y de manejar que los fallos de software, pero deben ser manejados para que el modelo pueda seguir en la etapa de producción. Los fallos más comunes son los que se conocen como variaciones en la distribución y pueden terminar afectando prácticamente a cualquier modelo de machine learning. Cuando entrenamos el modelo en la etapa de desarrollo, lo hicimos usando un set de entrenamiento y prueba. Idealmente en la etapa de producción los datos usados deberían tener las mismas características usadas en la etapa de desarrollo, pero esto es casi imposible de controlar, lo que hace que estas ligeras diferencias terminen afectando el desempeño del modelo. Entre las variaciones en la distribución más comunes tenemos la deriva de datos y la deriva de concepto. En la deriva de datos hay cambios ligeros o significativos en las características o distribución de los datos de entrada con respecto a los usados durante el entrenamiento. Supongamos que entrenamos un modelo de reconocimiento facial, pero únicamente con imágenes tomadas durante el día. Lo llevamos a producción e inicialmente funciona bastante bien, pero de repente comenzamos a usarlo para detectar rostros en la noche y es acá donde comenzamos a ver una degradación del desempeño. En este caso tenemos precisamente una deriva de datos, pues los datos de entrenamiento provienen de una distribución diferente de la de los datos recibidos por el modelo en producción. Por otra parte la deriva de concepto se da cuando la distribución de los datos de entrada permanece sin variación, pero a pesar de ello las predicciones hechas por el modelo comienzan a cambiar. Por ejemplo, supongamos que desarrollamos un modelo para predecir el precio de un inmueble con base en algunas de sus características como el área, el número de habitaciones y el número de baños. Para un inmueble en particular el modelo predice un costo de $500,000 dólares, pero resulta que tiempo después hubo una crisis inmobiliaria y el modelo, a pesar de usar datos de entrada con la misma distribución usada en el entrenamiento, predice ahora que el costo es de $200,000 dólares. Perfecto, ya tenemos claros los principales factores que pueden degradar el desempeño de un modelo, así que podemos definir el monitoreo como la fase del Machine Learning Operations, en la cual medimos diferentes variables de desempeño del modelo y las comparamos con valores de referencia para determinar si continúa generando predicciones adecuadas o si es necesario tomar acciones que busquen mejorar el desempeño. Y hay varias formas de realizar este monitoreo, algunas bastante simples y otras más sofisticadas. La más sencilla de todas es registrando continuamente una métrica global del desempeño del modelo y comparándola con un nivel de referencia. Por ejemplo, si tenemos un sistema de detección de rostros que en la etapa de desarrollo tenía una exactitud del 97%, entonces podemos registrar periódicamente, por ejemplo a diario, este desempeño en el modelo desplegado y si se observa que cae debajo de este nivel de referencia, se podría generar una alerta indicándonos que debemos tomar alguna acción antes de que las cosas sigan empeorando. El inconveniente de realizar el monitoreo utilizando una métrica de desempeño global es que no nos permite ver las razones de fondo que expliquen esa degradación, es decir, si el problema de fondo es una deriva de datos o una deriva de concepto. Una forma más sofisticada de realizar este monitoreo es, por ejemplo, obtener la distribución estadística de los datos de entrada antes del despliegue y periódicamente calcular esta distribución, pero para los datos usados por el modelo desplegado y luego aplicar una prueba estadística para determinar si existen diferencias significativas entre una y otra. En el caso de encontrar diferencias, podríamos concluir que el origen de la degradación está en la deriva de datos o podemos hacer algo similar, pero para las distribuciones de datos a la salida del modelo antes y después del despliegue. De tal forma que si encontramos diferencias estadísticamente significativas, podemos concluir que la degradación del desempeño se debe en este caso a una deriva de concepto. Muy bien, en este video hemos visto que después del despliegue es muy probable que el desempeño del modelo comience a decaer y esto se debe precisamente a que tanto los datos como el entorno en el cual se encuentra el modelo son dinámicos y pueden comenzar a tener variaciones en el tiempo. Así que el monitoreo permite detectar esta degradación en el desempeño bien sea usando métricas globales o técnicas más sofisticadas como la estimación de las distribuciones y pruebas estadísticas aplicadas a los datos de entrada o de salida del modelo. Pero este proceso no termina en el monitoreo porque si confirmamos que efectivamente hay una degradación del desempeño, tenemos que tomar acciones correctivas para garantizar que el modelo siga estando en la fase de producción. Esta fase correctiva se conoce como el mantenimiento del modelo y de ella hablaremos en un próximo video. Así que por el momento esto es todo, no olviden darle un pulgar hacia arriba de me gusta al video y compartirlo con sus amigos y conocidos, pues ya saben que esto me ayudará a seguir desarrollando este tipo de contenido y a que el canal siga creciendo. Y también los invito a suscribirse al canal si aún no lo han hecho y activar la campanita para recibir las notificaciones cada vez que suba nuevo contenido al canal. Por el momento esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0081|Mlops|En videos anteriores hemos visto en qué consiste el despliegue y el monitoreo en el Machine Learning Operations. Y hemos visto que en la práctica es inevitable que el modelo desplegado se degrade con el tiempo generando cada vez peores predicciones. Así que tenemos que actualizar cada cierto tiempo este modelo para combatir esta degradación y seguir generando buenas predicciones. Así que en este video hablaremos del concepto del mantenimiento de un modelo de Machine Learning. Veremos qué es y por qué se requiere, cuándo se debe llevar a cabo y cómo realizarlo. Pero antes de continuar los invito a visitar codificandovids.com, en donde encontrarán la Academia Online con cursos que les permitirán construir su carrera en ciencia de datos, Machine Learning e Inteligencia Artificial. Este mes estamos con el curso de Introducción al Machine Learning, pensado para todos los interesados en comenzar a adentrarse en este campo de la Inteligencia Artificial. Además en codificandovids.com podrán contactarme si están interesados en servicios como el desarrollo de proyectos, asesorías personalizadas y cursos de formación a la medida para personas o empresas. Y ahora sí comencemos. Cuando hablamos del monitoreo en un video anterior, vimos que un modelo de Machine Learning es altamente dinámico, es decir que si cambian los datos o cambia su entorno, cambiará también su desempeño. Es decir que si por ejemplo tenemos un clasificador, una vez entrenamos el modelo y lo desplegamos por primera vez, muy probablemente va a tener una exactitud bastante alta, pero a medida que pasa el tiempo se comienza a degradar este desempeño y progresivamente entonces disminuirá esta exactitud. Entonces lo que va a ocurrir es que tarde o temprano tendremos que actualizar este modelo para poder mantener esos niveles mínimos de desempeño y para permitir que el modelo siga estando en la fase de producción, y esto es precisamente el mantenimiento. Teniendo esto en cuenta, podemos definir el mantenimiento de un modelo de Machine Learning como el proceso mediante el cual actualizamos el modelo desplegado para mantener su desempeño. Bien, teniendo claro el concepto de mantenimiento, la siguiente pregunta que tenemos que responder es ¿Cuándo debemos realizar el mantenimiento de este modelo de Machine Learning? Y la respuesta es, depende, porque debemos tener en cuenta diferentes factores que nos permitirán determinar el momento más adecuado para hacer este mantenimiento. El primer factor a tener en cuenta es la rapidez con que el modelo debe responder a los cambios. Supongamos que construimos un modelo que determina el precio del alquiler de vehículos basado en el comportamiento histórico de los precios de dichos alquileres. Y supongamos que en general el modelo predice con una buena exactitud los precios, pero que de repente hay un gran evento en la ciudad y la demanda se incrementa. Si el modelo no puede responder a este cambio lo suficientemente rápido, incrementando el precio predicho, los usuarios podrían cambiarse a la competencia o se podrían reducir los ingresos para la compañía. En este caso lo ideal es que el modelo se pueda actualizar en cuestión de minutos para responder rápidamente a estos cambios en los datos. Pero por el contrario, si tenemos un sistema de recomendación como el usado por ejemplo por Netflix, para sus sugerencias de contenido nos daremos cuenta de que se requiere un tiempo de al menos unas cuantas horas o incluso algunos días para que el usuario consuma parte de ese contenido y el sistema genere las predicciones correspondientes. En este caso lo ideal es que el modelo se pueda actualizar en periodos de horas o unos pocos días. Otro factor a tener en cuenta al momento de decidir cuándo actualizar el modelo es determinar qué tanto incremento de desempeño alcanzaremos con dicha actualización. Por ejemplo, si en lugar de actualizar el modelo cada mes lo hacemos cada semana o cada día, cuál será la ganancia en desempeño y si esta ganancia en desempeño genera un incremento en los ingresos. Y entonces acá tenemos que poner en una balanza el costo que implica actualizar el modelo, la periodicidad con que lo haremos y las ganancias que esta actualización representaría para la empresa. Y en tercer lugar otro factor que determina cuándo podemos actualizar el modelo es la periodicidad misma de los datos. Si tenemos un e-commerce al estilo de Amazon, tendremos nuevos datos cada vez que una persona adquiera un producto, es decir que al día probablemente tendremos nuevos datos que permitirán actualizar el modelo constantemente. Pero por ejemplo en una tienda física de comercio al por menor, probablemente para un producto en particular solo tendremos datos del nivel de ventas cada semana o cada quincena o incluso cada mes. Así que probablemente tendremos que extender la periodicidad con que actualizaremos el modelo. Bien hasta este punto ya tenemos claro lo que es el mantenimiento de un modelo de Machine Learning y cuándo deberíamos realizarlo. Y entonces lo que veremos ahora serán algunas de las principales estrategias que podemos llevar a cabo para realizar dicho mantenimiento. La idea general es que estas actualizaciones no las vamos a realizar directamente sobre el modelo desplegado, porque esto impediría el acceso por parte del usuario. Entonces en lugar de ello lo que se hace es generar una réplica de ese modelo que está desplegado, realizar las actualizaciones sobre esa réplica y si vemos que esas actualizaciones realizadas sobre la réplica terminan generando un mejor desempeño que el modelo desplegado originalmente, pues lo que hacemos es desplegar esta réplica reemplazando así el modelo original. Y para realizar este mantenimiento se pueden usar dos enfoques, la afinación del modelo existente y el entrenamiento desde cero. En la afinación del modelo existente tomamos como punto de partida el modelo ya desplegado, lo replicamos y continuamos su entrenamiento pero con nuevos datos, muy parecido a lo que se hace en la transferencia de aprendizaje. En el entrenamiento desde cero, como su nombre lo indica lo que hacemos es generar una réplica de la arquitectura original del modelo, recolectamos un nuevo set de datos completo y ejecutamos el entrenamiento desde cero. La ventaja del mantenimiento con afinación del modelo es que generalmente requeriremos recolectar menos datos para poder tener un modelo que se pueda llevar rápidamente a producción, así que este enfoque podrá responder más rápidamente a una degradación repentina en el desempeño del modelo. Por otra parte en el mantenimiento con entrenamiento desde cero, necesitaremos muchos más datos para poder entrenar el modelo completamente, así que este enfoque podría resultar útil si observamos que con la afinación no se alcanza el desempeño esperado. Y con esta fase de mantenimiento ya tenemos todos los elementos que hacen parte del ciclo de vida de un proyecto de Machine Learning, que como vimos cuando iniciamos esta serie sobre el Machine Learning Operations, no se reducen únicamente al entrenamiento del primer modelo. Realmente una vez desplegado el modelo tendremos que hacer un monitoreo continuo y un mantenimiento periódico, y repetir esto una y otra vez para garantizar que el modelo que se encuentra en la etapa de producción satisfaga las necesidades para las cuales fue creado. Muy bien y con este bloque de mantenimiento ya tenemos todos los elementos que hacen parte de lo que se conoce como el Machine Learning Operations. Si aún no lo han hecho los invito a ver los videos anteriores en donde hablamos de todos estos elementos que debemos tener en cuenta para llevar a producción un modelo de Machine Learning. En la descripción les voy a dejar todos los enlaces a esos videos. Así que en los próximos videos de esta serie de Machine Learning Operations ya veremos aplicaciones más prácticas de todos estos conceptos, es decir que tomaremos modelos pre-entrenados, haremos el despliegue, el monitoreo y el mantenimiento usando diferentes tipos de plataformas. Como siempre si les gustó el video no olviden darle un pulgar hacia arriba de me gusta y dejar abajo sus comentarios o todas las preguntas que tengan y por supuesto compartirlo con sus demás amigos y conocidos porque ya saben que esto me ayudará a llegar cada vez a más y más personas con el contenido de este canal. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les avise cada vez que publique nuevo contenido en el canal. Así que por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0082|LLM|En los últimos meses han aparecido sistemas de inteligencia artificial como ChatGPT, GPT-4 o BART que tienen una capacidad impresionante de interpretar y de generar lenguaje natural logrando una comunicación muy similar a como lo hacemos nosotros los seres humanos. Y en el fondo estos sistemas usan lo que se conocen como los grandes modelos de lenguaje o Large Language Models que es el tema precisamente que da comienzo hoy en el canal a una serie de videos en donde poco a poco iremos explorando diferentes aspectos de estos modelos. En esta lista de reproducción iré publicando videos en donde abordaremos no sólo los aspectos técnicos de estos modelos sino también herramientas de software y programación que nos permitirán desarrollar diferentes tipos de soluciones de inteligencia artificial usando esta tecnología y donde también analizaremos las diferentes implicaciones que este tipo de desarrollos pueden tener en nuestra vida cotidiana. Y en este primer video de la playlist comenzaremos con lo esencial que es precisamente entendiendo qué son los Large Language Models. Así que los invito a ver este primer video y a seguir la playlist que iré alimentando poco a poco con videos sobre este tema de los grandes modelos de lenguaje. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online concursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan sólo 10 dólares. Así que listo, comencemos. Comencemos aclarando algunos conceptos básicos que nos permitirán entender posteriormente qué son estos grandes modelos de lenguaje. El concepto más sencillo es el del lenguaje natural que es simplemente el lenguaje escrito o hablado que usamos los seres humanos cuando nos comunicamos. Y de aquí se desprenden los conceptos de procesamiento del lenguaje natural y generación del lenguaje natural que son dos campos de las ciencias computacionales y de la inteligencia artificial. El procesamiento del lenguaje natural busca dotar a los computadores de herramientas que les permitan comprender y procesar el lenguaje humano. Es decir, que a través del texto o la voz podamos comunicarnos con un computador. Pero además de esto es necesario que la comunicación se de en la otra dirección del computador a nosotros los humanos. Que es lo que busca precisamente la generación del lenguaje natural. Que permite dar herramientas a los computadores para que generen texto o incluso voz usando el lenguaje natural. Y la combinación del procesamiento y la generación del lenguaje natural permite desarrollar un amplio rango de aplicaciones como la traducción de voz y texto, el reconocimiento de voz, el análisis de texto y los chatbots entre otros. Y generalmente detrás del procesamiento y la generación del lenguaje natural está lo que se conoce como un modelo de lenguaje. Un modelo de lenguaje permite estimar la probabilidad de ocurrencia de un carácter. Una palabra, una frase o un símbolo dentro del texto. Y esta probabilidad es simplemente un número entre 0 y 1. Que nos indica que tan adecuada o inadecuada puede resultar una secuencia de elementos en el lenguaje natural. Por ejemplo, un modelo de lenguaje podría asignar diferentes probabilidades a diferentes combinaciones de las palabras comió, gato, queso, él y ratón. Por ejemplo, el modelo debería asignar una alta probabilidad a la frase el ratón comió queso, pues se trata de una secuencia de palabras que podríamos encontrar en una conversación entre humanos. Pero a su vez debería darle una baja probabilidad a la frase el queso comió gato, pues se trata de una frase que no tiene mucho sentido para nosotros los humanos. De hecho estos modelos de lenguaje básicos los podemos encontrar por ejemplo en las aplicaciones de chat, en los dispositivos móviles. Pues por ejemplo cuando escribimos uno o varios caracteres o algunas palabras inmediatamente la aplicación nos sugiere los siguientes caracteres o las siguientes palabras para completar esta secuencia de texto. Y para entender cómo funcionan estos grandes modelos es necesario también que entendamos la evolución que han tenido los diferentes modelos de lenguaje a lo largo del tiempo. Los primeros modelos de lenguaje fueron desarrollados hacia los años 90 y la idea era construir modelos capaces de predecir la siguiente palabra en una secuencia de texto, usando diferentes técnicas estadísticas. El problema de estos primeros modelos es que eran entrenados con sets de datos que se conocen como corpus que eran relativamente pequeños y además en esos momentos la capacidad de cómputo también era limitada. Y entonces esto hacía que estos modelos estadísticos no estuviesen en capacidad de capturar las diferentes relaciones que existían entre los elementos de ese texto escrito y por tanto su capacidad predictiva era limitada. Es decir que en la práctica no funcionaban muy bien. Luego a comienzos de los años 2000 se implementaron los primeros modelos de lenguaje basados en redes neuronales donde la idea era entrenar una red neuronal para que aprendiera a predecir la siguiente palabra en una secuencia de texto. Sin embargo este intento tuvo las mismas limitaciones de los modelos estadísticos es decir que al usar muy pocos datos de entrenamiento y tener unas capacidades de cómputo limitadas se obtuvieron modelos que no funcionaban muy bien en la práctica. Pero hacia el año 2010 hubo un cambio muy importante pues en ese momento ya se contaba con equipos de cómputo mucho más potentes y con muchísimos más datos disponibles precisamente en la nube. Así que en ese momento se comenzaron a usar las redes neuronales recurrentes y las redes LSTN que son tipos de redes neuronales especializadas en el procesamiento de secuencias como lo es precisamente el texto. Y con este tipo de redes fue posible entrenar modelos de lenguaje más robustos que mejoraron la capacidad de interpretación y generación del lenguaje natural. Sin embargo este tipo de redes tiene una gran limitación su reducida memoria de largo plazo. Esto quiere decir que funcionaban bien consecuencias de texto relativamente cortas pero cuando el texto era demasiado extenso los modelos no estaban en capacidad de recordar las primeras palabras del mismo y por tanto no eran capaces de procesarlo adecuadamente. Pero en el año 2017 surgieron las redes transformer un tipo de red neuronal que revolucionaría el campo de la generación y procesamiento del lenguaje natural y que es la base de los grandes modelos de lenguaje que conocemos hoy en día. En la descripción del vídeo les voy a dejar un enlace en donde explico en detalle cómo funcionan estas redes transformer pero por ahora lo que nos interesa tener claro es que estas redes a diferencia de las redes recurrentes y LSTN tienen una memoria de largo plazo muchísimo más grande. Así una red transformer puede analizar secuencias de texto mucho más extensas pero no solo eso pues por la forma como procesa los datos también está en capacidad de analizar y codificar numéricamente las relaciones que pueden existir entre los elementos del texto a diferentes niveles. Entonces la red transformer puede analizar relaciones entre las palabras de la secuencia pero también entre las diferentes frases que hacen parte del texto e incluso entre diferentes párrafos. Y esto en últimas permite que la red transformer interprete el texto de forma similar a como lo hacemos los humanos pues para dar significado a una palabra necesitamos tener claro el contexto dentro del cual se encuentra escrita. Las primeras redes transformer fueron entrenadas para realizar tareas muy específicas como por ejemplo la clasificación de sentimientos la traducción de texto o la generación de texto en una temática particular pero en el año 2018 se dio otro gran hito pues varios investigadores de Google desarrollaron BERT un modelo basado precisamente en redes transformer de este modelo también hablo en detalle en un video que encontrarán acá en el canal pero de momento lo que nos interesa es entender claramente cuáles fueron los principales aportes de este desarrollo. El primer aporte fue la idea del pre-entrenamiento que consiste en entrenar un modelo con cientos de millones de parámetros con un corpus gigantesco y para una tarea genérica que consistía en completar palabras faltantes y predecir la siguiente palabra en el texto. Esta fase requiere muchos equipos de cómputo con altísimas capacidades de procesamiento y con este pre-entrenamiento a gran escala fue posible crear un modelo de lenguaje mucho más robusto que los que existían hasta ese momento. El segundo aporte fue el aprendizaje autosupervisado. Al entrenar el modelo para que aprende a completar la palabra faltante o a predecir la siguiente palabra en el texto No es necesario que este set de datos sea etiquetado previamente por un ser humano y al no requerir este tipo de preparación resulta posible recolectar precisamente un set de datos gigantesco. Y el tercer aporte fue la afinación. Una vez pre-entrenado el modelo se puede usar un corpus más pequeño para re-entrenarlo y lograr especializarlo en una tarea determinada como la clasificación de sentimientos, una tarea de pregunta-respuesta o la sumarización por ejemplo. Y en esta fase de afinación se requieren menos equipos de cómputo y menos tiempo de entrenamiento en comparación con la etapa de pre-entrenamiento. Así que BERT fue el primer gran modelo de lenguaje creado después del cual vinieron GPT, versiones 1, 2, 3 y 4, ChatGPT, BART y muchos otros más. Entonces en este punto ya hemos visto implícitamente que es un gran modelo de lenguaje así que vamos un poco más en detalle esta definición. Entonces podemos definir un gran modelo de lenguaje como un modelo de lenguaje basado en redes transformer que contiene cientos o miles de millones de parámetros que ha sido entrenado con un corpus gigantesco y que es de propósito general pero que puede ser afinado para tareas específicas de procesamiento y generación de lenguaje natural. Y acá vale la pena desglosar varios elementos de esta definición. Al hablar de cientos o miles de millones de parámetros y de un corpus de entrenamiento gigantesco estamos diciendo que se requieren altas capacidades de cómputo y muchas horas, días e incluso semanas de entrenamiento. Esto quiere decir que no podemos entrenar uno de estos modelos en nuestro computador personal o ni siquiera contratando un servicio en la nube, pues el costo sería demasiado alto. El término propósito general hace referencia a que el modelo al contener muchos hiperparámetros y haber sido entrenado con un set de datos inmenso es un modelo de lenguaje lo suficientemente robusto que logra modelar adecuadamente los patrones y estructuras del lenguaje humano. Y todo esto le permite al modelo aprender relaciones estadísticas entre palabras, frases y textos enteros para procesar y generar texto de manera coherente y muy similar a como lo haría un ser humano. Y el término afinado hace referencia a que podemos tomar este modelo gigantesco pre-entrenado y entrenarlo con un corpus más pequeño para especializarlo en una tarea específica de procesamiento o generación del lenguaje natural. Muy bien, entonces para finalizar este video hablaremos de algunas habilidades que son únicas de estos grandes modelos de lenguaje. Y la primera de ellas es el aprendizaje en contexto que quiere decir que el modelo es capaz de generar un texto coherente a partir de tan solo una instrucción o de unos cuantos ejemplos. Por ejemplo, le podemos indicar al modelo que queremos que traduzca un texto de inglés a español, pero antes de que lo haga le mostramos unos cuantos ejemplos del resultado esperado. Y con esto el modelo ya logrará generar la respuesta esperada. La segunda habilidad que tienen estos modelos es el seguimiento de instrucciones. Es decir que podemos darle al modelo una instrucción simple y el modelo generará la respuesta esperada. Por ejemplo, podemos indicarle que escriba un poema corto y el modelo generará dicho poema. Y la tercera habilidad es el razonamiento paso a paso. Esto quiere decir que el modelo logra resolver tareas relativamente complejas que involucran múltiples fases de razonamiento. Como por ejemplo problemas matemáticos escritos en lenguaje natural. Por ejemplo, en este sencillo ejercicio le podemos indicar al modelo el número de bolas de tenis que Miguel tenía inicialmente y la cantidad de tubos que compró, indicando cuantas bolas hay en cada tubo. Y al final podemos pedirle al modelo que nos indique cuantas bolas de tenis habrá en total. Y en la respuesta entregada por el modelo se observa claramente el razonamiento paso a paso. Pues descompone el enunciado del problema en sus elementos esenciales y luego ejecuta las operaciones requeridas para obtener la respuesta correcta. Muy bien, en este video hemos entendido la evolución y el significado de lo que es un gran modelo de lenguaje. Así como muchas de sus características. En esencia estos modelos están construidos sobre una red transformer gigantesca que ha sido entrenada con un set de datos inmenso y que permite que se tenga entonces un modelo de lenguaje muy muy robusto. Y esto es precisamente lo que ha permitido desarrollar muchas de estas aplicaciones que hemos visto recientemente, que permiten procesar y generar lenguaje de forma muy similar a como lo hacemos nosotros los seres humanos. Recuerden que en esta nueva lista de reproducción encontrarán inicialmente este video, pero que poco a poco irá alimentándola con videos en donde exploraremos aspectos técnicos de estos grandes modelos de lenguaje, así como herramientas de programación y de software que nos permitirán desarrollar diferentes aplicaciones de inteligencia artificial para el procesamiento del lenguaje natural. Así que si tienen alguna sugerencia de alguna temática que quieren que incluyan esta lista de reproducción me pueden dejar la sugerencia acá abajo en los comentarios del video. Y también si les gustó el video no olviden darle un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos. Pues ya saben que esto me ayudará a llegar cada vez a más y más personas con este tipo de contenidos. Y desde luego si aún no lo han hecho los invito a suscribirse al canal y a activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Así que por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0083|LLM|El surgimiento de los grandes modelos de lenguaje como GPT-4 o BART ha abierto recientemente la posibilidad de crear aplicaciones de inteligencia artificial capaces de interpretar el lenguaje natural y esto ha motivado a muchas personas a aprender a desarrollar este tipo de aplicaciones. Sin embargo, como se trata de un campo que se mueve vertiginosamente resulta muy difícil tener claros cuáles son los conceptos y las herramientas que debemos aprender para comenzar a desarrollar este tipo de aplicaciones. Así que en este vídeo les voy a sugerir una ruta de aprendizaje de lo que considero son los elementos mínimos que deberían aprender para poder comenzar a desarrollar aplicaciones usando estos grandes modelos de lenguaje. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online con cursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Antes de ver la guía en detalle hagamos algunas aclaraciones. La primera de ellas es que la guía está organizada para lo que yo considero son dos tipos de desarrolladores. Un desarrollador nivel básico y un desarrollador nivel avanzado. El desarrollador básico es una persona que quiere comenzar a crear aplicaciones usando estos grandes modelos de lenguaje y que tiene conocimientos de programación, pero que desconoce los fundamentos técnicos y conceptuales de cómo funcionan en el fondo estos modelos. La ventaja en este caso es que la ruta de aprendizaje es más corta, pero la gran desventaja es que este desarrollador no estará preparado para entender los cambios que se vendrán con los nuevos modelos que vayan surgiendo progresivamente. Por otra parte, el desarrollador avanzado es una persona que no solo tiene los conocimientos de programación necesarios, sino también los principios conceptuales de cómo funcionan en el fondo los sistemas existentes que dan origen a estos modelos. La desventaja en este caso es que la ruta de aprendizaje es un poco más larga, pero la gran ventaja es que este tipo de desarrollador estará mucho mejor preparado para implementar aplicaciones con los nuevos modelos que vayan surgiendo. En la ruta de aprendizaje que van a poder descargar en formato PDF en el enlace que les dejo abajo en la descripción, he indicado con una estrella los elementos adicionales que considero debería aprender un desarrollador avanzado, mientras que los bloques que no contienen estrella son comunes para los dos perfiles. Y la segunda aclaración importante es que esta ruta la construí a partir de los avances que hemos tenido hasta este momento, es decir junio de 2023. Pero como este campo de los grandes modelos de lenguaje se mueve tan rápidamente, muy probablemente para el año 2024 actualizaré esta ruta de aprendizaje con los avances que tengamos hasta ese momento. Y la tercera aclaración es que esta ruta hace parte de una lista de reproducción acá en el canal, la cual iré alimentando poco a poco con varios elementos conceptuales y prácticos para la construcción de aplicaciones basadas en estos Large Language Models. Así que sin más preámbulos veamos cuál es esta ruta de aprendizaje. Comencemos con lo esencial que son todos los fundamentos de programación y conceptuales que cualquier desarrollador debería conocer para comenzar a desarrollar aplicaciones usando estos grandes modelos de lenguaje. El primer elemento es comenzar entendiendo qué son los grandes modelos de lenguaje, para lo cual les sugiero un video acá en el canal en donde explico en detalle este concepto. El segundo elemento es saber programar en Python, pues las interfaces para el desarrollo de aplicaciones están basadas precisamente en este lenguaje de programación. Acá en el canal podrán encontrar una ruta de aprendizaje que sugiero para este lenguaje de programación y en el sitio web de la Academia Online podrán encontrar además tres cursos para aprender a programar en Python, niveles básico, intermedio y avanzado. El tercer elemento es comprender conceptos y herramientas básicas de desarrollo web, pues la mayoría de las aplicaciones que podemos desarrollar con estos grandes modelos de lenguaje son precisamente a través de aplicativos web que podemos abrir desde cualquier navegador de internet. Entre estos se encuentran conceptos como las API REST, los procesos de autenticación y librerías de Python para el desarrollo rápido de aplicaciones web como Streamlight o Fast API. El cuarto elemento es entender varios conceptos básicos del procesamiento y generación del lenguaje natural, pues estos grandes modelos de lenguaje están precisamente construidos con este propósito, es decir que acá la idea es comprender en qué consisten tareas como la clasificación de texto, el clustering, el named entity recognition, que permite por ejemplo extraer información del texto escrito y la generación de texto entre otras. Y el quinto elemento que sugiero para los desarrolladores avanzados es comprender los conceptos y la implementación práctica de modelos de deep learning, especialmente los relacionados con las redes transformer y los modelos atencionales, que son la base de los grandes modelos de lenguaje existentes en la actualidad. Si les interesa en la academia online pueden encontrar un curso teórico práctico muy completo enfocado precisamente en este tema. Estas bases técnicas generales son conceptos muy sencillos que considero cualquier desarrollador debería tener claros, pues resultan esenciales cuando queremos construir aplicaciones usando estos grandes modelos de lenguaje. Entre estas bases técnicas tenemos el concepto de tokenización, que es la primera fase del procesamiento del texto por parte de estos modelos, los embeddings, que es la forma como los modelos representan numéricamente los tokens, y los conceptos de temperatura, top-K y top-P, que resultan claves al momento de la generación del texto, así como el concepto de ventanas de contexto, que determina esencialmente la cantidad de texto que puede procesar a la entrada y que puede generar a la salida cada uno de estos modelos. De cada uno de estos conceptos hablaré en detalle en próximos videos, que también van a encontrar en la lista de reproducción que les mencioné hace un momento. Esta parte de la ruta la sugiero únicamente para los desarrolladores avanzados, y aquí la idea es entender desde el punto de vista práctico y conceptual cómo funcionan los principales grandes modelos de lenguaje creados hasta el momento. Así que la idea es entender cómo funciona BERT, el primer gran modelo de lenguaje que existió y que fue creado por Google en 2018, y del cual hablo en detalle en un video que encontrarán acá en el canal, así como los diferentes modelos GPT creados por OpenAI, hasta llegar por supuesto al más reciente, GPT-4, y también entender cómo funciona el modelo Pawn de Google, junto con el uso del aprendizaje por refuerzo que ha permitido el desarrollo de las aplicaciones que más han llamado la atención en los últimos meses, como ChatGPT de OpenAI y BART de Google. De estas dos últimas aplicaciones hablaré en próximos videos que también encontrarán próximamente en esta lista de reproducción. Además, sugiero entender cómo funcionan varios de los modelos de código abierto, y aunque el panorama es inmenso, sugiero comenzar por ejemplo con Ljama, GPT-4ALL y Alpaca, así como algunas de sus variantes. Este tercer elemento de la ruta también está enfocado únicamente para los desarrolladores avanzados, y aquí la idea es entender cómo afinar estos grandes modelos de lenguaje. La idea de la afinación es sencilla, y consiste en tomar cualquiera de estos modelos pre-entrenados y reentrenarlo con un set de datos más pequeño para lograr especializarlo en una tarea o en una temática particular, usando lo que se conoce como la transferencia de aprendizaje. Esta afinación resulta útil cuando queremos contar con un modelo que genere respuestas de forma más rápida, lo que se conoce como un modelo de menor latencia, o para reducir la extensión del texto introducido por el usuario al momento de realizar consultas al modelo. De este proceso de afinación también hablaré en detalle en un próximo video que también era parte de esta playlist. El cuarto elemento de la ruta es una alternativa a la afinación que acabo de hacer, y se conoce como el prompt engineering, y este procedimiento busca ser algo similar a la afinación, es decir, que el modelo pre-entrenado funcione adecuadamente en una tarea para la cual no fue entrenado inicialmente. Sin embargo, en este caso no se requiere el reentrenamiento del modelo, y en su lugar lo que hacemos es usar técnicas que nos permitan escribir el texto que ingresamos al modelo con la estructura adecuada para lograr generar las respuestas adecuadas dependiendo del aplicativo que tengamos. La elección de la alternativa más adecuada, es decir, entre la afinación o el prompt engineering, dependerá del problema que queramos resolver o incluso de restricciones económicas, pues en general ambos procesos cuestan dinero, aunque de estos detalles también hablaré más adelante en otro video. Y habiendo completado las fases anteriores, en este punto ya estamos listos para comenzar a entender cómo funcionan las herramientas de software que se pueden utilizar para realizar consultas. Las herramientas de software que nos permitirán construir diferentes tipos de aplicaciones usando estos grandes modelos de lenguaje. Estamos hablando de las APIs, que esencialmente son herramientas de software que permiten que nuestra aplicación pueda conversar, es decir, enviar y recibir solicitudes a los grandes modelos pre-entrenados y disponibles en la nube, así como librerías de Python que permiten interconectar diferentes fases de procesamiento de los datos con los grandes modelos de lenguaje, para desarrollar una aplicación en particular. En este caso existen dos vertientes, las APIs de pago y las librerías de código abierto. En el caso de las APIs de pago, sugiero aprenderlas de OpenAI, y en el caso de las librerías de código abierto, sugiero a los desarrolladores avanzados aprender a usar Lankchain y las herramientas de Hogging Face. De ambos tipos de herramientas hablaré también más adelante en próximos videos. Y habiendo desarrollado todas estas habilidades de programación, de manejo de librerías, de manejo de herramientas de software, y conociendo todos los conceptos y herramientas técnicas de estos grandes modelos de lenguaje, ya estamos listos para comenzar a desarrollar diferentes tipos de aplicaciones. Y en este caso lo que sugiero es, primero identificar un problema a resolver, que guarde relación precisamente con el procesamiento del lenguaje natural, luego elegir uno de todos estos modelos que existen, así como cualquiera de las herramientas de software o herramientas de programación o librerías mencionadas anteriormente, y luego entonces construir una aplicación pensando precisamente en un usuario final. Y aquí existe un amplio abanico de áreas de aplicación. Se podrían desarrollar por ejemplo soluciones de búsqueda semántica que consisten en encontrar información en un texto relativamente extenso a partir de consultas realizadas usando el lenguaje natural. O el modelamiento de tópicos, donde lo que se busca es identificar y extraer automáticamente tópicos o grupos de temáticas a partir de una colección de documentos. Así como aplicaciones más conocidas como la síntesis de texto, y desde luego los chatbots, los asistentes virtuales, los agentes inteligentes o los sistemas de generación de contenido, a los cuales dedicaremos videos más adelante acá en el canal. Muy bien, espero que esta ruta que les acabo de compartir les sirva como punto de partida si están interesados en comenzar a desarrollar aplicaciones de procesamiento del lenguaje natural con estos grandes modelos de lenguaje. Realmente no resulta fácil consolidar una única ruta de aprendizaje. Pues como les he venido contando, este campo de los grandes modelos de lenguaje es muy, pero muy dinámico. Y cada semana estamos viendo nuevos desarrollos y nuevos modelos y diferentes tipos de aplicaciones. Sin embargo, en esta ruta están los elementos técnicos y conceptuales que considero esenciales y que además les darán las bases para poder estar a la par con los nuevos desarrollos que vayan apareciendo poco a poco. Recuerden que este video hace parte de una lista de reproducción de grandes modelos de lenguaje que van a encontrar acá en el canal y que esta lista la irá alimentando poco a poco, con contenido relacionado con los grandes modelos de lenguaje y con muchos elementos que acabo de mencionar precisamente acá en esta ruta de aprendizaje. Y también recuerden que pueden visitar la Academia Online de CodificandoVids.com en donde encontrarán cursos relacionados con muchos de estos temas que acabo de mencionar en esta ruta de aprendizaje, como por ejemplo la programación en Python o el curso Teórico Práctico de Deep Learning. Y no olviden dejarme acá abajo sus comentarios o dudas acerca de lo que acabo de contarles. Y si les gustó el video, los invito a compartirlo con todos sus amigos y conocidos para que me ayuden a llevar este contenido a cada vez más y más personas. Y también los invito a suscribirse al canal y activar la campanita para recibir notificaciones cada vez que publique un nuevo video. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video. ¡Suscríbete al canal!
DOC0084|LLM|Muchos hemos oído hablar del término token cuando nos referimos a los grandes modelos de lenguaje, como por ejemplo GPT-4, o cuando usamos aplicaciones como ChatGPT. Y este concepto de los tokens resulta fundamental para poder usar este tipo de modelos, pues lo que hacen en lugar de procesar directamente el texto es recibir y generar precisamente tokens. Así que en este video veremos todos los elementos esenciales relacionados con este concepto de los tokens. Veremos qué son, cómo se generan y por qué resultan esenciales cuando queremos usar estos grandes modelos de lenguaje. Así que prepárense porque en este video vamos a ver varios conceptos que resultan muy sencillos, pero que a la vez son fundamentales para poder construir aplicaciones basadas en estos grandes modelos de lenguaje. Pero antes de comenzarlos invito a visitar codificandobits.com, en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning, que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Como lo vimos en el primer video que encontrarán acá en esta playlist, los grandes modelos de lenguaje están basados en las redes Transformer, que son un tipo de red neuronal capaz de procesar datos, pero estos datos deben estar precisamente en formato numérico. Sin embargo estos modelos están diseñados para procesar el lenguaje natural, es decir que a la entrada reciben texto y a la salida generan texto. Así que es necesario que internamente haya algún tipo de procesamiento que permita tomar ese texto que introduce el usuario, convertirlo al formato numérico para que esta red neuronal lo pueda procesar, luego esta red neuronal generará unas predicciones también en formato numérico y esos números tendrán que ser convertidos a texto para que nosotros podamos entender la respuesta generada por el modelo. Esta equivalencia de texto a números o de números a texto se conoce como el vocabulario del modelo y existen diferentes formas de construir este vocabulario. Una primera alternativa sería simplemente representar cada carácter en el texto con un número e ingresar esta secuencia de números al modelo, es decir codificar el texto a nivel de caracteres. Pero este enfoque tiene varios inconvenientes. Uno de los principales problemas es que si el texto de entrada es extenso, es decir, si tiene miles o millones de caracteres, tendríamos una secuencia de números inmensa que requeriría una gran capacidad de cómputo y de memoria para que el modelo pueda procesarla. Es decir, que en términos simples no resulta eficiente representar cada carácter con un número. La segunda alternativa es representar cada palabra con un número, es decir, codificar el texto a nivel de palabras. Este enfoque es un poco más eficiente que la codificación a nivel de caracteres. Por ejemplo, en el texto, el gato duerme plácidamente, tenemos cuatro palabras y un total de 27 caracteres, incluyendo espacios en blanco. La codificación a nivel de palabras requeriría solo cuatro números, mientras que a nivel de caracteres requeriría 27 números. Sin embargo, esta codificación a nivel de palabras tampoco es la mejor solución. Uno de los problemas es que los lenguajes naturales tienen una inmensa cantidad de palabras únicas, así que al tener muchas palabras seguiríamos necesitando un vocabulario inmenso. Otro problema es qué hacer con palabras que no estén en el vocabulario. Si dentro del texto aparece una palabra desconocida que no esté inicialmente en el vocabulario, el modelo no podrá procesarla adecuadamente. Y otro problema es que en el lenguaje natural muchas palabras están relacionadas a través de lo que se conoce como las variantes morfológicas. Por ejemplo, las palabras gato, gata, gatito, gatos, gatitos, tienen una secuencia de caracteres que es común a todas, las letras G, A y T. Y de alguna forma el vocabulario usado debería aprovechar estas similitudes para hacer la codificación. Así que la idea al momento de representar el texto numéricamente a través del vocabulario es que esta codificación sea lo más compacta posible, para evitar presentarle demasiados datos al modelo. Y además que esta codificación aproveche las particularidades del lenguaje. Y acá aparece precisamente el concepto de tokens, del cual vamos a hablar en detalle a continuación. Acabamos de ver que no siempre la codificación del texto a nivel de caracteres o a nivel de palabras resulta siendo la más adecuada. Así que en los grandes modelos de lenguaje lo que se usa es la codificación del texto a través de tokens, que son simplemente el resultado de tomar el texto y partirlo en pequeños pedazos, teniendo en cuenta las particularidades del lenguaje que se está usando. Entonces un token puede ser una palabra, una sub-palabra, es decir, una parte de una palabra o por ejemplo un signo de puntuación. Es decir que esta codificación con tokens es un punto intermedio entre la codificación a nivel de caracteres y la codificación a nivel de palabra de la cual hablamos hace un momento. Entendamos este concepto con algunos ejemplos. La palabra una es tokenizada, es decir, es convertida a tokens como una. Es decir, es exactamente igual a un token. Pero la palabra puntos es tokenizada con dos tokens, punt y os. Y esto se debe a que la secuencia de caracteres punt aparecen varias palabras del español y por tanto por ser relativamente común se codifica como un solo token. Por ejemplo podemos encontrar las palabras puntillas y puntapié que tienen exactamente la misma secuencia inicial de caracteres, P, U, N y T. Así dependiendo del lenguaje y de la palabra en particular un token puede ser exactamente una palabra o una porción de una palabra. Y esta codificación termina siendo más eficiente que la codificación a nivel de palabra o a nivel de caracteres y resuelve inconvenientes como la aparición de vocabularios inmensos que requieren más capacidad de memoria y más capacidad de procesamiento o problemas como la presencia de palabras desconocidas que no se encuentren en el vocabulario. Entonces lo que hacen los algoritmos que permiten obtener estos tokens es analizar una gran cantidad de texto que se conoce como el corpus y buscar pequeñas secuencias de caracteres que se repitan dentro de ese texto. Y estos tokens son precisamente la unidad fundamental de información que es procesada y generada por estos grandes modelos de lenguaje. La tokenización es precisamente el proceso de convertir el texto en tokens. Así que entendamos este proceso de tokenización a través de varios ejemplos. En primer lugar es importante tener en cuenta que la tokenización implica el uso de un vocabulario. Es decir que cada uno de los tokens tendrá un número equivalente y este es el número que precisamente será procesado de forma interna por la red transformer o la red neuronal que conforma este modelo de lenguaje y que luego de procesarlo generará un nuevo token también numérico que posteriormente tendrá que ser convertido al formato de texto a través de ese vocabulario para que nosotros podamos entender la respuesta generada por el modelo. Así en el texto el gato duerme tendremos un total de seis tokens cuyos equivalentes numéricos son 417, 308, 5549, 7043, 263 y 1326. En segundo lugar es importante tener en cuenta que la tokenización se lleva a cabo incluyendo los espacios en blanco. Por ejemplo en el texto este y este la primera palabra este no contiene un espacio en blanco al comienzo y por tanto es un token diferente al de la segunda palabra este que sí contiene un espacio en blanco al comienzo. Además como lo mencioné anteriormente una palabra puede representarse con varios tokens. Por ejemplo la palabra esto se representa con dos tokens EST el primer token y O el segundo token. También muchas veces en el texto escrito podremos encontrar emojis. Estos emojis también son representados a través de tokens y al igual que con el texto el número de tokens usados dependerá de qué tan frecuente sea el uso del emoji en el lenguaje de interés. Por ejemplo el emoji de la cara sonriente requiere dos tokens para ser representado mientras que el emoji con la mano hacia arriba requiere cuatro tokens. Y además es posible encontrar números en el texto escrito. En este caso durante la tokenización lo que puede ocurrir es que secuencias de números encontradas comúnmente en el texto sean representadas con un token. Por ejemplo la secuencia de números 1 2 3 4 5 6 7 8 9 y 0 es representada con cuatro tokens. Uno para el grupo 1 2 3 otro para el grupo 4 5 otro para el grupo 6 7 8 y otro para el grupo 9 y 0. Además la equivalencia texto tokens depende del idioma. Así por ejemplo la frase Pone una canción que me haga sonreír tiene 35 caracteres y 7 palabras. En español esta frase es representada con 14 tokens pero en inglés requeriría tan sólo 8 mientras que en chino se necesitan 15 tokens. En general esta equivalencia dependerá del lenguaje en el que se haga el procesamiento y del tipo de modelo que estemos usando pues cada modelo usa un algoritmo de tokenización en particular. Por ejemplo para GPT-4 en promedio cada token equivale a 7 caracteres en inglés 9 caracteres en español y 12 caracteres en chino. Y entender este concepto de tokens resulta esencial para poder construir aplicaciones con estos grandes modelos de lenguaje que se encuentran disponibles. Y la primera razón es porque estos modelos tienen un límite máximo de tokens a la entrada y a la salida. Por ejemplo, ChatGPT se basa en el modelo GPT 3.5 Turbo el cual tiene un límite de 4096 tokens. Esto quiere decir que si sumamos los tokens de entrada y de salida al momento de usar la aplicación el valor máximo será precisamente de 4096. Así que si por ejemplo tenemos un texto en español de 25000 caracteres a la entrada de ChatGPT esto equivaldrá aproximadamente a 2800 tokens. Por tanto a la salida la aplicación podrá generar máximo 1296 tokens es decir, unos 11600 caracteres. Y la segunda razón por la cual resulta importante entender que son los tokens es porque muchos de estos modelos tienen precisamente un costo asociado al número de tokens que usemos o que introduzcamos a la entrada y que esperemos generar a la salida de estos modelos. Es decir, que el costo en dinero de utilizar el modelo se va a incrementar a medida que usemos más tokens es decir, más texto tanto a la entrada como a la salida. Por ejemplo, supongamos que queremos usar el modelo GPT 3.5 Turbo en nuestro sitio web para crear una aplicación de chat para nuestros usuarios. En este caso tendremos que pagar a OpenAI por el uso de este modelo. Si un usuario ingresa a nuestro sitio web e introduce un texto en español de 10.000 caracteres a la aplicación lo que es aproximadamente igual a unos 1100 tokens y el modelo genera una respuesta también de unos 10.000 caracteres entonces el costo de procesamiento será según los precios actuales de uso establecidos por OpenAI 0.0015 dólares por cada 1000 tokens de entrada. Es decir, que el costo de procesar los 1100 tokens de entrada serán 0.00165 dólares y 0.002 dólares por cada 1000 tokens de salida. Es decir, que el costo para generar los 1100 tokens de salida serán 0.0022 dólares. Así que para esta solicitud simple el costo total serían apenas 0.0038 dólares. Esto parece muy económico pero tengamos en cuenta que estamos considerando solo una pregunta del usuario y una respuesta generada por el modelo. Pero si la conversación tiene 10 preguntas y 10 respuestas y asumimos que cada una tiene más o menos la misma cantidad de tokens ya estaríamos hablando de 0.038 dólares. Y si asumimos que en lugar de un usuario tenemos por ejemplo 50 usuarios interactuando diariamente con la aplicación el costo sería de casi 2 dólares diarios. Así que es evidente que entre más solicitudes hagamos al modelo tendremos más tokens procesados que se sumarán al costo total de uso del modelo. Muy bien, acabamos de revisar los conceptos esenciales acerca de los tokens y esto nos permite entender por qué resultan fundamentales al momento de usar estos grandes modelos de lenguaje. Hemos visto que en el lenguaje natural un token puede ser una palabra, una subpalabra o también nos puede permitir representar absolutamente cualquier elemento dentro del texto escrito como por ejemplo emojis, números o signos de puntuación. También hemos visto que el número total de tokens usados para representar el texto va a depender en últimas del idioma y que hay idiomas para los cuales el número promedio de tokens es menor como por ejemplo en el caso del inglés y otros para los cuales el número de tokens es mucho mayor como por ejemplo el chino. Y finalmente es importantísimo entender que el número de tokens determinará en última instancia el costo que pueda tener el uso de estos modelos para el procesamiento y generación de texto. Y que además los tokens determinan ese límite máximo de datos que podremos ingresar al modelo y que podremos esperar como respuesta del modelo al momento de la generación de texto. Si tienen alguna duda de lo que acabamos de ver en este video no olviden dejarla abajo en los comentarios y recuerden que si les gustó el video los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con todos sus amigos y conocidos pues esto me ayudará a llegar a cada vez más y más personas con este tipo de contenido. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0085|LLM|Los grandes modelos de lenguaje como GPT-4 o BART, en realidad no procesan el texto como lo vemos nosotros los seres humanos, y en lugar de ello lo que hacen es transformarlo a una representación numérica que se conoce como embeddings, así que internamente estos modelos lo que hacen es procesar y generar embeddings para posteriormente realizar diferentes tareas de procesamiento del lenguaje natural. Entonces en este vídeo vamos a entender qué son estos embeddings y cómo se utilizan para desarrollar diferentes tipos de aplicaciones de procesamiento y generación del lenguaje natural usando estos grandes modelos de lenguaje. Pero antes de comenzar los invito a visitar codificandovids.com, en donde encontrarán la academia online con cursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Para entender qué son los embeddings comencemos con un ejemplo sencillo, supongamos que queremos describir la apariencia de dos personas diferentes usando algunas características. Podemos decir por ejemplo que la primera persona tiene una altura de unos 72 centímetros, el cabello corto y de color negro, su peso es 67 kilogramos y sus ojos son de color café. Y la segunda persona tiene una altura de 183 centímetros, el cabello largo y de color castaño, su peso es 78 kilogramos y sus ojos son de color verde. Así que lo que hemos hecho ha sido tomar el dato original, es decir cada persona, y lo hemos representado a través de cuatro características. Y si organizamos estas cuatro características en un listado, lo que tendremos será un vector de cuatro elementos. Ahora veamos cómo lograríamos hacer algo similar pero usando redes neuronales, que son la base de los grandes modelos de lenguaje. Por ejemplo, si queremos hacer un sistema de verificación de identidad, lo que necesitamos es comparar la imagen del rostro de una persona con imágenes de referencia en una base de datos, para determinar si la persona es o no quien dice ser. Pues en estos sistemas realmente no comparamos directamente las imágenes, en lugar de ello lo que se hace es construir una red convolucional, que es un tipo de red neuronal, que procesa estas imágenes, y que a la salida genera una representación vectorial de la imagen de entrada. Y esta representación vectorial, que es simplemente un listado o un arreglo de números, es precisamente lo que se conoce como un embedding. Y la idea es que imágenes con rostros similares tendrán embeddings similares, e imágenes con rostros diferentes tendrán embeddings diferentes. Y este principio nos permitirá construir precisamente el sistema de verificación de identidad. Y este mismo principio lo podemos aplicar para el procesamiento y análisis del lenguaje natural. Así que en resumen y para lo que nos interesa en este video, un embedding es una representación vectorial del dato de entrada y que se obtiene tras el entrenamiento de una red neuronal. Como acabamos de ver el texto también puede ser procesado por algún tipo de red neuronal para generar embeddings y desarrollar aplicaciones como por ejemplo el análisis de sentimientos, la generación de texto o los chatbots. Y un enfoque utilizado hace algunos años era generar embeddings a partir de palabras. Es decir que la idea era tomar cada palabra dentro del texto, generar un token que es una representación numérica inicial de esa palabra, de esos tokens hablamos anteriormente en algún video, y después tomar ese token y convertirlo precisamente en un embedding o una representación vectorial de esa palabra. Con este tipo de embeddings, usualmente palabras con significados similares, como por ejemplo manzana y pera, tendrán embeddings similares, pero que a su vez son diferentes de los embeddings para las palabras perro o gato, que tienen un significado diferente. Sin embargo en el lenguaje natural las palabras no se encuentran aisladas, sino que se encuentran en un contexto, en una frase por ejemplo. Y dependiendo de ese contexto, una palabra determinada podría tener diferentes significados. Por ejemplo en las frases debo ir al banco a retirar dinero y estoy sentado en el banco, la palabra banco tiene significados completamente diferentes. Y ese significado lo establecemos nosotros los humanos a partir precisamente del contexto, leyendo la totalidad de la frase. Esto quiere decir que el uso de embeddings a nivel de palabra no es capaz de capturar la información del contexto y por tanto en las frases anteriores la palabra banco tendría el mismo embedding en ambos casos, lo cual no resulta ideal. Así que como alternativa al uso de embeddings a nivel de palabra, los grandes modelos de lenguaje existentes actualmente hacen uso de las redes transformer que permiten generar embeddings capaces de capturar la información del contexto, es decir de la totalidad del texto. De estas redes transformer hablo en detalle en un video anterior, pero para resumir la idea principal podemos decir que una red transformer es un tipo de red neuronal diseñada específicamente para procesar secuencias como el texto y que incorpora algo que se conoce como un mecanismo atencional. Al momento de decidir cuál será el embedding para una palabra o una frase, lo hace analizando la relación entre la palabra o la frase y los demás elementos del texto a diferentes niveles. Así que de alguna manera los embeddings usados por estos grandes modelos de lenguaje intentan imitar la forma como nosotros los seres humanos interpretamos el texto. Con la diferencia de que estos modelos generan internamente representaciones vectoriales, es decir, arreglos de números correspondientes a esa palabra, a esa frase o a ese texto que se está procesando. La ventaja de esto es que con los embeddings generados, frases o textos que tengan significados similares, tendrán representaciones vectoriales similares. Por ejemplo, frases como me gustaría conocer las tarifas de vuelos entre Madrid y Milán o quiero encontrar tiquetes de primera clase ida y vuelta de Nueva York a Miami tienen embeddings similares, pues ambas se refieren, por ejemplo, al concepto de tiquetes de avión. Mientras que frases como que aerolíneas vuelan de Los Ángeles a Tokio o de todas las aerolíneas que llegan a Hong Kong, cuál es la más económica, se refieren a un concepto relacionado con el anterior, pero que es ligeramente diferente, pues se enfoca más en la temática de aerolíneas. Lo anterior quiere decir que embeddings que son numéricamente similares equivalen a frases semánticamente similares, es decir, con significados muy parecidos. En la práctica, los diferentes grandes modelos de lenguaje disponibles utilizan diferentes tamaños para esos embeddings, dependiendo de la forma como hayan sido entrenados. Por ejemplo, los últimos modelos de OpenAI usan embeddings de 1536 o 2048 elementos, mientras otros, como el LLAMA por ejemplo, tienen embeddings que oscilan entre los 4096 y los 8192 elementos. Bien, como hemos visto, hasta ahora los embeddings del texto son la materia prima de los grandes modelos de lenguaje y son lo que ha permitido el desarrollo de muchas de las aplicaciones de procesamiento de lenguaje natural que hemos visto recientemente. Y los tipos de aplicaciones que se pueden construir con este concepto van más allá incluso de las aplicaciones más conocidas, como por ejemplo, ChatGPT. Por ejemplo, podemos usar los embeddings para realizar lo que se conoce como Es decir, que podemos tomar un texto fuente y obtener su representación a través de embeddings y luego un usuario puede escribir una búsqueda en lenguaje natural, es decir, tal como lo haría por ejemplo en una conversación, que también se representa con embeddings. Luego el sistema realiza la comparación vectorial de ambos embeddings y genera la respuesta a la búsqueda también en lenguaje natural. También se pueden realizar las compasiones de los dos embeddings también se pueden realizar tareas de clustering o agrupamiento. Es decir, que podemos usar los embeddings del texto para pedirle a uno de estos modelos de lenguaje que encuentre diferentes temáticas o tópicos o agrupaciones de conceptos que se encuentren dentro del texto. O podemos usar los embeddings para generar sistemas de síntesis de texto, es decir, que extraigan las ideas principales de un texto y generen a la salida un texto más corto, es decir, un resumen, pero usando el lenguaje natural. O también podríamos implementar aplicaciones de recomendación de contenido. Por ejemplo, con base en los artículos que más busca o que más lee un usuario, podríamos recomendar nuevo contenido similar en la web para su lectura. E internamente todo este procesamiento se puede hacer precisamente a través de los embeddings. Así que el rango de aplicaciones que tienen los embeddings generados por los grandes modelos de lenguaje es realmente inmenso y en los próximos vídeos de esta serie comenzaremos a ver algunas de esas aplicaciones. Muy bien, acabamos de ver que son los embeddings, que es la forma como los grandes modelos de lenguaje aprenden a representar el texto al momento de procesarlo. En últimas, un embedding es simplemente un vector, es decir, un arreglo de números que captura la información esencial de la totalidad del texto y que permite no solo que ese texto sea procesado por un computador, sino también desarrollar múltiples aplicaciones de procesamiento del lenguaje natural. Si quieren entender internamente cómo es este proceso de generación de los embeddings a partir de una Red Transformer que es la base de los grandes modelos de lenguaje, les sugiero ver el vídeo sobre Red Transformer en donde explico esto detalladamente. Y como siempre, si tienen alguna duda, comentario o sugerencia sobre esta serie de vídeos, lo pueden dejar abajo en los comentarios. Además, si les gustó el vídeo, no olviden darle un pulgar hacia arriba de me gusta y compartirlo con sus amigos y conocidos, pues esto me ayudará a seguir llevando este tipo de contenido cada vez a más y más personas. Y si aún no lo han hecho, los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique un nuevo vídeo. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo vídeo.
DOC0086|LLM|El término ventana de contexto es un concepto muy muy importante en los grandes modelos de lenguaje, pues en últimas determina la cantidad de texto que estos modelos pueden procesar y generar, así que es fundamental entender este concepto si queremos desarrollar aplicaciones usando estos grandes modelos de lenguaje. Entonces en este video vamos a entender, ¿qué es la ventana de contexto? ¿qué ocurre cuando superamos los límites establecidos por esta ventana? y ¿qué alternativas tenemos cuando queremos procesar texto que es más extenso que el límite establecido por esta ventana de contexto? Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la academia online con cursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Para entender qué son las ventanas de contexto necesitamos primero entender qué significa el concepto de contexto cuando hablamos de los grandes modelos de lenguaje. Recordemos que en un video anterior vimos que estos grandes modelos de lenguaje se basan en las redes transformer, un tipo de red neuronal especializada en el procesamiento de secuencias, como lo es precisamente el texto. Y recordemos que al momento de procesar el texto estas redes no analizan cada palabra de manera individual, sino que por el contrario tienen en cuenta la relación de cada palabra con las otras palabras del texto. Y dependiendo de esto codifica numéricamente esta información para luego realizar el procesamiento. Por ejemplo, en la frase el perro está jugando mientras que el gato acaba de comer y ahora duerme plácidamente. Para determinar a qué sujeto se refiere la palabra duerme, la red transformer debe analizar la totalidad del texto y las relaciones entre palabras a diferentes niveles para determinar en últimas que el sujeto es el gato y no el perro. Así que en últimas el contexto es la relación que existe entre las diferentes palabras y que permite interpretar y codificar adecuadamente la información del texto analizado por el modelo. Teniendo claro este sencillo concepto, ahora sí podemos ver qué son las ventanas de contexto. De forma sencilla podemos definir la ventana de contexto de un gran modelo de lenguaje como la cantidad de tokens que el modelo puede procesar al momento de interpretar una secuencia de texto. Y acá es necesario que veamos algunos detalles de esta definición. En primer lugar hablemos del término tokens, al cual le dediqué un video anterior. Recordemos que un gran modelo de lenguaje antes de procesar el texto lo divide en pequeños segmentos que pueden corresponder a una palabra o a una porción de una palabra. Estos segmentos son precisamente los tokens y la cantidad de tokens que se obtendrá a partir de un texto depende del idioma. Por ejemplo para GPT-4 en promedio un token equivale a 7 caracteres en inglés a 9 en español y a 12 en chino. En segundo lugar el término procesamiento se refiere a la cantidad de tokens que el modelo puede recibir a la entrada y generar a la salida. Así que en últimas la ventana de contexto es el número total de tokens que el modelo de lenguaje podrá procesar en un momento determinado. Entendamos esto con un ejemplo. El modelo GPT-3.5 de OpenAI, que es la base de la aplicación ChatGPT tiene una ventana de contexto de 4097 tokens. Esto quiere decir que si escribo una pregunta y espero una respuesta de esta aplicación al sumar el texto introducido y generado este no podrá sobrepasar los 4097 tokens que son poco menos de 37 mil caracteres en español. Así que la ventana de contexto nos impone un límite en la cantidad de texto que podemos introducir y esperar a la salida del modelo. Y esto es súper importante cuando queremos desarrollar aplicaciones con este tipo de modelos pues simplemente nos indica que no podemos introducir una cantidad arbitraria de texto y esperar que mágicamente el modelo nos genere la respuesta esperada. Bien, en este punto ya tenemos claro que es la ventana de contexto pero qué pasaría por ejemplo si en una aplicación determinada introducimos al modelo un texto que supere ese límite establecido por la ventana de contexto pues simplemente lo que ocurriría es que en primer lugar el modelo no podría ni siquiera procesar el texto de entrada y mucho menos generar una respuesta. Por ejemplo, supongamos que queremos usar ChatGPT para que nos genere el resumen del texto completo de Don Quijote de la Mancha. Entonces copiamos y pegamos el texto completo y le pedimos que haga el resumen. Y al hacer la solicitud de ChatGPT vemos que nos genera un mensaje de error indicando que el texto introducido es demasiado extenso. Es decir que ni siquiera nos genera una respuesta. Lo que ocurre en este caso es que el texto completo de Don Quijote de la Mancha contiene más de 2 millones de caracteres, es decir más de 226 mil tokens. Y como la ventana de contexto de GPT 3.5, el modelo usado por ChatGPT es de 4096 tokens pues simplemente no resulta posible procesar esa cantidad de texto. Y también lo que puede ocurrir generalmente es que cuando ese texto supere el límite de la ventana de contexto al ingresarlo al modelo pues este va a truncar el texto, es decir lo va a recortar para garantizar que quepa dentro de esa ventana de contexto y que haya un espacio suficiente para generar la respuesta. Sin embargo en este caso lo más probable es que la respuesta del modelo no sea la adecuada porque el texto de entrada está incompleto. Acabamos de ver que es clave que el texto a procesar o a generar por parte del modelo sea acorde con el tamaño de la ventana de contexto. Pero también puede ocurrir que tengamos un texto demasiado extenso y que queramos procesar ese texto con un modelo determinado. Así que el primer paso es asegurarnos de que el modelo que vamos a utilizar para procesar ese texto tenga una ventana de contexto del tamaño adecuado para la longitud del texto que queremos procesar. Por ejemplo, modelos como GPT 3.5 y 4.0 tienen ventanas de contexto que van de los 4096 a los 32768 tokens. Pero recientemente modelos como Cloth de la empresa Anthropic tienen ventanas de contexto de hasta 100.000 tokens. Así que en principio existen diferentes alternativas y tamaños de ventanas de contexto que se podrían ajustar a nuestras necesidades. Sin embargo muchas veces el texto que queremos procesar puede incluso sobrepasar ese límite de los modelos existentes. Así que en este caso podemos usar otras alternativas para intentar hacer este procesamiento. La primera de ellas consiste simplemente en dividir el texto en pequeños fragmentos, lo que se conoce como chunking, donde cada fragmento no supera el tamaño de la ventana de contexto del modelo que estemos usando. Otra alternativa es combinar el chunking con la generación de resúmenes. Por cada fragmento de texto se genera un breve resumen y luego todos los resúmenes se concatenan en un nuevo texto de menor extensión que el original y que puede caber en la ventana de contexto. Y una tercera alternativa es el uso de bases de datos vectoriales. En un próximo video hablaré de este tipo de bases de datos, pero la idea general es que lo que se hace es tomar el texto extenso y representarlo usando lo que se conoce como embeddings, que son simplemente vectores o arreglos de números. De estos embeddings también hablé en un video anterior, pero lo importante acá es que al usarlos, logramos representar el texto de forma compacta a través de vectorias. Y generalmente con estas técnicas lograremos evitar las limitaciones impuestas por las ventanas de contexto. Muy bien, acabamos de entender que son las ventanas de contexto de estos grandes modelos de lenguaje y por qué resultan importantes cuando queremos construir aplicaciones con este tipo de modelos. En últimas la idea general es que todos los modelos de lenguaje tienen un límite máximo de tokens que pueden procesar y generar. Así que no podemos ni introducir ni generar ni generar así que no podemos ni introducir ni esperar que generen una cantidad de texto totalmente arbitraria. Y este límite lo pone precisamente la ventana de contexto. Entonces si sobrepasamos este límite lo que generalmente hará el modelo será truncar el texto de entrada y por tanto terminará generando una respuesta que no es la que estamos esperando. Así que al momento de desarrollar una aplicación lo que les sugiero es primero determinar el tamaño del texto que queremos introducir y generar, ese tamaño la idea es medirlo en tokens y con base en esto es coger un modelo de lenguaje que tenga una ventana de contexto adecuada para ese tamaño. Y si en todo caso el texto supera ese límite de la ventana de contexto pues podemos usar alguna de las técnicas que les mencioné anteriormente para lograr hacer ese procesamiento. Recuerden que acá abajo me pueden dejar sus dudas y comentarios de este video y también recuerden que si les gustó le pueden dar un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos pues esto me ayudará a seguir llevando este tipo de contenido cada vez a más y más personas. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique un nuevo video. Por ahora esto es todo les envío un saludo y nos vemos en el próximo video.
DOC0087|LLM|Los grandes modelos de lenguaje contienen una serie de parámetros que permiten controlar la calidad y la aleatoriedad del texto generado. Estos valores son la temperatura, el muestreo TopK y el muestreo TopP y son valores que nosotros podemos definir al momento de solicitar al modelo que nos genere un determinado texto. Así que es clave conocer cómo funcionan estos parámetros al momento de construir una aplicación con estos grandes modelos de lenguaje. Entonces en este video vamos a entender precisamente que son la temperatura y los muestreos TopK y TopP y el efecto que los valores que escojamos tiene al momento de la generación del texto por parte de estos grandes modelos de lenguaje. Pero antes de comenzarlos invito a visitar CodificandoVids.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Para entender que son la temperatura y los muestreos TopK y TopP debemos entender el principio de generación de texto de estos grandes modelos de lenguaje de los cuales ya hablé en un video anterior. Estos modelos son entrenados entre otras cosas para generar texto en lenguaje natural y esta generación consiste simplemente en predecir de manera recursiva la siguiente palabra dentro de una secuencia de texto. Por ejemplo, supongamos que introducimos a uno de estos modelos la frase a los gatos les gusta. Y queremos que el modelo prediga la siguiente palabra. Tanto a la entrada como a la salida el modelo utiliza una representación intermedia del texto que se conoce como tokens y de los cuales ya hablé en un video anterior. Pero para no complicarnos con este detalle nos enfocaremos únicamente en el texto de entrada y el texto generado por el modelo, obviando este procesamiento intermedio a través de tokens. Cuando el modelo recibe el texto de entrada lo procesa a través de una red transformer y a la salida genera no solo una palabra sino realmente un conjunto de palabras que podríamos llamar palabras candidatas y cada una de estas palabras tiene asociado una probabilidad de ocurrencia. Por ejemplo, volviendo a la frase de entrada a los gatos les gusta, tras procesarla el modelo genera por ejemplo un total de 18 palabras candidatas. Comer, jugar, dormir, abrazar, ronronear, etcétera, etcétera. Cada una de las cuales tiene una probabilidad asociada. Estas probabilidades son simplemente valores numéricos entre 0 y 1 y si sumamos todas estas probabilidades obtendremos un resultado exactamente igual a 1. Cada uno de estos valores nos indica que tan probable es encontrar cada palabra candidata en el lenguaje natural. Y para calcular estas probabilidades el modelo tiene en cuenta el significado completo de la frase introducida por el usuario. Así, una probabilidad cercana a 1 nos indica que es altamente probable encontrar esa palabra en el lenguaje natural y junto a la frase de entrada. Mientras que una probabilidad muy cercana a 0 indica que es poco común encontrar esa palabra junto al texto de entrada. Y luego la idea es que el modelo debe seleccionar una de estas palabras candidatas la cual será precisamente la palabra predicha. Y en este último paso podríamos pensar que la palabra seleccionada sería simplemente aquella con la probabilidad más alta, que en nuestro caso sería la palabra comer. Sin embargo esto no es del todo cierto y aquí es donde entra en el juego la temperatura y los muestreos top-k y top-p que en últimas van a determinar cuál de todas estas palabras candidatas será finalmente seleccionada por el modelo y predicha como la siguiente palabra dentro de la secuencia. Así que teniendo esto en cuenta entendamos qué son estos parámetros y cómo afectan el proceso de generación de texto. La temperatura es simplemente un número que es mayor que 0 y menor que 1 y que permite escalar la distribución de probabilidades de la cual hablamos hace un momento. Un valor de temperatura cercano a 0 hace que la palabra con la probabilidad más alta en la distribución original tenga tras el escalamiento un valor muy cercano a 1 mientras que las palabras restantes tendrán valores cercanos a 0. Por ejemplo si volvemos a nuestro listado de 18 palabras candidatas originalmente la palabra comer era la más probable con una probabilidad de 0.135 pero ahora usando una temperatura muy cercana a 0 la probabilidad es de 0.9999 casi 1 y todas las demás palabras tienen una probabilidad prácticamente igual a 0. ¿Y qué quiere decir que una de las palabras candidatas tenga una probabilidad de ocurrencia tan alta es decir muy cercana a 1? Pues que al momento de escoger la palabra para completar el texto de entrada lo más probable es que el modelo seleccione precisamente la palabra comer. Todo esto quiere decir que con un valor de temperatura cercano a 0 tendremos un modelo más determinístico es decir menos aleatorio lo cual implica que el modelo no hará un uso tan creativo del lenguaje. Analicemos ahora la otra situación extrema. Supongamos que la temperatura tiene un valor muy cercano a 1. De nuevo al generar y escalar la distribución de probabilidades con este valor de temperatura tendremos las 18 palabras pero ahora observamos que sus probabilidades son muy similares. Así que al momento de determinar la siguiente palabra a predecir el modelo tendrá más opciones para escoger pues buena parte de las palabras candidatas tienen probabilidades muy cercanas. Esto quiere decir que con una temperatura cercana a 1 tendremos un modelo más determinístico y con una temperatura cercana a 1 tendremos un modelo que generará texto de forma un poco más aleatoria es decir será un poco más creativo al momento de utilizar el lenguaje. Y esto se debe a que al momento de predecir la siguiente palabra dentro del texto considerará un conjunto más amplio de palabras candidatas. Entonces en resumen con una temperatura cercana a 0 tendremos un modelo que genera texto de forma más determinística es decir un poco más predecible Mientras que con una temperatura cercana a 1 tendremos un modelo que genera texto de forma más aleatoria es decir más impredecible pero además este parámetro de la temperatura lo podemos mezclar por ejemplo con el muestreo top-k del cual vamos a hablar a continuación. Para entender en qué consiste el muestreo top-k volvamos a la distribución de probabilidades de las palabras candidatas que teníamos originalmente cuando ingresamos la frase A los gatos les gusta. En principio esta distribución contiene un total de 18 palabras así que el modelo tendría en principio la libertad de escoger cualquiera de estas pero resulta que podemos también limitar ese listado de palabras candidatas a solo unas cuantas. Por ejemplo podemos indicarle que en lugar de considerar las 18 palabras tenga en cuenta únicamente las 5 más probables al momento de la generación. ¿Y cómo podemos controlar este comportamiento? Pues precisamente usando el muestreo top-k que lo que hace es esencialmente es tomar las K palabras más probables dentro de la distribución de probabilidad original. Entonces cuando usamos el muestreo top-k lo que hace el modelo es primero generar la distribución de probabilidad para un valor específico de temperatura luego organiza de manera descendente cada palabra de acuerdo a su probabilidad de ocurrencia En el tercer paso selecciona las primeras K palabras de dicha distribución y finalmente selecciona aleatoriamente una de estas K palabras. Por ejemplo si fijamos el parámetro K con un valor de 5 el nuevo conjunto de palabras candidatas sería comer, jugar, dormir, abrazar y ronronear y el modelo hará la predicción de la siguiente palabra escogiendo aleatoriamente una de estas 5 palabras candidatas. Y acá vemos que aunque la selección de la palabra generada es aleatoria realmente estamos limitando esta selección a las 5 palabras más probables así que la generación de texto no es del todo aleatoria. Y con esto podemos ver que dependiendo del valor que escojamos para este parámetro K también podremos controlar la aleatoriedad del texto generado. Por ejemplo si escogemos un valor extremo de K igual a 1 el modelo siempre escogerá la palabra más probable dentro del conjunto de palabras candidatas y por tanto la generación de texto será totalmente determinística. Pero por otra parte si para el ejemplo que estamos analizando fijamos por ejemplo el valor de K en 18 el modelo podrá escoger cualquiera de las 18 palabras candidatas al momento de generar el texto es decir que con un valor de K relativamente alto la generación de texto será más aleatoria. Y una tercera forma de controlar la aleatoriedad del texto generado es usando el muestreo top P. En este caso lo que hacemos es definir un umbral que va a ser un valor entre 0 y 1 y que corresponde a lo que se conoce como una probabilidad acumulada. Y las palabras candidatas cuyas probabilidades sumen al menos esa probabilidad acumulada serán las únicas tenidas en cuenta al momento de generar la siguiente palabra. Entendamos esto con un ejemplo. Volvamos a la distribución original de probabilidades que teníamos para un valor determinado de temperatura. Y supongamos que definimos un umbral P de 0.3 con base en el cual haremos el muestreo top P. Esto quiere decir que para generar la siguiente palabra el modelo debe hacer lo siguiente. Primero debe tomar la distribución original y organizarla de manera descendente de la palabra más probable a la menos probable. Luego crea un nuevo listado de palabras candidatas que inicialmente estará vacío. Para llenar este listado se añaden iterativamente una a una de las palabras sumando en cada iteración el valor correspondiente de la probabilidad. Las iteraciones se detienen cuando se sobrepasa el valor de P definido anteriormente. Así por ejemplo, en la primera iteración se añade la palabra comer al nuevo listado de candidatas y la suma de probabilidades será en este caso 0.135. Como este valor es inferior a P igual a 0.3 continuarán las iteraciones. En la segunda iteración se añade la siguiente palabra del listado original, jugar, cuya probabilidad es de 0.125. Esto quiere decir que ahora la probabilidad acumulada es de 0.135 más 0.135. Es decir 0.26. Como este valor sigue siendo menor que 0.3 continuarán las iteraciones. En la tercera iteración se añade la palabra dormir que tiene una probabilidad de 0.115. Ahora la probabilidad acumulada será de 0.135 más 0.125 más 0.115. Es decir igual a 0.375. Como este valor sobrepasa el límite de P igual a 0.3 en este punto se detienen las iteraciones. Entonces como resultado de este muestreo top P tendremos ahora solo tres palabras candidatas comer, jugar y dormir. Y para finalizar la palabra generada se escoge aleatoriamente de este nuevo listado. Entonces vemos que el muestreo top P es otra manera de controlar la aleatoriedad del texto generado por el modelo. En esencia si el valor de P es pequeño, es decir es bastante cercano a 0, las palabras candidatas consideradas por el modelo al momento de la generación serán muy pocas. Y por tanto el modelo terminará escogiendo casi siempre las mismas palabras. Es decir que tendremos un modelo que genera texto de forma un poco más grande que el de la generación. Y por el contrario si el valor de P es relativamente grande, es decir muy cercano a 1, el modelo tendrá en cuenta prácticamente todas las palabras candidatas al momento de seleccionar la próxima palabra predicha. Y por tanto la generación de texto será mucho más aleatoria. Así que en este punto ya tenemos claro el significado y el impacto que tienen la temperatura y los muestreos top K y top P al momento de generar texto. Entonces en la práctica tenemos que tener en cuenta que el modelo genera texto de forma al momento de generar texto. Entonces en la práctica podemos jugar un poco con todos estos parámetros para controlar esa aleatoriedad al momento de la generación del texto. Como lo vamos a ver a continuación en el siguiente ejemplo. Tanto la temperatura como los valores de K y P los podemos definir bien sea al momento de usar una aplicación como por ejemplo ChatGPT o BART, es decir cuando estamos escribiendo la solicitud a esa aplicación, o también los podemos definir dentro del código mismo o en otros mismos estamos desarrollando la aplicación, usando alguno de estos grandes modelos de lenguaje. Por ejemplo supongamos que usamos BART de Google para generar una corta estrofa de una canción comenzando con la frase la luna oculta y que además de eso utilizamos diferentes valores de temperatura y de los parámetros K y P. En un primer caso le podemos decir por ejemplo al modelo. Escriba una corta estrofa de una canción comenzando con la frase la luna oculta. Use una temperatura de 0.2 con un muestreo TOP-K con un valor de K igual a 2 y un muestreo TOP-P con un valor de P igual a 0.1. Observemos que estamos usando valores relativamente bajos para estos tres parámetros, así que el texto generado será más determinista. Como resultado de esto obtendremos esta estrofa. La luna oculta tras las nubes, su luz brumosa ilumina la noche, un secreto que guarda la luna oculta, un secreto que guarda para nosotros un misterio que nos hace soñar. Vemos que la estrofa si tiene algo de lírica pero que el texto es bastante descriptivo y digamoslo así poco creativo. Y esto lo vemos en las frases la luna oculta tras las nubes o la luz de la luna que ilumina la noche. Ahora incrementemos un poco los valores de la temperatura y de los parámetros K y P para generar un texto un poco más aleatorio, es decir un poco más creativo. Y ahora creamos la siguiente solicitud. Ahora escriba una nueva estrofa corta de una canción comenzando con la frase la luna oculta pero en este caso use una temperatura de 0.9, un muestreo top K con un valor de K igual a 15 y un muestreo top P con un valor de P igual a 0.8. En este caso la estrofa generada por Barth es la siguiente. La luna oculta tras el velo de la noche, su luz tenue baña el mundo de sombras como un sueño que se desvanece como un deseo que nunca se cumple. Observemos que aparecen términos como el velo de la noche o el mundo de sombras o el sueño que se desvanece. Definitivamente se trata de un texto mucho más creativo. Bien, acabamos de entender estos tres sencillos conceptos de temperatura, muestreo top K y muestreo top P que como vimos nos permiten controlar la aleatoriedad del texto generado por estos grandes modelos de lenguaje. En últimas lo que vimos es que entre más pequeños sean estos valores, más determinístico o más predecible será el texto generado por el modelo. Mientras que entre más altos sean esos parámetros, el texto generado será más aleatorio o más impredecible. Y esos parámetros los podemos definir al momento de escribir la solicitud que hagamos sobre la aplicación, por ejemplo, sobre Barth o ChatGPT o al momento de programar el código si es que estamos desarrollando nosotros mismos la aplicación. Además, ya los valores que escojamos para esos parámetros pues dependerán precisamente de la aplicación que estemos desarrollando. Como siempre no olviden dejarme abajo sus dudas y comentarios y si les gustó el video no olviden compartirlo con todos sus amigos y conocidos y darle un pulgar hacia arriba de me gusta pues todo esto me ayudará a seguir llevando este contenido cada vez a más y más personas. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video. Subtítulos realizados por la comunidad de Amara.org
DOC0088|Machine Learning|Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán toda acerca de la inteligencia artificial, el deep learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com. Hoy hablaremos del algoritmo del gradiente descendente que es el algoritmo esencial en la etapa de aprendizaje de la mayor parte de los modelos de deep learning que veremos más adelante. Así que comencemos. Primero, una idea general de qué es el gradiente descendente. El gradiente descendente es un algoritmo que permite de forma automática, es decir, una vez programado de forma automática encontrar el valor mínimo de una función. Acá por simplicidad les he mostrado pues una función muy sencilla, una función que llamamos E, que depende de una variable que se llama W, está en el eje horizontal y esa función tiene una forma de una parabola. Entonces el gradiente descendente permite calcular o encontrar de forma automática ese punto de color verde que estamos dibujando ahí que corresponde precisamente al mínimo de la función que les mostraba anteriormente. He dibujado tres puntos y al lado de cada uno de esos puntos he dibujado unas líneas rectas, unas líneas tangentes a la gráfica de la función. En el primer caso, en el primer punto vemos una línea de color gris con una inclinación, esa inclinación recibe el nombre también de pendiente, o sea la pendiente es una medida de qué tan inclinada está esa línea recta en ese punto y aquí lo que podemos decir es que para ese punto 1 la pendiente es positiva. Para el punto 2 vemos que la línea es totalmente horizontal, no tiene ningún tipo de inclinación, entonces ahí hablamos de que la pendiente es exactamente igual a cero. Y en el punto 3 vemos que la pendiente tiene una dirección opuesta a la del punto 1, es decir que estamos hablando de que la pendiente en ese punto 3 tiene un valor negativo. Entonces ¿qué es el gradiente? Pues el gradiente simplemente es una medida de qué tan grande o qué tan pequeña o el signo que tiene la pendiente. Otros nombres que recibe ese gradiente o esa pendiente, matemáticamente eso se conoce como la derivada. Entonces si yo conozco la expresión matemática para esa parábola que vemos en la gráfica del lado izquierdo, yo puedo aplicar la operación matemática de la derivada y con eso entonces tengo otra función matemática que me permite calcular la pendiente o el gradiente para diferentes valores de W. Ahora bien, ¿esto para qué nos sirve? Entender ese concepto del gradiente o la pendiente, para entender precisamente cómo funciona el método del gradiente descendente. Es muy muy sencillo, recuerden que el objetivo de ese método, el gradiente descendente, es encontrar el mínimo de una función matemática. En este caso la función que estamos considerando es esta parábola que está acá dibujada del lado izquierdo y el método consiste en lo siguiente. Inicialmente el usuario tiene que definir dos parámetros. Un parámetro que se llama la tasa de aprendizaje, ya vamos a ver qué papel juega esa tasa de aprendizaje en inglés de learning rate y define el número de iteraciones, o sea cuántas veces se va a repetir el mismo procedimiento en el algoritmo. Esos son los dos únicos parámetros que la persona tiene que introducir al momento de programar el algoritmo. Entonces, ¿en qué consiste? Antes de iniciar las iteraciones el algoritmo de forma aleatoria escoge un punto cualquiera de la gráfica. Es decir, define un valor aleatorio para este parámetro W, que en nuestro caso es el que controla o el que define los valores de la función E. Entonces escoge aleatoriamente un valor inicial de W, puede ser muy grande, puede ser muy pequeño, totalmente aleatorio y a medida que se van ejecutando las iteraciones el algoritmo lo que hace es usar la ecuación que vemos acá en la parte superior derecha y es en cada iteración el valor W se va actualizando obedeciendo a la siguiente ecuación. Se toma el valor actual de W y a ese valor se le resta el gradiente en ese punto W multiplicado por la tasa de aprendizaje que fue el parámetro que introdujo el usuario al programar el algoritmo. Entendamos un poco esta ecuación. Entonces digamos que en la primera iteración el algoritmo escoge aleatoriamente este punto que estamos mostrando acá en el lado izquierdo, el punto 0. Ese punto 0 lo escoge aleatoriamente, es decir, el define de forma arbitraria el valor de W sub 0, el algoritmo, y fíjense que en este punto he dibujado una línea blanca que corresponde a la pendiente de la parábola en ese punto 0. Esa pendiente como veíamos anteriormente es una pendiente negativa, eso es un número negativo. Entonces, ¿qué pasa? Cuando vamos a la siguiente iteración y aplicamos la ecuación que está del lado derecho, el siguiente valor de W, es decir W sub 1, ¿a qué va a ser igual? Va a ser igual al W sub 0 que tenemos acá definido inicialmente, menos una cierta cantidad de veces, que es el learning rate, que es un valor positivo, una cierta cantidad de veces el gradiente. Como en este punto el gradiente es negativo, al multiplicar ese gradiente negativo por la tasa de aprendizaje que es positiva, sigo obteniendo un número negativo y al multiplicar ese resultado por el menos de la ecuación acá del lado derecho, lo que tengo realmente es un número positivo. Entonces lo que me dice este algoritmo es que en la siguiente iteración a este valor W sub 0 original le voy a sumar en últimas un pequeño valor, luego el valor W sub 1 va a estar ubicado del lado derecho del punto inicial. Entonces, observen lo que ocurre en esta primera iteración, en últimas estamos pasando del punto 0 al punto 1 y si miramos la altura del punto 0 y la altura del punto 1, lo que estamos viendo es que tras una iteración obtenemos un valor de la función más pequeño que el que teníamos originalmente. Si repetimos el mismo procedimiento, es decir, si el W sub 2 que vamos a calcular es el W1 que tenemos acá dibujado del lado izquierdo, menos alfa veces el gradiente, en este punto 1 el gradiente sigue siendo negativo, luego menos alfa veces el gradiente nos da una cantidad positiva, luego lo que vamos a hacer es a W1 sumarle un número positivo y por tanto el W2 va a estar ubicado más a la derecha. Entonces, fíjense que poco a poco y progresivamente nos vamos acercando a ese valor mínimo de la función E que es el que estamos buscando. Si repetimos el procedimiento para 1.3 y para 1.4 pues ya era un momento en la iteración que ya llegamos al mínimo de esa función. ¿Qué pasa si el algoritmo al definir el valor inicial de W lo hubiese escogido del lado derecho? Porque recuerden que este valor inicial se define de forma aleatoria. Hubiera ocurrido exactamente lo mismo, aquí la diferencia ¿cuál es? Que en ese punto inicial la pendiente en este caso es positiva, esa línea recta de color blanco dibujada al lado del punto cero es una pendiente positiva. Entonces ese término alfa por el gradiente es una cantidad positiva y al agregar el signo menos lo que vamos a hacer es que al W0 le vamos a restar una cierta cantidad. Al restarle una cierta cantidad lo que estamos diciendo es que ese W0 se va a mover ahora en la siguiente iteración hacia el lado derecho. Y fíjense que nuevamente entonces poco a poco desde el otro extremo vamos a comenzar a acercarnos a ese valor mínimo. Si repetimos las iteraciones vamos a ver que progresivamente vamos llegando a ese valor mínimo. Entonces esta es la idea general del algoritmo del gradiente descendente, este es un ejemplo sencillo para una función que depende de una sola variable que en este caso se llama W, pero el algoritmo se aplica a cualquier tipo de función que dependa de una de dos o de más variables, es decir que sea de dos o de tres o de cuatro o de más dimensiones y también funciones que tengan diferentes formas. Esta función es muy sencilla porque es evidente que tiene un único mínimo que es el punto cuatro que estamos aquí ilustrando, pero si la función tiene otras características, otra forma, otro comportamiento también se va a poder aplicar el algoritmo del gradiente descendente. Entonces para resumir este algoritmo del gradiente descendente permite detectar el mínimo de una función, de una función matemática de forma automática, lo único que requiere al momento de ser programado son dos parámetros, la tasa de aprendizaje o learning rate y el número de iteraciones o la cantidad de veces que yo quiero ejecutar el algoritmo para poco a poco ir llegando a obtener ese valor mínimo de esa función. Bien esto ha sido todo, gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com. Hasta luego.
DOC0089|Machine Learning|Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán todas acerca de la inteligencia artificial, el deep learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com. Hoy vamos a hablar del método, el algoritmo de la regresión lineal, vamos a explicar todos los conceptos asociados a este algoritmo que es bien importante para entender más adelante cómo se implementarán diferentes modelos de deep learning. Entonces comencemos. ¿Qué es lo que hace este algoritmo de la regresión lineal? Aquí tenemos un ejemplo de una serie de datos, cada uno de esos datos está representado con un punto acá dentro de esta gráfica y la idea de este algoritmo ustedes observan que los datos tienen un comportamiento o una relación lineal ven por ejemplo que a medida que x la variable que está en el eje horizontal se incrementa y la variable y en el eje vertical crece o se incrementa también en la misma proporción. La idea del algoritmo de la regresión lineal es encontrar una línea recta que me permita matemáticamente representar estos datos aquí vemos ese concepto, hay diferentes líneas rectas que pueden existir pero de estas tres líneas rectas que tenemos en este ejemplo ustedes observan que sólo una de ellas, la de color blanco, es la que realmente se ajusta mejor a esos datos, es la que está digamos en promedio más cercana a la totalidad de los datos la que lo representa de forma más adecuada. La idea del algoritmo de la regresión lineal es que de forma automática el algoritmo sea capaz de encontrar precisamente una línea recta como la línea blanca que estamos viendo que estamos viendo ahí. Para entender cómo funciona el algoritmo recordemos inicialmente cómo es la ecuación o cuál es la ecuación de una línea recta en este caso tenemos una variable en la gráfica sobre el eje y, una variable que depende de una variable llamada x en el eje ubicada en el eje horizontal si yo hago una representación de y en términos de x a través de una línea recta la ecuación que observamos en la que está aquí arriba wx más b eso va a ser y donde w lo que representa es la inclinación que tiene esa línea recta eso se conoce como la pendiente acá en la gráfica les muestro el significado de esa pendiente entonces entre más inclinada es decir entre más hacia arriba esté la línea recta más alto es el valor de la pendiente y dice entre más abajo más entre más cerca hacia el eje horizontal se encuentra esa línea recta pues menos pendiente tendrá y el parámetro b en este caso tiene un valor de 20 y lo que indica es simplemente cuando la variable x es igual a 0 esa línea recta donde cortaría o cruzaría el eje y en este caso es un valor igual a 20 entonces el objetivo del método de la regresión lineal es encontrar estos dos parámetros w y b de forma totalmente automática partiendo únicamente de los datos o de la nueve de puntos que les mostraba yo anteriormente cómo se logra esto entonces estamos diciendo que el objetivo es encontrar una línea recta pero no cualquier línea línea recta es encontrar la línea recta que mejor se ajuste a la totalidad de los datos entonces tenemos que definir de alguna manera una cantidad numérica que mida que también o que tan mal está ese ajuste o esa línea recta que estamos obteniendo en el caso de la regresión lineal aplicaba algoritmos de machine learning y de deep learning la función que más se usa o la métrica que más se usa se conoce como el error cuadrático medio el sm esta función no sólo el error cuadrático medio sino esa métrica que usamos para determinar qué tan bien o qué tan mal está hecha la aproximación se conoce como función de costo o pérdida este es un término bien importante sobre el que más adelante en otros tutoriales seguiremos hablando la función de costo función de pérdida entonces cómo se define ese error cuadrático medio en este caso la ecuación la tenemos aquí del lado derecho pero entendamos un poco el significado de ese término que aparece elevado al cuadrado dentro de la sumatoria veámoslo acá en la gráfica del lado izquierdo ahí es señalado dos puntos un punto de color azul acá en la parte inferior este punto de color azul que es el punto que conocemos el dato que tenemos de entrada lo vamos a llamar y porque y porque tenemos una cierta cantidad de puntos en total esos puntos azules a eso se refiere este término n entonces para cada uno de esos puntos le vamos a dar este nombre y el punto correspondiente a esa línea recta que estamos tratando de determinar lo vamos a llamar y y gorro si entonces lo que lo que mide esta diferencia es que tanto se acerca el punto que estamos obteniendo de forma automática con el algoritmo que tanto se acerca al punto original y ustedes ven que para ciertos puntos esa diferencia es pequeña para otros puntos como el que estamos viendo acá la diferencia relativamente grande entonces lo que hace la el error cuadrático medio es promediar todos esos valores todas esas diferencias y eso da una cantidad numérica que es el error el objetivo es que ese error sea lo más pequeño posible y para que ese error sea pequeño pues precisamente estos esta línea recta tiene que ajustarse en promedio bastante bien a la mayoría de los datos esa es la idea de el error cuadrático medio porque se eleva al cuadrado porque estas diferencias a veces pueden ser positivas a veces pueden ser negativas si si no la elevo al cuadrado esos signos positivos y negativos pues al sumarse me van a dar una cantidad que probablemente es pequeña pero que realmente cuando hago el ajuste el error total es bastante grande entonces para evitar esa cancelación debida a los signos se eleva al cuadrado bien ahora entonces sabiendo que lo que queremos es minimizar este error cuadrático medio que aquí ya lo estoy reescribiendo en términos de los parámetros que nos interesan w y b como calculamos automáticamente esos dos parámetros entonces fíjense ustedes acá que tenemos la función de error cuadrático medio pero esa función depende de estas dos variables de w y b si yo escojo esas dos variables habrá alguna serie de valores para los cuales el error es más grande habrá una serie de valores para los cuales el error es relativamente pequeño que lo que ocurre que si yo hago la gráfica de esta función tiene un comportamiento cuadrático y depende de dos variables que tenemos acá en este w y b entonces cuando hacemos la gráfica eso tiene una forma como de un tazón esto matemáticamente o geométricamente se conoce como un paraboloide y acá en este punto de color rojo yo les he señalado el mínimo de esa función es decir fíjense que aquí esta altura me define el valor del error y este punto está en la altura más baja es decir corresponde al mínimo de ese error entonces el objetivo de la regresión lineal es obtener la ubicación de este punto es decir el valor de b y de w que hacen que este error alcance este valor mínimo y eso cómo se logra con el método del gradiente descendente que lo vimos anteriormente en otro tutorial y esos enlaces los van a encontrar en la descripción del vídeo para para que lo puedan revisar más adelante con el gradiente descendente encontramos ese mínimo y lo que vemos es que entonces cuando se aplica ese método del gradiente descendente se toman los datos originales y se hace iterar el algoritmo una cierta cantidad de veces ustedes ven acá del lado izquierdo que progresivamente es error cuadrático medio a medida que avanzan las iteraciones se va haciendo más pequeño estoy minimizando el error me estoy moviendo hacia el fondo de ese tazón y a su vez ven acá del lado derecho el resultado que se va obteniendo en cada iteración entonces fíjense que la línea recta cada vez se ajusta mejor a los datos a medida que el error va disminuyendo entonces esta es la idea principal del algoritmo de la regresión lineal bien esto ha sido todo gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com hasta luego
DOC0090|Machine Learning|Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán todo acerca de la Inteligencia Artificial, el Deep Learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com Hoy vamos a hablar del algoritmo de la Regresión Logística. ¿En qué consiste este algoritmo? Simplemente es un modelo de aprendizaje que permite clasificar una serie de datos. En este caso tenemos el ejemplo de un emoji con un rostro feliz, entonces lo que permitiría hacer la Regresión Logística en este caso sería clasificar ese emoji correspondiente a la categoría feliz. Por su parte si es una carita triste pues simplemente permitiría clasificarlo como un rostro triste. Entonces la Regresión Logística permite realizar este tipo de clasificación cuando a la salida tenemos dos categorías. Por eso también esto se llama Regresión Binaria, porque las categorías son únicamente dos. Miremos un ejemplo un poco más concreto. Aquí les muestro una nube de puntos de color rojo y de color gris y cada punto se caracteriza por tener dos coordenadas x1 y x2. Eso también lo llamamos características, cada uno de esos puntos tiene dos características y cada uno de estos puntos pertenece a una categoría diferente. En rojo les estoy mostrando la categoría cero, en gris la categoría uno. ¿Qué es lo que hace en últimas el algoritmo de Regresión Logística? Separar esos datos, es decir clasificarlos. Aquí les muestro una línea que de forma aproximada permite separar una categoría de la otra, lo que está por encima de la línea de color verde es la categoría uno, lo que está por debajo es la categoría cero. Lo que hace el algoritmo de Regresión Logística es encontrar automáticamente esta línea o esta frontera de decisión como también se llama. ¿Cómo lo logra hacer el algoritmo de Regresión Logística? Ejecuta o lleva a cabo dos pasos sobre esos datos. Toma cada uno de esos puntos, que recordemos que tenía dos características x1 y x2, y los pondera inicialmente con un coeficiente w1 y w2, con una serie de coeficientes. Y a eso le suma un tercer coeficiente llamado b. Y con eso se obtiene un valor z, que es lo que estamos llamando aquí la transformación. ¿Cómo se calculan estos tres coeficientes w1, w2 y b? Se calculan a través del entrenamiento, que es lo que vamos a ver a continuación. Una vez transformados estos datos, lo que vamos a lograr es comenzar a separarlos, es decir el objetivo de esta transformación es comenzar a separarlos para poderlos clasificar posteriormente. Ahora, ¿cómo se hace en la clasificación final? Entonces se toma ese dato de la transformación y se lleva a algo que se conoce como una función de activación. Esa función de activación tiene en este caso una característica importante y es que como nos interesa tener a la salida dos categorías, uno o cero, esa función de activación tiene que ser en primer lugar no lineal y en segundo lugar entonces tiene que tener ese rango de valores a la salida, es decir tiene que arrojarnos o el uno o el cero dependiendo del valor de z. Aquí en la parte de abajo les muestro la función usada en este caso de la regresión logística que se conoce como función sigmoidal. Tiene una definición matemática en el post que les dejo en la información de este video, en la descripción del video podrán encontrar todos los detalles, pero la idea básica es que si z en el eje horizontal de esta función, si z que es la transformación tiene valores negativos la salida entonces de esa función sigmoidal va a tener un valor cero y si z tiene valores positivos pues esa salida de la función sigmoidal va a ser muy cercana a uno, esto lo que permite es hacer ese tipo de clasificación binaria. Estos dos elementos que acabo de mostrarles, la transformación y la función de activación se combinan en una unidad que en deep learning en inteligencia artificial se conoce como la neurona artificial, ¿por qué neurona? porque su funcionamiento, su principio de funcionamiento es muy similar al de las neuronas humanas. Tiene un dato de entrada o unos datos de entrada en este caso lo llamamos x que puede tener una, dos o n características, cada una de esas características se transforma, se obtiene entonces un valor z, una cantidad numérica z que es esa transformación que permite comenzar a separar los datos y luego se aplica la función de activación que define entonces si el dato corresponde a la categoría 1 o a la categoría 0, esencialmente entonces esto es lo que hace una neurona artificial, un proceso de transformación y luego aplica una función de activación. Cómo se realiza el entrenamiento, entonces yo les decía que en la transformación se calculan unos coeficientes W y B, ese entrenamiento se hace a través del algoritmo del gradiente descendente que vimos en un video anterior y ese gradiente descendente lo que permite es minimizar algo que se llama la función de error o la función de pérdida que compara la predicción que hace el modelo, o sea el valor y o la salida que genera con el valor que realmente o la categoría de la que realmente pertenece el dato de entrada, comparando esos dos valores se minimiza esa función de error y al minimizarlo estamos encontrando los coeficientes que permiten que el error sea mínimo precisamente. En la regresión lineal vista anteriormente en otro video hablábamos del error cuadrático medio era una forma de medir ese error, ¿qué ocurre en este caso? Como los datos son binarios, categoría 1, categoría 0, cuando se calcula el error cuadrático medio esa función puede tener no un solo mínimo absoluto sino que puede tener varios mínimos locales que son los puntos verdes que estoy mostrando acá. Idealmente nosotros querríamos llegar al mínimo absoluto que es el punto de color rojo pero como el algoritmo del gradiente descendente no sabe dónde está ese mínimo puede ser que al momento de inicializarlo y de ejecutar el algoritmo el algoritmo converja a alguno de los puntos verdes, es decir no al mínimo absoluto, lo cual querría decir que el entrenamiento no sería adecuado, por eso esta función no se usa, la del error cuadrático medio, por eso usamos la que se conoce como la entropía cruzada que en codificando bits.com explica un poco más en detalle esta entropía cruzada pero la idea es que al usar esta función de error vamos a tener entonces un único mínimo absoluto que es este punto de color rojo que estamos mostrando acá, con eso siempre vamos a garantizar que el gradiente descendente funciona correctamente durante el entrenamiento. Finalmente aquí les voy a mostrar un ejemplo en tiempo real del proceso de entrenamiento y cómo a través de cada iteración de ese proceso de entrenamiento en los datos que les mostraba anteriormente se va refinando esa frontera de decisión entre los dos tipos de datos, es importante recordar que esta frontera de decisión como ustedes ven es una línea recta, siempre cuando se usa una sola neurona como es el caso de la regresión logística siempre esa frontera de decisión va a ser lineal y eso tiene algunas desventajas dependiendo de cómo estén distribuidos los datos, a veces funcionará bien la clasificación a veces no tan bien. Bien, esto ha sido todo, gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com. Hasta luego.
DOC0091|Machine Learning|Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán todo acerca de la inteligencia artificial, el deep learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com Hoy vamos a hablar de la Regresión Multiclase. Así que comencemos. En primer lugar miremos que eso en que consiste la Regresión Multiclase es un algoritmo que permite clasificar datos y esta clasificación la da en tres o más categorías. A eso se referencia la Regresión Multiclase. Si tenemos, recuerden que si tenemos dos categorías hablamos de una Regresión Binaria o Regresión Logística que vimos en un video anterior. Pero en este caso la Regresión Multiclase me permite clasificar los datos en tres o más categorías. En este ejemplo entonces vemos datos de entrada como un triángulo, un rayo, una estrella y un corazón y la idea del algoritmo de Regresión Multiclase es tomar esos datos de entrada y clasificarlo en una de esas cuatro posibles categorías. Vemos un ejemplo un poco más específico. Aquí les dibujo una serie de puntos. Cada uno de esos puntos está definido por dos características, x1 y x2. Y ven ustedes en la gráfica tres agrupaciones. Una de color rojo correspondiente a la categoría 0, una de color morado correspondiente a la categoría 1 y una de color gris correspondiente a la categoría 2. Es decir, tenemos en total tres categorías para este set de datos. ¿Qué es lo que idealmente haría el algoritmo de Regresión Multiclase con estos datos? Encontrar de forma automática estas fronteras de decisión, estas líneas que separan una categoría o una agrupación o una clase de la otra. Veamos cómo hace esto el algoritmo de Regresión Multiclase, cómo lo logra hacer de forma automática. Vemos acá el esquema general. Vamos a mirarlo ahora en detalle. En este esquema general tenemos los datos de entrada, x, cada uno de esos datos, como en la gráfica anterior, veíamos que tiene dos características. Y tenemos tres salidas. La salida y1 correspondiente a la categoría 0, y2 correspondiente a la categoría 1 y y3 correspondiente a la categoría 2. Tenemos tres posibles categorías de salida. Esencialmente lo que hace el algoritmo es tomar esos datos de entrada, esas características de entrada y hacer una transformación de los datos. ¿Cuántas veces se hace la transformación? Depende de las categorías que tengamos. En este caso tenemos tres categorías de salida y por tanto se hacen tres transformaciones. Las transformaciones después se convierten a un número o a una probabilidad y posteriormente se determina de todas estas tres probabilidades cuál es la más alta y se asigna la categoría correspondiente a ese dato de entrada. Este es el esquema general. Luego vemos que es similar a lo que ocurre con la regresión logística. Es decir, primero hay un proceso de transformación de los datos que permite hacer una separación inicial de esos datos para luego clasificarlos y después usamos una función de activación que es no lineal que en este caso se llama la función softmax. Veamos esto con un ejemplo más detallado. Supongamos que tenemos este dato de entrada x con dos características, con dos valores numéricos que lo caracterizan. Vamos a aplicar primero una transformación. Como son tres categorías tenemos que aplicar esta ecuación tres veces y vemos que en esta transformación se tiene la misma ecuación o la misma función usada en la regresión logística donde el valor x de entrada lo estamos multiplicando por unos coeficientes w y le estamos sumando un coeficiente b. Cómo se obtienen estos coeficientes? Se obtienen al igual que en la regresión logística por un proceso de entrenamiento donde se usa el gradiente descendente para minimizar una función de error que en este caso también al igual que en la regresión logística se llama la entropía cruzada. Entonces esos coeficientes se calculan de forma automática durante el entrenamiento. Miremos paso a paso entonces la transformación. Se toman los dos datos, las dos características del dato de entrada, se multiplican por dos coeficientes w calculados automáticamente durante el entrenamiento a eso se le suma este coeficiente calculado también de forma automática y eso nos da una cantidad numérica. Y esa transformación se hace tres veces para otros coeficientes w y b. Se obtienen entonces tres cantidades numéricas. Fíjense que estas tres cantidades numéricas no están normalizadas en el sentido de que la suma de ellas no es igual a uno. Evidentemente aquí si sumamos esos coeficientes el valor resultante va a ser mayor que uno. Para normalizar esos valores usamos una función que se llama softmax que ustedes ven acá definida en esta ecuación donde es evidente que es una función no lineal. Cómo hacemos entonces esa transformación de cada uno de los tres datos usando la función softmax. Pues tomamos el primer dato, calculamos su exponencial y el valor resultante lo dividimos entre la suma de la exponencial de los tres datos de entrada y eso nos da entonces una primera cantidad. Tomamos el segundo dato, calculamos la exponencial de ese segundo dato, lo dividimos entre la suma de las tres exponenciales, eso nos da otra cantidad y así con el tercer dato. Pues observamos que esos tres datos ya están normalizados y sumamos esas tres probabilidades el valor resultante va a ser muy cercano a uno. El siguiente paso es entonces con base en esas tres probabilidades determinar la categoría a la que pertenece este dato de entrada. Cómo se hace eso? Es muy sencillo, simplemente de las tres probabilidades determinamos cuál de ellas es más alta, en este caso la que aparece en esta posición y a esa entonces le asignamos la categoría del dato entrante, luego el vector de salida o el dato de salida de ese modelo softmax será un vector 0, 1, 0 donde el 0 me indica que el dato de entrada no corresponde a estas categorías y el 1 precisamente me indica que ese dato de entrada corresponde a la segunda categoría que teníamos nosotros definida inicialmente. Miremos finalmente cómo durante el entrenamiento, cómo varían estas fronteras de decisión, observen primero que esas fronteras de decisión son lineales al igual que en el caso de la regresión logística y que a medida que avanzan las iteraciones se van refinando esas fronteras de decisión hasta que la totalidad de los datos queda clasificada correctamente. Bien, esto ha sido todo, gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com. Hasta luego.
DOC0092|Machine Learning|En este video veremos una explicación detallada y muy completa de lo que son las máquinas de soporte vectorial o máquinas de vectores de soporte, que es uno de los algoritmos clásicos del Machine Learning y que en la actualidad tiene un uso muy extendido, por ejemplo, en combinación con arquitecturas más complejas como las redes neuronales o las redes convolucionales. Así que sin más preámbulos, comencemos. El principal uso de las máquinas de soporte vectorial se da en la clasificación binaria, es decir, cuando queremos separar un set de datos en dos categorías diferentes. Para entenderlo, veamos de forma intuitiva este concepto. Supongamos que queremos conformar nuestro propio equipo de ciclismo y que estamos en el proceso de selección de nuestros corredores y que cada ciclista puede pertenecer a una de dos categorías. Escalador, es decir, especialista en la montaña o embalador, es decir, especialista en las llegadas con terreno plano. Para asignar la categoría a la que pertenece cada ciclista, es decir, para clasificarlo, podemos usar varias características. Consideremos una primera característica, el peso. Generalmente, los corredores más livianos son buenos en la montaña y los que tienen un peso un poco mayor son buenos para el embalaje. En este caso, podemos definir un umbral y al momento de la clasificación simplemente definimos la categoría, dependiendo de si el peso del ciclista está a la izquierda o a la derecha de este umbral. Pero para mejorar la precisión de nuestra clasificación, podemos agregar más características. Por ejemplo, si añadimos ahora la potencia que desarrolla el ciclista en cada pedalazo, entonces ahora en dos dimensiones podemos trazar una línea que divida a estos dos grupos. Si además del peso y la potencia incluimos, por ejemplo, la capacidad pulmonar, entonces tendremos una distribución de puntos en tres dimensiones, es decir, tres características. Y en este caso tendremos un plano que separa una categoría de otra. Y con cuatro o más dimensiones no resulta fácil dibujarlo, pero la idea es que en todos los casos idealmente tendremos una frontera de decisión que permite determinar la categoría a la que pertenece cada ciclista. En adelante vamos a llamar a esta frontera el hiper plano y en la explicación que viene ahora nos vamos a enfocar únicamente en dos dimensiones porque resulta más fácil entender los conceptos gráficamente. Pero este mismo concepto de hiper plano se aplica para una, tres, cuatro o más dimensiones. Ok, volviendo a dos dimensiones, nos enfocaremos en dos características. El peso y la potencia de cada pedalazo. Si volvemos a nuestro grupo inicial de ciclistas, podemos ver que los dos grupos están separados por un hiper plano, que en este caso es simplemente una línea recta. Si analizamos esta línea recta, veremos que cualquier punto que pertenezca a ella tiene una característica importante. Al reemplazarlo en la ecuación, el resultado será exactamente igual a cero. Pero si tomamos, por ejemplo, los puntos en los cuales están ubicados los embaladores, veremos que al reemplazarlos en la ecuación del hiper plano, todos son mayores que cero. Mientras que al hacer lo mismo para los puntos donde están los escaladores, se obtienen valores menores a cero. Así que partiendo de esto, podemos definir un algoritmo muy sencillo para la clasificación de los datos. En primer lugar, tenemos que obtener la ecuación del hiper plano, es decir, calcular los coeficientes. Y en segundo lugar, usar esta ecuación para reemplazar el dato que queremos clasificar en dicha ecuación y dependiendo del signo que obtengamos, lo clasificamos en una o en otra categoría. Si miramos en detalle este sencillo algoritmo, vamos a encontrar que el problema realmente se reduce a encontrar la ecuación de ese hiper plano. Y es aquí donde entran precisamente las máquinas de vectores de soporte. Para entender cómo funciona este algoritmo, debemos comprender un concepto muy importante, el del mejor hiper plano. Al intentar separar nuestro set de datos, podemos obtener diferentes líneas o hiperplanos y todos ellos logran dividir correctamente el set en dos categorías. Pero cuál de ellas es mejor? Vemos que las líneas uno y dos están demasiado cerca de una de las categorías, mientras que la línea tres está en un punto intermedio entre las dos agrupaciones. Esta línea es precisamente el hiper plano óptimo, pues es la que se encuentra más alejada de todas las observaciones. Y esto hace que al momento de clasificar un nuevo dato, no exista un sesgo hacia una u otra categoría, pues el algoritmo de máquinas de soporte vectorial permite precisamente obtener este hiper plano óptimo. Y aunque existen diferentes maneras de implementarlo computacionalmente, en esencia lo que en el fondo logra hacer este algoritmo es primero detectar los puntos más cercanos entre una clase y otra. Luego encuentra la línea que los conecta y finalmente tras una frontera perpendicular que divide esta línea en dos. La línea que se obtiene es precisamente el hiper plano óptimo. Y acá debemos resaltar tres definiciones importantes. Los vectores de soporte, que son precisamente los puntos más cercanos entre una clase y otra y son los que le dan el nombre al algoritmo. El margen, que es la distancia entre el hiper plano y los vectores de soporte y el mismo hiper plano óptimo, que es la frontera de separación que consigue el mayor margen posible. Así que podemos pensar en el algoritmo de máquinas de soporte vectorial como un método que nos permite trazar una carretera con dos carriles entre nuestros datos. El hiper plano óptimo es el separador entre esos dos carriles y el ancho de cada uno de esos dos carriles, que es el mismo en ambos casos, se conoce precisamente como el margen. Y la idea de las máquinas de soporte vectorial es precisamente obtener los carriles con el máximo ancho posible. Bien, y con esto ya tenemos el principio básico de cómo funcionan las máquinas de vectores de soporte. Pero este algoritmo funciona solo para el caso ideal, es decir, cuando tenemos nuestros datos lo suficientemente separados de manera tal que podemos trazar una línea recta con el margen más amplio posible, es decir, cuando podemos obtener una frontera de decisión óptima. Pero volvamos a nuestro ejemplo. Qué pasa si llega a nuestro equipo un corredor excepcional, es decir, que es muy bueno en la montaña, pero que también se destaca como envalador. En este caso, ese nuevo corredor se comporta como un outlier, es decir, un valor atípico, pues no obedece el comportamiento esperado para un escalador. Si aplicamos el algoritmo explicado anteriormente, veremos que se obtendrá un hiper plano óptimo, pero que el margen será muy pequeño. Esto se debe a que los vectores de soporte están más cerca y esto puede llevar al overfit, es decir, que si introducimos un dato nuevo que no haya sido nunca antes visto por el algoritmo, muy probablemente será clasificado incorrectamente porque el margen es muy reducido y no hay una adecuada separación entre las clases. Así que el algoritmo que vimos inicialmente conocido como hard margin o margen duro no resulta muy flexible y no se puede utilizar cuando tenemos outliers, como el caso de nuestro corredor excepcional. La manera de resolver esto es ensanchando el margen, es decir, buscando que la mayoría de los datos sean correctamente clasificados y aceptar la posibilidad de que existan unos cuantos errores al momento de la clasificación. Esto se logra introduciendo una modificación al algoritmo para el cálculo del hiper plano óptimo. Originalmente, en el clasificador hard margin se buscaba maximizar el margen y para esto se modificaban únicamente los coeficientes del hiper plano. Ahora, para contrarrestar el efecto del outlier se incluye un término adicional a esta función, lo que permite flexibilizar el margen. Este término depende de un parámetro C, un hiperparámetro que yo como diseñador elijo durante el entrenamiento. La idea es que un C relativamente pequeño permite generar margenes más amplios y a medida que su tamaño aumenta, el margen se va reduciendo poco a poco. Este parámetro se escoge de manera empírica, analizando el error que se obtiene en la clasificación comparado con diferentes valores de C. Y a este algoritmo de máquina de vectores de soporte se le conoce como soft margin. Bien, en este punto ya tenemos un algoritmo mucho más versátil sobre el cual tenemos un control al momento de obtener el ancho de ese margen o de esos carriles. Sin embargo, todavía estamos considerando una situación bastante ideal porque nuestros datos están lo suficientemente separados y basta con trazar una línea recta para clasificar correctamente la mayor parte de ellos. Pero en aplicaciones reales la situación es más complicada porque podemos tener fronteras de división que no son necesariamente líneas rectas y que tienen formas mucho más complejas. Y el problema es que hasta donde hemos visto, las máquinas de soporte vectorial solo permiten obtener hiperplanos o fronteras de decisión lineales. Pero entonces, ¿qué se puede hacer en este caso? Una alternativa sería agregar más dimensiones a cada dato. Es decir, ¿qué pasaría si encontramos una forma de agregar una o más dimensiones adicionales para así lograrse separar las dos categorías? En este caso podríamos usar el mismo algoritmo de máquinas de vectores de soporte para clasificar los datos en esas tres o más dimensiones. Pero ¿cómo logramos agregar más dimensiones a los datos para así lograr su clasificación? Pues el método usado en las máquinas de soporte vectorial se conoce como el truco del kernel. Básicamente consiste en tomar el set de datos original, que no es separable, y mapearlo a un espacio de mayores dimensiones usando una función no lineal. En un momento veremos las funciones más usadas. La idea es que con esta transformación el dataset ahora será linealmente separable. Es decir, que se puede usar una máquina de vectores de soporte, tipo SoftMargin, como la que vimos anteriormente, para obtener el hiperplano óptimo. Una vez lo hayamos obtenido hacemos la transformación inversa para volver al espacio original y así llevar a cabo la clasificación. En la práctica el truco del kernel no implementa todos estos pasos, sino que hace el mapeo y el cálculo del hiperplano, usando algo de álgebra lineal para obtener de forma simplificada estos cálculos y hacer más rápido el algoritmo. Las transformaciones más usadas en este truco del kernel se logran con dos tipos de funciones. Las polinomiales, que implican obtener combinaciones de los vectores de características usando potencias mayores que uno o usando funciones gaussianas, con forma de campana que se conocen como funciones de base radial. Aunque las máquinas de soporte vectorial fueron desarrolladas a comienzos de los años 90, en la actualidad son muy usadas porque permiten separar de manera óptima un set de datos en dos categorías diferentes, es decir, obteniendo una frontera de decisión que está equidistante a los dos grupos que yo quiero separar. Esto hace que una máquina de vectores de soporte tenga usualmente una precisión más alta que una neurona o un perceptrón convencionales. De hecho, esto ha permitido que en la actualidad muchos sistemas de deep learning que usan redes convolucionales para el procesamiento de imágenes, como por ejemplo para el reconocimiento de rostros, extraigan características de las imágenes usando estas redes convolucionales y luego alimenten estas características a una máquina de soporte vectorial para realizar la clasificación final del rostro. Bien, y con esto espero que tengan una idea completa y detallada de qué son y cómo funcionan las máquinas de soporte vectorial. En un próximo video veremos cómo usarlas para resolver un problema de machine learning en Python. Y si les gusta el contenido del canal, los invito a ver los videos que les estoy sugiriendo aquí en la pantalla y nos vemos en el próximo video.
DOC0093|Machine Learning|Los árboles de decisión son tal vez el algoritmo más sencillo, pero a la vez uno de los más poderosos del Machine Learning y son muy usados cuando tenemos sets de datos relativamente complejos, pues en este video veremos en detalle qué son y cómo usar los árboles de decisión para resolver un problema de clasificación. En particular vamos a hablar del algoritmo CART, que es el más usado en el Machine Learning para implementar en la actualidad árboles de decisión. Así que sin más preámbulos, comencemos. En la clasificación buscamos entrenar un modelo que sea capaz de determinar la categoría a la que pertenece un dato en particular. La idea es que el modelo aprenda a calcular una frontera de decisión que permita asignar el dato a una u otra categoría. Un ejemplo típico de este tipo de tarea es la clasificación de correos electrónicos entre normales y spam. En videos anteriores vimos cómo por ejemplo la regresión logística o las redes neuronales podrían ser usadas para clasificar sets de datos relativamente complejos. Sin embargo el problema de la regresión logística es que nos permite clasificar datos que son solo linealmente separables, es decir, donde la frontera de decisión es una simple línea recta en dos dimensiones o un plano o un hiper plano en tres o más dimensiones. Mientras que el gran inconveniente de las redes neuronales es que son generalmente modelos de caja negra, que son excelentes clasificadores pero no son fáciles de interpretar. Es decir que usualmente resulta difícil explicar en términos simples por qué la red clasificó un dato de una forma o de otra. Precisamente los árboles de decisión son una solución a este inconveniente porque además de permitirnos clasificar datos relativamente complejos, también nos permiten interpretar fácilmente los resultados de esa predicción. El algoritmo más usado en la actualidad para implementar árboles de decisión se conoce como CARP o árboles de clasificación y regresión por sus siglas en inglés. Lo que vamos a ver de aquí en adelante es en detalle cómo funciona este algoritmo para las tareas de clasificación y en un próximo video veremos cómo funciona para una tarea de regresión. Partamos de un ejemplo sencillo para entender cómo funciona el algoritmo. Supongamos que somos fanáticos del anime y que queremos diferenciar demonios de asesinos de demonios. Cada uno de estos personajes posee ciertas características como el nivel de fuerza o el nivel de letalidad y cada una en una escala de 0 a 20. Para representar esto gráficamente dibujaremos un plano en dos dimensiones, con el eje horizontal igual al nivel de fuerza y el eje vertical igual al nivel de letalidad. Los demonios estarán representados con puntos rojos y los asesinos de demonios con puntos verdes y en total tendremos 20 personajes. A los niveles de fuerza y de letalidad los llamaremos características y aunque por simplicidad estamos considerando únicamente dos características, es importante tener en cuenta que los árboles de decisión funcionan igualmente con sets de datos mucho más complejos, con tres o más características. La idea es encontrar una forma de separar los demonios de los asesinos de demonios, es decir, calcular unas fronteras de decisión que permitan posteriormente clasificar nuevos personajes en una de estas dos categorías. La idea de la clasificación con árboles de decisión es bastante simple. Iterativamente se irán generando particiones binarias, es decir, de a dos agrupaciones, sobre la región de interés, buscando que cada nueva partición genere un subgrupo de datos lo más homogéneo posible. Así, primero se establece una condición y dependiendo de si los datos cumplen o no la condición, tendremos una primera partición en dos subregiones, de ahí el término particiones binarias. Y luego se repite el procedimiento anterior una y otra vez, hasta que al final se obtengan agrupaciones lo más homogéneas posible, es decir, con puntos que pertenezcan en lo posible a una sola categoría. Y esta serie de particiones la podemos representar precisamente a través de un árbol de decisión. El punto de partida del árbol se conoce como la raíz y contiene la primera condición. Este nodo genera la primera partición binaria, lo que se representa gráficamente como dos flechas indicando si se cumple o no la condición. Luego tenemos los nodos internos que corresponden a condiciones adicionales para continuar realizando la partición. Y también las hojas que corresponden a subregiones más allá de las cuales no realizaremos más particiones. La profundidad del árbol es simplemente la trayectoria más larga entre la raíz y una de las hojas, que en nuestro caso corresponde a un árbol de profundidad 3. Ok, hasta acá tenemos una idea general de cómo funciona un árbol de decisión, pero todavía nos quedan por responder algunas preguntas de fondo. Por ejemplo, ¿cómo se construye de forma automática este árbol? ¿Y cómo lograr que en este proceso se generen las agrupaciones lo más homogéneas posibles? Pues aquí precisamente es donde entra en juego el algoritmo CART, que es capaz de generar automáticamente este tipo de particiones. Para medir esta homogeneidad se usa el índice Gini, que mide el grado de impureza de un nodo. Índices Gini iguales a cero indican nodos puros, es decir, con datos que pertenecen a una sola categoría, mientras que índices mayores que cero y con valores hasta de 1 indican nodos con impurezas, es decir, con datos de más de una categoría. Por ejemplo, volviendo a nuestro set de datos, analicemos dos posibles particiones iniciales, X0 menor o igual que 6.5 y X1 menor o igual que 11. Para el primer umbral veremos que la partición del lado izquierdo contendrá únicamente 4 puntos rojos y ningún punto verde, mientras que la partición del lado derecho tendrá 6 puntos rojos y 10 verdes. Esto corresponde a un nodo izquierdo totalmente puro, es decir, un nodo hoja con un índice Gini igual a cero y a un nodo derecho con impurezas con un índice Gini de 0.469. Para el segundo umbral veremos que el nodo izquierdo será puro, pues contendrá 2 puntos rojos y ninguno verde, mientras que el nodo derecho contendrá impurezas, 8 puntos rojos y 10 verdes, que equivalen a un índice Gini igual a 0.494. Para saber cuál de estas dos particiones es la mejor, el algoritmo CART define una función de costo, que asigna un puntaje al nodo padre dependiendo de los valores de los índices Gini individuales de sus nodos hijos. Para entender esto veamos nuevamente las dos posibles particiones. Para calcular la función de costo en cada caso debemos obtener el valor promedio ponderado de la impureza de los nodos hijo a izquierda y derecha. Esto se calcula tomando el índice Gini correspondiente a cada nodo y multiplicándolo por el resultado de dividir el número de datos que pertenecen a la nueva agrupación entre el número total de datos antes de la partición. Así para la primera partición el nodo hijo del lado izquierdo tiene un índice Gini igual a cero, el número de datos resultantes de la partición es 4 puntos rojos y cero verdes y el número total de datos antes de la partición es simplemente el set de datos original, así que para este nodo hijo se tiene una impureza ponderada igual a cero. Para el nodo del lado derecho el índice Gini era 0.469, tras la partición se obtuvieron 6 puntos rojos y 10 verdes y el número total de puntos antes de la partición sigue siendo 20, lo que nos arroja una impureza ponderada de 0.375. Al sumar estos valores ponderados de las impurezas de cada uno de los nodos obtenemos el valor de la función de costo del nodo padre que es igual a 0.375. Si repetimos el mismo procedimiento para la segunda opción, es decir si calculamos las impurezas ponderadas individuales de los nodos hijos y las sumamos, obtenemos un valor para la función de costo igual a 0.446. Y ahora sí tenemos un criterio para definir cuál de las dos particiones es mejor, simplemente tomamos la que tenga el menor valor posible para la función de costo, lo que indica un menor nivel de impureza y por tanto una mejor clasificación. Y con esto ya tenemos la base del algoritmo CART para la clasificación con árboles de decisión, ahora la idea es aplicar este mismo principio de funcionamiento de forma iterativa para clasificar los datos hasta que tengamos las agrupaciones lo más homogéneas posible. Así que si volvemos al nodo raíz seleccionado vemos que el nodo hijo de la izquierda es una hoja y por tanto no haremos más particiones, sin embargo el nodo de la derecha aún contiene impurezas y podemos intentar hacer más particiones. Si repetimos el procedimiento anterior y analizamos todos los umbrales posibles, veremos que la condición X0 menor o igual a 17 es la que generará el menor costo posible. Al aplicar esta condición se generará un nodo izquierdo impuro con tres puntos rojos y diez puntos verdes y un nodo derecho totalmente puro con tres puntos rojos y ningún punto verde. El costo promedio de este nuevo nodo padre será de 0.288 y vemos que es inferior al costo del nodo raíz con lo cual podemos verificar que progresivamente estamos logrando obtener agrupaciones cada vez más homogéneas. Como el nodo izquierdo recién obtenido es impuro podemos continuar dividiéndolo. Si repetimos el mismo procedimiento veremos que la condición X1 menor o igual a 11 generará un nodo padre con el menor costo posible. Esta nueva partición generará un nodo hijo izquierdo impuro con un punto rojo y diez puntos verdes y un nodo derecho totalmente puro con tres puntos rojos y cero puntos verdes. Al calcular la función de costo para el nodo padre recién obtenido tendremos un valor de 0.139 que de nuevo es inferior a la de los nodos anteriores y podríamos continuar subdividiendo este último nodo para mejorar la precisión pero podríamos llegar al extremo de tener overfitting en este árbol de decisión, aunque de esto les voy a hablar en un momento. Pero por ahora resumamos las principales ideas de este algoritmo CART. Para crear la raíz del árbol, es decir la primera partición, se toman todas las características y para cada una de ellas se definen todos los posibles umbrales a que haya lugar. Cada umbral será simplemente el punto intermedio entre dos valores consecutivos de cada característica. Por ejemplo en el caso particular de nuestro set de datos tenemos dos características X0 y X1. Como en total tendremos 20 datos, por cada característica existirán 19 umbrales, así que en total tendremos 38 umbrales por evaluar. Para cada uno de estos umbrales se calcula la partición, nodo izquierdo y nodo derecho, y para cada nodo hijo se calcula el índice GINI. Con esto se calcula la función de costo del nodo padre, que es el promedio ponderado de los índices GINI de sus hijos. Y se toma el umbral o nodo padre resultante, que tendrá la función de costo con el menor valor posible, indicando que la partición obtenida es la más homogénea de todas las analizadas. Una vez se haya realizado esta partición, se repite el mismo procedimiento de forma iterativa para los nodos resultantes, exceptuando los que sean nodos hoja. Bien, una vez entrenado nuestro árbol de decisión podemos hacer una predicción, es decir podemos clasificar un nuevo personaje. Supongamos que tenemos uno con un nivel de fuerza igual a 13 y un nivel de letalidad igual a 17. Para clasificarlo con el árbol ya entrenado simplemente llevamos estas dos características al modelo y comenzamos a evaluar una a una las condiciones en cada uno de los nodos. La categoría en la que será clasificado el personaje será simplemente el nodo en el cual resulte ubicado después de evaluar estas condiciones. Ok, pero recuerdan que en el árbol que acabamos de entrenar teníamos al final un nodo impuro, ¿por qué no continuamos haciendo más particiones en ese nodo? Pues perfectamente podríamos hacerlo, pero el problema es que esto nos llevaría al overfitting. Es decir, en caso de dividir este nodo estaríamos obteniendo dos nuevos nodos que mejorarían la clasificación. Pero las regiones obtenidas serían muy pequeñas y es muy probable que al introducir un nuevo dato para ser analizado por el modelo, este resulte clasificado incorrectamente. Es decir, que el árbol de decisión se ajustará perfectamente a sed de entrenamiento, pero al momento de hacer predicciones no lo hará tan bien. Para evitar este overfitting tenemos esencialmente dos opciones. Una de ellas es restringir el crecimiento del árbol durante el entrenamiento y la segunda es, después del entrenamiento, eliminar algunos nodos, es decir, de alguna forma podar ese árbol. Para restringir el crecimiento del árbol durante el entrenamiento lo que podemos hacer es usar unos hiperparámetros, es decir, unas variables numéricas que definimos al inicio del algoritmo cuando lo estamos programando. Podemos por ejemplo definir la profundidad máxima o el mínimo número de datos que debe tener un nodo o el mínimo número de datos de una hoja. Con estos hiperparámetros podemos evitar la aparición descontrolada de nodos en el árbol. La otra forma de evitar el overfitting en nuestro árbol de decisión se hace después del entrenamiento y consiste en podar o eliminar algunos nodos. Uno de los métodos más usados es el de poda de complejidad de costos, que consiste en definir un hiperparámetro alfa que controla el nivel de overfitting. Con un alfa igual a cero tendremos el árbol de decisión sin ningún recorte y por tanto con un alto overfitting, mientras que a medida que aumenta alfa se eliminarán algunos nodos del árbol, hasta lograr un balance adecuado entre la precisión con el set de entrenamiento y la que se logra con el set de validación. Además de tener la posibilidad de clasificar sets de datos relativamente complejos, una de las grandes ventajas de los árboles de decisión comparado por ejemplo con la regresión logística o las redes neuronales, es la facilidad con que se pueden interpretar los resultados que arroja el modelo. Volviendo a nuestro ejemplo resulta muy sencillo entender la serie de reglas que hacen que un dato sea clasificado en una categoría o en otra, basta simplemente con mirar en detalle cada una de las condiciones en los nodos del árbol de decisión, claro esto resulta sencillo siempre y cuando el árbol y el número de características sean relativamente pequeños. Y otra gran ventaja es que una vez entrenado el árbol podemos determinar fácilmente cual o cuales fueron las características que tuvieron mayor impacto, mayor relevancia al momento de la clasificación. De nuevo si miramos el árbol de decisión obtenido podremos ver que dos de sus tres nodos usan X0, el nivel de fuerza del demonio o del asesino, mientras que solo uno usa X1, el nivel de letalidad, así que podemos decir que la primera característica es más relevante que la segunda al momento de la clasificación, de hecho existen métricas que permiten cuantificar estos niveles de importancia, pero esto lo veremos en detalle en el video tutorial sobre cómo programar árboles de decisión para clasificación en Python. La grandes ventajas es que CART es un algoritmo codicioso, para calcular el umbral óptimo en cada partición evalúa todas las posibles opciones, para nuestro caso fue sencillo porque solo teníamos dos características y 20 datos, pero entre más características y más datos tengamos el algoritmo requerirá más tiempo de entrenamiento. Bien, espero que con esta explicación hayan entendido todos los detalles de cómo funcionan los árboles de decisión y cómo usar el algoritmo CART para la clasificación de datos, de hecho estos mismos árboles de decisión son la base de algoritmos mucho más potentes del Machine Learning, como los bosques aleatorios o los métodos del Gradient Boosting o potenciación del gradiente, que veremos en próximos videos. También en un próximo video veremos un tutorial en Python de cómo usar estos árboles de decisión para clasificar un set de datos. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0094|Machine Learning|Los árboles de decisión son uno de los algoritmos de aprendizaje supervisado más importantes del Machine Learning. En el video anterior vimos cómo usar el algoritmo CART para implementar un árbol de decisión capaz de clasificar un set de datos. En este video veremos la segunda parte de este algoritmo, es decir, cómo implementar árboles de decisión para una tarea de regresión. Vamos a ver en detalle cómo funciona paso a paso el algoritmo, cómo se construye el árbol, cómo usarlo para realizar predicciones y cómo se realiza la poda en caso de que sea necesario simplificarlo. Así que sin más preámbulos, comencemos. En una tarea de regresión lo que buscamos es entrenar un modelo que sea capaz de predecir el valor de una variable continua. Por ejemplo, podríamos usar características como el área de un inmueble, el número de habitaciones y de bañas y la antigüedad para predecir una variable continua que es el costo del inmueble. Si hablamos de medicamentos podríamos usar como características la dosis en miligramos del medicamento, el peso y la edad del paciente para intentar predecir la variable continua que es el porcentaje de efectividad del medicamento. Para simplificar las cosas asumamos un problema de regresión con una sola característica. Con esto va a ser suficiente para entender los árboles de regresión y el algoritmo CART. Y luego simplemente será repetir esto paso a paso cuando tengamos dos o más características. Volviendo al problema de los medicamentos vamos a suponer que la única característica es la dosis en miligramos y que la usamos para predecir el porcentaje de efectividad. Si la relación entre estas dos variables es lineal, es decir, a mayor dosis mayor efectividad, el problema se resuelve fácilmente usando el algoritmo de regresión lineal que vimos en un video anterior. Pero compliquemos las cosas intencionalmente porque en el mundo real es así. Supongamos que la relación entre la efectividad y la dosis tiene el comportamiento que vemos en la figura. Para un rango de dosis la efectividad es baja, para otro rango tiene un valor alto y para otro tiene un valor medio. Acá necesitamos un modelo que sea capaz de tener en cuenta estos diferentes rangos y que logre predecir con precisión el porcentaje de efectividad para una dosis en particular. Y es aquí donde precisamente entran los árboles de decisión y el algoritmo CART. En la regresión con árboles de decisión de manera iterativa vamos a comenzar a generar particiones binarias sobre el espacio de características que en este caso es nuestro eje horizontal y que corresponde a la dosis en miligramos del medicamento. La idea es que estas particiones generen las agrupaciones de datos con los niveles de efectividad del medicamento lo más uniformes posible. Veamos esto en detalle. Si establecemos por ejemplo una condición de una dosis menor o igual a 37.5 miligramos, los puntos a la izquierda no estarán muy dispersos y en promedio tendrán una efectividad del 10%. Pero los del lado derecho que no cumplen la condición estarán más dispersos y por tanto el promedio de la efectividad no será una representación muy precisa de esta región. Así que dividimos esta región usando como criterio una dosis menor o igual a 60 miligramos. La partición resultante del lado izquierdo tiene baja dispersión y una efectividad promedio del 90%. Pero la del lado derecho aún está muy dispersa así que la dividimos nuevamente usando como condición una dosis menor o igual a 80 miligramos. Y acá resultan dos nuevas agrupaciones un poco más homogéneas. La de la izquierda con un promedio de 38.3% y la de la derecha con uno de 11.7%. Y con este simple ejercicio acabamos de construir nuestro primer árbol de regresión. El nodo inicial se conoce como la raíz, los demás nodos se conocen como nodos internos y las hojas son las terminaciones, es decir donde no hay más particiones. En este caso decimos que nuestro árbol tiene una profundidad igual a 3 que es la distancia máxima entre la raíz y la hoja más alejada. Bueno ya tenemos una idea general de cómo se entrena un árbol de regresión pero todavía nos quedan varias preguntas por responder. Por ejemplo ¿cómo lo usamos para realizar la regresión? ¿O cómo se construye de forma automática este árbol? ¿O cómo definimos en qué momento se dejan de hacer más particiones? En el caso de la regresión, que en nuestro caso consiste en predecir el porcentaje de efectividad del medicamento para un nuevo dato, basta con tomar la dosis correspondiente y recorrer el árbol entrenado evaluando en cada nodo la condición establecida hasta llegar a una de las hojas. En este caso la predicción será simplemente el valor promedio de los datos que pertenecen a esta hoja y que fue obtenido durante el entrenamiento. Ahora respondamos la segunda pregunta, es decir ¿cómo construir automáticamente este árbol? Y acá es donde precisamente entra en acción el algoritmo CART. El primer paso es organizar ascendentemente los valores de la característica y luego calcular el punto medio entre cada par de características consecutivas. A estos puntos los llamaremos umbrales candidatos. La idea ahora es seleccionar el mejor de estos umbrales candidatos, es decir el que genere las particiones con la menor dispersión posible de la variable continua, que en este caso es la efectividad. Para entender cómo medir esta dispersión analicemos el primer umbral, una dosis menor o igual a 10 mg. Al usar esta condición la región izquierda tendrá un solo punto y su dispersión será nula, porque su promedio será exactamente igual al valor de la efectividad, es decir del 10%. Pero la región de la derecha tiene un comportamiento diferente, pues tendrá 11 puntos con una efectividad promedio del 40%. Sin embargo la mayor parte de estos puntos se encuentra alejada del promedio, así que habrá una alta dispersión. Para medir esta dispersión podemos simplemente promediar la diferencia existente entre el porcentaje de efectividad de cada punto y el valor de efectividad promedio de la región. Pero para evitar que algunas diferencias positivas se anulen con diferencias negativas, elevaremos al cuadrado estas diferencias y luego si las promediaremos. Y esta métrica que acabamos de definir se conoce como el error cuadrático medio. Un error cuadrático medio igual a 0 es la condición ideal, es decir una dispersión nula, mientras que entre más alto sea este error mayor será el grado de dispersión. Si volvemos a las dos regiones que acabamos de obtener y calculamos su error cuadrático medio, veremos que la de la izquierda tiene un error igual a 0, mientras que la de la derecha tiene una dispersión bastante alta, pues su error es mucho mayor, de 1127.3. Ahora conociendo el grado de dispersión de cada una de las agrupaciones podemos calcular un puntaje para el umbral seleccionado, lo que llamaremos en adelante la función de costo. Para esto primero calculamos la dispersión promedio en cada región, es decir tomamos el valor del error cuadrático medio y lo multiplicamos por la fracción de los datos que se encuentran en cada agrupación con respecto al número original de datos antes de realizar la partición. Así por ejemplo, la región izquierda tenía un error cuadrático medio igual a 0 y en total había uno de los 12 datos iniciales, lo que equivale a una dispersión promedio igual a 0. En el lado derecho el error era igual a 1127.3 y se tenían 11 de los 12 datos iniciales, lo que equivale a una dispersión promedio igual a 1033.4. Y con esta información calculamos la función de costo para este umbral, que será simplemente la suma de las dos dispersiones promedio que acabamos de obtener, es decir igual a 1033.4. Perfecto, ya tenemos una métrica para evaluar qué tan buena o no es una partición. La idea es ahora repetir el mismo procedimiento del cálculo de la función de costo para cada umbral y una vez hecho esto tomar el umbral con la menor función de costo posible, que será precisamente el que genere las particiones con la menor dispersión. En este caso particular veremos que la mejor condición de todas es una dosis menor o igual a 37.5 miligramos. Y bien con esto ya tenemos una primera partición, lo que podemos seguir haciendo es ahora refinar cada una de estas regiones haciendo más y más particiones. Si por ejemplo nos enfocamos en la región del lado derecho veremos que no se puede subdividir, así que repetimos el algoritmo anterior. Definimos los umbrales candidatos para esta región de interés y luego calculamos la función de costo de cada uno y elegimos el umbral con el menor costo posible, que en este caso equivale a una dosis menor o igual a 60. Y con esto ya tenemos la base del algoritmo CART para la regresión, que se puede resumir de la siguiente forma. Primero calcular los umbrales candidatos, que corresponden al punto intermedio entre dos valores consecutivos de las características. Segundo para cada umbral candidato obtener las dos particiones y calcular sus errores cuadráticos medios. Tercero calcular la función de costo de cada umbral candidato y elegir aquel con el menor costo. Y cuarto repetir los pasos 1 a 3 de forma iterativa hasta cumplir con un criterio de parada. Pero nos quedaba una pregunta por responder, que era ¿cómo sabemos en qué momento dejamos de hacer más particiones? Pues la respuesta está en el término que acabamos de mencionar, el criterio de parada. Si volvemos al árbol que acabamos de entrenar veremos que algunas hojas tienen algo de dispersión, así que podríamos seguir subdividiéndolas. Pero al hacerlo estaríamos obteniendo regiones cada vez más pequeñas, que se ajustarían prácticamente a un solo dato de entrenamiento, pero que no funcionarían también cuando intentemos hacer la predicción. Es decir, que el árbol estaría haciendo overfitting, se ajustaría perfectamente al set de entrenamiento, pero no funcionaría de forma adecuada cuando tenga nuevos datos que nunca antes había visto. Para evitar que haya overfitting, es decir, que el árbol crezca de forma indiscriminada, usamos precisamente un criterio de parada, que básicamente es una condición que establece hasta dónde se va a subdividir un nodo. El criterio de parada más usado es el mínimo número de datos por hoja. Por ejemplo, en el árbol que acabamos de entrenar este criterio era igual a 3, lo cual quiere decir que no se realizarán más particiones a un nodo que alcance este mínimo número de datos. Si por ejemplo cambiamos este número a 2, tendremos un árbol con unas hojas, mientras que si es igual a 4, el tamaño del árbol será menor. Este criterio de parada también se conoce como prepoda, pero también existe otra forma de controlar el tamaño del árbol, que es primero entrenarlo y después eliminar algunas hojas, lo que se conoce precisamente como post-poda. El algoritmo más usado para realizar este recorte es la poda de complejidad de costos, que funciona de la misma forma como lo vimos en el video anterior para el caso de la clasificación con árboles de decisión. La idea es que se usará un parámetro alfa que controla el nivel de poda. Con un alfa igual a cero no se elimina ninguna hoja, y a medida que aumenta se eliminarán más hojas y se controlará por tanto el tamaño del árbol de regresión. Bien, y con esto acabamos de ver detalladamente cómo funciona el algoritmo CART para entrenar y usar un árbol de decisión para una tarea de regresión. El principio de funcionamiento es bastante sencillo, pero más importante aún el árbol de decisión resultante es fácil de interpretar, porque para entender cómo hizo la predicción basta con mirar las condiciones establecidas en cada uno de los nodos del árbol. Y aunque para facilitar la explicación vimos el funcionamiento del algoritmo para una sola característica, la misma idea la podemos extender de forma iterativa a dos o más características. Si combinamos lo que acabamos de ver en este video con lo que vimos en el video anterior sobre los árboles de clasificación, tenemos dos de los algoritmos más poderosos del Machine Learning, que son la base de un algoritmo incluso aún más poderoso que se conoce como los bosques aleatorios, y que será el tema del próximo video. Así que si les gustó este video no olviden darle un pulgar hacia arriba de me gusta, porque esto me ayudaría un montón para que la red neuronal de YouTube le recomiende este mismo contenido a otras personas. Por el momento esto es todo, los invito a continuar viendo los videos que les voy a sugerir de este lado, les envío un saludo y nos vemos en el próximo video.
DOC0095|Machine Learning|En los videos anteriores hablamos de los árboles de decisión y de cómo usarlos para resolver problemas de regresión y clasificación. El entrenamiento de estos árboles se hace generando de forma recurrente particiones sobre el espacio de características, para obtener agrupaciones de datos cada vez más uniformes. Y esta uniformidad se mide con el índice Gini en el caso de la clasificación y con el error cuadrático medio en el caso de la regresión. Y para clasificar un dato nuevo o para realizar la regresión simplemente verificamos las condiciones para ese dato en cada nodo del árbol, hasta que sea asignado a una hoja en particular. Pues a pesar de que estos árboles de decisión son arquitecturas muy poderosas, tienen una gran desventaja, el overfitting. Esto quiere decir que funciona muy bien cuando los estamos entrenando, pero no tanto cuando queremos hacer predicciones con nuevos datos. Y es esta limitación la que precisamente dio origen a los bosques aleatorios, que son uno de los algoritmos más usados y más poderosos del Machine Learning. Entonces en este video hablaremos de esos bosques aleatorios, de cómo entrenarlos y validarlos y de cómo usarlos para hacer una predicción, de cómo elegir los parámetros para su entrenamiento y de las cosas en común y las ventajas que tienen frente a los árboles de decisión. Para entender todos los detalles les recomiendo ver los dos vídeos anteriores, en donde les explico cómo funcionan los árboles de clasificación y los árboles de regresión. Pero antes de entrar en todo esto les quiero contar que el próximo 10 de julio voy a hacer el lanzamiento de la Academia Online de Cursos de Machine Learning y Ciencia de Datos. Así que si están interesados pueden visitar codificandovids.com y diligenciar el formulario para que puedan recibir más información a vuelta de correo. Bien, hablemos primero del gran problema de los árboles de decisión, el compromiso entre el bias o sesgo y la varianza. Los árboles de decisión tienen la característica de que al momento de entrenarlos se ajustan bastante bien a ese set de entrenamiento, lo que se conoce como un bias bajo, pero al momento de hacer la predicción el error o varianza es relativamente alto y esto se debe a que son muy sensibles a los datos de entrenamiento. Una ligera variación en tan solo algunos de ellos puede dar origen a un árbol totalmente diferente, lo que dificulta aún más hacer predicciones con datos nuevos. Y cuando tenemos un bias bajo y una varianza alta estamos precisamente ante un problema de overfitting, pues resulta que los bosques aleatorios permiten resolver este problema, preservando lo mejor de los árboles de decisión que es su bias bajo pero además logrando reducir su varianza. Bueno, ¿y cómo se logra esto? Pues esencialmente introduciendo dos variantes a estos árboles de decisión, la primera es que en lugar de entrenar un único árbol se entrenan varios, usualmente decenas o cientos, de ahí precisamente el término bosques, pero esto no es suficiente porque si entrenamos cada árbol del bosque con el mismo set de datos, seguiríamos teniendo el mismo problema de la varianza alta. Con la aleatoriedad cada árbol será entrenado con un subset diferente, así que si el set de entrenamiento tiene algo de ruido, probablemente este afectará a algunos árboles pero no a la totalidad del bosque. Además, al agregar los resultados para generar la predicción, los árboles que no funcionan también no tendrán un impacto significativo en ese resultado final, así que al combinar estos dos elementos, la aleatoriedad y la agregación se logra reducir la varianza de los árboles individuales. Entonces un bosque aleatorio es como el equipo de jueces que evalúa a los clavadistas en los Juegos Olímpicos, habrá jueces poco exigentes o demasiado estrictos, es decir que generarán una alta varianza que asignarán puntajes o muy altos o muy bajos a los participantes, pero al tener 7 u 8 jueces y obtener la puntuación de forma conjunta, es decir agregando los resultados, evitamos tener puntajes excesivamente altos o demasiado bajos. Bien, ya tenemos una idea general de cómo funcionan estos bosques aleatorios, ahora veamos cómo hacer el entrenamiento, y para eso vamos a suponer que vamos a resolver una tarea de clasificación, aunque si la tarea es de regresión el procedimiento será equivalente. Partamos de un set de datos con 6 ejemplos de entrenamiento, es decir 6 filas, cada uno con 4 características, es decir 4 columnas. El primer paso es definir el número de árboles que tendrá el bosque, para facilitar la explicación supongamos que este número es 5, aunque en realidad se usan decenas o cientos de árboles, pero de esto hablaremos más adelante. El segundo paso es crear el subset de entrenamiento de cada árbol, introduciendo precisamente un componente aleatorio, como tenemos 5 árboles la idea es crear 5 subsets de entrenamiento a partir del set original, para lograr esto debemos hacer algo que se llama el muestreo con reemplazo o bootstrapping, en cada caso vamos a tomar al azar observaciones, es decir filas, del set original hasta completar un total de 6 observaciones, el término reemplazo hace referencia a que una misma fila podrá aparecer varias veces en el subset que estamos creando, por ejemplo digamos que para crear el primer subset se toman aleatoriamente las observaciones 1, 2, 5, 1, 1 y 3, y acá el reemplazo implica que la primera observación se repite 3 veces, para el segundo subset tomaremos al azar y con reemplazo otro grupo de observaciones, por ejemplo las filas 6, 3, 2, 5, 6 y 2, de nuevo el reemplazo implica que las filas 6 y 2 aparecerán repetidas en el subset, y así repetimos este procedimiento hasta obtener los 5 subsets de entrenamiento, después de esto viene el entrenamiento que consiste en tomar cada subset y realizar de forma recurrente las particiones en el espacio de características, para crear cada uno de los árboles, y aquí es donde introducimos un segundo elemento de aleatoriedad, porque en lugar de tomar todas las características o variables disponibles, vamos a tomar aleatoriamente solo una parte, así que para obtener cada nodo de cada árbol, es decir las particiones, en lugar de escoger las cuatro columnas que tenemos disponibles, al azar vamos a escoger dos o tres de ellas, supongamos que para este ejemplo en particular fijamos ese número en 2, pero más adelante vamos a ver cuáles son los criterios para determinar cuál es el número más adecuado de características a extraer, si vamos al primer árbol para determinar su primer nodo, seleccionamos entonces de forma aleatoria dos de las características, supongamos que la 1 y la 4, y obtenemos la mejor partición, para el segundo nodo nuevamente escogemos de forma aleatoria otras dos características, supongamos que la 2 y la 4, y calculamos la mejor partición, y repetimos este procedimiento una y otra vez hasta construir el árbol completo y alcanzar el criterio de parada, y esto lo repetimos para cada uno de los árboles que conforman el bosque, y con esto tenemos listo el entrenamiento, que es muy similar a lo que se hace convencionalmente con los árboles de decisión, con la única diferencia que en lugar de tomar todas las características, escogemos solo unas cuantas de ellas de forma totalmente aleatoria, con el bosque aleatorio ya entrenado es fácil realizar la predicción, simplemente se introduce el nuevo dato a cada árbol, se realiza la clasificación individual, y se escoge la categoría asignada por la mayoría de los árboles, si la tarea fuese de regresión el procedimiento sería muy similar, se realiza la regresión individual y el valor final sería simplemente el promedio de la predicción hecha por cada árbol, acá es importante que resumamos los dos mecanismos esenciales detrás del funcionamiento de estos bosques aleatorios, el bootstrapping durante el entrenamiento y la agregación de los resultados durante la predicción, si miramos en detalle el bootstrapping nos daremos cuenta de que no todas estas observaciones seleccionadas aleatoriamente quedarán incluidas al final en los subsets de entrenamiento, en realidad aproximadamente una tercera parte de ellas quedará por fuera y no será usada durante el entrenamiento, así que a diferencia de otros algoritmos del machine learning donde al inicio el set de datos se divide entre entrenamiento y validación, en el caso de los bosques aleatorios no ocurre lo mismo, porque el set de validación se extrae precisamente de las muestras que no fueron tomadas aleatoriamente del set de entrenamiento original, estos ejemplos de entrenamiento se conocen como muestras por fuera de la bolsa o out of back samples, pero si nos devolvemos al ejemplo que acabamos de ver nos quedan todavía tres preguntas por resolver, ¿cómo sabemos cuántos árboles usar? ¿cómo sabemos hasta dónde hacer crecer los árboles? y ¿cómo sabemos cuántas características seleccionar aleatoriamente durante el entrenamiento? Para ajustar estos hiperparámetros podemos tomar las muestras que están fuera de la bolsa, introducirlas al bosque aleatorio y mirar cómo se comporta el error de la predicción con el cambio de estos parámetros. Podemos primero fijar el número de características y el criterio de parada, y entrenar múltiples bosques cambiando progresivamente el número de árboles que lo conforman, en este caso veremos que a medida que se tienen más y más árboles el error irá disminuyendo, usualmente en aplicaciones prácticas se usan entre 100 y 200 árboles. Con el número de árboles ya determinado fijamos el criterio de parada y repetimos el procedimiento cambiando el número de características y escogemos aquel que genere el menor error. Finalmente con el número de árboles y de características fijo variamos el criterio de parada y escogemos el que arroje el menor error, recuerden que el criterio de parada puede ser por ejemplo el mínimo número de datos de una hoja. Otra de las ventajas interesantes de estos bosques aleatorios es que, al igual que con los árboles de decisión, podemos definir el orden de importancia de las características, para lo cual podemos tomar por ejemplo el índice Gini en el caso de la clasificación o el error cuadrático medio en el caso de la regresión, porque en ambos casos ellos miden precisamente la calidad con que se está realizando esa partición. Así que si durante el entrenamiento del bosque aleatorio mantenemos un registro de todas estas puntuaciones, al final podremos obtener el puntaje individual de cada característica y podremos determinar cuales de ellas generan las mejores particiones y cuales no. Y esta información puede resultar súper útil al momento de desarrollar un modelo de Machine Learning, porque podemos seleccionar las características más relevantes y descartar otras que tengan menor importancia. Bien, y con esto ya tenemos un panorama completo y detallado de qué son y cómo funcionan estos bosques aleatorios, uno de los algoritmos más importantes del Machine Learning y que en la práctica es de los más usados para resolver una gran variedad de problemas. En resumen, lo que hace este método es usar el bagging, es decir el bootstrapping y la agregación para entrenar múltiples árboles y luego combinar sus resultados individuales, logrando así un bosque aleatorio que va a tener un bias y una varianza mucho más bajos. Bien, y esto es todo. Como siempre, los invito a compartir este video con sus amigos y conocidos y a darle un pulgar hacia arriba de me gusta. Les envío un saludo y nos vemos en el próximo video.
DOC0096|Machine Learning|En el video anterior en donde hablamos del Machine Learning Engineering vimos todas las etapas involucradas en el desarrollo de un proyecto de Machine Learning y además vimos que la etapa de preparación de los datos es una etapa fundamental y que requiere usualmente entre el 60 y el 70% del tiempo de desarrollo y una de esas tareas fundamentales es el análisis exploratorio de los datos que es precisamente el tema de este video. Y aunque en internet se encuentran muchos recursos y tutoriales sobre ese tema, realmente no existe una guía clara de cómo hacer este análisis exploratorio de los datos. Así que en este video, en lugar de un tutorial, veremos una guía paso a paso de cómo hacer precisamente este análisis exploratorio de los datos en Machine Learning y Ciencia de los Datos. Vamos a ver en qué consiste el análisis exploratorio, cuáles son los tipos de datos y las herramientas estadísticas para describirlos. Hablaremos de las herramientas de visualización, del análisis bivariado y multivariado y de la sumarización. En la descripción del video les voy a dejar el enlace para que puedan descargar esta guía. Pero antes de todo esto, les quiero contar que el próximo 10 de julio haremos el lanzamiento de la Academia Online de Cursos de Machine Learning y Ciencia de Datos. Así que si están interesados, pueden ingresar a codificandovids.com, diligenciar el formulario con sus datos y a vuelta de correo les voy a enviar más detalles. Bien, primero tengamos en cuenta que todo lo que les voy a contar de aquí en adelante aplica únicamente para datos estructurados, es decir, que vienen en formato tabular. Para datos no estructurados o para series de tiempo, el análisis exploratorio es totalmente diferente. Así que si les interesa, me pueden dejar un comentario abajo para preparar más adelante otros videos sobre esos temas. Ok, el principal propósito del análisis exploratorio de datos es tener una idea completa de cómo son nuestros datos antes de decidirnos por una técnica en particular de ciencia de datos o por un modelo de machine learning. Y como en la práctica los datos no son ideales, debemos organizarlos, entender su contenido, entender cuáles son sus variables y cómo están posiblemente relacionadas. Comenzar a ver algunos patrones, determinar qué hacer con los datos faltantes y con los datos atípicos y finalmente extraer conclusiones acerca de todo este análisis. Y todo esto es precisamente el análisis exploratorio de datos, que en últimas es una forma de entender, de analizar los datos, de visualizarlos, de extraer información relevante para posteriormente elegir cuál es la ruta o técnica más adecuada para su posterior procesamiento. Y este es siempre el paso cero en cualquier proyecto de machine learning o ciencia de datos. Siempre tenemos que comenzar por acá. Bien, teniendo esto claro, podemos resumir las fases del análisis exploratorio en siete pasos. El primero es tener clara la pregunta que queremos responder. El segundo es tener una idea general de nuestro data set. Después debemos definir los tipos de datos que tenemos y luego elegir el tipo de estadística descriptiva y la visualización a utilizar. En el sexto paso debemos analizar las posibles interacciones entre las variables del data set y finalmente debemos extraer algunas conclusiones de todo este análisis. Para entender todas estas fases usaremos un data set clásico de Kaggle, el del Titanic, un set de datos que contiene información de los pasajeros como nombres, edades, género y obviamente la categoría a la que pertenece, es decir, si sobrevivió o no al hundimiento. El primer paso, la pregunta que queremos responder en este caso es qué tipo de personas tenían la probabilidad más alta de sobrevivir al hundimiento del Titanic. Y para responder esta pregunta debemos echar primero un vistazo al data set, mirar su tamaño, determinar cuáles son las características o variables, es decir, las columnas de la tabla y dar un primer barrido a los registros u observaciones, es decir, las filas del data set. Y con esto nos haremos una idea general de los datos, viendo que por ejemplo cada pasajero estará caracterizado por variables como el nombre, la edad, el género, etcétera. Bien, después de esto podemos empezar a analizar en detalle el data set, así que el paso tres es definir a qué tipo de variable pertenece en nuestros datos y aquí tenemos dos opciones, las variables numéricas y las variables categóricas. Los numéricos pueden ser discretos cuando toman solo valores enteros, como por ejemplo la edad de cada pasajero o continuos cuando pueden tomar cualquier valor dentro de un intervalo, como por ejemplo la tarifa del tiquete. Los datos categóricos pueden ser nominales, binarios u ordinales. Los nominales se usan para etiquetar el dato, pero no pueden ser ordenados ni medidos, como por ejemplo el género de los pasajeros, que puede ser hombre o mujer. Los datos binarios indican una de dos posibles categorías, como por ejemplo sobreviviente o no sobreviviente. Y finalmente están los datos ordinales, que como su nombre lo indica, corresponden al orden en el que vienen representados los datos, como por ejemplo la categoría del tiquete. Uno, dos o tres. El cuarto paso es iniciar con la descripción estadística, que depende precisamente del tipo de dato que tengamos en cada una de las variables. Y para esto usamos dos grandes tipos de medidas, las de tendencia central y las de variabilidad. Las de tendencia central nos dan una idea general del valor típico que pueden tener nuestros datos, y las principales son la media y la mediana. La media es simplemente el promedio de los datos y por tanto se puede aplicar a datos discretos, como por ejemplo la edad de los pasajeros, o continuos como por ejemplo el valor de los tiquetes. La desventaja de la media es que es muy sensible a valores atípicos. Si por ejemplo la mayor parte de los tiquetes tenían precios bajos, pero sólo unos cuantos tenían unos precios muy altos, al calcular el promedio daría la impresión de que la mayoría de los pasajeros compraron tiquetes un poco más costosos. La mediana resuelve este inconveniente, y es simplemente el valor que divide los datos en dos mitades, y se puede aplicar para datos ordinales o discretos, como la categoría del tiquete o la edad. Para calcularla debemos primero organizar los datos de manera ascendente y luego encontrar el valor tal que la mitad de los datos estarán por debajo de dicho valor y la otra mitad por encima. Pero resulta que no es suficiente con conocer la media o la mediana de la distribución, porque también debemos tener una idea de qué tan agrupados o dispersos se encuentran los datos. Para determinar esto usamos las medidas de variabilidad, donde las principales son la desviación estándar y el rango intercuartiles, y nos indican que tanto se alejan los datos del valor medio o de la mediana respectivamente. La desviación estándar se puede calcular para cualquier tipo de dato numérico, entre más bajo sea su valor tendremos datos más agrupados y viceversa. La desventaja de la desviación estándar es la misma de la media, es muy sensible a los valores atípicos. Una alternativa es usar el rango intercuartiles, que es la diferencia entre el percentil 75 y el percentil 25. Si la mediana es el punto medio de los valores observados, el percentil 75 es el valor por debajo del cual se encuentra el 75% de los valores, mientras que el percentil 25 corresponderá al 25% de dichos valores. Al igual que la mediana esta diferencia intercuartiles también es menos sensible a valores atípicos en comparación con la desviación estándar. Así, en nuestro dataset el percentil 75 es 38 años y el 25 es 20 años, y por tanto el rango intercuartiles será de 18 años, y entre más grande sea este rango, más dispersos estarán los datos. Los percentiles 25, 50, es decir la mediana y 75, dividen la distribución exactamente en cuatro partes llamadas cuartiles. El primer cuartil cubre del 0 al 25% de la distribución, el segundo del 25 al 50%, el tercero del 50 al 75% y el cuarto del 75 al 100%. Tengan en cuenta esta definición porque la usaremos más adelante. La limitación de las medidas centrales y de variabilidad es que son solo un número, entonces nos pueden dar solo una idea muy general de cómo se están comportando nuestros datos, Así que el quinto paso de este análisis exploratorio es precisamente visualizar los datos para poder empezar a encontrar más detalles. Para datos continuos y discretos podemos calcular y dibujar el histograma, que se obtiene tras organizar los datos en diferentes grupos o bins y realizar el conteo del número de datos en cada uno. Con el histograma podemos verificar que la distribución es normal, es decir que tiene forma como de campana, como por ejemplo la edad, o si está sesgada, como una campana pero asimétrica, como por ejemplo la tarifa. La desventaja del histograma es que no permite ver los valores atípicos porque quedan enmascarados cuando los datos se introducen en uno de sus bins, así que la alternativa en este caso, o cuando los datos están sesgados, es usar lo que se conocen como los diagramas de caja o los boxplots, que se pueden usar para datos tanto continuos como discretos. En un boxplot se dibujan los presentiles, las barras superior e inferior corresponden a los presentiles 75 y 25, mientras que la línea en medio de la caja es la mediana. Por fuera de la caja hay dos líneas, conectadas por líneas punteadas que se llaman whiskers o bigotes en español, y cada una de ellas es igual al presentil 75 o 25, más o menos 1.5 veces el rango intercuartil. Si por ejemplo dibujamos el boxplot para la edad y la tarifa y superponemos los datos originales, podemos fácilmente interpretar estas variables. Vemos que el rango de edades es uniforme entre 0 y 68 años, mientras que la mayor parte de los pasajeros tenía etiquetes económicos entre 0 y 30. También podemos ver los outliers que están más allá de las líneas de los extremos. El tratamiento de los outliers lo veremos en detalle en otro video, pero por ahora recordemos que es un elemento fundamental de este análisis exploratorio de los datos. Y bien, pero qué pasa cuando tenemos datos categóricos? Pues en este caso podemos visualizar un gráfico de barras y mostrar, por ejemplo, el conteo de ocurrencias en cada categoría o el porcentaje que estas categorías representan del total de datos. Por ejemplo, el gráfico de barras para la variable supervivencia nos muestra que fueron más los no sobrevivientes que los sobrevivientes. Esto nos da una pista del posible esquema utilizar en la predicción. El set de datos está desbalanceado y probablemente si usamos un clasificador convencional tendremos problemas para entrenarlo. Hasta el momento hemos analizado y visualizado una sola variable, lo que se conoce precisamente como el análisis univariado, pero también podemos empezar a mirar interacciones y posibles relaciones entre dos o más variables, lo que se conoce como el análisis bivariado y multivariado. El análisis bivariado consiste en comparar pares de variables y aquí podemos aprovechar los tipos de gráficas que vimos anteriormente para analizar esas interacciones. Por ejemplo, si queremos comparar dos variables numéricas como la tarifa y la edad del pasajero, podemos usar una gráfica de dispersión. Donde cada punto es representado por un dato y podemos verificar si existe alguna tendencia lineal, es decir, si el aumento de una variable genera un aumento o disminución de la otra o podemos calcular el índice de correlación entre estas dos variables, donde un valor cercano a uno nos indica una relación lineal, uno cercano a menos uno una relación lineal inversa y un valor cercano a cero indica que no hay correlación lineal entre los datos, que es precisamente lo que ocurre en este ejemplo en particular. También podemos comparar una variable numérica como la tarifa con una variable categórica como la variable supervivencia y usar, por ejemplo, un gráfico de barras para determinar si la tarifa está relacionada con la probabilidad de supervivencia o para la misma comparación podemos usar una gráfica de violín que es similar a un boxplot, pero además de mostrar la mediana y los límites de los cuartiles, incluye una gráfica de densidad de la distribución, que es como una gráfica continua del histograma. También podemos comparar dos variables categóricas, como por ejemplo el título del pasajero y la variable supervivencia, usando gráficos de barras apiladas, lo que nos permite ver que en este caso la mayor parte de las pasajeras con categoría señorita sobrevivieron al hundimiento. Por otra parte, en el análisis multivariado comparamos simultáneamente todos los posibles pares de variables para encontrar algún tipo de relación. Para cada comparación calculamos el índice de correlación entre diferentes pares de variables y dibujamos los resultados en una matriz de correlación. En la diagonal principal de esta matriz tendremos valores iguales a uno, porque estamos comparando una variable consigo misma. Pero lo que nos interesa es lo que está por fuera de esta diagonal. Por ejemplo, para el caso de nuestro dataset podemos ver que no existe relación alguna entre la clase del pasajero y la probabilidad de supervivencia y podemos analizar en detalle diferentes pares de variables para ver si hay relaciones más relevantes que otras. Y la última fase de este análisis exploratorio consiste en sumarizar las observaciones, es decir, en extraer las conclusiones más importantes de todo este análisis que hemos venido haciendo. En este caso, lo que les sugiero es simplemente escribirlas como frases muy cortas. Esto nos servirá para identificar, por ejemplo, las variables o características que están correlacionadas o cuáles de ellas de pronto son más relevantes. Y esto es fundamental para las etapas posteriores del proyecto, como por ejemplo el preprocesamiento de datos, la extracción de características o el desarrollo mismo del modelo en el caso de que tengamos un proyecto de machine learning. Bien, con esto ya tenemos las principales fases del análisis exploratorio de los datos. Recuerden que todos estos pasos que les acabo de contarlos podrán descargar en la guía que van a encontrar el enlace que les dejo aquí abajo en la descripción. Pero también recuerden que se nos quedan por fuera dos etapas importantísimas de este análisis exploratorio, de las cuales vamos a hablar en los próximos videos, que son el manejo de datos faltantes, es decir, cuando nuestro dataset está incompleto y el manejo de los Outliers. Bien, por el momento esto es todo. No olviden darle un pulgar hacia arriba de me gusta al video, compartirlo con sus amigos y conocidos y dejar sus comentarios acá abajo. Les envío un saludo y nos vemos en el próximo video.
DOC0097|Machine Learning|En el video anterior hablamos del análisis exploratorio de los datos, y allí mencionamos que en cualquier proyecto de Machine Learning o de ciencia de datos nos tenemos que enfrentar a una situación ineludible, y es que los datos no son ideales, pues en este video nos vamos a enfocar en uno de esos problemas ineludibles y que es una tarea fundamental del análisis exploratorio de datos, que es el manejo de datos faltantes. Así que veremos en detalle las dos grandes técnicas para el manejo de datos faltantes, la eliminación y la imputación. En la descripción del video les voy a dejar un enlace para que puedan descargar una guía con todo lo que vamos a hablar ahora, para que fácilmente puedan utilizarla en sus proyectos cuando sea necesario. Pero antes de entrar en todo esto les quiero contar que el próximo 10 de julio haré el lanzamiento de la Academia Online de cursos de Machine Learning y Ciencia de Datos, así que si están interesados, pueden entrar a codificandovids.com y diligenciar el formulario con sus datos para que a vuelta de correo yo les envíe más detalles de estos cursos. Para saber cuál técnica podremos usar para el tratamiento de estos datos faltantes, primero tenemos que determinar cuál es el mecanismo detrás de esta pérdida de datos, y estos mecanismos se dividen en tres. Primero están los datos faltantes completamente aleatorios, y en este caso la razón de la falta de los datos es ajena a los datos mismos. Para entender esto supongamos que tenemos un set de datos y que los datos faltantes aparecen tanto en la categoría A como en la B o en la C, y los valores faltantes pueden ser altos o bajos. Esto quiere decir que estos datos faltantes no dependen ni de la categoría ni del valor mismo de los datos, por lo que podemos decir que el mecanismo es completamente aleatorio. El segundo mecanismo, es decir, los datos faltantes no aleatorios, es totalmente opuesto al anterior. Esto quiere decir que la razón detrás de la falta de esos datos depende precisamente de los datos que hemos recolectado. De nuevo esto se entiende mejor con un ejemplo. Volviendo a nuestro set de datos hipotético, podemos ver que sistemáticamente los datos con valores menores a 100 faltan, tanto para las categorías A como B como C. Es decir que los valores faltantes dependen de la variable B2, y por tanto la razón de la falta de datos no es aleatoria. Este mecanismo es el más complicado de todos, porque como la pérdida de datos es sistemática, es decir hay una razón de fondo, tenemos que encontrar esa razón, intentar corregir el problema y muy probablemente adquirir los datos nuevamente. Y el tercer mecanismo, es decir, el de los datos faltantes aleatorios, es un punto intermedio entre los dos anteriores. Esto quiere decir que la razón detrás de la falta de estos datos no depende de los mismos datos faltantes, pero sí puede depender de otras variables o otras columnas dentro del mismo set de datos. De nuevo en el ejemplo vemos que los datos faltantes corresponden únicamente a datos en la categoría B, y que estos datos faltantes van desde los más pequeños a los más grandes. Esto quiere decir que los valores faltantes dependen sólo de la variable B1, la categoría, y no de la propia variable B2. Para este mecanismo y para el primero también podremos usar las técnicas que vamos a ver a continuación. Estas técnicas se dividen en dos grandes grupos, que son el descarte y la imputación. El descarte consiste simplemente en eliminar los registros que contengan datos faltantes, mientras que en la imputación lo que buscamos es estimar el valor del dato faltante, bien sea usando la información de los registros vecinos o la información de las columnas o variables que hacen parte de nuestro set de datos. En términos generales, estas técnicas las podemos aplicar a datos faltantes aleatorios o completamente aleatorios, es decir que en el fondo asumen que existe algún grado de aleatoriedad en este mecanismo responsable de la pérdida de los datos. Sin embargo, para el caso de datos faltantes no aleatorios, no es aconsejable usar estas técnicas, porque como les comenté antes, en este caso la pérdida de datos es sistemática. Entonces ahora sí hablemos de los algoritmos más usados en cada caso cuando implementamos un proyecto de Machine Learning o de ciencia de datos. En el caso del descarte, la eliminación se puede hacer de dos formas, la primera se conoce como eliminación de la lista, y consiste en remover del set de datos las filas que contengan datos faltantes, con la desventaja de que al eliminar una fila completa eliminaremos también algunos datos existentes, lo que puede llevar a una pérdida significativa de información. La segunda forma es la eliminación por pares, que es un método menos agresivo que el anterior, pues en lugar de eliminar la fila completa se quitarán únicamente las casillas con el dato faltante. La ventaja es que preservaremos los datos conocidos, pero la desventaja es que podremos tener características, es decir columnas, con diferente cantidad de datos, lo que puede complicar el entrenamiento de un modelo de Machine Learning, pues el número de datos debe ser el mismo para cada característica. De todos modos les sugiero usar estos métodos con pinzas, porque si tenemos demasiados datos faltantes, generalmente un poco más del 10%, lo que puede ocurrir es que al eliminarlos estaremos cambiando la distribución de nuestros datos originales, y esto puede afectar el modelo de Machine Learning que entrenemos más adelante, o el análisis que hagamos posteriormente. Por ejemplo, si estamos hablando de un problema de clasificación, al final con esta eliminación podríamos tener más datos de una categoría que de otra. Para evitar una pérdida significativa de datos, lo mejor es usar la imputación. Y ojo, porque esto no es lo mismo que inventar datos. Cuando inventamos no tenemos en cuenta ningún criterio y el valor asignado es totalmente arbitrario, pero en la imputación lo que hacemos es mirar los valores de los datos vecinos para tener una estimación aproximada del valor del dato faltante. Idealmente esta imputación no debería cambiar la distribución de nuestros datos, así que si originalmente teníamos una distribución normal con forma de campana, entonces después de la imputación se debería mantener esta forma original. Para esta imputación podemos usar dos técnicas, la imputación simple y la imputación múltiple, y en este caso sugiero, al igual que con el descarte, usarlas solo si estamos seguros que los mecanismos detrás de la pérdida de datos corresponden a datos faltantes aleatorios o completamente aleatorios. En la imputación simple se usa un algoritmo para hacer una única estimación, y el valor obtenido se usa para reemplazar el dato faltante correspondiente. En este caso las tres técnicas más usadas en el Machine Learning y la ciencia de datos son la imputación por la media o la mediana, la imputación por regresión y la imputación hotdeck. La imputación por la media o la mediana es la más sencilla de todas. Simplemente se toman los valores conocidos en la variable donde están los datos faltantes, se calculan la media o la mediana y se reemplazan estos datos faltantes con cualquiera de estos dos valores. Aunque es muy fácil de implementar, este método tiene la desventaja de que al reemplazar muchos datos faltantes con un único valor estaremos cambiando la distribución de los datos. Una alternativa es hacer la imputación por regresión, en este caso cada dato faltante es reemplazado con el valor predicho por un modelo de regresión. Para esto primero se combina la información de la columna con los datos faltantes con columnas en donde los datos están completos, para así ajustar un modelo de regresión y luego se usa este modelo para predecir los datos faltantes. Este método es mejor que la imputación por la media o la mediana, porque los datos faltantes no serán reemplazados con un único valor en todos los casos, lo cual nos permite preservar la distribución original de los datos. La desventaja es que para poder realizar la regresión, lineal, logística o polinómica, debemos garantizar que hay algún tipo de correlación entre los datos que usamos para construir ese modelo de regresión. El tercer método de imputación simple es la imputación hotdeck. En este caso el dato faltante es reemplazado con valores tomados de datos cercanos a ese dato faltante. Dentro de esta categoría el método más usado es el de k vecinos más cercanos o knn por sus siglas en inglés. Este algoritmo busca los k valores más cercanos, donde k es un número entero como 2, 3 o 10 por ejemplo y reemplaza el valor faltante con el promedio de estos vecinos. La ventaja de este método es que es mucho más preciso que la media o la mediana e incluso lo podemos usar en lugar de la regresión cuando no podemos garantizar que haya correlación entre los datos. La desventaja es que si tenemos demasiados datos el tiempo de cómputo es elevado porque tenemos que calcular para cada dato faltante su distancia con respecto a cada uno de los demás datos del set. Una alternativa más robusta que todas las técnicas que vimos anteriormente es precisamente la imputación múltiple que de hecho es una de las técnicas más usadas en la actualidad y es que todas las técnicas de imputación siempre tienen un gran problema y es que para reemplazar el dato faltante se fían de una única estimación. Es como intentar reemplazar la altura desconocida de una persona con el promedio de otras mil personas, muy probablemente esa estimación va a estar alejada del valor real. Así que en lugar de hacer una sola estimación en la imputación múltiple como su nombre lo indica se hacen múltiples estimaciones y luego todas ellas se combinan en un único valor que es el que reemplazará el dato faltante con lo cual se evita el sesgo precisamente en esta estimación. El método de imputación múltiple más usado es el algoritmo de imputación múltiple con ecuaciones encadenadas o MICE por sus siglas en inglés. En este algoritmo de forma iterativa se harán progresivamente cada vez mejores estimaciones de los datos faltantes. Inicialmente la primera estimación no es muy buena y se hace con la imputación por la media que vimos anteriormente, pero en los pasos restantes se aplica una regresión lineal entre pares consecutivos de columnas y el procedimiento se repite una y otra vez hasta completar un número predefinido de iteraciones. La idea es que progresivamente las estimaciones sean cada vez más precisas y se acerquen más y más al valor real. Como les decía este algoritmo MICE es mucho más robusto que cualquiera de los de imputación simple y en la práctica no cambia la distribución obtenida. Sin embargo tampoco es perfecto porque para poderlo utilizar tenemos que garantizar que las variables están relacionadas linealmente. De no ser así debemos usar modelos estadísticos más sofisticados o incluso entrenar un modelo de machine learning para que pueda hacer esa regresión. Bien y con esto ya tenemos una guía completa de los métodos más usados para el manejo de los datos faltantes. En resumen recordemos que el primer paso es determinar el mecanismo que da origen a estos datos faltantes. Si este mecanismo es aleatorio o completamente aleatorio entonces podemos usar las técnicas que les acabo de mencionar, pero si el mecanismo es no aleatorio en este caso lo que les recomiendo es recolectar más datos porque si usamos cualquiera de las técnicas anteriores con este tipo de datos podemos introducir un sesgo en la distribución del data a ser resultante. Si miramos en detalle las técnicas creo que las más robustas son la de cabecinos más cercanos para la imputación simple y obviamente el algoritmo MICE para la imputación múltiple. En todo caso la técnica que escojan va a depender mucho del tipo de datos que ustedes tengan en su proyecto. Recuerden que en el enlace que les dejo en la descripción del video van a poder descargar la guía completa con las recomendaciones para que puedan utilizar estas técnicas en sus proyectos de machine learning o ciencia de datos. También tengan en cuenta que existen otras técnicas para el manejo de datos faltantes cuando estamos hablando de series de tiempo, pero estas son totalmente diferentes de las técnicas que acabamos de ver hace un momento, así que si están interesados déjenme sus comentarios para preparar un video sobre este tema. Por ahora esto es todo, si les gustó el video les agradezco un montón darle un pulgar hacia arriba y compartirlo con sus amigos y conocidos y si aún no lo han hecho suscribirse también al canal. Les envío un saludo y nos vemos en el próximo video.
DOC0098|Machine Learning|Cuando resolvemos un problema de clasificación con Machine Learning, muchas veces nos debemos enfrentar a la decisión de determinar cuál de los múltiples modelos entrenados resulta más adecuado. Y comúnmente la métrica más usada es la exactitud, que es la proporción entre los datos clasificados correctamente y la totalidad de los datos. Sin embargo esta métrica a veces no es una fiel representación del desempeño real del modelo. Así que en este vídeo hablaremos de la matriz de confusión, una herramienta muy útil para evaluar el desempeño de modelos de Machine Learning. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Bits, en donde encontrarán los cursos que les permitirán construir su carrera en ciencia de datos y Machine Learning. Este mes estamos con el curso Python Nivel Avanzado, en donde aprenderemos a realizar el manejo de archivos y de excepciones y todos los detalles de la programación orientada a objetos en Python. Así que los invito a suscribirse a la Academia por un valor mensual de tan solo 10 dólares. Y ahora sí, comencemos. Supongamos que queremos crear un modelo para determinar si un sujeto tiene o no una enfermedad cardiaca, a partir del análisis de su electrocardiograma. Así el modelo clasificará al sujeto como normal o anormal. Y supongamos que para resolver este problema entrenamos varios modelos. Una red neuronal, un bosque aleatorio, una red convolucional y una máquina de soporte vectorial. Pero entonces ¿cómo sabemos cuál es el mejor clasificador? Para responder a esta pregunta necesitamos alguna herramienta que nos permita cuantificar este desempeño. Así podemos aplicar esta herramienta a cada modelo y luego realizar una comparación objetiva para escoger el mejor clasificador. Una de las métricas más usadas en estos casos es la exactitud, que es simplemente la proporción entre el número de aciertos del clasificador y la cantidad total de datos. Si por ejemplo entrenamos la red neuronal y luego la validamos con un set de 10 pacientes y encontramos que 8 de ellos han sido clasificados correctamente, podemos decir que el modelo tiene una exactitud del 80%. Y si repetimos este procedimiento con los otros tres modelos y encontramos que por ejemplo tienen exactitudes del 90, 70 y 60%, podríamos entonces concluir que el bosque aleatorio es el más adecuado para este problema, porque simplemente tiene un mayor número de aciertos, es decir una mayor exactitud. Hasta acá no hay ningún inconveniente. La exactitud parece ser la métrica más adecuada, aunque tiene una gran limitación. En el ejemplo anterior hemos asumido una condición ideal. Los sets de entrenamiento y prueba estaban balanceados, es decir que tenían prácticamente la misma proporción de normales y anormales. Pero en la práctica no siempre podremos tener sets de datos balanceados. Por ejemplo, en un sistema anti-spam, generalmente habrá más correos normales que spam. O en un sistema de detección de fraudes en transacciones bancarias, generalmente habrá más transacciones normales que fraudulentas. Así que muchas veces los sets de datos estarán desbalanceados, es decir con más datos de una categoría que de otra. Y si entrenamos nuestro modelo con este set de datos tendrá un sesgo, es decir que clasificará mejor la categoría con más datos, pero no lo hará también para la otra categoría. Y si luego validamos este modelo y calculamos su exactitud, no tendremos una medida fiable de su desempeño. Para entender esto volvamos al caso de los cuatro modelos y supongamos que ahora los entrenamos y validamos con sets desbalanceados. En el caso del set de prueba tendremos 100 datos, 90 normales y tan solo 10 anormales. Al clasificar estos datos con por ejemplo el bosque aleatorio, encontramos que 89 de los 90 normales fueron clasificados correctamente, mientras que en el caso de los anormales tan solo hubo un acierto. Si calculamos el desempeño obtendremos una exactitud del 90%, lo que nos llevaría a concluir de manera errónea que en el 90% de los casos nuestro modelo realiza una clasificación correcta. Pero si miramos en detalle este resultado, encontraremos que la exactitud está enmascarando el comportamiento real del modelo, pues la proporción de normales y anormales clasificados correctamente no es la misma. En el primer caso tendremos 89 de 90 aciertos, mientras que en el segundo tan solo 1 de 10. Así que en realidad el modelo lo está haciendo muy bien con los normales, pero tremendamente mal con los anormales. En conclusión podemos decir que la exactitud no es la forma más adecuada de medir el desempeño de un modelo cuando tenemos sets de datos desbalanceados, así que tenemos que buscar otra alternativa, y una manera de hacerlo es precisamente usando la matriz de confusión. Como vimos en el ejemplo anterior no basta simplemente con determinar el número de aciertos, pues debemos ser capaces de diferenciar el desempeño entre una categoría y la otra, y la matriz de confusión nos permite lograr precisamente este objetivo. Esta matriz de confusión es simplemente una tabla que nos permite ver qué tan confundido está nuestro modelo al momento de la clasificación, mostrándonos tanto los aciertos como desaciertos cometidos para cada una de las categorías. Para entender cómo construir esta matriz debemos tener claros cuatro conceptos básicos, los verdaderos y falsos positivos y los verdaderos y falsos negativos. Volvamos al caso del clasificador de enfermedades cardíacas y supongamos que nos interesa usarlo para detectar casos normales. A estos casos les asignaremos la etiqueta positivo y a los anormales la etiqueta negativo. Entonces al momento de la clasificación se podrán presentar estas situaciones. Un dato normal es clasificado correctamente como normal, a esto lo llamaremos verdadero positivo. Un dato normal es clasificado incorrectamente como anormal, a esto lo llamaremos un falso negativo porque realmente no es un caso negativo. Un dato anormal es clasificado correctamente como anormal, a esto lo llamaremos un verdadero negativo. Y un dato anormal es clasificado indecisivo como normal, a esto lo llamaremos un falso positivo porque realmente no es un caso positivo. Es decir que los verdaderos positivos y los verdaderos negativos serán simplemente los aciertos, mientras que los falsos positivos y los falsos negativos serán los desaciertos. Y con estas definiciones ya estamos listos para construir nuestra matriz de confusión. Para construir la matriz de confusión seguimos estos pasos. Primero tomamos el set de prueba y lo clasificamos con el modelo entrenado y realizamos el conteo de los aciertos y desaciertos por cada categoría. Luego organizamos este conteo de aciertos y desaciertos en una tabla donde las columnas representan las categorías a las que realmente pertenece cada dato y las filas representan las categorías predichas por el modelo. Y por último ubicamos los aciertos y desaciertos en la celda correspondiente. Así la primera celda contendrá la cantidad de normales que fueron clasificados como normales, la segunda la cantidad de normales clasificados como anormales, la tercera la cantidad de anormales clasificados como normales y la cuarta la cantidad de anormales clasificados como anormales. Y podemos ver que esta matriz de confusión será de 2x2 porque tenemos precisamente dos categorías. Además en la diagonal principal tendremos los aciertos, verdaderos positivos y verdaderos negativos y en las celdas restantes la cantidad de desaciertos, falsos positivos y falsos negativos. Con la matriz de confusión ya construida podemos ver en detalle el desempeño del modelo para las diferentes categorías y acá resulta evidente que lo hace mucho mejor para los datos normales y que lo hace muy mal con los anormales. Y si obtenemos la matriz de confusión para los modelos restantes podremos concluir que la red neuronal es la que tiene el mejor desempeño, pues el número de aciertos para cada categoría es superior a todos los demás modelos. Y podemos extender esta idea de la matriz de confusión para la clasificación binaria para casos en los cuales tengamos más de dos categorías. Volvamos a nuestro ejemplo y supongamos que en lugar de dos tendremos ahora cinco categorías, una para los sujetos normales y cuatro para los anormales. En este caso ya no podremos hablar de positivos y negativos pues hay más de dos categorías, así que simplemente hablaremos de aciertos y desaciertos. Entonces al construir la matriz obtendremos un arreglo de 5x5, pues tenemos cinco categorías y en la diagonal principal tendremos el conteo de aciertos mientras que por fuera de ella estarán los desaciertos o clasificaciones erróneas. Por ejemplo la celda de la fila 3 columna 1 nos indica la cantidad de sujetos anormales tipo 2 que fueron clasificados incorrectamente como normales, mientras que la fila 4 columna 4 nos indica cuántos sujetos anormales tipo 3 fueron clasificados correctamente en esta categoría. Muy bien ya sabemos cómo construir una matriz de confusión para un modelo de clasificación binaria o multiclase y cómo usar esta información para determinar el modelo con el mejor desempeño para el problema que estemos resolviendo, pero en ocasiones no resulta fácil determinar la ventaja de uno u otro modelo tan sólo con la información proporcionada por esta matriz. Por ejemplo volvamos al caso de la clasificación binaria y supongamos que obtenemos las matrices de confusión para tres modelos diferentes. Al comparar las matrices es evidente que el modelo 2 tiene el peor desempeño, pero no resulta fácil determinar cuál de los dos modelos restantes es el mejor, si el 1 o el 3. El modelo 1 tiene una mayor tasa de verdaderos positivos mientras que el 3 tiene una mayor tasa de verdaderos negativos. En este caso la decisión de cuál es el mejor modelo dependerá por un lado del uso final que esperemos darle, es decir si lo que nos interesa es la detección de casos normales o anormales, pero además de esto tendremos que recurrir a métricas adicionales que nos permitan escoger el mejor modelo de manera objetiva. De estas métricas que se pueden calcular precisamente a partir de la matriz de confusión hablaremos en detalle en el próximo vídeo. Muy bien, acabamos de ver qué es y cómo usar la matriz de confusión para comparar el desempeño de diferentes modelos de Machine Learning y determinar cuál es el más adecuado para la aplicación que estemos desarrollando. Sin embargo vimos que es difícil diferenciar un modelo de otro cuando se tienen tasas de aciertos y desaciertos similares, así que tendremos que recurrir a herramientas como el precision o el recall de las cuales hablaremos en detalle en el próximo vídeo. No olviden dejar sus dudas o comentarios acá abajo y tampoco olviden darle un pulgar hacia arriba de me gusta al vídeo y suscribirse al canal porque esto me ayudará a llegar cada vez a más personas. Por ahora esto es todo les envío un saludo y nos vemos en el próximo vídeo.
DOC0099|Machine Learning|Cuando queremos resolver un problema de Machine Learning tenemos a nuestra disposición tantos algoritmos que a veces resulta difícil decidir cuál de ellos usar. Así que en este video haremos un tour por las principales familias de algoritmos de Machine Learning organizadas dependiendo de su principio de funcionamiento. Para cada familia veremos también los algoritmos más representativos y en la descripción del video encontrarán el enlace de descarga con este mapa que le servirá como guía para el desarrollo de sus proyectos. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Vídeos donde encontrarán cursos para construir su carrera en ciencia de datos, inteligencia artificial y Machine Learning. Este mes estamos con el curso Python Nivel Avanzado donde aprenderemos el manejo de excepciones así como el procesamiento de diferentes tipos de archivo y todo lo relacionado con la programación orientada a objetos. Recuerden que el acceso a este y a todos los cursos de la Academia es a través de una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Como punto de partida supongamos que ya hemos determinado que el Machine Learning es la ruta adecuada para resolver nuestro problema. De hecho en la descripción les dejo un video donde explico cómo hacerlo. También es importante tener en cuenta que existen muchísimos algoritmos de Machine Learning así que en esta guía no tendremos un listado exhaustivo sino más bien los algoritmos más representativos de cada familia. Comencemos entonces hablando de lo que significa resolver un problema de Machine Learning. En un problema de Machine Learning lo que buscamos es crear un modelo que sea capaz de tomar unos datos de entrada, encontrar ciertos patrones en esos datos y con base en ello que logre realizar una predicción a partir de dichas propiedades. Por ejemplo podemos pensar algo tan simple como un modelo alimentado con imágenes y que buscamos que logre clasificarlas en diferentes categorías. Y para que el modelo aprenda a detectar patrones usamos lo que se conoce como un set de entrenamiento. Así una vez entrenado el modelo lo podremos poner a prueba con datos que nunca ha analizado, es decir el set de prueba. Y este proceso de aprendizaje puede ser supervisado, no supervisado o por refuerzo. En el aprendizaje supervisado tenemos un grupo de datos de entrada y para cada uno de ellos conocemos con antelación los atributos que deseamos predecir. Por ejemplo para entrenar un sistema de clasificación de imágenes debemos tener varias imágenes y para cada una de ellas conocer la categoría a la que pertenecen. O por ejemplo para un sistema de puntuación de crédito tenemos datos de potenciales clientes de un banco pero también tenemos que conocer con antelación la probabilidad de que cada cliente adquiera un producto crediticio con el banco. Por otra parte en el aprendizaje no supervisado solo tenemos los datos pero desconocemos el atributo que queremos predecir. Por ejemplo podemos tener una base de datos de clientes con diferentes comportamientos financieros pero desconocemos cuáles de ellos tendrán más o menos probabilidad de adquirir por ejemplo un crédito con el banco. En este caso los algoritmos de aprendizaje no supervisado pueden detectar diferentes patrones de comportamiento y de forma automática agruparlos en ciertas categorías para que podamos posteriormente interpretar estos resultados y tomar decisiones. Y por último está el aprendizaje por refuerzo en donde lo que se busca es usar algoritmos que permitan a un agente aprender a interactuar con su entorno. Como cuando por ejemplo queremos que un robot aprenda por sí solo a ejecutar acciones y a moverse de forma autónoma en un espacio determinado. Este aprendizaje por refuerzo tiene un espectro tremendamente amplio de algoritmos así que en este vídeo no hablaremos de este enfoque aunque en el canal podrán encontrar un vídeo introductorio a este tema. Ahora sí estamos listos para comenzar a explorar las diferentes familias y sus algoritmos más representativos. Estas familias están conformadas por grupos de algoritmos que obedecen al mismo principio de funcionamiento aunque hay que tener en cuenta que no son totalmente excluyentes es decir que algunos algoritmos bien podrían pertenecer a diferentes familias. Pero más allá de este criterio lo importante es que estoy intentando darle un cierto orden lógico a la gran cantidad de algoritmos existentes. Y como se trata de una guía lo que haremos será describir las principales características de cada familia y mencionar sus algoritmos más representativos. Sin entrar a mirar en detalle el funcionamiento de cada uno de ellos. Comencemos con los algoritmos de regresión que son algoritmos de aprendizaje supervisado y donde lo que buscamos es entrenar un modelo capaz de predecir el valor de una variable continua. Por ejemplo podemos entrenar un modelo que aprenda a tomar la edad, el género y el peso de una persona e intente predecir su altura. La regresión lineal, la regresión ordinaria por mínimos cuadrados y la regresión logística son algunos de los principales algoritmos de esta familia. Los algoritmos basados en árboles de decisión también son de aprendizaje supervisado y se pueden usar para tareas de regresión o clasificación. El nombre de esta familia se debe a que usan una estructura básica, los árboles precisamente, que con simples reglas de decisión aplicadas sobre los datos les permite generar predicciones. Por ejemplo podemos entrenar un árbol para que aplique de forma secuencial reglas de decisión sobre diferentes características relacionadas con el comportamiento financiero de un cliente y con esto prediga si adquirirá o no un producto. Entre los principales algoritmos tenemos los árboles de clasificación y los árboles de decisión que usan como base lo que se conoce como el algoritmo CART. Otra familia son los algoritmos probabilísticos que intentan construir una distribución de probabilidades de las características de los datos para realizar tareas de clasificación y regresión o también de aprendizaje no supervisado. Por ejemplo pensemos en un simple clasificador de imágenes para determinar si la fruta en una imagen es una banana o un kiwi. Si se analiza la forma y el color de cada fruta se tendrán distribuciones diferentes y con esta información el clasificador probabilístico podrá diferenciar una de la otra. Los algoritmos más representativos son Knifebase, las redes vallesianas y los modelos de mezcla gausiana. Muchos modelos de Machine Learning sufren en mayor o menor grado el problema de overfitting que hace referencia a que el modelo funciona bastante bien con el set de entrenamiento pero no tanto con el set de prueba. Para reducir este overfitting se puede intentar controlar el impacto que tiene cada una de las características del dato que está siendo procesado por el modelo. Por ejemplo volviendo al sencillo clasificador de tipos de fruta puede ser que le de más importancia al color que a la forma al realizar la clasificación y tal vez el desempeño no sea el adecuado. Lo ideal sería que ambas características tuviesen niveles de importancia similares. Los algoritmos de contracción buscan reducir el overfitting contrayendo el peso o la importancia de algunas características imponiendo una penalidad sobre su tamaño. Entre los algoritmos más importantes encontramos la Regresión Reach, la Regresión Lazo y ElasticNet que pueden ser usados para tareas tanto de regresión como de clasificación. Los algoritmos de agrupamiento permiten realizar tareas de aprendizaje no supervisado para lo cual generan agrupaciones basadas en el grado de similitud entre los datos de forma tal que datos similares pertenecerán a una misma agrupación. Por ejemplo podemos usar algoritmos de clustering para encontrar diferentes perfiles de cliente de una entidad bancaria. Aquellos más interesados en adquirir productos crediticios o aquellos con un perfil de inversionistas. Los principales algoritmos en esta familia son K-means, el agrupamiento espectral y los mapas auto-organizados. En los algoritmos combinados se construye un modelo a partir de múltiples modelos más simples con lo cual se busca mejorar el desempeño del modelo en tareas de regresión o clasificación. Por ejemplo podemos tener un árbol de decisión que tiene un desempeño regular como clasificador pero si construimos un modelo con múltiples árboles, unos diferentes de otros, podremos aprovechar las fortalezas de cada árbol para que el modelo combinado realice una mejor clasificación que cada modelo individual. Los principales algoritmos en esta categoría son los bosques aleatorios y los algoritmos de bagging y boosting. Los algoritmos de aprendizaje basado en instancias se usan en tareas de aprendizaje supervisado y en lugar de construir un modelo almacenan instancias o ejemplos de los datos de entrenamiento para posteriormente comparar el dato procesado con dichas instancias. Por ejemplo podemos intentar determinar si una imagen corresponde a un producto en buen estado o defectuoso comparando sus características con las de múltiples objetos almacenadas previamente. Los algoritmos más usados en esta familia son los K-vecinos más cercanos y las máquinas de soporte vectorial. Los algoritmos de reducción de dimensionalidad de aprendizaje no supervisado permiten analizar los patrones presentes en los datos para simplificar su representación pero preservando la información que nos interesa. Por ejemplo, supongamos que tenemos una población de sujetos y para cada uno tomamos los datos de su altura y su peso. Estas dos variables no son totalmente independientes y en su lugar están correlacionadas. Es de esperar que a mayor altura se tenga mayor peso y viceversa. Así que podríamos pensar que para describir al sujeto podríamos usar una sola variable que combine la información tanto del peso como de la altura. Al usar una en lugar de dos variables lo que estamos haciendo es reducir la dimensionalidad de nuestro set de datos pero sin pérdida de información. Entre los algoritmos más usados tenemos el análisis de componentes principales o el análisis discriminante lineal. Hasta este punto hemos hablado de los algoritmos clásicos del machine learning que funcionan bastante bien cuando tenemos datos estructurados, es decir que vienen representados en formato tabular o cuando tenemos relativamente pocos datos, es decir unas cuantas decenas o cientos de miles. Pero cuando tenemos datos no estructurados, es decir como las imágenes, el audio, el video o el texto nos resulta mucho más difícil usar estos algoritmos clásicos para realizar tareas de aprendizaje supervisado o no supervisado. En este caso la alternativa más adecuada son las arquitecturas del deep learning que han tenido una evolución impresionante en los últimos 10 años y que se derivan de una estructura básica que son las redes neuronales. Estas arquitecturas requieren una gran cantidad de datos de entrenamiento del orden de cientos de miles o millones y son adecuadas cuando tenemos datos no estructurados. Los principales representantes de esta familia son las redes neuronales, las redes convolucionales, las redes recurrentes y LSTM y las redes transformer. Muy bien, ya tenemos un panorama general de las principales familias y algoritmos del Machine Learning. Aunque tengamos en cuenta que este listado de algoritmos es mucho más amplio y lo que he hecho en esta guía ha sido incluir aquellos más representativos de cada familia. De muchos de estos algoritmos ya hemos hablado en detalle y encontrarán videos asociados en el canal, pero si no los encuentran me pueden dejar abajo sus comentarios para incluir próximamente un video sobre ese tema. También recuerden que en la descripción del video podrán encontrar el enlace de descarga de esta guía Y si les gustó el video no olviden darle un pulgar hacia arriba de me gusta y compartirlo con sus amigos y conocidos porque esto me ayudará a seguir creando este tipo de contenido y también los invito a suscribirse al canal si aún no lo han hecho. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0100|Machine Learning|Imaginen que trabajan para una empresa en donde les interesa analizar el comportamiento histórico de las ventas y para eso cuentan con un dataset que contiene el monto total facturado cada día para un año en particular. La idea es lograr predecir el nivel de ventas para un mes determinado y para años posteriores. Pero ¿se podría usar el Machine Learning para este caso? ¿Y cómo sabemos si es realmente la alternativa más adecuada? Pues este sencillo ejemplo es un caso típico de lo que ocurre cuando nos enfrentamos a una situación en la cual tenemos que resolver un problema usando una gran cantidad de datos. Y como el Machine Learning es un área que en los últimos años ha tenido unos desarrollos impresionantes, a veces la tendencia es usarlo para resolver absolutamente cualquier problema que involucre el uso de datos. Pero esto no quiere decir que lo podamos usar prácticamente para cualquier situación. De hecho, si lo usamos en el caso donde realmente no es necesario, la empresa para la que trabajamos puede perder tiempo y dinero y podemos llegar a un modelo que no tiene un buen desempeño o que simplemente no resulta útil. Es decir, hay casos en donde el Machine Learning simplemente no es necesario. Pero entonces ¿cómo saber cuándo usarlo y cuándo no? Pues ahora voy a tratar de resolver esta pregunta. Y para hacerlo he organizado una hoja de ruta que nos guiará paso a paso en el proceso de determinar cuándo usar y cuándo no el Machine Learning. En la descripción del video les voy a dejar el enlace de descarga. Y para esto debemos partir de esta definición súper importante. El Machine Learning es un enfoque que permite aprender patrones complejos a partir de datos existentes y usar estos patrones para realizar predicciones sobre datos nunca antes analizados. Traten de recordar los términos aprender patrones complejos y realizar predicciones porque son esenciales para lo que vamos a hablar ahora. La primera condición que debemos verificar está precisamente relacionada con los datos que son el punto de partida de cualquier proyecto de Machine Learning. En este caso debemos verificar que se cumplan al menos dos condiciones. Que sean accesibles y que sean asequibles. Que sean accesibles quiere decir que los podemos recolectar fácilmente. Por ejemplo podemos intentar crear un modelo para predecir el volumen de ventas de una empresa pero es imposible construirlo a menos que tengamos acceso a datos de varios años atrás. Y que los datos sean asequibles significa que en muchos casos no serán de uso libre y probablemente será necesario comprarlos. Por ejemplo, imaginemos que queremos desarrollar un sistema para prevenir fraudes con tarjetas de crédito. Para poderlo construir necesitamos un dataset con datos reales que muy probablemente pertenecerán a una entidad bancaria. Lo más probable es que este dataset no sea de acceso libre y si queremos utilizar esos datos puede resultar necesario adquirirlos. Así que en resumen el paso cero es tener certeza de que contamos con los datos que necesitamos. Bien, si se cumple esta condición debemos analizar ahora otra situación que es menos de tipo técnico y más de tipo humano, el tema ético. Y este es un aspecto súper importante porque independientemente del problema que queramos resolver tenemos que asegurarnos de que a los datos y al modelo que vamos a entrenar se les dé un uso precisamente ético. Imaginemos por ejemplo el caso de los vehículos autónomos. ¿Qué pasa si uno de estos vehículos atropella a una persona? ¿A quién podemos culpar de este accidente? Es un dilema ético de fondo que realmente ha generado una discusión en torno a este tema y ha creado una barrera que ha dificultado el uso masificado de estos vehículos. Y dilemas como este se vienen dando en diferentes ámbitos de nuestra vida diaria. Desde el mismo uso de las redes sociales, pasando por la medicina y llegando incluso a temas militares. Así que no podemos confiar ciegamente ni en los datos, ni en los algoritmos, ni en el Machine Learning. Así como resulta benéfico en muchas situaciones, en otras como en el caso de los vehículos autónomos un error del modelo puede llevar a consecuencias catastróficas. Y en estos casos mi sugerencia es simple. Ante cualquier dilema ético que pueda tener consecuencias negativas sobre cualquier actor de nuestro entorno, es mejor no usar ni el Machine Learning y de hecho ningún otro enfoque. Resuelto el tema ético, el siguiente paso es tener claridad sobre el proceso que está detrás de la generación de los datos. Que puede ser determinístico o estocástico. Un proceso determinístico quiere decir que puedo predecir con una certeza del 100% un evento futuro. Es decir, es un proceso donde no tengo ningún tipo de aleatoriedad. Por ejemplo, para calcular el saldo de mi cuenta de ahorros a final de mes simplemente debo tomar los aportes hechos y sumarle lo que ha ganado por cuenta de la tasa de interés. Por otro lado, en un proceso estocástico no tendré una certeza del 100% y en su lugar tendré una estimación o una probabilidad de que ocurra el evento. Es decir, que en estos procesos tengo un cierto grado de aleatoriedad. En el caso anterior, un ejemplo de un proceso estocástico sería el comportamiento de la tasa de interés. Pues ésta depende de las condiciones del mercado, de dónde invierte el dinero el banco, de la volatilidad de las acciones, etcétera, etcétera. Y por tanto tendrá ese grado de aleatoriedad. Pero un proceso estocástico es diferente de un proceso aleatorio, como lanzar un dado. En este último caso es imposible predecir en cada lanzamiento en qué número caerá. Mientras que la tasa de interés tiene un mayor grado de predictibilidad. Por ejemplo, con un alto grado de confianza, puedo decir que prácticamente nunca tendremos una tasa de interés del 1000%. Así que si el proceso es determinístico no necesitamos recurrir al Machine Learning. Podemos usar el mismo modelo o las ecuaciones que tengamos para resolver el problema. Pero el Machine Learning resulta viable si el proceso es estocástico. Si los datos se generan a través de un proceso estocástico, el siguiente paso es definir qué es lo que queremos hacer con esos datos. Es decir, queremos hacer un análisis de comportamiento histórico o queremos hacer algún tipo de predicción. Volvamos al ejemplo inicial, el del volumen de ventas de la empresa. Si simplemente queremos saber en qué mes o día se tuvieron más ventas, basta con consultar el registro histórico que tenemos. Acá realmente no necesitamos el Machine Learning. Pero si por el contrario lo que queremos es tomar estos datos históricos y predecir lo que podría ocurrir en años posteriores, en este caso el Machine Learning podría funcionar. Y digo podría porque realizar predicciones no es realmente la única condición. Recordemos que podemos predecir el saldo de nuestra cuenta simplemente usando una ecuación. Y acá vale la pena retomar la definición que les mencionaba al comienzo. El Machine Learning es un enfoque que permite aprender patrones complejos a partir de datos existentes y usar estos patrones para realizar predicciones sobre datos nunca antes analizados. Entonces es evidente que una de las tareas del Machine Learning es realizar predicciones. En tareas como la clasificación o la regresión. Pero además debe haber algún tipo de patron en los datos. Y la idea es que con el Machine Learning el modelo pueda aprender a identificar este patron. Por ejemplo, aprender a predecir el número ganador de la lotería resulta imposible porque es un proceso totalmente aleatorio. No hay patrones. Pero aprender a predecir con cierta precisión la variación de la tasa de interés sí sería posible con el Machine Learning. Pero además estos patrones deben ser complejos. Predecir cuándo habrá un eclipse de sol requiere recurrir a la física para predecir las trayectorias de la tierra y del sol. Pero predecir el valor de un inmueble a partir de su área, del número de habitaciones, del año en que fue construido, de si hay o no escuelas o supermercados en el vecindario es posible con el Machine Learning porque allí tenemos un patrón más complejo. Bien estando seguros de que queremos hacer una predicción la siguiente condición a analizar sería la interpretabilidad que es uno de los grandes signos de interrogación del Machine Learning. La interpretabilidad es el grado con que un ser humano puede comprender las razones de una decisión tomada por un modelo de Machine Learning. Por ejemplo, si trabajamos en una entidad bancaria y estamos desarrollando un sistema de detección de fraudes allí resulta fundamental poder explicar los motivos por los cuales una transacción fue clasificada como fraudulenta. De esta forma la entidad puede tomar los correctivos necesarios para evitar este tipo de fraudes a futuro. Desafortunadamente la mayor parte de los modelos de Machine Learning no son fácilmente interpretables exceptuando posiblemente los árboles de decisión y los bosques aleatorios en algunos casos particulares. Estos modelos generalmente funcionan como una caja negra que logra altos niveles de precisión pero donde lo que ocurre en el interior de esa caja tiene todavía un cierto halo de misterio para nosotros los humanos. Aunque vale la pena aclarar que se están haciendo esfuerzos para lograr que estos modelos sean más interpretables. Así que en resumen, si requerimos interpretabilidad realmente la mayoría de las veces el Machine Learning no es la vía recomendada. Pero si la interpretabilidad no es un inconveniente el Machine Learning podría ser una alternativa. Así que poco a poco nos vamos acercando a lo que estamos buscando que es saber cuándo podemos usar el Machine Learning. Para llegar a este punto debemos determinar si la cantidad de datos que hemos recolectado es suficiente porque para que el modelo de Machine Learning aprenda a detectar esos patrones generalmente necesita una gran cantidad de datos. Aunque el número depende del problema particular que estemos resolviendo podemos usar tres reglas básicas para tener una primera aproximación del número de datos que se requeriría. Si es un problema de clasificación podemos definir un factor dependiendo del número de clases. Por ejemplo, podemos definir que por cada categoría se requieren al menos 100, 1000 o 10,000 ejemplos de entrenamiento. También se puede definir un factor dependiendo del número de características de cada dato. Es decir, si cada ejemplo de entrenamiento está representado como un vector de por ejemplo 9 características entonces el número de ejemplos puede ser de al menos 100, 1000 o 10,000 veces ese número. Por último se puede definir un factor dependiendo del número de parámetros del modelo. Por ejemplo, si estoy entrenando una red neuronal con 100,000 parámetros podríamos definir una cantidad de ejemplos de entrenamiento que puede ser 5 o 50 veces ese número. Pero esto siempre será una aproximación y es necesario entrenar el modelo para verificar si se requieren más datos o no. Desafortunadamente es imposible calcular de forma precisa y con antelación un valor exacto para el tamaño del dataset, pues en el Machine Learning este tamaño depende de muchas variables que son propias del problema a resolver, como el tipo de datos o el tipo de modelo a implementar. Para resumir, si encontramos que los datos no son suficientes la única opción que nos queda es recolectar más. Pero si la cantidad es adecuada entonces estamos mucho más cerca de confirmar que efectivamente el Machine Learning es la opción. Así que ya estamos llegando a lo que nos interesa, que es saber en qué situaciones podemos usar definitivamente el Machine Learning. Y para esto tenemos que verificar el tipo de datos que tenemos, si son estructurados o si son no estructurados. Los datos estructurados, como lo mencioné en el video sobre cómo usar SQL en el Machine Learning, vienen usualmente presentados en forma de tabla, es decir dentro de una estructura predefinida, de ahí su nombre. En este caso debemos verificar si se requiere efectivamente la complejidad de un sistema de Machine Learning o si podemos realizar algo más sencillo, por ejemplo un modelo con una serie de reglas para poder realizar la predicción. Un ejemplo sencillo, volvamos al caso de las ventas de la empresa. Para predecir el mes y el volumen de ventas a futuro, podríamos crear una regla como esta. Si el mes es mayo o diciembre, entonces se tendrán los mayores niveles de ventas. Y estos niveles serán iguales al valor del año anterior con un incremento porcentual igual a la inflación más un punto. Así que la sugerencia es, si el patrón es fácil de identificar y podemos fácilmente codificar las reglas necesarias para realizar la predicción, entonces optar por este método. Pero si los patrones no son tan obvios y no podemos codificar de manera explícita estas reglas, la alternativa es finalmente el Machine Learning. Bien, y esto para datos estructurados. Ahora vamos con el segundo grupo que son los datos no estructurados. Y acá tenemos ejemplos como el texto o datos que provienen de procesos perceptuales como el audio, las imágenes o el video. Por ejemplo, para desarrollar un sistema de análisis de sentimientos a partir del texto o un sistema de visión artificial o uno de reconocimiento de voz, los patrones son extremadamente complejos. Los caracteres conforman palabras que pueden estar relacionadas de diferentes formas. La imagen o el video contienen millones de pixeles o frames. Cada uno con diferentes colores y con escenas con múltiples objetos. La entonación y pronunciación depende de la persona que esté hablando y puede haber incluso ruido de fondo. Los patrones son tan complejos que resulta imposible codificar de forma explícita todas las reglas para resolverlo. Incluso podemos tener casos en los cuales combinamos datos no estructurados con datos estructurados, como por ejemplo el sistema de recomendación de YouTube que analiza tanto el contenido de los videos, que son datos no estructurados, como el comportamiento del usuario en la plataforma, que son datos estructurados. Así que en resumen, si los datos son estructurados pero para realizar la predicción no podemos codificar las reglas explícitamente, la solución más viable es el Machine Learning. Y si los datos son no estructurados o están combinados con datos estructurados, definitivamente la alternativa a usar es también el Machine Learning. Bien, espero que esta guía les resulte muy útil cuando tengan que enfrentarse a un proyecto donde tengan que usar datos y necesiten unas pautas para determinar si es viable o no utilizar precisamente el Machine Learning. Creo que esta guía se ajusta a la mayoría de los casos de aplicación en el mundo real, pero puede haber situaciones particulares en las cuales de pronto no funciona. Si es así, déjenme sus comentarios acá abajo. Recuerden que abajo en la descripción les voy a dejar el enlace para que puedan descargar este roadmap. Y eso es todo, si les gustó este video les agradecería un montón que lo compartan con sus amigos y conocidos, que le den un pulgar hacia arriba de me gusta y que dejen abajo sus comentarios. Por ahora les envío un saludo y nos vemos en el próximo video.
DOC0101|Machine Learning|Aunque parezca obvio, cuando desarrollamos un proyecto de Machine Learning muchas veces no tenemos en cuenta un aspecto fundamental y es que al inicio debemos tener absolutamente claros todos los elementos que hacen parte de ese problema que queremos resolver. Al no tener claros estos elementos estaremos gastando tiempo y muy probablemente dinero de manera innecesaria y probablemente el desarrollo que hagamos no tendrá un uso final o no responderá a las necesidades que dieron origen a ese proyecto. Entonces en este video les voy a compartir una serie de sugerencias que considero importantes tener en cuenta al momento de definir claramente el problema que queremos resolver en un proyecto de Machine Learning, que como les comenté hace un momento es el primer paso antes de comenzar el desarrollo de nuestro proyecto. Pero antes de comenzar los invito a visitar mi sitio web codificandovids.com, en donde encontrarán la Academia Online con cursos en Ciencia de Datos, Machine Learning e Inteligencia Artificial que les permitirán construir su carrera en estas áreas usando un enfoque totalmente práctico y por tan solo 10 dólares mensuales. Este mes estamos con un nuevo curso Introducción al Machine Learning que he diseñado para todos los que estén interesados en comenzar a incursionar en este campo donde además de ver qué es el Machine Learning y algunos conceptos básicos hablaremos de la ruta de aprendizaje que se requiere para especializarnos en esta área. Además en codificandovids.com podrán ponerse en contacto conmigo si están interesados en servicios como el desarrollo de proyectos, asesorías online o cursos de formación a la medida para personas o empresas. Y ahora sí comencemos. Antes de desarrollar cualquier proyecto en Machine Learning es esencial que entendamos claramente el problema que queremos resolver, porque esto nos permitirá ahorrar tiempo y en muchos casos dinero. Si no tenemos claro el problema a resolver y en lugar de eso comenzamos a hacer el desarrollo, es decir comenzamos a recolectar datos o comenzamos a programar o escribir el código de nuestro desarrollo, es probable que en el camino de pronto nos demos cuenta o que los datos que recolectamos no eran adecuados o que eran insuficientes o que incluso posiblemente el Machine Learning no era la ruta más adecuada para resolver ese problema o incluso puede ser que terminemos construyendo un modelo pero que no responda a las necesidades que dieron origen a ese proyecto. Mientras que si tenemos totalmente claro el problema a resolver, en primer lugar aprovecharemos mejor los recursos que tengamos disponibles, probablemente requeriremos menos tiempo de desarrollo y en últimas estaremos construyendo un modelo que responde a las necesidades que nos planteamos inicialmente. Partamos de un ejemplo hipotético que nos permitirá entender fácilmente los pasos involucrados en esta definición del problema. Supongamos que trabajamos como científicos de datos en una empresa de manufactura y que la empresa quiere determinar cómo maximizar las ganancias resultado de la venta de su producto estrella durante los próximos meses. Como vemos esta es la necesidad del negocio y no está definida usando un lenguaje de un científico de datos y más bien ha sido definida muy probablemente por el equipo de ventas o los administrativos de la empresa. Partiendo de esta necesidad del negocio podemos llevar a cabo estos pasos para definir nuestro problema. En este paso simplemente debemos intentar describir el problema con pocas palabras de forma muy concisa como si se lo estuviéramos contando a un colega o a una persona que no es experta en el tema. Volviendo a la necesidad del negocio en este paso podríamos por ejemplo sintetizar nuestro problema de la siguiente forma. Queremos saber el nivel de ventas del producto estrella de la empresa durante los próximos meses. Observemos que acá estamos comenzando a delimitar la necesidad original usando un lenguaje más específico. Ya estamos hablando del nivel de ventas y estamos diciendo que nos interesa analizar ese producto estrella y que nuestro horizonte de tiempo será en meses, aunque aún nos falta especificar cuantos. Al definir el problema de esta forma ya tenemos un excelente punto de partida para poco a poco formalizarlo como un problema de Machine Learning. En este segundo paso debemos traducir el problema que definimos anteriormente a un lenguaje propio del Machine Learning usando terminología más técnica. Así que en este caso deberíamos especificar por ejemplo la tarea que queremos realizar. Si es por ejemplo clustering, regresión, clasificación, el tipo de aprendizaje que vamos a utilizar. Si es supervisado, no supervisado, semi supervisado o aprendizaje por refuerzo. Cuales serán las entradas y las salidas que esperamos que genere el modelo. Así como la forma como vamos a medir el desempeño de ese modelo. Para entender esto partamos del problema que acabamos de plantear de manera informal que consistía en determinar el nivel de ventas del producto estrella durante los próximos meses. En este caso tendríamos una tarea de regresión. Pues lo que queremos es predecir valores continuos correspondientes al nivel de ventas de un producto. Como entrada podemos definir que nos interesa tomar el registro histórico consecutivo de por ejemplo 9 meses a partir del cual realizaremos la predicción de los siguientes 3 meses, que será la salida del modelo. Observemos que en este caso ya estamos definiendo nuestros horizontes de tiempo que nos permitirán establecer las restricciones sobre el set de datos que requeriremos para entrenar nuestro modelo. Como métrica de desempeño podríamos por ejemplo usar la raíz cuadrada del error cuadrático medio. Es decir que tomaremos las sumas de las diferencias cuadráticas entre los valores reales y los valores predichos por el modelo. Las promediaremos entre la cantidad de predicciones realizadas y obtendremos la raíz cuadrada de este resultado. Así que vemos que en este segundo paso estamos siendo mucho más técnicos con el lenguaje utilizado y ya hemos delimitado muchas variables como el horizonte de tiempo, las entradas y salidas del modelo y la forma como mediremos su comportamiento o desempeño al momento de hacer las predicciones. Con el problema ya definido usando un lenguaje técnico, la siguiente fase es una etapa de planeación en donde debemos definir todos los requerimientos necesarios para posteriormente poder resolver este problema usando el Machine Learning. En este punto tenemos que pensar por ejemplo qué tipos de datos y cuántos datos requerimos, dónde vamos a recolectar esos datos, cómo preprocesaríamos esos datos y por ejemplo qué tipos de modelos podríamos utilizar para analizarlos y generar las predicciones. Y lo importante de este paso es que todo lo vamos a hacer de forma manual, es decir que hasta este punto no escribiremos nada de código, simplemente definiremos un listado de requerimientos y de estrategias que después cuando estemos realizando la programación implementaremos en ese proceso de desarrollo. Para entender este paso volvamos al problema que ya tenemos definido, donde nos interesa realizar una tarea de regresión para predecir el nivel de ventas del producto durante tres meses consecutivos, tomando como entrada el comportamiento en los nueve meses previos. En este caso los datos que requerimos para entrenar el modelo corresponden precisamente al registro histórico de las ventas del producto y podemos establecer que como requerimiento necesitamos ese comportamiento histórico de por ejemplo al menos los últimos cinco años, día tras día. Para adquirir estos datos deberíamos verificar si están disponibles en la base de datos de la empresa y al momento de preprocesarlos deberíamos al menos tener en cuenta que puede haber datos incompletos. Y debemos entonces comenzar a pensar en estrategias para la imputación de esos datos. Además dependiendo del modelo que vayamos a usar podría ser necesario hacer algún tipo de escalamiento o estandarización de los datos. También podríamos definir que las redes LSTM o el método ARIMA por ejemplo, podrían ser los modelos que resultarían viables al momento de hacer las predicciones. Y observemos que con este proceso de planeación, así no hayamos implementado absolutamente nada de código ya podemos comenzar a determinar si resulta viable o no resolver nuestro problema usando precisamente el Machine Learning. Por ejemplo volviendo a nuestro caso hipotético de la predicción del nivel de ventas de un producto podremos darnos cuenta en este proceso de planeación que por ejemplo no tenemos los datos suficientes o que estos datos no se encuentran centralizados y tendremos que ir a adquirirlos en los diferentes puntos de venta o que tal vez no podremos utilizar redes LSTM porque no tenemos definitivamente la cantidad suficiente de datos o que incluso tendremos que reformular nuestro horizonte de tiempo y en lugar de generar predicciones a tres meses lo podremos hacer tan solo a un mes. Así que este proceso de planeación es súper importante porque nos permitirá con antelación determinar por ejemplo vacíos que podamos tener en ese proceso de recolección de los datos o vacíos que existan en esa solución que estamos planteando en nuestro problema y nos va a permitir entonces hacer los ajustes que sean necesarios antes de comenzar ese proceso de desarrollo, es decir antes de comenzar a escribir el código para resolver ese problema. Incluso podríamos llegar a darnos cuenta de que el problema es más sencillo de lo que pensábamos en un comienzo y de que no es ni siquiera necesario el uso del Machine Learning o que definitivamente no vamos a poder realizar las predicciones porque simplemente no contamos con la cantidad suficiente de datos. Muy bien, como lo mencioné al inicio del video, considero que este listado de sugerencias es un paso importantísimo antes de comenzar el desarrollo de un proyecto de Machine Learning, pues al tener totalmente claros todos los elementos que hacen parte del problema, podremos definir con antelación una estrategia para resolverlo antes de comenzar ese proceso de desarrollo y esto en última nos permitirá ahorrar tiempo y otro tipo de recursos en el desarrollo de ese proyecto. En últimas, si no tenemos totalmente claros todos los elementos del problema a resolver y los requerimientos para la solución de ese problema, no vale la pena que comencemos la etapa de desarrollo porque probablemente no estaremos llegando a una solución que responda a las necesidades que nos planteamos inicialmente. Y bien, espero que les haya gustado este video, si es así no olviden dejar sus comentarios acá abajo y darme un pulgar hacia arriba de me gusta y compartir este video con todos sus amigos y conocidos, pues ya saben que esto me ayudará a difundir el contenido del canal y llegar cada vez a más y más personas. Y si aún no se han suscrito al canal, los invito a hacerlo y a activar la campana de notificaciones de YouTube para que les envíe una notificación cada vez que publique un nuevo video. Así que les envío un saludo y nos vemos en el próximo video. ¡Suscríbete al canal!
DOC0102|Machine Learning|En un video anterior hablamos de la matriz de confusión y de cómo utilizarla para caracterizar el desempeño de un clasificador. Sin embargo vimos que cuando el set de datos está desbalanceado, la matriz de confusión no es suficiente para caracterizar ese desempeño. Así que en este video vamos a partir de esos conceptos vistos acerca de la matriz de confusión y vamos a hablar de dos nuevas métricas de desempeño, el precision y el recall, que permiten caracterizar un clasificador cuando tenemos un set de datos desbalanceado. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online y servicios en ciencia de datos y machine learning. Además en codificandovids.com me podrán contactar si están interesados en servicios de formación a la medida para personas o empresas o en el desarrollo de proyectos y asesorías en las áreas de ciencia de datos, machine learning o inteligencia artificial. Y ahora sí comencemos. Entonces recuerden que el notebook que vamos a usar en este video lo van a tener disponible el enlace en la descripción del video. Entonces de qué vamos a hablar acá, vamos a ver una breve introducción en donde hablaremos de lo comentado en el video anterior acerca de la exactitud y la matriz de confusión. Luego vamos a ver limitaciones de esas dos métricas o de esas dos herramientas para evaluar el desempeño de un clasificador. Luego vamos a hablar de la primera métrica de importancia en este caso que es el precision y vamos a ver cómo calcularla. Luego hablaremos del recall y veremos cómo calcularla. Luego analizaremos en qué situaciones es mejor usar el precision y en cuáles es mejor usar el recall. Y dependiendo de esto entonces encontraremos que en ocasiones las dos métricas son igualmente importantes. Entonces ahí hablaremos en el punto 6 del F score y cómo calcularlo. Y luego veremos todas estas ideas cómo llevarlas fácilmente a clasificadores multiclase. Y finalizaremos entonces con una conclusión de los puntos más importantes a tener en cuenta al momento de usar el precision, el recall y el F score. Entonces como lo vimos en un video anterior, en ese caso hablábamos de la exactitud y de la matriz de confusión. Cuando teníamos un clasificador que el clasificador podía ser binario o multiclase, pero podríamos usar entonces esas dos métricas o esas dos herramientas. Entonces ¿qué es la exactitud? Pues básicamente mide la cantidad de aciertos o del total de datos que tenemos, cuántos fueron clasificados correctamente, la proporción de aciertos. Eso independientemente de la categoría. La matriz de confusión nos permite ver de forma un poco más detallada el número de aciertos y desaciertos para cada categoría. Permite, digamos, a través de una tabla diferenciar esos aciertos y desaciertos en cada una de esas categorías. Sin embargo, cuando tenemos datos desbalanceados, es decir, por ejemplo, en el caso que tengamos dos categorías clasificación binaria, si tenemos muchos más datos de una categoría que de otra, en esos casos no resultan lo suficientemente adecuados la exactitud y la matriz de confusión. Entendamos eso un poco más en detalle. De esto hablamos en un video anterior, pero acá lo vamos a resumir. Entonces, por ejemplo, acá tenemos un set de datos desbalanceados. Supongamos que tenemos una gran cantidad de sujetos normales. Supongamos que estamos hablando de una aplicación donde queremos clasificar al sujeto como normal o anormal, dependiendo, digamos, de su ritmo cardiaco. Entonces acá tenemos una gran cantidad de sujetos normales. Tenemos 90 sujetos normales, los de color verde, y tenemos tan solo 10 sujetos, entre comillas, anormales. Entonces, si llevamos eso a un clasificador, supongamos que ese clasificador nos entrega para los sujetos normales 89 aciertos y tan solo un desacierto. Y para los sujetos anormales, los 10 anormales, tenemos un solo acierto y 9 desaciertos. Si calculamos la exactitud de este clasificador, pues simplemente tenemos que sumar el número de aciertos que tendríamos 89 y 1, es decir, 90 aciertos en total de la cantidad total de datos que serían en total 100. Entonces tendremos una exactitud de .9 o del 90%. ¿Qué nos diría esa exactitud? Que en el 90% de los casos, el sujeto va a quedar clasificado correctamente. Lo cual no es cierto porque como estamos poniendo todo en una misma bolsa, no estamos diferenciando normales de anormales, sino que simplemente estamos calculando el número de aciertos, pues acá ese 90% nos va a enmascarar el comportamiento inadecuado que tiene el clasificador con los sujetos anormales. Porque en el caso de los anormales está clasificando incorrectamente el 90% y solo el 10% de ellos está siendo clasificado correctamente. Pero esto no lo podemos ver a través del accuracy o de la exactitud. Si construimos la matriz de confusión, por ejemplo acá tendríamos un caso de una matriz de confusión, podemos ver un poco más en detalle esos comportamientos, entonces podemos ver de los que realmente son normales, cuántos fueron clasificados como normales, lo que está en verde, este 89, y de los que son anormales, cuántos fueron clasificados correctamente como anormales, digamos que fueron 8. Y tenemos entonces los falsos positivos y los falsos negativos, es decir, de los normales, ¿cuántos fueron clasificados incorrectamente como anormales? ¿Eso serían los falsos negativos? Si suponemos que los normales son los positivos y los anormales son los casos negativos, tendríamos dos falsos negativos. y acá tenemos un anormal que fue clasificado incorrectamente como normal esto sería un falso positivo, entonces ahí tenemos la matriz de confusión, nos permite ver un poco más en detalle esa situación pero no nos permite determinar o cuantificar si este número de falsos negativos o este número de falsos positivos es adecuado o no es adecuado entonces ahí es donde aparecen las dos métricas de precision y recall hablemos del precision inicialmente y veamos entonces cómo calcular entonces partamos de algunas definiciones previas, lo que acabamos de mencionar en el contexto del problema que estemos resolviendo, acá todo esto va a ser referencia inicialmente a clasificadores binarios es decir, sólo tenemos dos categorías y luego en la parte final del video veremos cómo extender esto a la clasificación multiclase suponiendo que tenemos dos categorías, entonces una primera definición es hablar de los positivos y los negativos eso depende mucho de la aplicación que estemos desarrollando si volvemos a este ejemplo vamos a suponer que los positivos entonces van a ser los sujetos normales que no tienen problemas cardíacos y los negativos son los sujetos anormales ¿cuáles son entonces los verdaderos positivos? los que eran normales, los que sabemos que son normales y fueron clasificados correctamente como normales ¿cuáles son los falsos positivos? los que eran anormales y que incorrectamente fueron clasificados como normales esos son los falsos positivos, los verdaderos negativos entonces eran o serán la cantidad de anormales que fueron clasificados correctamente como anormales y los falsos negativos son aquellos sujetos normales que fueron clasificados incorrectamente como anormales entonces aquí tenemos esas cuatro posibles situaciones y esto es importante porque estas definiciones las necesitamos para poder calcular el precision y el recall entonces, calculemos el precision ¿qué es el precision? veamos primero una definición entonces de todo lo que haya sido clasificado como positivo o sea como normal en nuestro caso particular ¿qué es realmente positivo? entonces si volvemos acá a nuestra tabla ¿qué es lo que tenemos clasificado como positivo? acá tenemos las categorías predichas y positivo para nosotros es normal entonces todo esto, esta columna del lado izquierdo es todo lo que ha sido clasificado como positivo por el modelo este 89 y este 1 pero de todo esto, acá vemos que hay una porción lo que está en color rojo en esa columna que realmente no es positivo porque es un sujeto anormal que fue clasificado incorrectamente como normal un sujeto negativo que fue clasificado incorrectamente como positivo entonces acá la idea del precision es mirar de todo esto que fue clasificado como positivo ¿qué es lo que realmente, qué proporción realmente es positivo? en nuestro caso normal entonces ¿cómo se calcula? el precision se calcula simplemente dividiendo los verdaderos positivos sobre la suma de los verdaderos positivos más los falsos positivos, la suma de los verdaderos positivos más los falsos positivos es precisamente todo aquello que fue clasificado como positivo y eso es un número que está entre 0 y 1 o que si lo representamos como un porcentaje va de 0 a 100 idealmente el valor máximo debería ser 100% si fuese 100% quiere decir que no tenemos falsos positivos y que todo lo que fue clasificado como positivo realmente es positivo ese sería el valor ideal acá veamos un cálculo simple entonces vamos con non-py vamos a introducir esa matriz de confusión que tenemos acá en el dibujo introducimos la matriz de confusión y simplemente entonces acá vamos a calcular de manera individual los verdaderos positivos entonces los verdaderos positivos son normales, clasificados como normales es decir el primer elemento de esta matriz es decir el elemento 00 y los falsos positivos son los anormales que fueron clasificados como normales es decir, segunda fila, primera columna de esa matriz que la indexamos con 1, 0 y entonces acá tendremos 89 verdaderos positivos y un falso negativo y el precision es simplemente entonces calcular la relación de todos los que fueron clasificados como casos positivos que proporción realmente son datos positivos y entonces tendremos acá un precision del 98.9% y esta es entonces una primera métrica una segunda métrica es el recall entonces si el precision se enfocaba acá en tener o en medir los falsos positivos fíjense que indirectamente está midiendo los falsos positivos el recall se va a enfocar en los falsos negativos entonces ¿qué es lo que ocurre con el precision? que no nos dice nada acerca del desempeño del clasificador frente a esas predicciones que fueron negativas los falsos negativos entonces ¿cómo definimos el recall? de todo lo que sabemos que es positivo en nuestro caso por ejemplo sujetos normales ¿qué proporción fue clasificada realmente como positivo? entonces volvamos acá a nuestra tabla ¿qué cantidad de datos sabemos que son normales o positivos? son esta fila todo lo que sabemos que es normal tenemos 89 que fueron clasificados correctamente y 2 que fueron clasificados incorrectamente como anormales pero estos 91 datos sabemos que realmente son positivos entonces de estos 91 datos positivos ¿cuántos fueron realmente clasificados como positivos? estos 89 entonces esa proporción de 89 entre el 91 que es el total de positivos es lo que es precisamente el recall entonces ¿cómo se calcula ese recall? la tasa de positivos o los verdaderos positivos sobre los verdaderos positivos más los que fueron clasificados incorrectamente como negativos y nuevamente esto tiene un rango de valores de 0 a 1 donde el valor ideal es 1 o 100% que sería cuando tenemos una tasa de falsos negativos exactamente igual a 0 es decir todos los que eran positivos fueron clasificados correctamente como positivos entonces veamos acá nuevamente cómo sería el cálculo simplemente tenemos que calcular los falsos negativos porque los verdaderos positivos ya los tenemos del cálculo del precision entonces los falsos negativos simplemente van a ser este número 2 de acá es decir los normales que fueron clasificados incorrectamente como anormales que es la fila 1 columna 2 de esta matriz de confusión que se indexa precisamente como 0,1 entonces esos son los falsos negativos vamos a imprimir acá en pantalla los falsos negativos los verdaderos positivos y calculamos el recall y entonces al hacer eso pues tenemos nuestra matriz de confusión tenemos acá este primer elemento son los verdaderos positivos y este elemento del lado derecho son los falsos negativos y el recall nos dio 97.8% un valor relativamente alto si esto es un ejemplo hipotético entonces tenemos un precision de 98.9 y un recall de 97.8% entonces cuál de los dos es mejor el precision o el recall es decir cuando yo esté caracterizando un modelo que ya entrenamos para clasificar datos binarios medimos el precision, medimos el recall pero cuál de los dos es mejor eso depende eso depende de la aplicación entonces ahí tendremos que mirar el uso que le vamos a dar a ese clasificador entonces acá por ejemplo si lo que nos interesa es minimizar la cantidad de falsos positivos es decir lo que nos interesa es reducir la cantidad de anormales en nuestro caso particular por ejemplo que sean detectados como normales es decir nos interesa reducir este número de falsos positivos entonces observemos que acá el precision está midiendo indirectamente el número de falsos positivos si reducimos el número de falsos positivos este precision tiende a ir hacia arriba se incrementa entonces la idea es para minimizar esos falsos positivos deberíamos entonces enfocarnos en el precision y en obtener el precision más alto posible por el contrario si en la aplicación que estemos desarrollando lo que nos interesa es reducir la tasa de falsos negativos este Fn que aparece acá entonces vemos que el recall se relaciona con esos falsos negativos en nuestro caso particular que serían los normales que fueron detectados como anormales entonces en ese caso si queremos minimizar esa cantidad de falsos negativos nos enfocamos en el recall y deberíamos buscar que ese recall sea lo más alto posible si es alto es porque precisamente estamos reduciendo el número de falsos negativos pero puede haber situaciones en la que de pronto nos interese reducir esas dos métricas es decir, reducir tanto los falsos positivos como los falsos negativos entonces no podemos darle más importancia o al precision o al recall sino que tenemos que darle igual importancia a los dos entonces acá es donde entra precisamente el Fscore que es una métrica que combina el precision y el recall en un solo valor entonces ese Fscore tiene una ecuación para el cálculo que depende de un parámetro beta y ese parámetro beta como lo vemos acá entonces simplemente lo que hace es aparece un valor constante que depende de ese valor beta que escojamos y vamos a tener la relación entre la multiplicación del precision por el recall en el numerador sobre beta al cuadrado veces el precision más el recall entonces entendamos como algunos valores extremos por ejemplo, que es el significado de beta entonces beta controla el nivel de importancia que le vamos a dar al precision o al recall por ejemplo si hacemos en esta ecuación del Fscore hacemos beta igual a cero pues acá simplemente nos quedaría un coeficiente de 1 y si hacemos beta igual a cero en este denominador desaparece el precision y nos queda precision por recall sobre el recall entonces nos queda simplemente el precision quiere decir que en este caso cuando beta es igual a cero vamos a descartar por completo el recall y vamos a enfocarnos o darle importancia únicamente al precision si ponemos por ejemplo un beta igual a 0.5 y lo reemplazamos en esta ecuación entonces tendremos esta combinación de acá 1.25 veces el precision por el recall y en el denominador entonces tenemos 0.25 veces el precision y el recall multiplicado por un factor de 1 entonces acá le estaremos dando más importancia al recall que al precision en ese caso particular de 0.5 y el caso especial es cuando beta es igual a 1 cuando beta es igual a 1 que es como la métrica más usada la llamamos el F1 score y es F1 score precisamente porque beta es igual a 1 y entonces en ese caso observemos que acá nos aparece el precision y el recall en el numerador y en el denominador nos aparecen el precision y el recall ponderados por un factor de 1 ambos tienen igual importancia entonces esta sería la métrica a usar cuando queremos darle la misma importancia al precision y al recall y en ese caso entonces en el caso del ejemplo que estamos analizando cómo lo calculamos por ejemplo si calculamos el F1 score entonces simplemente escribimos esta ecuación de acá dos veces el precision que ya calculamos anteriormente por el recall sobre la suma de estos dos y comparemos entonces esos valores entonces el precision nos daba 98.9 el recall nos daba 97.8 y el F1 score nos da 98.3 entonces si queremos que el precision y el recall tengan igual importancia nos enfocamos en esta métrica y tendríamos que buscar hacerla lo más alta posible si logramos hacerla lo más alta posible quiere decir que estamos reduciendo tanto los falsos positivos del precision como los falsos negativos del recall estamos incrementando esas dos métricas ¿cómo extendemos estas ideas entonces a los clasificadores multiclase? pues muy sencillo simplemente entonces ya no tendremos dos categorías sino que podremos tener 3 o 5 o 10 entonces construimos nuestra matriz de confusión nuestra matriz de confusión siempre va a ser una matriz cuadrada entonces si tenemos por ejemplo 5 categorías nuestra matriz será de 5 filas por 5 columnas la diagonal de esa matriz de confusión va a contener los aciertos y lo que esté por fuera de la diagonal va a contener los desaciertos entonces ahí lo que podemos hacer que esa matriz de confusión multiclase de esa ya hablamos en el vídeo anterior entonces construimos esa matriz de confusión multiclase y simplemente podemos calcular el precision o el recall para la categoría correspondiente para cada categoría recordemos que el precision si tenemos la matriz conformada de esta forma el precision es analizar por ejemplo por columnas y el recall es analizar precisamente por filas entonces es aplicar la misma idea pero a cada categoría de manera individual si tenemos 5 categorías calculamos 5 diferentes precision o 5 diferentes recalls o 5 diferentes F1 scores para poder caracterizar entonces el comportamiento del clasificador multiclase en cada una de esas categorías entonces esa es como la idea del precision y el recall cosas importantes a tener entonces acá en cuenta cuando tenemos un set de datos desbalanceados lo más recomendable es no usar la exactitud o el accuracy porque nos pueden mascarar el comportamiento del desempeño del modelo para diferentes categorías en lugar de eso entonces podemos usar el precision y el recall el precision recordemos entonces que es una métrica que indirectamente está midiendo la tasa de falsos positivos mientras que el recall tiene en cuenta la tasa de falsos negativos si queremos reducir la tasa de falsos positivos deberíamos darle prioridad al precision si queremos reducir la tasa de falsos negativos debemos darle prioridad al recall y si queremos darle el mismo nivel de importancia tanto al precision como al recall lo más recomendable es usar el F1 score que combina estas dos métricas en una sola cantidad numérica que es la métrica de desempeño para ese clasificador muy bien acabamos de ver cómo usar el precision y el recall para caracterizar el desempeño de un clasificador y vimos además que la métrica a utilizar depende de si nos interesa minimizar el número de falsos positivos o de falsos negativos sin embargo en algunas aplicaciones vamos a encontrar que este número de aciertos y desaciertos va a depender de un umbral que podemos fijar así que dependiendo del valor de este umbral irán cambiando precisamente los valores del precision y el recall para encontrar el umbral más adecuado dependiendo de nuestra aplicación debemos recurrir a otra herramienta que se conoce como la curva ROC o también podremos usar lo que se conoce como la curva precision recall de las cuales hablaremos en el próximo video recuerden que en la descripción de este video van a encontrar el enlace para poder descargar el notebook y no olviden también dejarme sus dudas y comentarios acerca de este video si les gustó también los invito a compartirlo con sus amigos y conocidos y a darle un pulgar hacia arriba de me gusta y si no se han suscrito los invito a hacerlo pues esto me seguirá motivando a desarrollar este tipo de contenido y a llegar cada vez a más y más gente así que les envío un saludo y nos vemos en el próximo video
DOC0103|Machine Learning|En ocasiones hacemos referencia a los términos algoritmos de Machine Learning y modelos de Machine Learning como si tuviesen el mismo significado. Y en realidad un algoritmo y un modelo de Machine Learning están relacionados pero son conceptos diferentes. Así que en este video veremos el significado y las diferencias entre estos dos conceptos. Pero antes de comenzar los invito a visitar codificandovids.com es donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Para entender estos dos conceptos partamos de un problema hipotético. Supongamos que una entidad bancaria ha recolectado información de muchos clientes y que para cada cliente ha recolectado datos específicamente del nivel de ingreso y del nivel de endeudamiento. Y supongamos que con base en esta información el banco quiere encontrar dos diferentes tipos de clientes. Aquellos a quienes puede ofrecer productos de inversión vamos a llamar a este grupo inversores y aquellos a quienes puede ofrecer algún tipo de crédito. A quienes llamaremos el grupo de consumidores. La idea ahora es formalizar el problema anterior usando el lenguaje del Machine Learning. Para hacerlo en primer lugar podemos decir que cada cliente estará definido por dos características. El nivel de ingreso y el nivel de endeudamiento. Y cada una de estas características será simplemente una cantidad numérica. Así que podemos representar cada cliente en un plano cartesiano donde un eje será el nivel de ingreso y otro eje el nivel de endeudamiento. Y cada punto dentro del plano corresponderá precisamente a un cliente con un nivel específico de ingreso y de endeudamiento. Al hacer esto con todos los clientes podríamos observar que habrá por ejemplo clientes con altos niveles de ingreso y bajos niveles de endeudamiento quienes podrían entrar a formar parte del grupo inversores mientras que podría haber otro grupo con niveles medios de ingreso y niveles medios de endeudamiento. Quienes podrían ser parte del grupo consumidores. Entonces en esta gráfica podemos observar dos agrupaciones relativamente separadas la una de la otra. Y la idea es usar el Machine Learning para encontrar una forma de definir automáticamente a qué grupo pertenecería cada cliente tomando como base sus dos características el nivel de ingreso y el nivel de endeudamiento. Una posible solución es intentar encontrar automáticamente y a partir de los datos una frontera que nos permita separar estas dos agrupaciones. Una de las soluciones más sencillas sería simplemente suponer que esta frontera es una línea recta. Idealmente si esta línea recta tiene la orientación correcta podremos separar correctamente un tipo de cliente de otro. Si miramos en detalle la línea recta que separa idealmente las dos agrupaciones veremos que todos los puntos que pertenecen a ella obedecen precisamente a la ecuación de una línea recta donde las variables son los niveles de ingreso y endeudamiento respectivamente. Al reemplazar en esta ecuación los valores correspondientes a un punto que pertenece a la línea recta tendremos como resultado un valor exactamente igual a cero. Pero por ejemplo al tomar un cliente con un perfil inversor y reemplazar sus niveles de ingreso y endeudamiento en la ecuación de la línea recta encontraremos que el resultado es una cantidad positiva. Mientras que si hacemos lo mismo pero con un cliente con un perfil consumidor el resultado será una cantidad negativa. Así que acá ya tenemos una forma de clasificar a un cliente como inversor o consumidor y podríamos resumir el procedimiento en estos pasos. Primero encontrar la ecuación de la línea recta que contenga el nivel de ingreso y el nivel de endeudamiento y que permita separar adecuadamente los dos tipos de cliente. Segundo tomar el nivel de ingreso y de endeudamiento de cada cliente que queramos clasificar reemplazar estos valores en la ecuación de la línea recta y si el resultado es mayor que cero clasificarlo como inversor y si es menor que cero clasificarlo como consumidor. Y listo, acá ya tendríamos una solución al problema usando precisamente el Machine Learning. Pero si analizamos en detalle la solución que acabamos de proponer veremos que el éxito de este clasificado radica en un componente esencial que es la ecuación de la línea recta. Vemos que esta ecuación de la línea recta no puede ser arbitraria pues dicha recta debe tener una orientación precisa que permita separar adecuadamente un tipo de cliente de otro. Así que para poder resolver nuestro problema debemos preguntarnos cómo encontrar la recta que tenga la orientación adecuada y que permita separar correctamente las dos agrupaciones de clientes. Y para responder a esta pregunta debemos analizar en detalle la ecuación de una línea recta. Esta ecuación dependerá no sólo de los valores correspondientes al nivel de ingreso y al nivel de endeudamiento sino que además contendrá unos parámetros que llamaremos W1, W2 y V y que son simplemente cantidades numéricas que dependiendo de sus valores generarán diferentes tipos de líneas rectas con diferentes orientaciones. Así que la solución al problema radica en encontrar el set de parámetros más adecuado que nos arroje como resultado la recta que estamos buscando. Y la idea es que este set de parámetros sea calculado de forma automática es decir sin nuestra intervención y a partir de los datos que tenemos de los clientes. Y para lograr esto necesitamos precisamente un algoritmo de Machine Learning así que hablemos en detalle de este algoritmo. Intuitivamente podemos decir que el algoritmo de Machine Learning es simplemente una serie de pasos que debemos programar en un computador y que de forma automática e iterativa nos ayudará a encontrar los parámetros que nos interesan. Para el problema que estamos resolviendo estos pasos o este algoritmo tomaría en esta forma. Debemos comenzar asignando unos valores iniciales totalmente aleatorios a los parámetros que queremos encontrar. Al asignar estos valores de forma aleatoria podemos garantizar que no vamos a influir de manera alguna en los resultados que arroja el algoritmo y con esto garantizamos que los parámetros serán encontrados de forma automática a partir de los datos. Pero lo más probable es que estos valores aleatorios iniciales no resulten adecuados. Así que la frontera de separación obtenida inicialmente no será perfecta y muchos clientes serán clasificados incorrectamente. Entonces nuestro algoritmo debe incorporar algunos pasos adicionales que permitan refinar estos valores de manera progresiva para que poco a poco vayamos modificando la orientación de la línea recta hasta que logremos clasificar a todos nuestros clientes de forma correcta. Este proceso de refinación se debe repetir un cierto número de veces conocidas como iteraciones con el fin de modificar los parámetros de nuestra frontera de separación para ir mejorando poco a poco la clasificación de nuestros clientes. Y si analizamos en detalle este proceso de refinación, los pasos serían tomar la primera recta construida con los parámetros aleatorios, reemplazar los niveles de ingreso y endeudamiento de cada cliente y con base en los resultados obtenidos, es decir, si los valores son positivos o negativos, clasificar a cada cliente como inversor o consumidor. Y todo esto se conoce como generar una predicción. La idea ahora es comparar las predicciones que acabamos de obtener con la categoría real a la que pertenece cada cliente y definir un error, es decir, un número que nos permita medir qué tan bien o qué tan mal resulta siendo la clasificación para la totalidad de los clientes. Y ahora la idea es tomar este error y modificar los parámetros de nuestra recta siguiendo esta lógica. Si el error es grande, tendremos que hacer grandes modificaciones a los parámetros, mientras que si el error es pequeño, solo tendremos que afinarlos ligeramente. Y si repetimos los pasos anteriores una y otra vez, es decir, por un cierto número de iteraciones, veremos cómo progresivamente el error se irá reduciendo, es decir, las predicciones se parecerán más y más a la categoría real a la que pertenece cada cliente y como resultado llegaremos a estos valores ideales para los parámetros. Y este es en esencia el algoritmo, una serie de pasos que debemos repetir de forma iterativa para encontrar los parámetros de esta línea recta que nos permitirán posteriormente separar adecuadamente un tipo de cliente de otro. Así que como resultado final de aplicar este algoritmo, es decir, después de evaluar todas estas iteraciones, lo que tendremos precisamente será un modelo. En nuestro caso, este modelo de Machine Learning nos permitirá clasificar a un cliente como inversor o consumidor, dependiendo de sus niveles de ingreso y endeudamiento. Y específicamente en nuestro caso, este modelo tendrá dos elementos. El primero será la ecuación de la línea recta con los parámetros finales encontrados por el algoritmo y el segundo será el criterio para la clasificación. Si al reemplazar el nivel de endeudamiento y de ingreso, obtenemos un valor positivo, la categoría será inversor, mientras que si el resultado es negativo, la categoría será consumidor. Y a esto nos referimos cuando hablamos de un modelo de Machine Learning. Es simplemente el producto final del algoritmo y con el cual podremos generar predicciones sobre nuestros datos. Muy bien, a través de este sencillo ejemplo hipotético, hemos entendido el concepto de lo que es un algoritmo y un modelo de Machine Learning. En términos generales, un algoritmo es una serie de pasos que podemos programar en un computador y que se repiten de forma iterativa y que al final nos permiten encontrar los parámetros más adecuados para el modelo que estamos desarrollando para poder resolver el problema que nos interesa. Por otra parte, el modelo es simplemente el producto final del algoritmo y con este producto final podremos tomar datos y generar nuevas predicciones que nos permitan responder al problema que estamos intentando resolver. Y desde luego, tanto el modelo como el algoritmo pueden tomar diferentes formas dependiendo de los tipos de datos que estemos procesando. Así, por ejemplo, podemos tener algoritmos y modelos tan sencillos como los usados en la regresión logística y la regresión lineal o modelos y algoritmos más complejos, como los usados para procesar voz, audio y texto y que podemos encontrar en las redes neuronales, en las redes convolucionales o en las redes Transformer. Si tienen dudas de lo que acabamos de hablar en este video, no olviden dejarlas en los comentarios aquí abajo. Y desde luego, si les gustó el video, no olviden darle un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos. Pues ya saben que esto me ayudará a seguir llegando cada vez a más y más personas con este tipo de contenido. Y si aún no lo han hecho, los invito también a suscribirse al canal y activar la campanita para recibir las notificaciones cada vez que publique un nuevo video acá en el canal de YouTube. Por ahora, esto es todo. Les envío un saludo y nos vemos en el próximo video.
DOC0104|Machine Learning|Cuando desarrollamos un modelo de Machine Learning es común que nos encontremos con los términos parámetros e hiperparámetros, que aunque tienen nombres similares significan dos cosas totalmente diferentes, así que en este vídeo veremos en detalle el significado de cada uno de estos conceptos que resultan fundamentales en el Machine Learning. Pero antes de comenzar los invito a visitar codificandovids.com, en donde encontrarán la academia online con cursos de inteligencia artificial, ciencia de datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan sólo 10 dólares. Así que listo comencemos. Partamos de un ejemplo hipotético para entender estas diferencias entre parámetros e hiperparámetros y para ello usaremos el mismo ejemplo que analizamos en un vídeo anterior cuando hablamos de las diferencias entre algoritmos y modelos de Machine Learning. Supondremos que una entidad bancaria ha recolectado información de muchos clientes y que usaremos el nivel de endeudamiento y el nivel de ingreso para determinar para cada cliente su perfil que puede ser una de dos categorías, inversionista o consumidor. Los clientes con perfil inversionista serán aquellos con ingresos relativamente altos y niveles de deuda relativamente bajos, mientras que los clientes con un perfil consumidor tendrán niveles intermedios tanto de ingreso como de deuda. Como lo vimos en el vídeo en el que hablamos de la diferencia entre algoritmos y modelos, vimos que una manera de resolver este problema es encontrar una frontera que nos permita separar un tipo de cliente de otro con base en sus niveles de ingreso y de deuda y en este caso la frontera puede ser simplemente una línea recta que deberá tener una orientación adecuada para que la separación sea correcta. Así si tenemos esta recta con la orientación ideal podremos clasificar a un cliente como inversor si sus niveles de ingreso y de deuda hacen que esté de un lado de la recta o como consumidor si dichos niveles hacen que esté del otro lado de la recta y esta línea recta se caracteriza por tener unos coeficientes que determinarán precisamente la orientación de esa recta y la forma de calcular estos coeficientes de manera automática es usando un algoritmo que en primer lugar define una recta con una orientación arbitraria, es decir con unos coeficientes totalmente aleatorios y luego de forma iterativa el algoritmo repite una serie de pasos con el fin de refinar la orientación de esta recta. En cada iteración se toma la recta construida y se generan las predicciones, es decir que se clasifica cada cliente de nuestro set de datos como consumidor o inversor dependiendo de si sus características hacen que esté de uno u otro lado de la recta. Luego se evalúa el error existente entre las predicciones y las categorías reales a las que pertenece cada cliente y finalmente se modifican los coeficientes de la recta dependiendo de la magnitud de este error, buscando que este ajuste permita mejorar progresivamente dichas predicciones. Al final de todo esto lo que buscamos es obtener el error más pequeño posible, es decir que las predicciones se parezcan al máximo a las categorías reales a las que pertenecen los clientes y tras ejecutar el algoritmo obtendremos el modelo, el producto final que nos permitirá predecir idealmente de forma correcta a qué categoría pertenece un cliente dados sus niveles de ingreso y de deuda. Pues resulta que implícitamente en este proceso de construcción del modelo a partir del algoritmo encontramos precisamente los conceptos de parámetro e hiperparámetro, así que entendamos a partir de este sencillo ejemplo cada uno de estos conceptos comenzando con los parámetros. En el ejemplo anterior vimos que la orientación de la recta depende de unos coeficientes que son elegidos inicialmente de forma aleatoria pero que iremos refinando a medida que ejecutamos cada paso del algoritmo una y otra vez hasta llegar a esos valores ideales, pues resulta que estos coeficientes se conocen precisamente con el nombre de parámetros y son variables internas del modelo que se pueden obtener de forma automática a través del algoritmo de entrenamiento. Al decir que estos parámetros se obtienen de forma automática nos estamos refiriendo que se obtienen o se pueden estimar a partir de los datos que en nuestro caso corresponde a la información que ha recolectado el banco de cada uno de los clientes y cuyos valores no podremos controlar directamente en cada iteración del algoritmo de entrenamiento, así que teniendo claro este concepto veamos ahora qué son los hiperparámetros y cómo se diferencian de los parámetros. Volviendo al ejemplo anterior vimos que en el caso del algoritmo los pasos que allí definimos se tienen que repetir un cierto número de veces, este número de repeticiones se conoce como el número de iteraciones y es una cantidad numérica que nosotros debemos definir al momento de programar el algoritmo y que debemos escoger cuidadosamente pues si definimos un número inadecuado de iteraciones no lograremos obtener la frontera de separación ideal al final del entrenamiento, así que podemos definir un hiperparámetro como una variable que es externa al modelo es decir que no se obtiene automáticamente a partir de los datos y que en su lugar debemos definir manualmente al momento de programar el algoritmo de entrenamiento. Bien teniendo estas definiciones de parámetros y hiperparámetros veamos ahora algunos ejemplos que nos permitirán entender de forma clara estos dos conceptos. Por ejemplo cuando tenemos una red neuronal cada neurona contiene unos coeficientes que permiten transformar los datos que estamos procesando, estos coeficientes son precisamente los parámetros del modelo pues se calculan de forma automática a partir de los datos durante el entrenamiento, sin embargo para lograr obtener los parámetros más adecuados debemos definir por ejemplo el número de iteraciones del algoritmo de entrenamiento o el número de capas ocultas de la red neuronal o el número de neuronas por capa, estos son los hiperparámetros del modelo pues se trata de valores que definimos al momento de crear la red neuronal. Las redes transformer que son la base de modelos como VARGT, GPT o ChatGPT son un tipo de red neuronal especializada en el procesamiento de secuencias como lo es por ejemplo el texto, en este caso los parámetros son los coeficientes de cada neurona que hace parte de la red, mientras que los hiperparámetros son por ejemplo el número de bloques atencionales o el tamaño del embedding. Una red convolucional es una arquitectura de deep learning que permite procesar imágenes, en este caso los parámetros son por ejemplo los coeficientes de los filtros encargados de extraer diferentes características de las imágenes, mientras que los hiperparámetros pueden ser el número de filtros, el tamaño de cada filtro o el número de capas convolucionales, además cada una de las redes mencionadas anteriormente es entrenada con un algoritmo de optimización que permite ajustar de forma automática e iterativa los parámetros de cada modelo para progresivamente mejorar las predicciones y usualmente este proceso de optimización se basa en el algoritmo del gradiente descendente o en alguna de sus variantes y en el fondo estos algoritmos contienen algo que se conoce como la tasa de aprendizaje que determina qué tan rápido se actualizan los parámetros del modelo en cada iteración de entrenamiento, pues esta tasa de aprendizaje es también otro ejemplo de hiperparámetro y que usualmente encontraremos en diferentes modelos de machine learning. Muy bien a lo largo de este vídeo hemos podido entender estos conceptos de parámetros e hiperparámetros que resultan fundamentales en el machine learning, en esencia un parámetro es una variable numérica interna del modelo que nosotros no podemos controlar al momento de programar el algoritmo de entrenamiento y que por el contrario se calcula de forma automática a partir de Y por otra parte tenemos los hiperparámetros que también son variables numéricas pero que a diferencia de los parámetros sí podemos controlar al momento de programar el algoritmo y antes de hacer el entrenamiento y dependiendo del valor que escojamos para estos hiperparámetros podremos mejorar o empeorar el desempeño del modelo, de hecho existen técnicas de machine learning enfocadas en la afinación de estos hiperparámetros que lo que buscan es encontrar los valores más adecuados de estas variables para generar las mejores predicciones posibles, pero de este tema hablaremos en detalle en un próximo vídeo. Recuerden que si tienen dudas de lo que acabamos de hablar en este vídeo las pueden dejar abajo en los comentarios y recuerden también que si les gustó el vídeo le pueden dar un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos pues ya saben que esto me ayudará a llegar cada vez a más y más personas con este tipo de contenido y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita de notificaciones para que youtube les avise cada vez que suba nuevo contenido al canal. Por ahora esto es todo les envío un saludo y nos vemos en el próximo vídeo.
DOC0105|Machine Learning|Cuando abordamos un proyecto de Machine Learning, usualmente debemos entrenar múltiples modelos y elegir cuál de ellos generará las mejores predicciones posibles. Y generalmente lo que hacemos es entrenar esos múltiples modelos, ponerlos todos a prueba para medir su desempeño, seleccionar el que tenga el mejor desempeño, pero a la vez garantizar que este modelo seleccionado funcione correctamente con datos que nunca antes ha visto. Y para lograr esto, usualmente debemos usar tres sets de datos diferentes. El de entrenamiento, el de validación y el de prueba. Así que en este video veremos por qué son necesarios estos sets de datos al momento de implementar una solución de Machine Learning. Pero antes de comenzar, los invito a visitar codificandobits.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. En un video anterior hablamos en detalle de los parámetros e hiperparámetros de un modelo, conceptos que resultan fundamentales para entender que son estos sets de entrenamiento, validación y prueba. En esencia los parámetros son los coeficientes internos del modelo, es decir, son unas cantidades numéricas que nosotros no podemos modificar y que en su lugar el modelo las aprende durante el proceso de entrenamiento. Y después del entrenamiento el modelo usa estos parámetros para generar predicciones. Y por otro lado tenemos los hiperparámetros que son unos coeficientes externos al modelo y que debemos definir al momento de programar el algoritmo de entrenamiento. Por ejemplo, en un bosque aleatorio los parámetros son los umbrales internos que usa el modelo para clasificar un dato en una o en otra categoría. Mientras que los hiperparámetros son, por ejemplo, el número de árboles que contendrá el bosque o el número máximo de características usado en la clasificación. Y con estos conceptos claros ya estamos listos para comenzar a ver en qué consisten los sets de entrenamiento, validación y prueba. Partamos de un ejemplo hipotético que nos permitirá entender el papel que juegan los sets de entrenamiento, validación y prueba en el desarrollo de un modelo. Supongamos en este sencillo ejemplo que hemos recolectado datos de 10.000 sujetos con información sobre su peso, su nivel de glucosa en sangre y si la persona es o no es hipertensa. Y supongamos que a partir de este set de datos queremos entrenar un modelo de clasificación que tome como entrada las variables peso y nivel de glucosa en sangre y que prediga si una persona es o no es hipertensa. Al intentar resolver este problema nos enfrentamos a una situación que usualmente encontraremos en diferentes proyectos de Machine Learning, que es elegir cuál es el modelo más adecuado para generar las predicciones. En este caso podríamos preguntarnos qué tipo de modelo resultaría más adecuado si por ejemplo una red neuronal, un bosque aleatorio o un modelo de regresión logística. Y realmente no podemos saber esto con antelación, así que lo que nos queda es entrenar todos estos modelos y posteriormente ponerlos a prueba. Así que veamos para este ejemplo hipotético cuál sería el procedimiento a seguir usando precisamente los sets de entrenamiento, validación y prueba. ¿Cómo se puede entrenar los tres modelos? Entonces supongamos que para este problema entrenaremos los tres tipos de modelos mencionados anteriormente. Una red neuronal, un bosque aleatorio y un modelo de regresión logística. Y entonces las primeras dos preguntas que debemos resolver es cómo encontramos los parámetros de estos modelos y cuáles datos usamos para encontrar esos parámetros. Pues si usamos la totalidad de los datos tendríamos un inconveniente porque la idea es no solo encontrar los parámetros del mejor modelo posible sino también ponerlo a prueba en un escenario real. Es decir que una vez entrenado ese mejor modelo lo que nos interesa es presentarle unos datos que no haya visto previamente para determinar si logra clasificar correctamente a un sujeto como hipertenso o no hipertenso. Así que en lugar de entrenar los modelos candidatos con la totalidad de los datos lo que haremos será una primera partición. Tomaremos aleatoriamente por ejemplo un 70% de los datos y se los presentaremos a cada modelo durante el entrenamiento para que logre calcular sus parámetros. Mientras que el 30% restante lo mantendremos por ahora oculto. Y a este set usado para obtener los parámetros de cada modelo durante el entrenamiento lo llamaremos precisamente el set de entrenamiento. Pero recordemos que cada modelo tiene unos hiperparámetros y que dependiendo de los valores que escojamos para esos hiperparámetros podremos mejorar o empeorar el desempeño de cada clasificador. Y recordemos además que nos interesa escoger el mejor modelo de los tres que estamos considerando. Así que en este segundo paso debemos resolver dos preguntas. Cómo elegir los hiperparámetros de cada modelo para que tenga el mejor desempeño posible y cómo elegir cuál es el mejor modelo de los tres que estamos considerando. De nuevo para responder estas preguntas necesitamos recurrir a los datos. Pero no podemos usar el set de entrenamiento pues al hacerlo tendríamos varios inconvenientes. En primer lugar si afinamos los hiperparámetros de cada modelo con el mismo set de entrenamiento podríamos llegar a una situación de sobreajuste. Cada modelo tenderá a memorizar los datos de entrenamiento. Aparentemente tendrá un buen desempeño pero en realidad no generará buenas clasificaciones cuando le presentemos datos que nunca antes ha visto. En segundo lugar para elegir de todos los modelos posibles cuál es el mejor deberíamos tomar cada modelo entrenado y con sus hiperparámetros afinados presentarle los datos y generar predicciones con cada clasificador y simplemente elegir aquel que tenga el mayor porcentaje de aciertos. Pero al usar el set de entrenamiento para generar estas predicciones tendríamos el mismo inconveniente mencionado anteriormente. El desempeño obtenido por cada modelo no sería una medida fiable de su verdadero comportamiento. De nuevo la idea es ponerlo a prueba con un set de datos que no haya visto previamente. Así que la forma adecuada de afinar los hiperparámetros de cada modelo y de elegir el mejor modelo posible es usando un set de datos que no haya visto previamente ninguno de los modelos. Y recordemos que en el problema que estamos resolviendo habíamos ocultado el 30% de los datos. Así que vamos a tomar este subset, lo vamos a mezclar aleatoriamente y lo vamos a partir en dos. Y vamos a tomar la mitad de estos datos, es decir el 15% del set de datos total y lo vamos a usar para afinar los hiperparámetros y para elegir el mejor modelo. Este subset se conoce precisamente con el nombre de set de validación y nos permitirá obtener los mejores hiperparámetros de cada modelo y seleccionar el mejor modelo de todos los que hayamos entrenado y de una manera objetiva. Muy bien, hasta este punto ya hemos entrenado cada uno de los modelos usando precisamente el set de entrenamiento y ya hemos afinado sus hiperparámetros y encontrado el mejor modelo de los tres que estamos considerando usando el set de validación. Así que ya hemos seleccionado el modelo que podría generar las mejores predicciones para nuestro problema en particular. Y entonces sólo nos queda resolver una pregunta, que es ¿cómo se comportaría el modelo seleccionado con datos que nunca antes ha visto? Es decir que la idea es poder usar nuestro modelo en el mundo real, así que si tenemos nuevos sujetos la idea sería recolectar información de su nivel de glucosa en sangre, de su peso, llevar esa información al modelo entrenado e intentar predecir con el mayor grado de fiabilidad si ese sujeto es hipertenso o no hipertenso. Y para poner a prueba este modelo seleccionado no podemos usar ni el set de entrenamiento ni el set de validación porque se trata de datos que ya ha analizado previamente ese modelo. Así que debemos ser totalmente imparciales y objetivos en este punto, para que al poner a prueba el modelo tengamos una medida certera de cuál sería su verdadero desempeño con datos que no ha procesado previamente. ¿Y cuáles son esos datos que aún no ha visto el modelo seleccionado? Pues recordemos que habíamos tomado un 70% de los datos para el entrenamiento y un 15% como set de validación, así que aún tenemos un 15% de los datos ocultos. Pues podemos precisamente usar estos datos restantes para poner a prueba el modelo y determinar cuál sería su desempeño en condiciones reales. Y este set se conoce precisamente con el nombre de set de prueba. Y acá es importante hacer hincapié en que este set lo debemos mantener en todo momento oculto al modelo y solo se lo debemos presentar al final de todo el proceso. Cuando ya hayamos realizado el entrenamiento, seleccionado el mejor modelo y afinado sus hiperparámetros. Si no hacemos esto podríamos obtener una medida poco fiable del desempeño real del modelo y esto podría afectar su fiabilidad al momento de hacer predicciones con datos totalmente nuevos. Muy bien, acabamos de ver que son los sets de entrenamiento, validación y prueba muy usados convencionalmente cuando entrenamos y construimos modelos de Machine Learning. En esencia el set de entrenamiento nos permite obtener los parámetros de cada modelo candidato, mientras que el set de validación nos permite encontrar los hiperparámetros más adecuados de cada uno de esos modelos candidatos y elegir de entre todos estos modelos que estamos considerando cuál es el que tiene el mejor desempeño para el problema que estamos resolviendo. Y una vez elegido el mejor modelo lo que podemos hacer es ponerlo a prueba precisamente con el set de prueba, lo que nos daría una medida de cuál sería su desempeño cuando llevemos el modelo a producción y comencemos a presentarle datos que nunca antes había visto. Esta partición del set de datos en entrenamiento, validación y prueba es un enfoque muy usado especialmente en el Deep Learning, cuando contamos con cientos de miles o incluso millones de datos y las proporciones usadas convencionalmente son 70, 15, 15 u 80, 10, 10. Sin embargo este enfoque no resulta adecuado cuando tenemos un set de datos pequeño, con unos cuantos cientos o miles de datos. En estos casos es recomendable usar lo que se conoce como la validación cruzada, de la cual vamos a hablar en detalle en un próximo video. Y bien, espero que les haya gustado este video y si es así los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con todos sus amigos y conocidos para seguir llevando este contenido a cada vez más y más personas. Y si tienen alguna duda de este video no olviden dejarla abajo en los comentarios. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido en el canal. Así que por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
DOC0106|Machine Learning|En un video anterior vimos cómo usar los sets de entrenamiento, validación y prueba para encontrar los parámetros y perparámetros de un modelo y para elegir el mejor modelo de entre varios posibles. Sin embargo, no siempre resulta conveniente usar este enfoque y muchas veces es más adecuado usar un enfoque que se conoce como la validación cruzada. Así que en este video vamos a entender qué es la validación cruzada y cómo funciona el K-Fault-Cross Validation que es uno de los principales métodos o algoritmos utilizados para realizar precisamente esta validación. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Antes de ver qué es la validación cruzada vale la pena recordar que son los parámetros y perparámetros de un modelo algo de lo que ya hablamos en detalle en un video anterior y recordemos también que uno de los objetivos del Machine Learning es construir un modelo que tome unos datos de entrada y que sea capaz de generar predicciones sobre esos datos. Cuando construimos el modelo lo que buscamos es encontrar una serie de parámetros que son simplemente los coeficientes numéricos internos del modelo y que se encuentran durante el proceso de entrenamiento. Además para construir ese modelo debemos definir los hiperparámetros que son también unos coeficientes numéricos pero que nosotros como programadores del algoritmo de entrenamiento tenemos que definir buscando generar las mejores predicciones posibles. Además usualmente es necesario entrenar múltiples modelos con diferentes parámetros e hiperparámetros para posteriormente elegir cuál es el más adecuado de todos estos modelos para el problema que estamos intentando resolver. Y para lograr esto debemos ajustar los parámetros e hiperparámetros de cada modelo medir su desempeño y escoger el modelo que genere las mejores predicciones posibles. Y en todo esto entra en juego un concepto muy importante que se conoce como la capacidad de generalización del modelo que es simplemente su capacidad de generar buenas predicciones sobre datos que no ha visto previamente. Entendamos esto con un ejemplo. Supongamos que queremos construir un modelo que tome variables como el peso, la edad y algunos hábitos de las personas como la cantidad de horas de ejercicio que hace semanalmente por ejemplo y con ello prediga si el sujeto tiene o no problemas cardíacos. De nada sirve contar con un modelo que durante el entrenamiento acierte en la clasificación el 99% de las veces si al momento de presentarle nuevos datos que nunca antes ha procesado esta tasa de acierto se reduzca tan solo el 70%. Es decir, lo ideal sería que en condiciones reales con datos nunca antes vistos este desempeño sea cercano al 99% obtenido en el entrenamiento. Y para entrenar el modelo y evaluar su capacidad de generalización podríamos usar precisamente los sets de entrenamiento, validación y prueba. Así que a continuación voy a hacer un pequeño resumen de este método pero si quieren ver en detalle en qué consiste les dejo abajo el enlace del video correspondiente. Con este enfoque tomamos nuestro set de datos y lo partimos en tres. entrenamiento con aproximadamente el 70% de los datos validación y prueba cada uno con aproximadamente el 15% de los datos. El set de entrenamiento nos permite obtener los parámetros del modelo y el de validación nos permite ajustar los hiperparámetros y seleccionar el mejor modelo entre varios posibles. Mientras que el set de prueba nos permite medir la capacidad de generalización del modelo. El problema de este enfoque es que para poder usarlo generalmente requerimos muchos datos generalmente cientos de miles o incluso millones y esto no resulta siempre viable pero además puede ocurrir que al hacer la partición no necesariamente estos tres subsets tengan las mismas distribuciones. Entonces es posible que por ejemplo el set de entrenamiento tenga datos ligeramente diferentes de los sets de validación y prueba y esto va a afectar el entrenamiento y validación del modelo. Y por otra parte al ajustar los parámetros y hiperparámetros del modelo siempre con los mismos sets de datos los de entrenamiento y validación podemos llevar al modelo algo que se conoce como el sobreajuste es decir que el modelo va a tender a memorizar esos datos de entrenamiento y validación y no va a funcionar adecuadamente cuando le presentemos por ejemplo los datos provenientes del set de prueba. Y la validación cruzada es precisamente una alternativa a todos estos inconvenientes que acabo de mencionarles así que veamos en detalle en qué consiste esta validación cruzada. La idea de la validación cruzada es muy simple en lugar de usar diferentes sets para entrenamiento, validación y prueba en la validación cruzada vamos a usar la totalidad de los datos y aunque existen muchos métodos para lograr este objetivo uno de los más usados es el de la validación cruzada de K iteraciones más conocida por su término en inglés K-Fault-Cross-Validation así que en lo que queda de este video nos enfocaremos en este tipo de validación entonces comencemos viendo en qué consiste el algoritmo y luego lo entenderemos en detalle a través de un ejemplo. El algoritmo comienza con la mezcla aleatoria de los datos y con la inicialización del parámetro K que es simplemente un número entero que definirá el número de particiones de nuestro set de datos así como el número de iteraciones de entrenamiento y validación que se usarán al construir el modelo esto es lo que le da el nombre precisamente a este algoritmo. Una vez definido el parámetro K inicia este proceso de entrenamiento y validación así que por un total de K iteraciones se repiten estos pasos se toma una de las K particiones y se mantiene oculta al modelo se toman las K-1 particiones restantes y con ellas se entrena el modelo es decir se calculan de forma automática a través del algoritmo de entrenamiento los parámetros y una vez entrenado el modelo se almacena su desempeño obtenido con el set de entrenamiento K-1 particiones y con el set de datos oculto la partición restante y luego se repiten los tres pasos anteriores pero cambiando la partición que se mantiene oculta y una vez terminadas las iteraciones tendremos K medidas de desempeño para los sets de entrenamiento y validación usados en cada iteración así que el desempeño final del modelo será simplemente el promedio de los desempeños anteriores Entendamos el algoritmo anterior con un ejemplo práctico entonces supongamos que volvemos al caso hipotético del modelo que queremos construir para determinar si una persona tiene o no problemas cardíacos y para simplificar las cosas supondremos un set de datos muy pequeño tan solo 12 datos y vamos a asumir que mediremos el desempeño como la exactitud del total de datos introducidos al modelo cuantas predicciones son correctas en este caso el algoritmo comienza con la mezcla aleatoria de los datos con lo cual podremos garantizar que el orden en el que presentamos los datos al modelo no afectará su desempeño ahora sumamos que el valor de K es igual a 3 es decir que partiremos el set de 12 datos en 3 bloques o faults cada uno con un total de 4 datos y ahora comenzamos las iteraciones de entrenamiento y validación como el valor de K es igual a 3 realizaremos 3 iteraciones en la primera iteración ocultamos el bloque 1 y usamos los bloques 2 y 3 para entrenar el modelo y una vez entrenado le presentamos el bloque 1 y lo validamos supongamos que tras este procedimiento obtenemos un desempeño con el set de entrenamiento igual al 82% y con el set de validación igual al 80% almacenamos estos dos valores en dos listas independientes y continuamos con las iteraciones en la segunda iteración repetimos los mismos pasos de la iteración anterior con la diferencia de que ahora debemos cambiar el bloque oculto supongamos que en este caso ocultamos el bloque 2 y que entrenamos el modelo con los bloques 1 y 3 tras el entrenamiento validamos el modelo con el bloque 2 y almacenamos en los listados anteriores los desempeños obtenidos 84% para entrenamiento y 81% para validación y llegamos finalmente a la tercera iteración en donde ocultamos el bloque 3 y usamos los bloques 1 y 2 para entrenar obteniendo al final de todo esto desempeños del 80% para los dos bloques de entrenamiento y del 79% para el bloque de validación perfecto terminadas las iteraciones el último paso del algoritmo es determinar el desempeño final del modelo durante el entrenamiento y la validación para el caso del entrenamiento simplemente tomamos los desempeños obtenidos durante las tres iteraciones de entrenamiento es decir 82, 84 y 80% y los promediamos obteniendo una exactitud promedio del 82% y hacemos algo similar con los desempeños obtenidos durante las tres evaluaciones es decir 80, 81 y 79% obteniendo una exactitud promedio del 80% acá vemos que los desempeños durante el entrenamiento y la validación son muy similares y por tanto podemos concluir que el modelo no tiene ningún sobre ajuste y que ha sido entrenado y validado correctamente y que además de esto tiene una buena capacidad de generalización pues la exactitud promedio de validación es muy cercana a la obtenida durante el entrenamiento una pregunta obvia que surge tras haber visto cómo funciona el algoritmo es cuál es el número adecuado de particiones es decir cuál es ese valor ideal que debe tener el parámetro K y en realidad no existe una única respuesta pues todo depende del modelo que estemos construyendo y de los datos con que lo estemos alimentando sin embargo podemos analizar algunas situaciones extremas y con base en esto definir un rango de valores de K que podría resultar adecuado si escogemos el valor de K más pequeño posible es decir 2 tendremos tan solo dos particiones y es posible que el modelo no detecte adecuadamente los patrones en los datos lo que hace que la mayoría de las predicciones sean erróneas por otra parte si escogemos un valor de K muy grande comparado con el tamaño del set de datos tendremos demasiadas particiones y cada una de ellas contendrá muy pocos datos esto hará que haya demasiada variabilidad en el desempeño entre una y otra partición y por tanto el entrenamiento y la validación no van a ser fiables además si nuestro set de datos tiene muchos datos y a la vez usamos demasiadas particiones requeriremos muchos más recursos computacionales para el entrenamiento y la validación así que usualmente lo que se hace es usar valores intermedios de K dependiendo del tamaño del set de datos es decir ni muy pequeños ni muy grandes por ejemplo cuando tenemos unos cuantos cientos o algunos miles de datos generalmente podríamos usar valores de K iguales a 5, 10 o 20 y lo que sí podemos hacer es entrenar y validar el modelo con estos tres diferentes valores de K y verificar cuál de ellos nos genera los mejores resultados para el modelo que estemos entrenando y para los datos que estemos utilizando muy bien, acabamos de ver en qué consiste la validación cruzada y uno de los algoritmos más usados en esta metodología que es el algoritmo de K-Faultless Validation esta validación cruzada es una alternativa a la partición del set de datos en entrenamiento, validación y prueba del cual hablamos en un vídeo anterior y tiene además varias ventajas con respecto a ese método la primera de ellas es que funciona en casos para los cuales tenemos relativamente pocos datos por ejemplo unos cuantos cientos o unos pocos miles la segunda ventaja es que nos da una validación más robusta porque estamos entrenando el modelo y validándolo con la totalidad de los datos y esto nos puede dar una medida más precisa de cuál es la capacidad de generalización del modelo y la tercera ventaja es que al entrenar y validar el modelo con la totalidad de los datos estamos reduciendo esa probabilidad de overfitting o sobreajuste del modelo y esto nuevamente mejora su capacidad de generalización sin embargo, tal vez una de las principales desventajas de este método es su costo computacional pues al tener muchos datos, muchas particiones tendremos que llevar a cabo múltiples iteraciones de entrenamiento y validación en comparación con el enfoque de el set de entrenamiento, validación y prueba muy bien, recuerden que si tienen alguna de lo que acabamos de ver en este vídeo me la pueden dejar abajo en los comentarios y recuerden también que si les gustó el vídeo los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con todos sus amigos y conocidos pues esto me ayudará a seguir llegando cada vez a más y más personas y si aún no lo han hecho, los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique un nuevo vídeo por ahora esto es todo, les envío un saludo y nos vemos en el próximo vídeo
DOC0107|Machine Learning|Para entrenar cualquier modelo de Machine Learning tenemos que fijar algo que se conocen como los hiperparámetros, que son unas variables numéricas externas que debemos escoger cuidadosamente para luego obtener el mejor desempeño posible. Así que para cualquier proyecto de Machine Learning tenemos que obtener ese set de hiperparámetros ideal que nos permita generar las mejores predicciones posibles para el modelo que estemos entrenando y para el problema que queramos resolver. Este proceso se conoce precisamente con el nombre de ajuste de hiperparámetros y existen esencialmente dos enfoques para lograr este objetivo, la búsqueda exhaustiva o grid search y la búsqueda aleatoria o random search. Así que en este video vamos a entender en detalle qué es este ajuste de hiperparámetros y cómo funcionan estos dos métodos que les acabo de mencionar. Pero antes de comenzar los invito a visitar codificandobits.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Comencemos recordando qué son los hiperparámetros de un modelo de lo cual ya hablamos en detalle en un video anterior. Supongamos que queremos construir una red neuronal para una empresa de streaming de video. La idea es tomar el perfil de cada cliente, como el número de meses que lleva suscrito al servicio, la cantidad de horas por semana que accede a la plataforma, la edad del cliente y su ubicación geográfica entre otras y queremos que la red aprenda a predecir si el cliente permanecerá o abandonará el servicio. Una de las decisiones que tenemos que tomar en este caso es por ejemplo determinar cuántas capas debemos agregar a esta red neuronal y cuántas neuronas tenemos que agregarle a cada una de esas capas. Estas variables son precisamente algunos hiperparámetros de esta red neuronal. Así que los hiperparámetros son en esencia unos valores numéricos externos que nosotros debemos definir al momento de escribir el código para crear esa red neuronal o ese modelo de Machine Learning. Volvamos al problema anterior y supongamos que definimos estos posibles valores para los hiperparámetros de la red neuronal. Un número de capas ocultas igual a 1 o 2. Si tenemos solo una capa oculta podremos usar 20 o 10 neuronas y si tenemos dos capas ocultas en la segunda capa podremos ubicar 10 o 5 neuronas. Y acá vale la pena preguntarnos cómo podemos determinar ese número de capas ocultas y ese número de neuronas por capa de la red neuronal. Y esta pregunta es fundamental, pues el desempeño del modelo estará precisamente relacionado con el valor que escojamos para estos hiperparámetros. Esto quiere decir que para ciertas combinaciones de capas ocultas y de número de neuronas por capa tendremos unas predicciones relativamente malas, pero para otras combinaciones podremos obtener predicciones muchísimo mejores. Desafortunadamente no existe algo así como una fórmula matemática mágica que nos permita encontrar el valor ideal de esos hiperparámetros, pues estos dependen del modelo que estemos utilizando y también de los datos. Y como cada problema tiene un set de datos totalmente diferente, pues resulta imposible tener una fórmula que funcione adecuadamente para todos los modelos y tipos de datos. Así que en la práctica lo que se hace es usar un enfoque de prueba y error, es decir, tomamos diferentes combinaciones de hiperparámetros, entrenamos y validamos el modelo con cada una de esas combinaciones, determinamos su desempeño y de todas las pruebas que hagamos escogemos aquel modelo y aquel set de hiperparámetros que nos genere el mejor desempeño posible. Y esto se conoce precisamente como el ajuste de hiperparámetros. Así que este ajuste de hiperparámetros es un proceso de prueba y error que dependiendo de las combinaciones que estemos probando nos permite encontrar el set de hiperparámetros más adecuado para el modelo y para los datos que estemos usando. Y para realizar este ajuste de hiperparámetros generalmente usamos uno de estos dos enfoques, la búsqueda exhaustiva que también se conoce en inglés como grid search y la búsqueda aleatoria que se conoce como random search. Así que vamos en detalle en qué consisten estos dos métodos. La idea de la búsqueda exhaustiva es sencilla, simplemente debemos probar todas las posibles combinaciones de hiperparámetros, es decir que para cada una de esas combinaciones entrenamos y validamos el modelo, calculamos su desempeño y al final elegimos la combinación que nos arroje el mejor desempeño posible. Entendamos esta idea volviendo a nuestro ejemplo de la red neuronal. Habíamos mencionado anteriormente que tenemos estas opciones de hiperparámetros, un número de capas ocultas igual a 1 o 2, un número de neuronas en la primera capa de 20 o 10 y un número de neuronas en la segunda capa de 10 o 5. Teniendo esto en cuenta podríamos tener en total seis combinaciones de hiperparámetros. En la primera podemos tener una capa oculta con 20 neuronas, en la segunda usaríamos una capa oculta pero con 10 neuronas, en la tercera tendríamos dos capas ocultas, la primera con 20 neuronas y la segunda con 10. En la cuarta combinación tendríamos dos capas ocultas, similar al caso anterior pero la primera capa tendría 20 neuronas, mientras que la segunda tendría 5. En la quinta combinación tendríamos dos capas ocultas, ambas con 10 neuronas. Y por último en la sexta combinación tendríamos dos capas ocultas, la primera con 10 neuronas y la segunda con 5. Ahora la idea es entrenar y validar la red neuronal con cada una de estas seis combinaciones por aparte, es decir, hacer el entrenamiento y la validación por aparte un total de seis veces. Para este entrenamiento y validación podemos usar los sets de entrenamiento, validación y prueba o la validación cruzada. De estos métodos ya hablen detalle en videos anteriores, así que les dejo el enlace en la descripción para que los revisen. Supongamos además que vamos a medir el desempeño del modelo como el porcentaje de aciertos al momento de determinar si un suscriptor permanece o abandona nuestro servicio. Así un desempeño del 95% nos dice que de cada 100 usuarios que analizamos, en 95% de estos casos el modelo predice correctamente el estado de ese usuario. Pues bien, supongamos que entrenamos la red con la primera combinación de hiperparámetros y obtenemos un desempeño del 92%. Hacemos lo mismo con la segunda combinación y obtenemos un 87%. Y repetimos esto para las cuatro combinaciones de hiperparámetros restantes, obteniendo desempeños de 91%, 96%, 88% y 92% respectivamente. Terminado este procedimiento, lo único que nos queda es elegir el set de hiperparámetros para el cual se obtuvo el mejor desempeño. En este caso esta sería la cuarta combinación, es decir la red con dos capas ocultas, la primera con 20 neuronas y la segunda con 5. Pues dicha combinación nos entrega el desempeño más alto que es del 96%. Y listo, esta es la idea básica de la búsqueda exhaustiva. La ventaja de este método es que al probar todas las combinaciones al final estaremos 100% seguros de que la combinación seleccionada es la mejor de todas las posibles y por tanto el modelo seleccionado será el mejor de todos. Sin embargo la desventaja es que si tenemos un modelo muy complejo y demasiados datos, el entrenamiento y validación con cada una de esas combinaciones requerirá demasiado tiempo y demasiados recursos computacionales. Así que como alternativa a este inconveniente tenemos el método de random search o búsqueda aleatoria. El principal inconveniente de la búsqueda exhaustiva que vemos anteriormente es que se tienen que probar todas las posibles combinaciones de hiperparámetros, lo que puede llegar a dificultar ese proceso de ajuste de hiperparámetros. Entonces como alternativa a este método tenemos precisamente la búsqueda aleatoria que lo que hace es en lugar de tomar todas las posibles combinaciones de hiperparámetros, selecciona una muestra aleatoria de esas posibles combinaciones, entrena y valida el modelo con esa muestra aleatoria, luego calcula el desempeño y elige el más adecuado de entre todas las muestras analizadas. Para entender cómo funciona este método volvamos una vez más a nuestro ejemplo de la red neuronal. Habíamos mencionado que existen seis posibles combinaciones de hiperparámetros, una capa oculta con 20 neuronas, una capa oculta con 10 neuronas, dos capas ocultas con 20 y 10 neuronas, dos capas ocultas con 20 y 5 neuronas, dos capas con 10 neuronas cada una o dos capas ocultas con 10 y 5 neuronas. Así que en este caso en lugar de analizar las seis posibles combinaciones, seleccionaremos por ejemplo una muestra aleatoria de tres. Digamos que estas son las combinaciones 1, 4 y 6. Una vez seleccionadas estas combinaciones lo que hacemos es entrenar y validar el modelo con cada una de ellas, y al hacerlo obtendremos desempeños de 92, 96 y 92 por ciento. Y una vez evaluados estos desempeños, lo único que nos queda es elegir la mejor combinación, es decir aquella con el desempeño más alto, que en este caso es la cuarta con un desempeño del 96 por ciento, y que corresponde a una red neuronal con dos capas ocultas con 25 neuronas cada una. Y listo, así es como funciona el método de búsqueda aleatoria para el ajuste de hiperparámetros, y la ventaja de este método es evidente comparado con la búsqueda exhaustiva, pues al probar menos combinaciones de hiperparámetros requeriremos menos tiempo de entrenamiento y validación, y menos recursos computacionales, pero la desventaja está en que al seleccionar aleatoriamente una parte de esas posibles combinaciones, no siempre podremos garantizar que dentro de la muestra seleccionada estará esa combinación ideal de hiperparámetros que genere el mejor desempeño posible. Muy bien, acabamos de ver en qué consiste el ajuste de hiperparámetros, que en esencia nos permite encontrar ese set de hiperparámetros que genera las mejores predicciones posibles para el modelo que estemos construyendo, y para lograr esto podemos generalmente usar uno de dos enfoques, la búsqueda exhaustiva que analiza todas las posibles combinaciones de hiperparámetros, y la búsqueda aleatoria que analiza una muestra aleatoria de todas esas posibles combinaciones. La búsqueda exhaustiva nos permite garantizar que al final del proceso vamos a encontrar ese set óptimo de hiperparámetros para construir el modelo, pero va a requerir mucho más tiempo de entrenamiento y más recursos computacionales, mientras que la búsqueda aleatoria será más rápida, va a requerir menos recursos, pero no siempre nos va a garantizar llegar a esa solución óptima en términos de los hiperparámetros. Así que la elección que hagamos va a depender precisamente de esos recursos computacionales, si tenemos tiempo suficiente y muchos recursos podríamos usar la búsqueda exhaustiva, o de lo contrario podríamos considerar la búsqueda aleatoria. Sin embargo, en proyectos reales con modelos muy complejos, con muchos hiperparámetros y muchos datos, generalmente el enfoque de búsqueda aleatoria funciona bastante bien, y es el que les recomiendo para sus proyectos. Bien, ya saben que si tienen algún adudó o comentario lo pueden dejar acá abajo, y recuerden que si les gustó el vídeo los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con sus amigos y conocidos, pues ya saben que esto me ayudará a llegar con este contenido a cada vez más y más personas, y si aún no lo han hecho los invito a suscribirse al canal y activar las notificaciones para que youtube les avise cada vez que publique nuevos vídeos acá en el canal. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo vídeo.
DOC0108|Machine Learning|Una de las principales decisiones que debemos tomar al momento de resolver un problema de Machine Learning es elegir el modelo más adecuado para los datos que estemos procesando. Es decir que el objetivo es encontrar el modelo que una vez entrenado genera las mejores predicciones posibles. Así que en este video les voy a mostrar la serie de pasos que sugiero para seleccionar el modelo más adecuado para nuestro problema de Machine Learning. Pero antes de comenzar los invito a visitar codificandovids.com Es donde encontrarán la Academia Online concursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Antes de ver este paso a paso es importante hacer algunas aclaraciones. En primer lugar cuando digo el mejor modelo me estoy refiriendo a que el modelo que genere las mejores predicciones. Es decir que no vamos a tener en cuenta por ejemplo situaciones en las cuales queremos que el modelo genere las predicciones lo más rápido posible. Es decir con el menor tiempo de cómputo o aquel modelo que ocupe el menor espacio en memoria. En segundo lugar es importante que tengan claros varios conceptos que hemos visto en videos anteriores pero que resultan fundamentales para lo que vamos a ver a continuación en este proceso paso a paso para la selección del modelo. Así que les sugiero ver esos videos, pero a continuación voy a hacer un breve resumen de los principales conceptos. El primero de ellos son los parámetros e hiperparámetros de un modelo. Los parámetros de un modelo son las variables numéricas internas que el modelo aprende con el algoritmo de entrenamiento. Mientras que los hiperparámetros son las variables numéricas externas que nosotros debemos fijar al momento de programar dicho algoritmo. Por otra parte existe algo que se conoce como la afinación de hiperparámetros que nos permite encontrar los valores más adecuados que deben tener los hiperparámetros para lograr generar las mejores predicciones posibles. Y tanto para entrenar el modelo como para ajustar sus hiperparámetros y evaluar su desempeño, podemos usar dos enfoques. La partición del set de datos en los sets de entrenamiento, validación y prueba o usando lo que se conoce como la validación cruzada. Entonces con estas aclaraciones iniciales ya estamos listos para entender cómo realizar la selección del modelo. Así que comencemos entendiendo cuál es el problema que queremos resolver. Supongamos que queremos resolver este problema hipotético, digamos que queremos predecir la eficiencia en el consumo de gasolina de un vehículo, es decir la cantidad de litros de gasolina que consume por cada kilómetro recorrido. Y esta eficiencia en el consumo es simplemente un número, esto quiere decir que por ejemplo si tenemos una eficiencia de 0.5 litros por kilómetro, esto quiere decir que si el vehículo recorre un kilómetro, en promedio requerirá un consumo de medio litro de gasolina. Y supongamos además que esta eficiencia depende de variables como el tamaño del motor del vehículo en centímetros cúbicos, su potencia medida en caballos de potencia, su peso en kilogramos, así como el número de cilindros y la edad misma del vehículo. Así que el objetivo es construir un modelo de Machine Learning que tome estas variables numéricas de entrada y que aprenda a la salida a generar una predicción del valor que tendría la eficiencia en el consumo. Esto se conoce como una tarea de regresión, que quiere decir que el modelo debe aprender a predecir una cantidad numérica. Y la idea es que esta predicción sea lo más cercana posible al valor real de la eficiencia del consumo. Así que con este panorama ahora sí podemos ver el procedimiento paso a paso para seleccionar el mejor modelo para este problema en particular. La idea básica de la selección del mejor modelo consiste en entrenar múltiples modelos, luego evaluar el desempeño de cada uno de ellos usando el set de datos y finalmente elegir aquel modelo que genere las mejores predicciones. Y el primer paso es establecer lo que se conoce como la métrica de desempeño que usaremos para comparar los diferentes modelos. Esta métrica de desempeño es simplemente una fórmula matemática que a la salida nos arrojará un número que nos permitirá evaluar que también o mal lo está haciendo el modelo al momento de generar predicciones. Y esta fórmula lo que hace en esencia es comparar las predicciones que hace el modelo con los valores de referencia, es decir que en este caso lo que va a hacer es comparar el valor predicho para esa eficiencia en el consumo de combustible con el valor real. Y la fórmula o métrica que escojamos dependerá del tipo de problema que queramos resolver. Así que en este caso como tenemos un problema de regresión tendremos a nuestra disposición diferentes métricas de las cuales voy a hablar más adelante en otro video, pero en nuestro caso podemos escoger lo que se conoce como la raíz cuadrada del error cuadrático medio o RMSE por sus siglas en inglés. Para entender en qué consiste este RMSE, supongamos que tenemos 200 datos de prueba para verificar el desempeño del modelo que estamos construyendo. Y supongamos que ingresamos al modelo entrenado cada uno de estos 200 datos generando 200 predicciones. Pero además por cada uno de estos datos tendremos el valor real de la eficiencia en el consumo. Así que en últimas tendremos 200 predicciones y 200 valores reales de eficiencia. Entonces para obtener el RMSE debemos hacer estos cálculos. Primero obtenemos las diferencias entre el valor real y el valor predicho para cada dato y las elevamos al cuadrado. Luego sumamos estas diferencias al cuadrado y dividimos el resultado entre el número total de datos que estamos comparando, que son 200 en este caso. Y finalmente obtenemos la raíz cuadrada de este resultado. La lógica de elevar al cuadrado cada diferencia es para evitar que al hacer la suma se anulen mutuamente errores individuales. Lo que podría llevar a la obtención de errores aparentemente más pequeños de lo que realmente son. Además al dividir la suma de estas diferencias cuadráticas entre el número total de datos, lo que estamos es calculando el valor promedio de estas diferencias. Y finalmente al obtener la raíz cuadrada estamos garantizando que el resultado final tendrá las mismas unidades originales de cada una de las variables que estamos comparando. Que en este caso será la eficiencia del consumo medida en litros por kilómetro. Por ejemplo si al hacer este cálculo para un modelo determinado encontramos que el RMSE es de 0.1 litros por kilómetro, quiere decir que en promedio para dicho modelo las predicciones se alejarán del valor real aproximadamente 0.1 litros por kilómetro. Esto quiere decir que entre más pequeño sea el valor del RMSE para un modelo en particular, mejores serán las predicciones generadas. Ahora el segundo paso es determinar qué tipos de modelos pueden resultar más adecuados para el problema que queremos resolver. Y esto es un desafío en sí mismo, pues elegir los modelos candidatos entre una amplia variedad de opciones requiere que tengamos experiencia y conocimiento en el uso de diferentes algoritmos y modelos de Machine Learning. Como guía les sugiero un video que encontrarán acá en el canal en donde hago un tour por las principales familias de algoritmos y modelos que existen para que tengan una idea general de los criterios a tener en cuenta para esta elección. Supongamos que consideramos tres opciones para este caso en particular. Una máquina de soporte vectorial, un bosque aleatorio y una red neuronal. Aunque podríamos elegir más, en este ejemplo nos centraremos solo en estas tres arquitecturas de las cuales ya habló en videos anteriores. En este punto no podemos decir con antelación cuál de estos modelos resultará más adecuado. Pues para ello debemos entrenarlos y medir su desempeño, que es precisamente el siguiente paso. Bien, en este tercer paso lo que tenemos que hacer es tomar cada uno de los modelos, entrenarlo, es decir encontrar sus parámetros, afinarlo, es decir encontrar el set de hiperparámetros que genera las mejores predicciones y luego validarlo, es decir calcular su RMSE. Y para lograr esto podemos usar los sets de entrenamiento, validación y prueba o la validación cruzada y al final de todo este procedimiento lo que tendremos será un número, el RMSE de cada modelo que nos indicará qué tan buenas predicciones está generando cada uno de estos modelos con datos que nunca había visto. Así que supongamos que nos enfocamos en el primer modelo, la máquina de soporte vectorial. Tomamos el set de datos original, entrenamos el modelo, ajustamos sus hiperparámetros y tras este procedimiento lo validamos y obtenemos un RMSE de por ejemplo 0.23 litros por kilómetro. Ahora vamos al segundo modelo que es el bosque aleatorio y supongamos que tras entrenarlo, afinarlo y calcular su error de validación obtenemos un RMSE de 0.26 litros por kilómetro. Y finalmente repetimos el mismo procedimiento para el tercer modelo, la red neuronal. La entrenamos y ajustamos sus hiperparámetros y luego la validamos, obteniendo un RMSE de 0.19 litros por kilómetro. Y después de este procedimiento ya tenemos tres diferentes modelos entrenados y para cada uno hemos obtenido el set de hiperparámetros que genera las mejores predicciones y además hemos medido su desempeño. Así que solo nos resta el último paso, seleccionar el mejor modelo. ¿Qué es el mejor modelo? Recordemos que cuando nos referimos al mejor modelo estamos hablando de aquel modelo que genere las mejores predicciones posibles y recordemos precisamente que el RMSE es un número que nos indica que tan buenas predicciones está generando cada uno de los modelos. Entonces volviendo al paso anterior recordemos que al final de cada validación obtuvimos valores de 0.23 litros por kilómetro para la máquina de soporte vectorial, 0.26 litros por kilómetro para el bosque aleatorio y 0.19 litros por kilómetro para la red neuronal. Así que solo nos resta decidir con base en este desempeño cuál de los tres modelos es el mejor. Y para ello recordemos que el RMSE mide en últimas el error promedio que tendrá la predicción hecha por cada modelo. Así que entre más pequeño sea el RMSE mejores serán las predicciones generadas por el modelo. Entonces en este caso en particular el mejor modelo será aquel con el menor RMSE que corresponde a la red neuronal que en promedio genera un error de 0.19 litros por kilómetro al momento de predecir la eficiencia en el consumo de combustible. Y listo en este punto y siguiendo los pasos que les acabo de mencionar ya habremos seleccionado el modelo que genera las mejores predicciones para el problema que estamos resolviendo. Muy bien, acabamos de ver en qué consiste el proceso de selección del mejor modelo de Machine Learning para resolver un problema usando como único criterio el desempeño al momento de hacer las predicciones. Y este procedimiento es bastante simple, comenzamos eligiendo la métrica de desempeño después seleccionamos las posibles arquitecturas o tipos de modelos más adecuados luego entrenamos, afinamos y validamos cada modelo y al final elegimos aquel modelo con la mejor métrica de desempeño. Sin embargo, cuando tenemos otras restricciones adicionales como el tiempo de cómputo al momento de hacer las predicciones o el tamaño en memoria que pueda ocupar el modelo pues ya tendremos que considerar esos elementos adicionales al momento de seleccionar el mejor modelo. Si tienen alguna duda de lo que les acabo de comentar no olviden dejarla abajo en los comentarios y si les gustó el vídeo no olviden darle un pulgar hacia arriba de me gusta y compartirlo con sus amigos y conocidos pues esto me ayudará a llevar este contenido cada vez a más y más personas. Y desde luego si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo vídeo.
DOC0109|Machine Learning|En esta primera lección del curso vamos a comenzar con lo básico. Vamos a entender qué es esto del aprendizaje reforzado o por refuerzo. Para comenzar a entender qué es esto del aprendizaje reforzado o por refuerzo partamos de un ejemplo que todos conocemos y es un bebé cómo aprende a caminar. Entonces si entramos en detalle como en la lógica y la secuencia de pasos que se llevan a cabo para que el bebé aprenda a caminar, pues el primer paso es que el bebé comience a interactuar con un entorno. Entonces el entorno pues es básicamente el lugar por donde se está moviendo el bebé y este bebé entonces a través de esa interacción con ese entorno pues se ejecuta una serie de acciones como mover por ejemplo los brazos o las piernas o la cabeza y luego coordinar todos estos movimientos desde el cerebro para que posteriormente entonces ese entorno le entregue como una recompensa al niño. Es decir, si el niño se cae pues esa recompensa es negativa, si el niño logra dar unos pasos la recompensa es positiva y ese aprendizaje entonces va evolucionando con el tiempo. Es decir que ese niño a través de su cerebro precisamente y de esos movimientos que va ejecutando y esas recompensas que le entrega el entorno pues se va dando cuenta de cuáles son como... cuál es la secuencia de acciones más benéficas que a la larga le va a permitir cumplir con ese objetivo final que en últimas es caminar. Entonces este es el primer ejemplo intuitivo y a partir de este ejemplo entonces tenemos que resaltar dos ideas importantes. La primera de ellas es el concepto de aprendizaje que en el caso del bebé que aprende a caminar lo podemos definir como el mapeo de situaciones a acciones. Es decir, las situaciones pues básicamente es lo que el niño observa cuando se mueve por ese entorno y las acciones pues son los movimientos que va a ejecutar. Los brazos de manera... el movimiento de los brazos de manera coordinada de las piernas de la cabeza y de todo su cuerpo para poder avanzar. Ese sería el aprendizaje, ese mapeo de situaciones a acciones. Y después entonces con base en esas acciones que ejecuta el niño pues va a recibir una recompensa que en adelante y usando mucho la terminología de neurociencias y de psicología que digamos el aprendizaje por refuerzo toma muchas de esas ideas de allá. Entonces lo vamos a llamar no recompensa sino refuerzo y ese refuerzo es como una especie de premio o castigo que le indica al niño si lo está haciendo bien o mal. Y en últimas entonces podemos resumir ese concepto de aprendizaje por refuerzo como aprender a partir de la interacción. Para ahondar un poco más entonces veamos un segundo ejemplo ya de un entorno virtual. Entonces acá lo que tenemos es un agente. El agente básicamente pues es como el cerebro de este personaje que aparece acá moviéndose, este personaje virtual que se está moviendo. Pero la lógica de aprendizaje va a ser muy similar al ejemplo que veíamos del bebé que aprendía a caminar. Entonces el agente tiene que aprender a mapear situaciones. En este caso son las situaciones van a ser las señales que recibe de este entorno. Entonces ustedes ven que el entorno tiene diferentes tipos de obstáculos. La superficie a través de la cual está caminando este agente que fue desarrollado por DeepMind de Google pues tiene diferentes obstáculos y entonces esos obstáculos le entregan una señal a este agente y este agente entonces tiene que interpretar esas señales y convertirlas en acciones o en movimientos. Vemos que inicialmente esos movimientos son relativamente torpes pero a medida que pasa el tiempo de forma secuencial el agente va aprendiendo a moverse por ese entorno con esos obstáculos. Acá la diferencia importante es que como este agente está implementado computacionalmente debemos crear un refuerzo o sea un premio castigo que sea cuantificable es decir que sea representado por cantidades numéricas de manera tal que por ejemplo si el agente lo hace bien si sortea el obstáculo correctamente le vamos a entregar una recompensa a un refuerzo positivo una cantidad numérica positiva y si lo hace mal entonces la recompensa será un número negativo será un castigo y al final de todo este proceso que se hace de manera repetitiva hasta que poco a poco el agente aprende a caminar por este entorno pues el objetivo final es que el agente logre obtener la recompensa positiva más alta posible la recompensa positiva que está asociada precisamente a sortear correctamente el obstáculo y de esta manera entonces entre comillas el agente aprende a caminar. Entonces si tomamos estos dos ejemplos del bebé que camina en el mundo real y de la gente que se mueve en este mundo virtual pues podemos definir el aprendizaje reforzado de la siguiente manera y esto es modificado de un libro básico de aprendizaje por refuerzo que es el libro de reinforcement learning an introduction de Sutton y Bartow del año 2020 entonces yo he modificado esa definición para tener los conceptos acá más importantes entonces podríamos decir que el aprendizaje por refuerzo o reforzado consiste en lograr implementar algoritmos computacionales que permitan a un agente más adelante definiremos formalmente el término de agente que permitan a ese agente mapear de situaciones a acciones es decir de lo que observa en el entorno a acciones como por ejemplo un movimiento en los dos ejemplos que vimos anteriormente con el fin de maximizar una señal de recompensa numérica acá lo importante es que esa recompensa o ese refuerzo tiene que ser una cantidad numérica para que el computador pueda operar sobre ella pero lo importante también es que este aprendizaje se da por la interacción con el entorno es decir que en ningún momento le vamos a indicar al agente qué acciones tiene que tomar él por sí solo y de manera autónoma tiene que darse cuenta digamos entre comillas cuáles son las acciones más benéficas y cuáles son las peores acciones en esa tarea que está llevando a cabo dentro de ese entorno entonces la gente tiene que tomar estas decisiones de manera secuencial es decir por prueba y error una y otra vez por sí solo y el objetivo final entonces es que genere la máxima recompensa posible esta sería entonces una definición entonces en la próxima lección pues vamos a ver ejemplos concretos de esta definición pero por el momento lo importante es quedarnos con la idea de que vamos a tener un agente computacional es decir como una inteligencia artificial digámoslo así implementada en un computador que de forma autónoma por sí solo tiene que aprender a ejecutar las mejores acciones posibles mediante la exploración a prueba y error que hace de su entorno teniendo clara esta definición entonces miremos algunas características del aprendizaje por refuerzo entonces el aprendizaje por refuerzo es un campo del machine learning que es el que el machine learning pues es un área de la inteligencia artificial la inteligencia artificial pues básicamente en términos muy generales es una disciplina que busca desarrollar máquinas o computadores que puedan actuar de forma muy similar al ser humano desde el punto de vista de la inteligencia misma del ser humano el machine learning es un área de esta inteligencia artificial que busca específicamente crear programas de computador que sean capaces de resolver problemas donde se requiera aprender a partir de los datos entonces eso es lo que es el machine learning y dentro del machine learning tenemos tres enfoques diferentes entonces tenemos el aprendizaje supervisado el aprendizaje no supervisado y lo que nos interesa en este curso que es el aprendizaje por refuerzo y cuáles son las diferencias importantes entre estas tres áreas del del machine learning entonces en el aprendizaje supervisado tenemos por ejemplo métodos tradicionales como los árboles de decisión las máquinas de soporte vectorial o los bosques aleatorios o herramientas incluso más modernas como las redes neuronales o las redes convolucionales y las redes recurrentes y lo que ocurre en este tipo de aprendizaje supervisado es que el ser humano tiene una influencia directa porque los datos de los cuales aprendes aprenden estos algoritmos son presentados y recolectados por el ser humano y el ser humano entonces decide qué tipo de datos recolectar y cómo etiquetarlos entonces este etiquetado en que consiste por ejemplo si queremos entrenar una red convolucional que está encargada de procesar imágenes para que aprenda a diferenciar gatos de perros pues el ser humano primero tiene que recolectar muchas imágenes de gatos y perros y etiquetar cada una de ellas es decir indicarle a la red para cada una de esas imágenes si pertenece a un gato o a un perro eso es precisamente etiquetar las imágenes entonces el ser humano tiene una influencia directa en los datos que se le presentan a estos modelos de aprendizaje supervisado por otra parte entonces tenemos en segundo lugar el aprendizaje no supervisado entonces en el aprendizaje no supervisado tenemos por ejemplo métodos como los algoritmos de clustering o de agrupación como por ejemplo k-means otros algoritmos que se conocen como reducción de dimensionalidad como por ejemplo el análisis de componentes principales acá en que se diferencia el aprendizaje no supervisado del supervisado entonces en el aprendizaje no supervisado el humano tiene que recolectar los datos para presentárselos al modelo pero no es necesario etiquetarlos es decir que la idea de estos modelos de aprendizaje no supervisado es que con esos datos que recolecta el ser humano los modelos aprendan a detectar patrones sin necesidad de que el ser humano les indique por ejemplo la categoría a la que pertenecen como en el caso anterior del aprendizaje supervisado cuando hablábamos de las imágenes de perros y gatos en este caso le presentamos los datos y el modelo de aprendizaje supervisado aprende a identificar patrones en esos datos y a diferenciar unos datos de otros o a clasificar unos en unas categorías y otros en otras entonces por ejemplo en este aprendizaje podemos entrenar un modelo que sea capaz de tomar datos de clientes y dividir esos clientes en diferentes agrupaciones lo que se conoce por ejemplo como segmentación de clientes pero acá lo importante en este caso del aprendizaje no supervisado es que el ser humano tiene que recolectar esos datos que se le van a presentar al modelo entonces en resumen aprendizaje supervisado no supervisado tienen una influencia directa del ser humano lo cual es diferente de lo que ocurre en el aprendizaje por refuerzo porque en el aprendizaje por refuerzo hay una diferencia importantísima con respecto a esos otros dos enfoques y es que como veíamos anteriormente en el caso del niño que camina o en el caso del agente que se aprende a mover aprende a moverse en ese entorno virtual el aprendizaje se hace por prueba y error mediante la interacción de ese agente con su entorno entonces ahí en este caso particular no tenemos datos etiquetados y el ser humano no recolecta esos datos es decir que en este caso el aprendizaje es reforzado por refuerzo el agente mismo determina de forma autónoma qué datos va a recolectar y cómo va a usarlos para su aprendizaje entonces en este sentido el aprendizaje por refuerzo no requiere la intervención humana pues simplemente más allá de la implementación del algoritmo de aprendizaje en el computador y por tanto este tipo de aprendizaje es más cercano al ideal que veíamos o que hemos visto en los libros de ciencia ficción acerca de la inteligencia artificial que es que el computador desarrolla su propia inteligencia para resolver un problema a partir de la interacción y ya para terminar entonces hablemos de cómo el aprendizaje por refuerzo se relaciona con otros campos del conocimiento acá tenemos algunos campos pero el campo los campos de aplicación y a partir de los cuales la teoría del aprendizaje por refuerzo se ha venido desarrollando en los últimos años pues son muchos más pero vamos a hablar acá de cuatro de ellos entonces tenemos la teoría de control en este caso todos conocemos por ejemplo los vehículos autónomos que lo que buscan es precisamente lograr que un vehículo no sea conducido por una persona sino por una máquina o por un computador precisamente a partir de que a partir de capturar las imágenes y las señales del entorno a través del cual se mueve el vehículo pues a partir de ellas tomar decisiones de forma autónoma y guiar el vehículo en la dirección que precisamente establezca el pasajero que ingresa a ese vehículo entonces esto usualmente venía siendo parte de lo que se conoce como la teoría de control pero en los últimos años se ha incorporado el aprendizaje por refuerzo precisamente en el desarrollo de estos vehículos autónomos y es precisamente porque esos vehículos autónomos la idea es que aprendan a interactuar con el entorno y a ejecutar acciones dependiendo de ese entorno entonces es una tarea específicamente digamos relacionada con lo que es la filosofía misma del aprendizaje por refuerzo en el área de matemáticas también en este segundo caso existe un campo que se conoce como la investigación de operaciones que básicamente lo que busca es o es un método de solución de problemas y de toma de decisiones que es muy usado en empresas y en organizaciones y muchas de estas técnicas pues básicamente lo que usan es esa interacción por prueba y error para la toma de decisiones o la toma de decisiones óptimas en un entorno específico lo cual precisamente es el aprendizaje por refuerzo entonces en el área de matemáticas tenemos también aplicaciones del aprendizaje por refuerzo pero también en la economía porque muchos problemas en el campo de la economía como por ejemplo el estudio de la relación que hay entre el ingreso que recibe una persona o un grupo de personas una población en un país determinado su nivel de consumo y el nivel de riqueza que alcanzan esa interacción con las variables externas que ofrece el entorno pues también es estudiada desde la economía precisamente con técnicas muy relacionadas con el aprendizaje por refuerzo y por último pues tenemos que buena parte de esta teoría del aprendizaje por refuerzo pues está relacionada con ese proceso esos procesos de aprendizaje a partir de la experiencia y a través de recompensas como lo vimos anteriormente en el caso del bebé que caminaba y todo esto estas ideas que son digamos sintetizadas en la teoría del aprendizaje por refuerzo pues son tomadas a partir de desarrollos que se han dado en las áreas de neurociencias y psicología que precisamente explican ese proceso de aprendizaje como se dan los humanos y en los animales y precisamente habla de cómo a través de esa interacción con el entorno ese proceso de toma de decisiones se basa precisamente en refuerzos o recompensas que pueden ser positivas o negativas y buena parte del aprendizaje en humanos y en animales está orientado por esos procesos de aprendizaje a partir de la experiencia y de esos sistemas de recompensa perfecto ya tenemos una definición bastante precisa de lo que es el aprendizaje por refuerzo que esencialmente busca desarrollar algoritmos que permitan a una gente aprender a tomar decisiones de manera óptima dependiendo de su interacción con el entorno en el siguiente vídeo veremos varios ejemplos de aplicación de este aprendizaje por refuerzo que nos permitirán comprender en su totalidad la definición que acabamos de dar y que además nos permitirán entender el potencial que este aprendizaje reforzado tiene en diferentes ámbitos de nuestra sociedad.
DOC0110|Machine Learning|En esta primera lección del curso haremos un repaso de los conceptos esenciales del aprendizaje por refuerzo vistos en el curso anterior, es decir que hablaremos de los elementos de un sistema de aprendizaje por refuerzo, de los procesos de decisión de Markov, del agente y de las ecuaciones de Bellman. Luego introduciremos algunos conceptos fundamentales como los problemas basados en modelos y libres de modelos, así como los problemas de predicción y de control, lo que nos permitirá cerrar la lección con un panorama de los algoritmos clásicos de aprendizaje por refuerzo que veremos a lo largo del curso y su relación con estos conceptos fundamentales. Comencemos entonces recordando los elementos principales que conforman un problema de aprendizaje por refuerzo, los componentes. Entonces tenemos dos elementos esenciales que son el agente y el entorno y estos dos interactúan constantemente enviando y recibiendo una serie de señales. Entonces cuando el agente ejecuta una acción, entonces esa es una señal que ese agente envía al entorno, el entorno recibe esa señal y como respuesta a esa señal de entrada entonces genera lo que es un cambio de estado y una recompensa que le indica entonces o juntos le indican entonces al agente qué tan buena o mala fue la acción que ejecutó y esto se repite de manera cíclica y de esta forma el agente aprende a interactuar con ese entorno. También hay un elemento importantísimo entonces en el agente que es esta política, todo esto lo vemos en detalle en el curso anterior, entonces la política del agente es como el cerebro que le permite ejecutar acciones con base en el estado que le está proporcionando el entorno. Y entonces para recordar también el objetivo principal de resolver un problema de aprendizaje por refuerzo pues es lograr en esencia que el agente encuentre una secuencia de acciones que maximice el retorno, cuál es el retorno es como la puntuación total que la gente obtendría al sumar precisamente todas las recompensas individuales de cada una de las interacciones con el entorno, entonces ese sería el retorno, entonces el objetivo es maximizar ese retorno y cómo se logra eso pues encontrando la secuencia de acciones más adecuada y esa secuencia de acciones pues está muy relacionada precisamente con esa política, entonces resolver un problema de aprendizaje por refuerzo en el fondo está muy relacionada con esa política del agente que es la que le permite tomar decisiones frente a las variables que le está presentando el entorno. Recordemos además entonces que en este proceso de interacción entre el agente y el entorno podemos obtener una representación matemática compacta de todas las variables que hacen parte de ese problema y eso se conoce como un proceso de decisión de Markov que esto también lo vimos en el curso anterior, entonces ese proceso de decisión de Markov contiene algo que se conoce como el espacio de estados que son todos los posibles estados que puede tener el entorno, el espacio de acciones que es el conjunto de todas las posibles acciones que puede ejecutar el agente, la función de recompensa que son esencialmente todas las posibles recompensas que puede obtener el agente tras ejecutar una acción y tras recibir como estímulo de entrada un estado, acá tenemos entonces también una función de transición que en últimas nos indica la probabilidad de que el entorno en el que se encuentra el agente alcance un nuevo estado dependiendo del estado anterior y de la acción misma que esté ejecutando el agente y por último entonces tenemos algo que se conoce como el factor de descuento que nos permite ponderar como en cada interacción el agente está recibiendo una recompensa que puede ser positiva o negativa, puede tener diferentes valores es una cantidad numérica que le entrega precisamente como señal el entorno, entonces al sumar todas esas recompensas pues vamos a tener lo que es el retorno pero entonces para optimizar ese proceso de interacción lo que hacemos es penalizar esas recompensas con un factor de descuento y entonces ese factor de descuento pues resulta siendo clave en el cálculo del retorno de la suma total de las recompensas y en el proceso de optimización de esa interacción como lo vimos en el curso anterior entonces estos cinco elementos hacen parte de lo que es el proceso de decisión de Markov que sintetiza todas las variables que hacen parte de nuestro problema de aprendizaje por refuerzo, también como parte de lo que vimos en el curso anterior entonces lo que mencionábamos era la política que es el cerebro que define la forma como va a interactuar el agente con el entorno dependiendo de las señales que reciba y en particular entonces esa política la estamos denotando con la letra pi y estamos usando una notación también de probabilidades es decir que lo que estamos diciendo es que la política por una parte es estocástica es decir casi siempre tendremos un grado de incertidumbre en ese proceso de interacción entre el agente y el entorno no siempre tendremos clara cuál es la acción que va a ejecutar el agente partiendo de un estado en particular sino que eso tendrá un componente relativamente aleatorio y por eso es una política estocástica y por eso se usa una notación de probabilidades también entonces teníamos una forma o definíamos unas funciones para cuantificar qué tan bueno es un estado en el que se encuentra la gente en un momento o en un instante de tiempo en particular con respecto al objetivo que quiere alcanzar o qué tan buena es la acción que ejecuta la gente en un instante de tiempo en particular tomando como referencia el objetivo al que quiere llegar entonces teníamos la función estado valor y la función acción valor que en últimas pues medían esas bondades y que tiene pues una ecuación matemática que tiene que ver directamente con esa ese retorno que se está obteniendo en cada una de esas interacciones y que lo vimos en detalle pues en el curso anterior y también entonces podemos introducir esto no lo vimos en el curso anterior pero tiene una definición sencilla y es entonces la función acción ventaja entonces además de medir la bondad de un estado o la bondad de una acción también entonces podemos definir la función acción ventaja que simplemente tomar la función acción valor y restarle la función estado valor y que nos mide o nos cuantifica la ventaja de tomar una acción definida por la política por eso aparece acá el sub índice pi en lugar de cualquier otra acción que no tenga relación alguna con la política entonces tenemos esas funciones valor que nos cuantifican esas decisiones que se toman dentro del proceso y finalmente como parte de este repaso entonces tenemos las ecuaciones de bellman de las cuales hablamos en el curso anterior que simplemente son una representación matemática un poco más detallada de estas funciones estado valor y acción valor que veíamos anteriormente entonces acá ya se involucra de manera explícita la política por ejemplo en el caso de la función estado valor y en el caso de ambas funciones estado valor y acción valor se involucra de manera explícita también la función de transición que tiene nuestro proceso de decisión de marcó pero además de eso tienen un componente importante y es que nos permiten descomponer esos cálculos en dos elementos un elemento que contiene la recompensa inmediata y que aparece en ambas ecuaciones y otro elemento que contiene los valores futuros observemos que este s prima hace referencia al estado futuro al siguiente estado que va a alcanzar el agente pero estamos incluyendo el descuento o la penalización todo relacionado precisamente con las tareas de optimización entonces acá tendremos los valores futuros con descuento en ambas ecuaciones de bellman entonces ese es un elemento importante del cual va a partir precisamente el primer algoritmo que veremos en la próxima lección que es el de programación dinámica y ahora sí entonces vamos a introducir un primer concepto que es los problemas o los algoritmos que involucran modelos o sin modelos entonces si volvemos acá al caso del proceso de decisión de marcó donde teníamos los cinco elementos que hacen parte de ese conjunto cuando hablamos de un modelo es porque en ese proceso de interacción con el entorno conocemos la función de recompensa del entorno y conocemos la función de transición entonces estos dos elementos hacen referencia precisamente al modelo del entorno y ese modelo del entorno pues en esencia nos da todas las reglas de juego como podrá interactuar el agente con ese entorno y cuál será la respuesta de ese entorno a esa interacción o esas acciones que toma la gente entonces si conocemos esos detalles decimos que conocemos el modelo del entorno y entonces dependiendo de eso podemos tener dos tipos de problema o dos familias también grandes de algoritmos que son los problemas o algoritmos basados en modelos acá colocamos el término en inglés porque en la bibliografía que se encuentra comúnmente todo esto se encuentra en inglés entonces son los model based y tenemos los problemas o algoritmos libres de modelos o model free cuáles son los problemas basados en modelos entonces donde tenemos absolutamente toda la información de nuestro proceso de decisión de markov entonces tenemos el espacio de estados el espacio de acciones tenemos el factor de descuento pero también tenemos el modelo del entorno y un ejemplo de eso pues es precisamente el caso del tablero bidimensional el pequeño juego que vimos en el curso anterior donde conocíamos todos los detalles del proceso incluso sabíamos las probabilidades de transición de un estado a otro y en cada caso que recompensa se iba a obtener cuando la gente se desplazaba de una casilla a otra entonces como conocíamos las recompensas y las probabilidades de transición de una casilla a otra pues teníamos el modelo completo de ese juego en ese caso lo que veamos era que la gente debía decidir cuál era la mejor secuencia de acciones si partía del inicio cuál era la mejor secuencia de acciones que le permitiese llegar a la meta entonces como era como ya digamos teníamos todos los detalles del modelo y del proceso de decisión de marcó se convertía todo esto esencialmente en un proceso de planeación encontrar la mejor ruta posible para ir del inicio a la meta y esa era una tarea entonces de planeación pero en casos prácticos en la mayor parte de los casos reales realmente no podemos conocer esta función de transición y esta función de recompensa no tenemos esa información completa entonces como no tenemos esa información completa tendremos problemas o algoritmos que tienen un enfoque libre de modelos en ese caso tendremos por ejemplo acceso a los diferentes estados del entorno a las diferentes acciones que puede ejecutar el agente y tendremos acceso entonces al factor de descuento pero no tendremos la información del modelo un ejemplo de esto entonces es un robot acá tenemos un robot que se desplaza por ejemplo por una bodega y entonces en ese caso cuando el robot interactúa con la bodega generalmente no conoce con antelación por ejemplo cuál es la penalidad o la recompensa que va a obtener por una acción determinada que está ejecutando y no conoce tampoco con respecto a esa bodega que es el entorno la probabilidad de que ese entorno cambie dependiendo de ciertas acciones es decir la función de transición y como no conoce eso entonces el agente tiene que comenzar a interactuar con el entorno e ir aprendiendo a identificar de manera progresiva la manera más adecuada de desplazarse por ese entorno como un proceso de prueba y error es decir que tiene que ejecutar acá en este problema de libre de modelos tiene que ejecutar una tarea de aprendizaje entonces en la mayor parte de los problemas de aprendizaje por refuerzo no nos vamos a enfocar tanto en la planeación sino en el aprendizaje porque efectivamente el objetivo del aprendizaje por refuerzo es que el agente por esa interacción que se da con el entorno progresivamente vaya aprendiendo a interactuar con él mismo entonces la mayor parte de los problemas de aprendizaje por refuerzo están enfocados en este tipo de problemáticas y usarán ese tipo de algoritmos con ese enfoque y también entonces tenemos dos tipos de tareas o dos tipos de problemas o dos tipos de algoritmos que son los de predicción o de control entonces en el caso de los algoritmos de predicción lo que tendremos como entrada será entonces ya la política del agente es decir con antelación de alguna manera ya tendremos la política del agente el cerebro del agente que le indica entonces cuál es la probabilidad de ejecutar una cierta acción partiendo de un cierto estado y entonces en esas tareas de predicción lo que se tendrá que hacer es calcular el retorno es decir la suma de todas las recompensas que se tienen ejemplo típico volvemos al tablero bidimensional entonces si ya definimos una política unas reglas de juego para que la gente se mueva por ese tablero pues entonces teniendo esa política lo que tenemos es que predecir cuál será el retorno del puntaje que obtendrá tomando diferentes rutas eso será una tarea entonces de predicción que es una tarea relativamente sencilla porque ya el problema fundamental se resolvió y es que se conoce la política pero entonces tenemos otra tarea otro problema otro tipo de enfoque para los algoritmos que es el enfoque de control y el enfoque el enfoque de control entonces es que podemos tener múltiples políticas es decir múltiples cerebros que le van a indicar al agente diferentes maneras de interactuar con su entorno y lo que tenemos entonces es que determinar de todas esas posibles políticas cuál va a ser la mejor de ellas es decir vamos a ponerle acá una anotación con asterisco que nos va a indicar entonces cuál es la política óptima es decir cuál de todas las políticas posibles es la más adecuada para permitir esa interacción del agente con el entorno entonces ahí tendremos una tarea de control que lo que busque es entonces determinar la política más adecuada de un abanico de posibles políticas existentes entonces esto es un problema mucho más complejo y que realmente es como el núcleo principal del aprendizaje por refuerzo encontrar la política más adecuada y ya después de eso entonces podremos hacer tareas de predicción y entonces teniendo en cuenta este tipo de tareas basadas en modelos o libres de modelos o tareas de predicción y de control pues es donde tenemos los algoritmos clásicos del aprendizaje por refuerzo que sustentan digamos toda la teoría de los algoritmos contemporáneos que se usan en la actualidad incluso los que involucran algoritmos de deep learning entonces para entender eso pues en este curso nos vamos a enfocar precisamente en esos algoritmos clásicos sobre los cuales se construye la teoría de algoritmos mucho más avanzados y esos algoritmos clásicos pues los podemos organizar dependiendo de si tenemos tareas de planeación es decir basadas en modelos o libres de modelos donde tendremos entonces tareas de aprendizaje y también entonces si tenemos problemas de predicción y de control y entonces dependiendo de esas categorías podremos tener un primer grupo grande lo que está acá en verde que son todos los algoritmos de la familia de la programación dinámica que fueron digamos los primeros algoritmos de aprendizaje por refuerzo que se desarrollaron que en la actualidad digamos tienen un uso limitado pero que es necesario que los veamos en este curso porque son la base de algoritmos más elaborados como los de monte carlo y las diferencias temporales que veremos también en este curso y de algoritmos más avanzados que veremos también en el próximo curso entonces estos no son muy usados en la práctica pero son esenciales para entender los algoritmos que vinieron después entonces en el caso de tener modelos algoritmos basados en modelos y que ejecuten tareas de predicción entonces tendremos un algoritmo de programación dinámica que se conoce como evaluación de la política y tendremos en del lado del control también basados en modelos entonces la iteración de la política y la iteración de valores y todos estos algoritmos de programación dinámica requieren que tengamos o que conozcamos en detalle el modelo del entorno con el cual está interactuando el agente si no tenemos una información completa del modelo es decir si no conocemos la función de recompensa en la función de transición pues entonces tendremos algoritmos enfocados en el aprendizaje o libres de modelos y acá tenemos dos grandes familias monte carlo y diferencias temporales si usamos tareas de si abordamos tareas de predicción entonces tendremos la predicción con monte carlo o el aprendizaje por diferencias temporales y si ejecutamos tareas de control entonces tendremos el control por monte carlo y dos variantes también de las diferencias temporales que son el algoritmo sarza y el algoritmo que learning todos ellos entonces los veremos precisamente en detalle a lo largo de este curso y veremos también entonces cómo implementarlos computacionalmente usando librerías específicas en python muy bien acabamos de ver un panorama de los principales algoritmos clásicos para la solución de problemas de aprendizaje por refuerzo y que se dividen en tres grandes familias la programación dinámica monte carlo y las diferencias temporales y cada una de estas familias puede ser usada para abordar problemas de predicción o de control o problemas donde tengamos o no la información del modelo del entorno así que con esta introducción ya estamos listos para comenzar a ver en detalle cada una de estas familias entonces en la primera sección del curso nos enfocaremos en los algoritmos de programación dinámica y en particular en la próxima lección veremos el primero de estos algoritmos de predicción para la evaluación de la política.
DOC0111|Machine Learning|El aprendizaje reforzado es una de las áreas más excitantes y más prometedoras del Machine Learning en la actualidad, porque tiene la capacidad de desarrollar agentes o máquinas inteligentes que pueden ejecutar tareas de forma muy similar a como lo hacemos nosotros los seres humanos. Pero ¿en qué consiste el aprendizaje por refuerzo? ¿Cuál ha sido su evolución? ¿Cuál es su potencial? ¿Qué tiene que ver el Machine Learning con todo esto? ¿Y cuáles son sus aplicaciones? Pues en este video les traigo una guía definitiva en donde les voy a aclarar todas estas dudas. Así que si quieren tener un panorama completo sobre lo que es el aprendizaje por refuerzo, no se pierdan ningún detalle. Así que listo, comencemos. En 1952, el matemático norteamericano Claude Shannon fue tal vez uno de los primeros en desarrollar una aplicación del aprendizaje reforzado. Creó un ratón artificial llamado Teseus que a través de prueba y error logró aprender a atravesar un laberinto, recordando la ruta más exitosa con la guía de imanes ubicados en el piso. Con el paso del tiempo se lograron otros avances, pero más que todo en la teoría. Pero en el año 2013 se dio inicio a una verdadera revolución. Los investigadores de DeepMind crearon un sistema capaz de aprender a jugar prácticamente cualquier juego de Atari desde cero y capaz también de superar a los humanos, usando solo como entrada los pixeles de cada escena, sin tener ningún conocimiento previo de las reglas mismas de estos juegos. Y esta fue la primera de una serie de logros cada vez más impresionantes, que continuaron en mayo de 2017 con AlphaGo, un agente inteligente que fue capaz de vencer al campeón mundial de Go, un juego de mesa extremadamente complejo inventado en China hace más de 2000 años. Pero ¿cómo lograron desarrollar este sistema? Pues la idea fue bastante simple, combinaron el poder de las redes neuronales, que es un área específica del Machine Learning, con las técnicas básicas de aprendizaje reforzado que se venían desarrollando desde los años 50. Y con esto precisamente nació el Aprendizaje Reforzado Profundo, que es una de las áreas del Machine Learning que tiene aplicaciones potenciales no solo en el tema de videojuegos, sino también en la robótica, en la automatización industrial, e incluso en áreas como el desarrollo de nuevos medicamentos para tratar diferentes enfermedades. Pero un momento, para entender lo que es el Aprendizaje Reforzado Profundo, primero tenemos que entender lo básico. Es decir, ¿qué es esto del Aprendizaje Reforzado? ¿Cómo es que una máquina inteligente puede aprender y empezar a ejecutar tareas similares a las que hace el ser humano? Para entender este concepto básico del Aprendizaje Reforzado, vamos a partir de un ejemplo intuitivo de los videojuegos. Supongamos que vamos a enseñar a un humano a jugar punk, el clásico juego de Atari. Si le mostramos por primera vez el juego a una persona, le podríamos dar una instrucción como esta. Con el teclado puede controlar una paleta que se mueve hacia arriba o hacia abajo. Su tarea es golpear la bola hasta que su oponente no logre alcanzarla. Cada vez que haga esto obtendrá un punto, y gana el jugador que logre obtener más puntos al final de la partida. Lentamente, el nuevo jugador humano aprenderá a controlar la paleta, a golpear la bola y a marcar puntos, a través de un proceso de prueba y error, y eventualmente logrará vencer al oponente. ¿Y cómo lograríamos esto con un computador? Pues en primer lugar deberíamos crear un programa, que en adelante vamos a llamar Agente, que logre hacer varias cosas. En primer lugar debe entender los elementos del juego, es decir, que hay por ejemplo dos oponentes, un tablero, una bola y dos paletas. Es decir, debe entender el entorno. Después de esto, el agente debe entender lo que está sucediendo en el entorno, es decir, por ejemplo, la dirección de la bola o los movimientos de su oponente. A esto lo llamaremos Estado. Dependiendo del estado, el agente deberá aprender a moverse en el tablero para tratar de vencer a su oponente. A esto lo llamaremos Acción. La manera de saber si lo hizo bien o mal, será a través del puntaje. Si el agente lo hace mal, recibirá un punto en contra, una penalización, pero si lo hace bien, recibirá un premio. A este premio o castigo les daremos el nombre genérico de recompensa. Así que con todas estas ideas, ya podemos empezar a dar una definición mucho más precisa de lo que es el aprendizaje reforzado. La idea en el aprendizaje reforzado es que un agente aprenderá de su entorno, mediante la observación de su estado y mediante su interacción a través de una serie de acciones por las cuales va a recibir una recompensa. Al final, la idea es que el agente aprenda a ejecutar la mejor acción posible, dependiendo del estado observado, lo que a la larga le permitirá obtener la mayor recompensa positiva posible. En el contexto de PUNK, esto equivale simplemente a ganar el juego. Y esta misma definición la podemos aplicar a diferentes conceptos. Por ejemplo, si pensamos en un robot, el agente será el programa que controla su movimiento, el entorno va a ser el mundo real, y los estados son los posibles obstáculos que vaya encontrando en el camino. Las acciones serán los movimientos que ejecuta el robot y la recompensa puede ser positiva o negativa. Positiva si llega la meta final al destino y negativa si se pierde en el camino o si de pronto cae con un obstáculo. Bien, con todo esto ya tenemos una idea general de lo que es el aprendizaje reforzado, pero nos queda una pregunta fundamental. ¿Cómo lograr que este agente logre aprender a través de su interacción con el entorno? ¿Y qué tiene que ver en todo esto el Machine Learning? En esencia, hay dos maneras de hacerlo, dependiendo si el agente conoce en detalle el entorno o solo una parte de él. Cuando se conoce en detalle el entorno y todas sus reglas de juego, lo que tenemos es el aprendizaje reforzado basado en modelos. Un ejemplo clásico de este tipo de aprendizaje reforzado es por ejemplo el juego Go. Con antelación en este juego, el agente puede conocer las reglas, los movimientos que puede realizar y el tamaño del tablero. Con este modelo, con estos detalles, el agente puede planear con antelación su siguiente movida y puede analizar las implicaciones de este movimiento o elegir otras alternativas. Alpha 0, desarrollado precisamente por DeepMind en 2017, es un ejemplo de un algoritmo de aprendizaje reforzado basado en modelos. El problema de este tipo de algoritmos es que solo en contadas ocasiones se tiene acceso completo a la información detallada del entorno para poder construir este modelo. En la mayoría de las aplicaciones reales solo se tiene acceso parcial. Y en este caso hablamos de aprendizaje reforzado libre de modelos, al que pertenecen la mayoría de los algoritmos que se usan en la actualidad. En este caso, el agente tiene que aprender a tomar decisiones por prueba y error, porque realmente no conoce todos los detalles del entorno. En realidad tiene acceso solo a dos elementos, los estados de este entorno y la recompensa que obtiene por sus acciones. Un ejemplo de esto es precisamente la inteligencia artificial que en 2013 desarrolló DeepMind y que fue capaz de vencer al ser humano en varios juegos de Atari. Pero para entender cómo funcionan los diferentes algoritmos de aprendizaje reforzado libres de modelos, necesitamos hablar de política. Pero no, no, no, no es la política tradicional y aburrida de nuestros países. En este caso se refiere al cerebro de nuestro agente, es decir, al programa de computador que le permite decidir qué acciones tomar dependiendo del estado observado. Por ejemplo, si tenemos un juego hipotético en el cual el agente debe recolectar un diamante y obtener al final el puntaje más alto posible, en este caso la política le permitirá determinar la ruta más adecuada para evitar la mayor cantidad de penalizaciones y así lograr al final la recompensa positiva más alta. Así que en el aprendizaje reforzado libre de modelos, la idea es poder contar con un método o un algoritmo que permita calcular esta política y que esto de su vez le permita al agente desplazarse o desenvolverse de la forma más óptima posible en este entorno. Y para esto existen esencialmente dos algoritmos que son los pilares fundamentales del aprendizaje reforzado moderno, las políticas de gradientes y el Q-Learning. Hablemos primero de la política de gradientes. La idea en este caso es que dado un estado en particular, el algoritmo sea capaz de predecir la acción a realizar, maximizando de esta forma la recompensa total. Volvamos a nuestro juego hipotético. En este caso, el agente puede ejecutar cuatro posibles acciones. Supongamos que diseñamos un algoritmo de política con cuatro parámetros, cada uno indicando la probabilidad de que el agente se desplace en una de esas direcciones. Una forma de entrenar al agente es definiendo, por ejemplo, un set inicial de valores para estos cuatro parámetros, donde cada valor indica la probabilidad de que el agente se mueva en dicha dirección. Luego, debemos hacer que el agente se mueva hasta que llegue a la meta, es decir, hasta el final del episodio y calcular la recompensa total obtenida. Después, modificamos ligeramente los parámetros, ejecutamos el episodio y calculamos la recompensa obtenida. Y repetimos una y otra vez hasta lograr afinar los parámetros de forma tal que se obtenga la recompensa más alta posible. Así que con la política de gradientes, la idea es reajustar iterativamente los parámetros de este algoritmo y la dirección en la que nos tenemos que mover con este ajuste es precisamente la de la máxima variación o el máximo gradiente de la recompensa calculada finalmente. Aunque para nuestro ejemplo hipotético, esta forma de actualizar los parámetros funciona bastante bien en un caso real, es decir, un agente que puede ejecutar múltiples acciones. Y en un escenario con muchísimos más estados, el problema se hace más complejo y es casi como buscar una aguja en un pajar. Afortunadamente, una solución a esta inconveniente no se puede realizar precisamente en el Machine Learning, pero esto lo vamos a ver un poco más adelante. Por ahora hablemos del segundo algoritmo más importante para entrenar nuestro agente, el Q-Learning. En ese caso, el algoritmo Q-Learning no generará directamente una predicción de la acción a realizar. En su lugar, el método permite calcular para cada par de estados y acciones la máxima recompensa que se obtendrá. Para entender esto, volvamos al caso de nuestro pequeño juego. Que nuestro agente se encuentra en el estado inicial y que a partir de ese estado puede ejecutar una de tres posibles acciones. Lo ideal sería seleccionar la acción que resultase en el máximo puntaje posible al final del juego. Si tuviéramos una función mágica que con antelación, es decir, muchas jugadas atrás, nos permitiera predecir la acción más inmediata a tomar, pues el agente tendría altas probabilidades de culminar exitosamente ese juego. Pues a esta función mágica la vamos a llamar la función Q, de quality o calidad. Y va a representar qué tan buena será la acción que tomará el agente en un estado determinado. Así que lo que hace el Q-Learning es encontrar iterativamente esta función a medida que el agente se va moviendo por el entorno paso a paso. Volviendo a las recompensas predefinidas para nuestro juego, vemos que las casillas blancas entregan una penalización de menos un punto, la que contiene fuego menos 100, mientras que la que corresponde al diamante dará un premio de 50 puntos. Este esquema de puntuación predefinido permitirá que el agente aprenda a moverse por las casillas blancas, que tienen una penalidad menor, a que en lo posible evite el fuego con mayor penalidad, y a que busque la ruta más corta, es decir, la que tenga menos penalización, para llegar al diamante con la máxima recompensa. Para lograr esto, en cada una de estas visitas se va llenando una tabla que tendrá tantas filas como estados allá disponibles, y tantas columnas como acciones puede ejecutar el agente. Al final, cada celda va a contener el valor máximo de la recompensa esperada para cada estado y cada acción. En nuestro juego se tendrá entonces una tabla con siete filas, que corresponden a los siete estados del juego, y cuatro columnas que corresponden a los cuatro posibles movimientos del agente. Al comienzo, todos los valores de la tabla serán cero, pues el agente no ha explorado el entorno. Luego comienza a desplazarse por el tablero, y en cada caso almacena el puntaje en la posición correspondiente de la tabla. Al comienzo del algoritmo se moverá de manera aleatoria, pero cuando el procedimiento se repite muchas veces, poco a poco nuestro agente irá descubriendo el patrón existente. Los valores más altos de la tabla corresponderán a los pares estado-acción que arrojen el mayor puntaje posible. Este algoritmo de QLearning usa un enfoque de fuerza bruta, es decir, se tienen que visitar todos los posibles estados y para cada estado calcular todas las posibles acciones, y el método funciona bastante bien para nuestro juego hipotético, porque teníamos tan solo 28 celdas, pero para un caso práctico realmente no resulta viable. Así que la gran desventaja del algoritmo del QLearning es similar a la de política de gradientes, que no es un algoritmo escalable, es decir, entre más acciones y más estados se tengan, más difícil va a resultar entrenar el agente. Afortunadamente, muchos de estos inconvenientes se pueden resolver usando el Machine Learning. Así que ahora vamos a ver cómo combinar estos dos algoritmos básicos con todo el potencial que tienen las redes neuronales. Una red neuronal es básicamente una arquitectura de Machine Learning que es capaz de generalizar conocimiento, es decir, que es entrenada con una serie de datos, a partir de ese entrenamiento es capaz de detectar unos patrones y luego es capaz de transferir ese conocimiento para detectar patrones en datos que previamente no ha visto. Si quieren aprender más sobre las redes neuronales tienen que ver por acá este video que les voy a dejar donde les explico en detalle cómo es que funciona. Así que si en los dos algoritmos reemplazamos los bloques funcionales por redes neuronales, en cada caso tendremos un agente con un cerebro más potente. Con este aprendizaje por refuerzo profundo, es como DeepMind y AlphaZero lograron vencer al ser humano. Al usar las redes neuronales en combinación con estos sistemas de aprendizaje reforzado, ahora es posible analizar escenarios mucho más complejos, tanto continuos como discretos, incluso con muchos más estados o muchas más acciones. Y todo esto gracias a la capacidad que tienen las redes neuronales de generalizar y de encontrar patrones en los datos, que para nosotros los seres humanos es difícil percibir a simple vista. El potencial del aprendizaje por refuerzo profundo es inmenso, porque ahora tenemos agentes que son capaces de aprender de forma autónoma de su entorno simplemente observándolo e interactuando con él, y que son capaces de desarrollar tareas relativamente complejas que incluso muchas veces superan la capacidad del ser humano. Y esto no se reduce simplemente a los videojuegos o la robótica, como hemos visto hasta el momento. El aprendizaje reforzado profundo tiene, por ejemplo, un potencial inmenso en la industria, en donde se pueden entrenar agentes capaces de optimizar el consumo de energía o el uso de materia prima, o mejorar el transporte en las bodegas y las cadenas de suministro, o logrando desarrollar robots capaces de ejecutar autónomamente tareas en entornos hostiles. Incluso resulta posible desarrollar sistemas autónomos que aceleren el proceso de desarrollo de medicamentos. Así que el aprendizaje reforzado y en particular el aprendizaje por refuerzo profundo es tal vez una de las áreas más prometedoras del machine learning en la actualidad. Y probablemente en los próximos años nos va a traer desarrollos que cada vez se van a parecer más a lo que muchas veces vemos solo en ciencia ficción, es decir, la capacidad de desarrollar máquinas inteligentes que aprendan de la experiencia de forma muy similar a como lo hacemos nosotros los seres humanos. Y bien, esto es todo por ahora. Espero que les haya gustado lo que les acabo de contar. Si todavía no se han suscrito los invito a suscribirse y nos vemos en el próximo video.
DOC0112|Data Storytelling|los siete pecados capitales de las presentaciones. Muchos de los datos que estamos manejando tienen que ver con ese tema de las presentaciones. Es el medio en el cual comunicamos todo ese mundo de información. Yo aquí hice un listado de calificativos que he escuchado de las presentaciones. Y la verdad es que la mayor parte no son positivos. ¿Por qué pasa esto? La información que me están presentando no tiene ningún valor para mí. Es tediosa, es cansada de leer. La persona habla una cosa y en la lámina hay otra cosa. Entonces la verdad es que empiezo a perder atención y la verdad es que ya no me es interesante lo que estoy viendo. Pues vamos a hablar los cuales son los siete pecados capitales de las presentaciones. ¿Por qué esto está pasando? ¿Por qué la gente sabe que no? Si es PowerPoint, olvídalo, no me invites, por favor. Vamos a empezar con el primero que es falta de definición de lógica. Nuestro pecado capital número uno. No nos sentamos previo a empezar la presentación a averiguar cuál es el propósito de mi presentación. ¿Qué es lo que quiero lograr? Una vez que tú cruces la puerta de la sala de juntas, te desconectas del meeting como presentador. ¿Qué ideas quiero que te crees en la cabeza? No sé ni cuántos slides, no sé si van gráficas, si van textos. Aún no tengo eso. Simplemente cuando acabe, ¿qué objetivos me entrezco el logrado? Generalmente llegamos, abrimos el PowerPoint, empezamos a entrar en información y así me lanzo sin ningún objetivo en mente. Por lo que es importante que te prepares. Y esta parte de prepararte no nomás es que tengas claridad en lo que decir, tengas claridad que vas a lograr, que solo llevas la información que te ayude a cumplir ese objetivo, sino que también estudies un poquito el lenguaje con el cual comunicas, la audiencia a la cual le vas a hablar. ¿Quiénes son estas personas que quieren saber? Esto entre más estudiar esto, entre más me prepare, pues ahora sí cumplo con evitar el primer pecado capital, que es hago estas cosas sin ningún objetivo. Vamos ahora a nuestro segundo pecado capital que tiene que ver con contenido irrelevante o contenido excesivo. Pues cuando tú ves tu presentación y te encuentras con estas cosas, no es tanto que la información no sea valiosa, no va por ahí. A veces queremos decir todo. Me dan 40 minutos y me los tomo hasta la hora completa ni siquiera respecto a los 40 minutos. Quiero decir, quiero decir, quiero decir y no hago ningún esfuerzo de sintetizar, de nomás irme al grano, de nuevamente está ligado con el primer pecado que no voy con ningún objetivo y no te creas que este el caso más complejo. Hay todavía cosas peores. Generalmente ahí estás sentado cerca de 45 minutos y obviamente tu audiencia dice por favor, dame un descanso. Tienes que definir si lo que estás haciendo es una presentación o estás haciendo un documento y se va a hacer los dos. PowerPoint es un programa muy versátil. A mí me gusta hacer más documentos en PowerPoint que en Word porque tengo más flexibilidad de trabajar con imágenes, de trabajar con formas, de me gusta más cómo acomodo los párrafos, me gusta más PowerPoint para hacer documentos. Realmente es esto no es una presentación, esto es un documento. Vas a hacer una presentación o va a ser un documento. Defínate primero antes. Si vas a hacer un documento, esta cosa que vas a hacer, aunque sea PowerPoint, lo el objetivo que tiene que lograr es que sea escaneable. Cuando me refiero que si escaneable es que de un distanso tú puedas saber de qué trata sin haber leído el documento, como que ves los títulos, ves más o menos los subtítulos, algunas cosas, algunos párrafos muy sencillos, algunas imágenes y ya me di una idea muy rápida de qué trata. Ahora, si al contrario lo que tú haces una presentación de un documento, esto tiene que tener otro tipo de lectura, tiene que ser como un espectacular en la carretera. Tú vas manejando y al final de cuentas es voltear a saber espectacular y regresa a manejar. Pero en ese instante tú te llevas toda la información. La persona que hizo el espectacular ya sabe eso, ya sabe que de un solo vistazo tienes que ver todo. Entonces, si te pongo información de más, no va a salir. El pecado capital es que ponemos demasiada información, nos excedemos en todo lo que estamos mostrando. Por ejemplo, si estoy arrancando con un slide como éstas, ve todo el texto que tiene. No es que sea malo, simplemente es a ver, esto es un documento, lo vas a hacer PDF y lo envías por correo, realmente es el objetivo, entonces, haz un buen documento. En primer lugar, elimina los púltes. Algo importante en documento es, use enunciados completos, no sintetices. En un documento no tienes que poner letra grande, al contrario, puedes meter letra chiquita de 10 puntos, 11 o 12 puntos, vete por columnas, como en un periódico. Y finalmente, quita estos colores y vete a blanco. Y ve cómo este documento es más escaneable. Es presentación o documento porque no cumple ni una otra, ni cumple presentación, ni cumple documento. Y si es por el camino de documento, hay técnicas para que tú hagas un excelente documento que realmente invita a ser ley. Nuestro siguiente pecado capital es abusar de los bullet points. Aquí es una colección de láminas de PowerPoint que están llenas de bullet points. Entonces, tengo aquí informaciones de diversos temas, pero todo el concepto de diseño, exactamente el mismo. Tengo, aunque ponga imágenes o no ponga imágenes, ahí están los bullet points. Entonces, voy a regresar a esta lámina de ejemplo. Las 5 etapas de la contratación de ejecutores. Obviamente, todo este texto, si yo hago leer a las personas, no va a funcionar porque va a estar demasiado enfocada en lectura y me van a dejar escuchar. Y esta lámina para desayunar los bullet points la puedo transformar en esto. El mensaje es, ¿quieres salir de estas cosas? ¿Quieres olvidarte de los bullet points? En primer lugar, no los uses. Pero, efectivamente, tienes que buscar alternativas de que otras formas puedo ejemplificar aquello que decir. El cuarto pecado capital de las presentaciones que es, demasiado ruido. ¿Qué me distrae de esta apostiga? Entonces, por un lugar, o que tiene un título, pero vean los colores, ¿no? Como tiene este verde que es altamente contrastante con este rojo, con el azul. Tiene una falta de orden. Y luego tiene una imagen que la imagen como que poco relacionada con el tema. Demasiado ruido. Mucho texto que leer, muchos colores raros, imágenes que a fin de cuentas no aportan al concepto. Entonces, esto tiene demasiado ruido. Otra forma de meter ruido son distracciones con animaciones. Si yo hago esta animación, pues obviamente, esto es un distractor, ¿no? ¿Cómo van entrando cada cosa? ¿Qué valor te está agregando esto? No más me distrae. De hecho, la presentación, cuando estoy en la quinta de apostiga, ya mis ojos están cansados. Ya estoy harto de estas cosas. Entonces, ¿a qué me refiero con evitar distractores? Esta lámina es muy interesante. Tiene mucho foco al mensaje, ¿no? El futuro de la automatización, muy limpia. Todo fondo negro. Mensajes muy claros. Pocos colores. Entonces, esta lámina no tiene distractores. Y al hacer que no tenga distractores, pues lo que hace es la vuelve bella. La vuelve interesante. Vuelve algo que vale la pena ver. Otro ejemplo. Ves esta imagen. Vemos este tucán. Y ahí tenemos un letrero que no se alcanza a ver. Y lo hice a propósito porque el mensaje es, elimina los distractores. Entonces, este fondo es un distractor de mi mensaje. Lo puedes usar, pero hay la forma en que no distraiga de lo que me interesa decir. Vamos al quinto pecado capital. Es fasta de consistencia en el diseño. ¿Y a qué me refiere esta palabra de consistencia? Ve esta lámina. Esta es un conjunto de láminas de una presentación. Parece un revoltijo de formas de presentar información. No estoy hablando si la información de aquí adentro es valiosa. No es mi foco de esa parte. Falta consistencia esta presentación. Ve otro caso. Fíjate cómo hay diferentes estilos. Hay láminas con muchísimo texto. Hay láminas que tienen unas como formas gráficas grandototas. Hay láminas que tienen diagramas. Entonces, y todo de colores diferentes. Ahí está la información. Puede ser valiosa, pero falta esta parte de consistencia. Cuando me refiero a la palabra consistencia, lo que quiero que te hayas en la cabeza es repetición. Si yo voy a hacer una presentación que tiene muchas depositivas, pues busca que haya elementos repetidos en las depositivas y le da coherencia. Fíjate en este caso. Decidí usar tonos de azules con blancos, entonces todos se quedan azul con blanco. Cada vez que meto una nueva positiva, tiene consistencia. El tipo de letra, no meto de todo. Meto el mismo tipo de letra en cada de las positivas. No significa que tú a partir de ahora todo el mismo color, no te vayas con esa impresión. Simplemente que todo sería como parte de lo mismo. Fíjate en este template. También tiene consistencia. La consistencia en el uso de imágenes. En las láminas en blanco es donde mete más diagramas. La imagen ocupa toda la depositiva. Entonces, eso me refiero con consistencia. Si hay como que cierta repetición en el diseño a lo largo de la presentación. Deja de mostrarte una presentación con esterolis. Tiene consistencia. Las láminas que ves en negro es cuando el diseñador está presentando un nuevo tema. Las láminas que ves en blanco tienen contenido. Ahí sí te explico, te estoy comentando qué es el tema. Las láminas que ves en verde es cuando hay una pregunta y quiero hacer partícipe de mi audiencia. A ver cómo resolverán esto. Las láminas que ves en rosa es un mensaje o una frase de alguien importante. Y por ahí aparecen las láminas en amarillo que es para descansos. Vamos a lunch, vamos a un descanso de unos minutos. Este diseño tiene consistencia. Cuando los veo, no me explican esto y lo voy averiguando conforme el presentador va avanzando. Entonces, recuerda, consistencia, mismos colores, fuentes. Entonces, también usa un solo tipo de electro. Pon negritas, pon cursivas, pon líneas, letra más con líneas delgadas. Pero, a fin de cuentas, el mismo tipo de letra. Íconos, el mismo estilo. Si el ícono es fotorrealista, todos fotorrealistas. Si es delineado el ícono, todos son delineados. Entonces, mantén el mismo estilo cuando usas íconos. Y nuevamente mencioné la palabra estilo. Y esta palabra estilo, ¿a qué me refiero? Que se note que es parte de lo mismo. Por ejemplo, hay presentaciones que son con el tema del arcoíris. Entonces, usan colores muy chillantes y tales más complejos de mezclar. Pero, a fin de cuentas, tiene el estilo. Hay presentaciones todas blanco y negro. No existen colores. Solo hay tonalidades de grises. Eso es un estilo. Mantén ese estilo a lo largo de la presentación. Siguiente precado capital, falta de conexión con el público. ¿Qué pasa cuando estamos presentando información? Un error que contemos mucho es creer que esto se trata de ti. Que, a fin de cuentas, están ahí para verte, ¿no? Que tú eres la estrella. La estrella es la audiencia. Ellos son los que importan. Tú eres el facilitador. Tú no eres la estrella. Y si hayas informaciones como estas, donde hablas de tu negocio, de tu proyecto, de lo que tú ofreces, de lo que te gusta, bien por ti, o sea, a fin de cuentas, es ¿cuándo vas a hablar de mis problemas? ¿Cuándo vas a hablar de lo que a mí me interesa ver? Estas cosas, ¿por qué las hacemos? A veces nos sentimos de que nos falta generar credibilidad. Entonces, como que me siento menos. Y en esta parte de, ¿me siento menos? Pues tengo que decir informaciones de que no soy menos. Estás tratando con un profesional. Estás tratando con una empresa seria. Estás tratando con lo mejor. Entonces, esto nuevamente nos lleva a generar esas depositivas que es todo ser creativo. Y la otra es que realmente no hago un estudio de qué están esperando esas personas de mí. Entonces, pues ofrecen cuentas que esperan de mí, pues que les platique por qué las cosas salieron como salieron. Ya sea un cliente que es para mí, ¿cómo le resuelves sus problemas? No como, no, ¿quién soy? Un proveedor que espera de mí, pues sabe exactamente de qué tamaño es la oportunidad de negocio. Entra al grano. No tienes que sacar estas cosas si no entender tu audiencia, ¿no? Entonces, eso es muy importante de este Pk tal de que generes esa conexión con el público. Y finalmente, nuestro último Pk tal, vamos a hablar de exceder en el tiempo. Cuando haces presentaciones, la única limitante que existe debe ser tiempo. Es decir, no te preocupes por la cantidad de slides. Lleva las depositivas que se te dé la gana. Quieres llevar 4, lleva 4. Quieres llevar 50, lleva 50. No importa cuántas depositivas lleves. Lo importante es el tiempo. Si te dan 15 minutos, son 15 minutos. Si te dan 30 minutos, 30 minutos. Si por algunas circunstancias no has tenido 5, entonces son 5. En esos 5 minutos, mete lo que tú quieras. Pero respeta que son 5 minutos. Como nadie hace caso de eso y no respetas tiempos, la única manera que se me ocurre de que respetes tiempos es, ¿sabes qué? No más tráete 4 depositivas. Y ya, oye, que no sé qué es que tengo que meter esto. No tengo idea. Lleva 4 depositivas. Acabas haciendo estas cosas. Como nomás tenía 4 depositivas, pues ahí está la tasca de informaciones en la depositiva. Pues, ¿cuánto tiempo tengo? Tienes 15 minutos. Perfecto. Yo me comprometo a respetar 15 minutos y no te metas con la información que yo llevo. Déjame entrar con todas las depositivas. Perfecto. ¿Pero qué pasa? Pues, yo empiezo mi presentación y quiero hablar de mí. Entonces, estoy hablando de quién soy. Y luego hablo de la historia de mi empresa. ¿Qué es lo que hacemos? Y ver todo el texto. La misión que tenemos. La visión, el sueño que estamos tirando. Los valores en los cuales seguimos. Y luego entro aquí a hablar de costo y retorno de inversión. Y tú, como el asistente, ay, ya, por favor, y algo que me interesa, ¿no? O sea, me tuve que aguantar los 15 minutos de introducción para por fin ver algo que realmente me interesa. Cuando llegas a este punto, pues, es que ya sabes que 15 minutos no alcanzo para presentarte lo quiero. Pero la gente que te veía, es lo que quería saber. Todo lo demás no le importaba. No le interesaba. Mi historia viene por ti, tus valores vienen por ti. ¿Cómo me ayudas a mí a resolver mis problemas? ¿Cómo sé o cómo puedo saber qué puedo cumplir con los tiempos? No hay una bola mágica, lamentablemente. Entonces, tómate el tiempo. Ensaia la presentación. Pon un cronómetro. Simplemente, pues, empieza a contar y ¿sabes qué? Tarde 20 minutos, tengo que bajarle 5. O digo menos cosas o le quito de apostivas. Significa, ensaya tu presentación. Considera tiempo para preguntas y respuestas. Entonces, ¿por qué es importante este tiempo? Se trata de dejar satisfechos a la audiencia. Estas personas llegan para verte y esperan respuesta a inquietudes. Entonces, dale espacio a que presentas inquietudes. Dale ese tiempo. O sea, es una cortesía que das a la audiencia. A ver, ahora sí, ya acabé. ¿Alguien tiene alguna duda? Sí, yo tengo una duda. ¿Qué pasa con esto? Te la resuelvo. Oye, tengo otra pregunta. Te la resuelvo. Tú no te vas de aquí sin que sientas que fuiste atendido. Oye, Rodríguez, que luego no pregunta nada. Es interesante eso porque significa que no genere las inquietudes adecuadas. Si cuando dicen, ya acabé, ¿hay alguna pregunta? No, ninguna. O soy un experto en difundir el tema o no logré captar el interés. Es deseable provocar esto. Cuando acabe y no recibo inquietudes es que tiene que ser el tema. Pues no generó el suficiente apego para que digan, sabes que a mí me gustaría que mencionaras más cosas porque esto sí me interesó. Los siete pecados capitales de las presentaciones. Resumiendo, número uno es no hiciste una definición de cuál es el objetivo, por qué haces esto, escríbelo. No lo pongas en PowerPoint, al menos que en tu cabeza esté muy claro qué esperas lograr. Meter contenido irrelevante o excesivo. ¿Para qué todas estas depositivas llenas de texto, llenas de imágenes, llenas de datos? Si al final de cuentas tú ya sabes que es paja, entonces ¿para qué lo metes? Abusar de los bullet points, pues todo eso es, busco otra forma diferente y no uses bullet points. Agarra y mete formas, mete colores, mete iconos y tú explica qué está pasando en lugar que la gente lo vea. Demasiado ruido. Los quita el exceso de colores, el exceso de tipo de letras, el exceso de imágenes, el exceso de elementos y hace slides limpias. Número 5, pecado número 5, falta de consistencia en el diseño. Repite. Entonces, los mismos detalles. Si tú usas ciertos colores, se repite en la presentación. Textos grandes, se repite en la presentación. Que va a la izquierda, que va a la derecha, que va centrado, se repite en la presentación. Falta de conexión con el público. Tú te sientes la estrella de la presentación, estás mal. Tú eres el facilitador, la estrella es la audiencia. Todo gira alrededor de ellos. Ellos van a ser los que terminen que tu presentación fue exitosa. Estas personas van a decir, sabes qué, me encantó lo que presentaste. No tú. Entonces, no es para ti, es para ellos. Y número 7, excederse en el tiempo asignado. No se trata de que me limiten a mí, la cantidad de postivas es que respete el tiempo. Y como dijo Paula, oye, si me dan menos minutos, a ver, o lo hice rápido, que no va a ser, no es lo mejor. O dices menos o llevas menos contenido. Pero tienes que respetar el tiempo. ¿Dónde aprendes todas esas cosas? Pues no tienes que a ningún otro lado. Con comunicación numérica tenemos un curso que es presentaciones que venden ideas. Es el primer curso que hacemos enfocado a presentaciones. Habíamos hecho cursos muy enfocados a datos. ¿Qué vemos en este curso? En este curso vamos a ver de temas para prepararte. Cómo lidiar con resistencia al cambio. Cómo conocer a la audiencia. Vamos a ver temas de uso de color, combinaciones y composición. Vamos a ver manejo de imágenes, de cortes. ¿Sáques todo el potencial que tienes de la imagen? ¿Dónde encontrar imágenes que sean de su gratito y de alta resolución? Vamos a hablar de poner excelencia a tus postivas de espacio vacío, de diseñar sin bullets, de iconografía, de tipografía. Vamos a hablar. Vamos a hablar de infografías también. Y vamos a hablar de tu momento con la audiencia. Si ya tengo todo hecho, ahora tengo que sentarme con las personas y presentarles esa información. Y aquí estamos en Comunicación Númerica a su orden. Nuestro correo es contacto.comunicacionnumerica.com.mex Cualquier duda, con todo gusto, ahí te ayudamos y te respondemos. Cuídense mucho y hasta luego.
DOC0113|Data Storytelling|Dale un boost a tu presentación de resultados. El motivo de este webinar es darte algunos tips para que ahora que tienes ejercicio de presentar mejores informaciones, tú tengas mejores herramientas de cómo distinguir la forma en que presentas esa información. Entonces quiero mostrarte aquí unos ejercicios de cómo darle un boost a tu presentación de resultados. Pero que te quiera hacer un disclaimer, un aviso antes de iniciar. Hay dos cosas que te quiero mencionar que debes tomar en cuenta. Vas a requerir invertirle tiempo y la otra es tienes que manejar tus herramientas con maestría. Entonces te voy a presentar algunos mensajes que tienes que seguir a lo largo de tu presentación de resultados. Y el número uno es simplifica. Siempre trata de meter la menor cantidad de elementos y enfocarte mucho más en la historia. Si estamos hablando de una lámina como esta, estoy llenando de pequeñas imágenes, eso son demasiados distractores. Y cuando te hablo de simplificar es, vete al gran, presenta algo así. Disminuya cantidad de imágenes, aquí esto es ya una sola imagen. Y aquí lo que quiero lograr es que mi audiencia me escuche. No se vea distraída con tantos elementos ni tantas fotografías. Simplemente es ya que lo viste, ahora escucha lo que tengo que decir sin tantos distractores. Siempre simplifica la información en tu presentación de resultados. Mete menos elementos y cuando es su imágenes, es mejor que uses una imagen de muy alta resolución. Una sola, bien seleccionada en lugar de meter muchas chiquitas que realmente es bueno. ¿Cuál es el punto? El siguiente punto es, haz un buen uso del orden y color en la información. Vean una lámina que esta lámina está con muchos elementos visuales. Arrancamos con un pie y me lo marca en tres tipos de colores. Me dice en azul lo que está concluido, pulsos concluidos, en amarillos que están en proceso, en naranja que están atrasados y en rojos que no hemos iniciado. Aquí tenemos cosas que se repiten dentro de la lámina. Cuando te hablo de orden, esta parte es importante. De todos estos elementos repetidos, que estas leyendas califican todas las gráficas, es lo mismo. Se vuelven demasiados elementos repetidos en esta información. Por último, estamos aquí metiendo información como que todo tenía un tipo de consistencia en colores. De repente te vas a ver una grafica que rompe un poco como se lee toda esta lámina. Ahora regresamos a la lámina inicial. Como que los colores no son muy claros. Que es lo que me quieres transmitir. Y este tipo de gama de colores que se repite, no me deja con mucha claridad que está pasando. Y algo que también está haciendo es el orden en la información. Que de alguna manera queremos distinguir que salió bien y que salió mal de nuestros resultados. Aquí cuando yo pongo la información ordenada por alfabético, o tal cual la sacó el Excel y así la pegué en estas gráficas sin preparar esa información de cómo yo llegué a estos resultados, no le estoy dando a esta audiencia distinguir con claridad que salió bien y de qué salió mal. ¿Qué podemos hacer por esta lámina para darle un boost a tu proceso de resultados? Entonces aquí vamos a enfocarnos en evidenciar con mucha claridad que tú distinguas como audiencia que salió bien y que salió mal. Voy a rediseñar esta lámina. Este es mi lenguaje de colores. Lo que veas en verde es que está bien. Lo que veas en gris, en amarillo está en proceso. En rojo está trazado. Y aquí puse un gris. Que no significa que no está iniciado. Que ni siquiera hemos avanzado en esa posición. Esta gráfica de Pi que me habla de estado general, prefiero mejor una gráfica de barras apiladas para que se vea el largo de las barras en 100%. Y aquí se ve con claridad cómo está en general. Qué tantos cursos están concluidos en proceso de atrasados y en gris que ni siquiera han empezado, pero no es ningún animal. Simplemente no han iniciado. Y después arranco con una gráfica, con esta parte de localidad. Pero fíjense que a diferencia de la gráfica anterior donde sólo estaba ordenado por alfabético, aquí están, cómo están ordenados por cursos concluidos. Que tú distinguas que Argentina está mucho mejor que todos los demás. Igual el puesto. Cuál es el puesto que tiene mayores cursos concluidos. Y finalmente la categoría que tiene la mayor cantidad de cursos concluidos. En cada una de estas gráficas están las novedades los verdes del mayor al menor. Y el código de color está uniforme en todas las gráficas. Me faltaban estas informaciones que hablaban de los puestos. Ya tengo espacio, puedo posar algo diferente y ponerlo aquí. Como son con íconos. Esto me refiero con darle un boost a tu presentación de resultados. Que es lo que yo tenía antes. Con quitando elementos que se repiten, uniformizando los colores. Sin embargo, qué más puedo hacer para meterle más color. Qué tanto buscan que cuando presentan información numérica, de alguna manera los colores de su presentación estén ligados a los colores de la compañía. Qué puedo hacer por esta lámina para manejar estos colores. Si yo quise hacer algo como que cambie un poquito de color. Puedo explorar informaciones de cómo sería, por ejemplo, si lo paso a que esta información se vea en color neg. En una sala muy oscura con poca luz, un fondo negro sea mucho más agradable a la vista. Pero de repente estoy manejando con los institucionales y el color no es negro. Me suena a veces que uno de los colores de la compañía que más se ocupan es este color mora. Ya no tiene esa parte agradable que tiene el negro. Incorpora con los institucionales, es una buena práctica. Que tú metas mal los colores de la compañía es bueno, porque tenés cuenta, las personas se sienten mejor identificadas con información. Pero tienes que buscar los colores que contrasten bien. Y esto semáforo que yo puse es horrible en este fondo morado. Pero qué pasa si yo agarro y manejo colores que me permitan trabajar mejor con el fondo que yo tengo. Y aquí me tuve que olvidar un poco de esos colores de semáforos que estoy acostumbrado a manejar. Y salirme fuera de la caja. El mismo colorcito morado que tengo institucional, lo llevé a oscuro, lo llevé a clarito y lo puse casi en gris. Ahora por ejemplo, si de repente me pide manejar un color rojo, porque el rojo es el color institucional. Como todo mi lánina estaba puesta como semáforo, pues todo lo que era rojo ya se perdió. Y nuevamente es busca que otros colores que contrasten ese rojo. Ya no podemos usar el rojo para marcarlo negativo. Tenemos que buscar otra forma de cómo representar ese semáforo. Siempre no tengas miedo de usar los colores institucionales. Simplemente es busca cómo hacer los contrastes. Lo que te quiero mostrar con esas cuatro láminas es que no tienes límites. Lo que sí tienes que fijarte es en el contraste de colores. Y el siguiente comentario es gráfica. Muchas veces lo que hacemos es meter información totalmente tabular. En esta información nos estamos perdiendo de cosas que podemos agilizar mucho con la vista. Porque algo que tiene la visión es la facultad más importante que tenemos para comunicar información. Procesamos 60 mil veces más rápido una imagen de la que tenemos contexto. Para tratar de poner esta información mucho más visual y más gráfica, debo entender exactamente qué están buscando. Por un lado, aquí tengo información. Está hablando de impuestos que hay que pagar en un año. Y está hablando por localidad cuántos impuestos tengo que pagar. Impuestos por empleado y aquí tengo que dar impuestos promedio. Después me está hablando el mismo ejercicio de cuánto tengo que pagar en total. Total de empleados que tengo y el total de impuestos promedio por empleado que tengo que pagar. Después tengo aquí una distribución por rango de impuestos, diferentes rangos de pagar y la cantidad de empleados que corresponden a ese rango. Y finalmente tengo un mensajito aquí que tengo que hacer que esto hay que pagarlo para el 25 de julio. Y esa es mi lámina de mi presentación de resultados. Vamos a darle un bus. Siempre trata de lo general a lo particular. Entonces empecemos hablando de cuántos son los totales. Que está esta parte ahí escondida y te sofiero que es una parte relevante. ¿Cómo va el negocio? Independientemente de hablar los pequeños componentes, ¿cómo vale todo? Y aquí lo voy a cambiar por algo distinto. Voy a poner este elemento cargado en color con algunos íconos que refuercen de qué se trata. Y exactamente las cifras muy grandes de cómo se muestra el todo. Tenía yo esta tablita. En esta tablita tengo dos comparativas. Cantidad de empleados y el impuesto promedio por empleados y para distintas ciudades. Aquí puedo hacer esto para mostrar en forma gráfica. Se me ocurre ponerlo en dos gráficas que se puedan comparar entre ellas. Para evidenciar que no el que tenga más empleados termina pagando más impuestos. Algunos que tienen menor cantidad de empleados termina pagando más impuesto promedio. De esta tablita algo que puedo sacar es que hay información que hace referencia a geografía. Y como mostraron una información adicional que no viene aquí, voy a crear una nueva variable que hable impuesto total. Voy a multiplicar la columna total empleado por impuesto promedio por empleado para obtener el impuesto total. Y como todo hace referencia a ciudades en Colombia, voy a meter un mapa. Y aquí en este mapa voy a reflejar con un gráfico de burbujas cómo va el impuesto total, dónde hay que pagar más impuestos de manera total. Finalmente, aquí en esta parte hablando de los impuestos que vienen aquí, los rangos, porque me está hablando de una distribución por las diferentes ciudades, eso funciona mucho mejor. Tengo mejores resultados con un histograma. Y en este histograma voy a presentar esta tabla de esta forma. Entonces aquí tengo cómo es la variación de empleados por rango de impuestos en las diferentes ciudades, las aquellas ciudades que más que tienen empleados en todos los rangos y las ciudades que más tienen empleados en pocos rangos. Y algo muy importante que quiero dejar claro es el mensaje. Voy a cambiar este mensaje que dice impuestos por empleado. Mejor va a quedar con el otro mensaje que ya en dinero que conozco la cifra. Y cuando se vea la cifra, eso hay que pagarlo el 25 de julio, que eres el letrito rojo que estaba hasta el fondo. Poniendo todo junto, lo que hice es generar esta parte mucho más visual, porque con visuales lees mucho más rápido. Si estoy buscando, pues hacer esa comparativa en lo que tenía antes a lo que tengo ahora aplicándolo en un bus a mi presentación de resultados. Siguiente punto, creatividad. Ya no quiero ser la típica lámina con gráficas y con las barras, con mis tablitas. Y ahora quiero aventarme algo diferenciado y voy a tratar de que mi lámina se vea más como un tipo infográfica. Y trato de usar otro tipo de elementos que no son comunes y que se vea diferente, que no se vea como la gráfica de todos los días. Es importante que tú trates de salirte fuera de la caja y plantear cosas distintas. A pesar de que las intenciones son buenas, también tiene muchas fallas en que realmente si esta lámina cautiva, si realmente interesa verla. Cómo puedo trabajar la parte creativa, pero sin olvidarme de los números. Primero vamos a ver que en esta lámina lo que me están presentando es un nuevo modelo tarifal. Estamos hablando de una industria de transporte y que en la actualidad hay una presión por incremento en las tarifas porque están altas en combustible. Y tenemos un sample de seguimiento de que hay que identificar las rutas más importantes y revisar qué hay que hacer. Debo aquí tengo una pirámide que me habla de que de todas las rutas que manejamos hay una ruta son clave, hay unas rutas que transportan mucho volumen y está el resto de rutas. Tengo aquí unas acciones que hay que hacer en cada de estos rubros. También aparecen acá informaciones de qué tipo de porcentaje están interviniendo estos datos dentro de la pirámide, cuánto se gasta. Y finalmente estoy rematando aquí esta columna del antiguo modelo de tarifas al nuevo modelo tarifas, cómo se va a comportar. Eso es la forma en que estoy mostrando mi información. Anímate a hacer cosas diferentes, pero hay que siempre priorizar cómo queda mucho más claro el mensaje. ¿Cómo le daría yo un boost a esta presentación y hacerla diferente? Siempre que una presentación hay dos componentes. El primer componente es la presentación y el otro componente es el presentador. Y esta persona también tiene mucha relevancia en cómo se comunica correctamente bien la información a tu audiencia. Entonces, si quieres ser algo creativo, siempre involucrate dentro de esa creatividad y trata de meter, partir más tus diapositivas en algo que vaya de la mano con lo que quieres presentar. Entonces aquí estás hablando de imágenes, estás hablando de formas, estás hablando de conceptos en esta lámina. Esto lo podemos cambiar diferente. Entonces podemos arrancar con esto y vamos a platicar de la actualidad. Y ahí pasa un tiempo hablándote de qué está pasando actualmente y cuál es el plan de seguimiento que hacer con la imagen visual de soporte. Después, la siguiente lámina, usando los mismos datos que ven a la misma pasada, vamos a hablar de esta pirámide. Mejor vamos a manejarla en una gráfica circular, donde vas a hablar de todas las rutas, cuánto es clave, cuánto tiene de alto volumen y cuánto es el resto. Después hablamos que en la misma lámina hablaba de porcentaje de perfección y porcentaje de rutas y porcentaje de gastos. Entonces aquí vamos a estar con una gráfica de barras apiladas. En un punto tenemos rutas y en el punto tenemos gastos. Y lo que queremos mostrar es que no las más rutas generan la mayor cantidad de gastos. Y finalmente tenemos un plan tarifario. Ese plan tarifario lo vamos a involucrar de esta forma. Y aquí estamos hablando de cómo estaban las proporciones en porcentaje de tarifas en el antiguo modelo y cómo son las nuevas proporciones de tarifas en el nuevo modelo. ¿Qué porcentaje tenías antes? ¿Qué porcentaje tenías ahora? Y finalmente había unas acciones que hacer. En una tabla vas a mostrar todas las acciones y las consideraciones que tenemos que tomar para que se cumpla nuestro plan. Esto es darle un boost a nuestra presentación de resultados. ¿Involucra una historia? ¿Qué podemos hacer para meter una historia? Este es el caso de tratar de meter en el sistema los datos que estamos usando que es por medio de animaciones. En mi historia viene así. Voy a hablar que tenemos un mercado, estamos en la categoría de snacks. Y aquí tenemos una categoría de galletas que nos interesa entrar. En el mercado de botanas, nuestra empresa, aquí no tenemos un producto de botanas. Realmente no competimos en esa parte de snacks. Viene la animación. Aquí en estas dos barras que son confetería y helados competimos. Somos con un 42 y un 60% de por ciento del mercado. Somos fuertes. El mercado de galletas aquí en particular lo que tenemos es que tenemos fortaleza, pero realmente participamos muy poquito. Y aquí ahí somos totalmente libres. Ahí somos los número uno al mercado, pero por mucho. Entonces queremos entrar al mercado de galletas y nuevamente con animación aparece aquí cuál es el momento de consumo o qué tan fácil es entrar a ese mercado. Entonces la gente consume snacks para que le de energía, para que le de placer o para que le de salud. Y cada uno de ellos tiene la energía o no da energía, da placer o no da placer, da salud o no da salud. Y con animaciones estoy mostrando cómo se ve todo este estudio de marketing. Y al fin y al cuento en galletas que realmente tenemos poca participación. Ahí fíjate que es para los que dicen que la energía y da placer dicen esto está muy fácil de entrar y por eso es sencillo entrar a este mercado. Esta lámina original lo que está tratando es tratar de contar una historia y lo que está haciendo es meter estas animaciones y poco a poco ahí va mostrándote. Se anima un punto, se anima otro punto, me va acompañando y ahí tengo la audiencia. Pero así como que a pesar de que tengo animaciones, tengo alguna secuencia, así como que la historia que esté cautivando y que realmente nos motive a poner atención y que realmente es lo correcto hacer, que es lo que la compañía debería ser, pues de alguna manera estos mismos datos pueden presentarse distintos y que realmente estamos buscando es poner atención. Quiero que te enganches, quiero que compres el proyecto y de buena manera de aquí no te salgas. Como le damos un bus a esta historia y que cautiva mi audiencia y ahí te quiero preparar a hablar de un concepto que es. Haz uso de secuencias, una historia es un espacio lineal, una historia de un inicio, una mitad y un final y todo va conectado y hay un programa como PowerPoint. Pues enfócate, apóyate más en las diapositivas de que tienes diapositiva 1, diapositiva 2, diapositiva 3 y todas estas diapositivas deben estar ligadas en cómo contar esa información. Más allá que te apoyes solo en cómo están las animaciones y eso, apóyate en más de postivas y que esas de postivas te vayan apoyando en cómo contar la historia. Cambiar de postiva te va a dar mucho más flexibilidad de cómo contar esa historia que meter animaciones. No digo que las animaciones sean malas, simplemente es flexibilidad, me refiero que tienes un mayor campo de acción y te lo quiero mostrar. Entonces esta lámina la voy a presentar diferente. Vamos a darle un boost, vamos a arrancar primero con una lámina de introducción y aquí arranco con una fotografía y que es lo que es snacks y que la categoría de galletas significa cento. Es un valor del mercado de 65 millones de pesos y es una gran oportunidad de crecimiento. Mi siguiente diapositiva voy a hablarles bueno hoy el mercado vale 410 millones de pesos. El mercado de snacks y está dividido de esta forma. La lámina es muy limpia con poca información. Luego viene el siguiente diapositiva y te quiero decir que en estas barras que están en gris aquí no tenemos producto. Después te quiero decir que en confitería tenemos un 42% de población al igual que en helado tenemos un 60% de población. Ve como van cambiando el color de las barras. En otras dos que tienen pastelillos en fruta seca la verdad es que hay prácticamente el mercado es nuestro y los estoy representando con otro color. Sin embargo, lo que realmente me interesa en atención es otra diapositiva y entra a hablar de las galletas y quiero que se vea que en galletas realmente tenemos muy poca participación. Y cómo es el consumo en esos mercados? Aquí meto la tablita y en el consumo lo que quiero mostrarte es que todos los alimentos es el mercado consumo de acuerdo si le da energía, si le da placer o si le da salud. Algunas cosas el mercado dice que si no le da energía, si le da placer y no le da salud cuando consume botanes. Y así cada uno de ellos con otra diapositiva entra a hablar de cómo está todo y en particular entra otra diapositiva en mercado de galletas. Lo que es energía y placer es un mercado fácil de entrar. Y esto es lo que estamos buscando con darle un boost a tu presentación de resultados. Esto es algo que está teniendo mucho amor al hablar de imbolcarse y teorías es involucrar una secuencia de más o menos avanzando lámina, lámina como hacer todo esto. Una persona que presenta resultados con esta calidad también gana puntaje. Tiene una lámina como esta, estamos viendo porcentaje de población que ve tales urbanas. Si hay una información que hace referencia a geografía, pues involucra mejor los mapas y puedes obtener algo distinto, diferenciado. Estas gráficas son bastante comunes, simples, sin chistes, son parcas. Pero ten en cuenta que aquí tenemos tres gráficas y te la voy a cambiar en lugar de tres gráficas, voy a usar nueve gráficas. Y mis nueve gráficas se ven más bonitas que cuando eran tres. Entonces aquí tengo tres plantas, las tres plantas tienen tres diferentes colores. Pero al final acaban en rojo porque en rojo significa una proyección, o sea lo rojo no ha pasado. Y tengo una línea negra que me abre el promedio semanal, siempre busca cómo presentar mejor tu información y que dé un mensaje mucho más claro que ver. Estoy poniendo unos triangulitos, unos escuditos para ver qué tan confiable es el pronóstico que me están dando. Entonces estoy poniendo información adicional de qué significa esta información. Cómo consigo resultados como los que vi en este webinar. Uno requiere capacitarse y aquí te quiero invitar a que te unas con nosotros en Comunicación América a capacitarte. Si quieres darle un boost a tus resultados, pues tienes que invertir en mejorar este conocimiento. Estamos en comunicacionamerica.com.mex, ahí nos puedes encontrar. Y te deseo la mejor y mucho éxito en todas las presentaciones de resultados que llegues a hacer.
DOC0114|Data Storytelling|Y hoy vamos a arrancar a hablar de este tema de presentaciones híbridas. En 2020 hubo una dilucción en el mundo de las presentaciones. Algo se quemó. Y a partir de ahí las cosas cambiaron. Llegó esta pandemia y de alguna manera nos obligó a todos a recluirnos y muchos esquemas de trabajo empezaron a cambiar. De arrancar en un modelo presencial, pues llega este virus y nos cambia la jugada y empezamos a transformar todo esto en algo conexión remota. Todo el mundo está conectado a través de equipos. Casi hacía lo mismo que hacías presencial, pues empezás a replicarlo tal cual de forma remota. Y luego llegó esta parte de regreso a oficinas. Pues ahora estamos, muchos de nosotros, regresando nuevamente a entornos físicos y ahora estamos en ambientes donde algunos van y otros no van a oficina. Entonces, en esos ejercicios de presentar información, tengo una audiencia que está ahí presente, la puedo ver, puedo interactuar con ella y tengo otra audiencia que está, pues, a través del monitor. ¿Y cómo hago esa compensación de balance? Cuando hablo de las presentaciones híbridas, combinan las dos cosas, combinan aspectos de la parte presencial porque tengo ahí gente, físicamente está ahí, y también tengo personas que físicamente están en otras cualidades. Para entender cómo llevar una presentación híbrida, hay que ver exactamente qué características tenemos con una presentación presencial, que es donde ya la mayor parte de nosotros estamos más que entrenados en este tipo de presentaciones y voy a dedicar un poquito más tiempo de hablar de presentaciones remotas. Y al final empezar a cerrar dónde se juntan estas dos. Entonces, empecemos hablando de presentaciones presenciales. En este año regresamos a hacer muchos cursos presenciales y algo que sientes mucho es la interacción con personas. Eso sí ha sido algo diferente, bueno, más bien no diferente, la palabra correcta es que extraña, vamos, esa parte de ver a las personas, convivir con ellas, ver sus reacciones y, pues, que todo se fomente ese ambiente donde todo mundo participa. Y lo que vino a cambiar las reglas son las presentaciones remotas. Está acostumbrado a interactuar de manera directa como presentador con esta audiencia. Pero cuando llegó la parte remota me encontré con un nuevo jugador que no tenía tan nominado, que es la tecnología. Son tres cosas en particular que hay que ver hablando de presentaciones remotas. Aún no hemos entrado a la parte de presentaciones híbridas. En la parte remota tenemos que ver esta presentación en tres aspectos. En el entorno, la gente ahora sí está enfocándose más a lo que está en pantalla. En esta parte de hablar, distancia y cómo hallar esta voz que llegue con claridad a su audiencia y en diapositivas mejoradas. Empezamos a hablar del entorno. Como estamos en una sala de juntas y fuera presencial, ya estaba la sala, ya estaba limpia, nada más me estuve que preocupar en qué condición estaba la sala, era alguien más en garras. Y cuando estamos en la parte remota, empezamos a vivir cosas como esta, ¿no? Y al final de cuentas es, pues nos empezamos a hacer este vlogs. Nos empezamos a acostumbrar, bueno, pues no me ven, apago la cámara, puedo estar donde quiera, puedo estar en las condiciones que me la ganen. Al final de cuentas es, yo nomás escucho y abre el micrófono cuando tenga alguna pregunta. En la parte de cuidar tu entorno, pues el punto, unos puntos principales es, tu iluminación. Es importante que la gente te vea. De alguna forma es su iluminar tu cara para que a fin de cuentas los demás puedan verte. Estás perorizando la experiencia de los demás. También, había que tener en cuenta, pues cuando tú salies en pantalla, todo lo que estaba alrededor de ti. Pero aparecían cosas que no, no, no cuidaba. Siguiente punto, hablar a distancia. Es algo que tampoco tengo que preocupar en las presenciales, siempre lo hacía de forma directa. Y ahora que venían nuevas cosas que fijaron. Veía, bueno, la parte de cómo me escuchas, la parte de audio, la parte de este contacto visual que tenía muy bien dejado con la parte presencial. Ahí tenía mi audiencia, interactúo con las personas y esta parte lograr mayor presencia. Si me quieres escuchar sin eco, si me quieres escuchar sin ruidos que están ocurriendo de la calle, gente que pasa aquí enfrente de la ventana. Entonces, hubo que mejorar esta parte del audio en qué es lo que va a hacer mi audiencia y pues empezar a buscar mejores formas. Entonces, hay que tener micrófonos. Estos micrófonos lo que hacían es, te encerraban el audio, o sea, digamos, tiene una determinada cobertura. Entonces, se cuenta que pasa medio metro y no se pasa ningún sonido fuera de ese medio metro. Bullen los ruidos, mucho mejor que lo pueda hacer la computadora. Y lo que hago es que la persona que está conectada a otro lado recibe mucho mejor mi voz. Y es algo muy importante que a fin de cuentas se me escuche con claridad. Es más importante el audio que el video. Es mejor que te salgas todo pixeleado a que te escuchen mal. Algo también importante era hacer esta parte de contacto visual. Que tú sientas que te estoy atendiendo, que sientas que te pongo atención. Entonces, una cosa que pongo que empezar a hacer en personas remotas es mirar hacia la cámara. Especialmente cuando tú estás presentando. ¿Cómo se siente cuando mira uno a la cámara directamente? Fíjate en esta imagen. ¿No sientes que esta persona te está poniendo atención? Todo lo que ocurre está a la persona para ti. Entonces, esto es súper importante en la parte de mirar directamente a la cara. La persona no está mirando a nadie. La persona está mirando totalmente a una cámara y toma la fotografía. Pero para nosotros que estamos del otro lado, nos sentimos como que está prestando toda la atención. Algo también importante en la parte de las presentaciones remotas era, bueno, ¿cómo lo hago para que sientan mi presencia? Que estoy ahí. Cuando ya llego a las herramientas virtuales, pues esa presencia, como esta imagen que tienes el presentador, moviéndose la audiencia, siendo dueño del tema, está en el centro de la sala. Ahora todo eso queda reducido a este pequeño espacio. Entonces, en este entorno virtual para aumentar mi presencia dentro de la sala, de la sala virtual, puedo incluirme a mí dentro de la sala. Esta es una de las formas. Como son reporteros, tengo ahí mi rostro, yo estoy mostrando mis slides y sigo presente. Ponte una versión de que me muestro lado a lado. Entonces, sigo ahí presente y estoy mostrando mis slides. Puedo mostrarme chiquito, como me has visto aquí. Sigo estando aquí presente, no me he ido a ningún lado, te sigo explicando. O bien puedo ponerme dentro del slide, que es lo que tú deberías tirar de una presentación remota. Que a fin de cuentas es que las personas que están conectando tengan que tener una experiencia al menos lo más semejante a la parte presencial. No deberías aspirar a que por ser una presentación vía remota fue más fea, más chafa, decimos aquí, menos interesante que por haber hecho presencia. El último tema de presencias remotas son de apositivas mejoradas. Pues veías presentaciones horribles, pero no te podías ir. Estabas ahí encerrado, podías contestar tu correo, pero no abusar porque tenías que poner al menos levantar la cara de 5 años. Podías contestar tus mensajes, pero no abusar. Podías irte a sanitario y luego regresar, pero no abusar. De alguna manera tenías que cuidar esa presencia dentro de la sala. Pero en el entorno virtual realmente ya no tengo que estar preocupado por eso. Entonces ahora sí, si me aborre esto, apago mi cámara, tú no ves qué hago y no tienes control. No tienes control de lo que la persona está haciendo. Ahora sí estoy al 100% que están de correos, ahora sí estoy sentado en la sala viendo el Netflix y al lado tengo la laptop con la presentación y no estoy poniendo atención. Cuando estoy compartiendo información, presento pues cómo está. O cómo está. Pues como te dije, en la sala de juntas no podías irte. Pero aquí definitivamente tu atención está en otro lado. Ya no vas a estas cosas, yo tengo que simplificar y hay un mensaje claro. Tenés que poner láminas interesantes de ver, atractivas. Lo que estás viendo está tan bien hecho y bien elaborado. ¿Sabes qué? Prefiero quedarme aquí conectado a la junta, que irme al Netflix, que ver las redes sociales, que atender cosas de la casa, que atender el apagafuegos en el trabajo. Prefiero estar aquí conectado porque eso está muy bueno. Pues déjame hablarte entonces de este animal nuevo que es las presentaciones híbridas. ¿Esto de las presentaciones híbridas llegó para quedarse? ¿O es sólo una moda pasajera? Consulté a Máquise de El Mundo Post Pandemia. Pues sé que 9 de cada 10 organizaciones cambiarán el trabajo a una parte híbrida, remoto más oficina. O sea, ya están 9 de cada 10 organizaciones que están ahí. Microsoft en su encuesta o su reporte anual de tenes de trabajo, 73% de los trabajadores deseaba continuar trabajando de forma remota. Zoom, que también es su proveedor de conferencias virtuales, también una encuesta con más de 7000 personas en 10 países, 2 tercios de las personas prefieren la combinación de entornos virtuales y entornos presenciales. O sea, ni uno ni del otro sino combinadito. Pues sí, esto llegó para quedarse. Y es un poquito evidente de por qué no se va a ir, ¿no? Una presentación a distancia me permite hacerla con mucho mayor velocidad. En menos tiempo tengo que anticipar esa junta. Hay ahorros. Antes era traer gente de todos lados y desplazamientos, hoteles. A veces tenés que apartar una sala porque en nuestra oficina no había salas, todos rentamos una sala afuera y esos son ahorros y también nos permite hacer más productivos. Porque, en fin y cuenta, la gente está cómoda en sus hogares y eso hace que trabajen mejor. Cuando te hago desplazarte, pues pierdes tiempo en el tráfico. Ese tiempo que voy a pasar manejando ya no tengo que... puedo dedicarlo a otras cosas. Son muchos beneficios para más dejarlos atrás y de repente volver las cosas como si no hubiera pasado nada. Y esta imagen me encantó porque es lo que refleja lo que está pasando. En la imagen parece que estamos viendo cuatro personas platicando ya solas en la... físicamente en la mesa y los que están conectados tras en pantalla como que nadie les hace caso. Predomina lo que está pasando en el entorno físico y los que están conectados ahí pues... Bien por ti. Estás ya... a veces ya lo saben. Hasta la aprovechas, entonces es que yo prefiero ver remota porque ni me hacen preguntas, ni me pelan, ni tengo que intervenir. Ojalá yo sea de los que esté remoto. Entonces esto es mucho lo que pasa y hay que ver entonces cómo resolverlo para lograr una mejor experiencia. Tenemos dos audiencias, una física y otra que está conectada vía remoto. Lo que tengo que manejar no de una forma distinta de entender los espacios dentro de ese entorno, de sala de juntas. Tenemos que también ver cosas de cómo mejoramos la interacción entre todas las personas, las que están físicas y las que están vía remota. Y tenemos que trabajar en el diseño de lo que estamos presentando. Hablando de espacios, pues déjame decirte que... la sala típica de juntas donde están todos sentados viendo un proyector, olvídalo. Eso... ya va a cambiar mucho porque ahora lo más importante es mejorar la experiencia global de todos, tanto los que están conectados de vía remota como los que están conectados de la presencia. Entonces esta sala de juntas, ten en cuenta, es cosa del pasado. Tenemos que pasar a sala de juntas con mayor tecnología. Sala de juntas que te permitan tener dos, incluso ya dos monitores, ya no puede ser más uno, uno donde tú estás viendo a las personas que están vía remota y otro donde estás viendo la presentación. Todos están viendo a todos. Los que están conectados externamente se sientan partícipes y los que están físicamente sientan a la persona cerca, que le puedes hablar y le puedes decir la palabra. Algo muy importante es que el entorno tecnológico es tal que esta camarita que está aquí tiene que girar, tiene que apuntar a la persona que está hablando de forma presencial. ¿Para qué? Para que la persona que está conectada de forma remota pues de alguna manera también tenga esa interacción contigo. O sea, debo de balancear y poner todo mundo en piso parejo. El siguiente punto es la interacción. Como presentador tengo que estar buscando esta interacción entre ambas audiencias. Entonces es muy importante que hagas saber a todo mundo que cuenta. Entonces el presentador puede estar siempre está recibiendo preguntas tanto de la parte física como del entorno remoto. Entonces está bien, pero siempre una buena práctica es repite tú la pregunta. El chiste es que todo mundo la escuche. Tú sí tienes que estar dándole espacio a cada persona. Los demás no. Entonces a ver, acabo de hacer esta pregunta. Me están preguntando qué va a pasar ahora con el proyecto. Y esa parte de repetir de alguna manera nivel el terreno y fomenta a ok, no escucha al compañero, pero sí estoy podiéndote ser al presentador. Siempre repite la pregunta que te hagan en ese tipo de entornos híbridos. Y algo que se nos pasa a todo mundo en entornos híbridos nos hacemos la junta aquí los físicos y los remotos. Pues bien, gracias. Hay que entender un poquito más el presentador. Tienes de repente que interactuar con el público, hacer las admisiones a la nuevo chat, responder las preguntas de audio, compartir la pantalla, hacer hablar a las personas, escuchar a la audiencia, silenciar a aquellos que están metiendo ruido en la pantalla que le llega el teléfono y no está apagado el micrófono. Pues todo eso te mete en mucho estrés. Entonces si están tus posibilidades, se para, busca un moreador que se encargue de toda la parte tecnológica de la parte del chat, de compartir pantalla, de darle admisión a las personas, de silenciar a que están metiendo ruidos y tú preocuparte por la parte de la audiencia. Entre menos hagas de la parte tecnológica, mucho mejor. Entonces aquí es, si estás en una posición importante, pues que otra persona se encargue de todo eso técnico. Algo muy importante es que todos te vean. Esto es algo que se pasa mucho por alto. O sea, he visto que me preocupo por los físicos, ellos sí me están viendo y los virtuales nomás me escuchan. O sea, lo único que ven la presentación, porque la compartí a través del equipo y me escuchan. Y a veces el micrófono me queda bien lejos y me escuchan horrible con un eco y súper mal. Esas personas probablemente van a acabar distraídas, van a hacer otras cosas. Lo importante es que me vean. Si me está hablando alguien físico, pues la voltea a ver, hago contacto visual. Si me está hablando alguien en el entorno remoto, lo que hago es voltear a ver a la cámara. A mí como presentador es importante que para ambas audiencias esté presente. Finalmente diseño. El diseño de mis apostivas sí tiene que ser de lo mejor. Un buen diseño de una presentación y todos podemos atender esto. Tiene contraste, tiene repetición, tiene alineamiento y tiene proximidad. Y eso te va a ayudar para hacer un buen diseño. Cuando me refiero a contraste es que las cosas sean diferentes, que se note eso. Entonces, por ejemplo, este es una mala relación de contraste. Prácticamente las palabras no se entienden con claridad. Contraste es hacer que se vea diferente. Entonces, en mis diseños tengo que sacar diferencia del mensaje más importante. Muy importante también esta parte de repetición. Que si yo hago un diseño que lo siga manteniendo, que siga, si estoy manejando tonos de azules, se mantienen puros tonos de azules la presentación. Entonces, si estoy manejando un tipo de letra, el tipo de letra se mantiene en toda la presentación. Eso es importante para tener buen diseño. El otro punto tiene que ver con alineamiento. Si este es el título de mi presentación, mi slide principal, pues tener alineamiento siempre andrás caminando por el lado de buen diseño. Y algo muy importante, proximidad. ¿Qué significa esta proximidad? Significa que si tú juntas cosas, asocias que se trata de lo mismo. No solo es la herramienta. Si tú quieres trabajar con diapositivas, que realmente transmitan mucho mejor los mensajes, no nomás basta esta parte de manejar cosas graficadas. Ponerle limpieza visual y elegir gráficas que vayan más enfocadas en el mensaje. Y no nos más basta aquí, si no sabes qué, adicional, hay que ponerle otro tipo de elementos que tiene en cuenta si cautiven más en audiencia. Son láminas que trabajan con mucha limpieza de información y que van muy enfocadas en el mensaje. Son criterios que a veces involucro imágenes que muestro con mis datos para darle esa presencia adicional. O si estoy usando algún mensaje que no requiere tantos datos, pues cómo elegir las imágenes más importantes. O bien, también de Otra PowerPoint, trabajar con infografías donde estoy manejando muchas formas. Y lo que quiero lograr es que la gente diga, ¿sabes qué? ¿Me interesa ver esto? No voy a abrir el celular, no voy a abrir el correo electrónico, quiero poner atención. Tanto tu parte que está presencial como la parte que estás remoda. ¿Pone qué requiero para mejorar mis reuniones? En particular, mis reuniones híbridas. Tienes que invertir en tecnología. Entonces tienes que mejorar la parte de cómo te den. Entonces tener cámaras de mejor resolución y aumentar la iluminación de tu rostro para que se vea con claridad. Un micrófono que te escuchen bien, que filtre ruidos en tu voz, llegue de la forma más natural. Audio, sin de cuentas que tú escuches mejor, que tengas una mejor precisión de lo que dicen, a veces te preguntan y no alcanzas a escuchar. Obviamente equipos, equipos que puedan soportar muchas aplicaciones abiertas. Pues tú no más estás viendo aquí mi PowerPoint, pero yo tengo aquí, tengo una plataforma de audio, tengo una plataforma de control de sala y tengo varios monitores. En todo eso me ayuda a tener control de reunión, pero sin de cuentas le exige más. Banda ancha, definitivamente es algo que tiene que manejarse. Muchas personas se conectan. Y aquello es lo importante. Si tú quieres enviar un video de buena calidad, tú tienes que tener banda ancha y la banda ancha que sale de tu equipo, no la que recibes. Y manejo de redes, especialmente en la oficina, por ejemplo es Todo Mundo Conectado, Todo Mundo con Access. Entonces son cosas que hay que invertir en tecnología. En el entorno presencial, estaba la sala de juntas típica, pues no más invertía en una buena sala. Pero en el entorno remoto, pues tenemos este animal nuevo y usted tiene que invertir en ellos para generar una mejor experiencia. ¿Qué puedo hacer para invertir en mí? Pues que según una vez tienes que mejorar tu forma en que comunites, en especial cuando se trata de números. Al estar tú presentando, también es importante que involucres historias. Pero aquí se trata de historias con datos. O sea, no no más es que te voy a contar la novela. Simplemente es a través de mis números te voy a ir contando exactamente qué está pasando. Y algo importante también es que todo esto lo hacemos via software. Estos también maestrían nuestros herramientas tanto en herramienta de análisis como en la herramienta de presentación. Son tres cosas que te pido, que bueno, que te pido, que te recomiendo, que manejes en lo individual. Ahora nosotros en Comunicación América podemos ayudarte con esto. Lo que no podemos ayudarte, pues es esta parte. Y eso estamos en Comunicación América para servirte.
DOC0115|Data Storytelling|convence, provoca más datos. Mi nombre es Rodrigo Márquez y yo ayuda a personas a transformar sus ideas, a expresar sus ideas a través de los datos. Efectivamente todo el tiempo está trabajando con datos y hoy estamos en un tsunami de información. Tenemos pero tablas y tablas enormes que de alguna manera lo que trató de sacar de ellos o lo que hacemos la mayor parte de nosotros, tratar de sintetizarlos con unos informes más simples de entender, tratar de que sean más autorespicables. Hay que preguntarnos bueno y para nuestra audiencia ¿y esto qué? ¿Qué esperas que yo obtenga de esta información? Independientemente de esto lo que busco es convencer a mi audiencia. Quiero provocar acciones en ellos pero usando mis datos. Cuando hablamos de convencer a qué nos referimos y quiero plantearte aquí una serie de pasos que tienes que tomar en cuenta cuando tú realmente quieres convencer y provocar a una persona usando tus datos. Y quiero hablarte del primero de ellos que es el más básico de todos pero que la mayor parte de nosotros pasamos por alto. Que es que tengas buenos datos, que es que tus datos realmente sirvan para hacer buenos análisis porque si esto no está bien hecho todo lo que hagas después va a estar mal. Y muchas veces pasa esto que no nos dedicamos el tiempo de crear una buena base de datos pero toda esta parte masiva de datos está sin orden. Imagínate tú que entras a tu carpeta de imágenes que te encuentras con esto. Ahí está toda la información pero es difícil que puedes acceder a ella. Uno requiere esta información, requieres clasificarla por diferentes categorías, fechas, lugares, lo que haga falta para que tú puedas sacar mayor provecho de ellos. Una base de datos podemos entenderla como una colección organizada de datos. Si tú quieres convencer a alguien tienes que empezar primero teniendo buenos datos. Le hablo como si quiera ser buen guiso, requieres buenos ingredientes. El siguiente punto te quiero hablar de poner foco en el mensaje. Y aquí es donde hemos perdido pues de alguna manera esta parte de poner atención a que es lo relevante. No la pasamos poniendo todo tipo de gráficas, tablas que estamos haciendo, mostrando información. Pero realmente, ¿cuál es el mensaje? Hemos perdido de vista que tenemos que hacer que estas cosas que son complejas de entender se vuelvan simples para otras personas. Aquí por ejemplo podemos tener una lámina que habla de la porcentaje de población que vive en áreas urbanas y con una proyección hace 2030. Esto lo puedo mostrar como esta gráfica de barras o bien puedo mostrarlo de esta forma poniendo foco al mensaje. Quiero que veas que los diferentes continentes como van avanzando cada vez más gente vive en áreas urbanas y finalmente norteamérica es la población que en el 2030 va a acabar más en áreas urbanas viviendo en ciudades. Es importante que demuestres dominio del tema. Tengas tu conocimiento de qué está pasando para que lo puedas expresar con mucha claridad. En el siguiente punto quiero hablarte de para lograr convencer a la persona es que nunca dejes un número solo. Imagínate que estamos haciendo resultados de 2021. Pero al final cuentas este resultado no nos está diciendo pues exactamente de qué se trata. Pero puedes tú compararlo. Podemos compararlo contra el objetivo. Si es que existe compararlo. No dejes solo nomás la cifra. Si no existe objetivo pues entonces búscate los históricos y compara contra esa información. Si no existen históricos porque es la primera vez que se hace entonces compara contra algún pronóstico, algún estimado, algo que diga de qué esperamos. Si no tienes idea de cómo comparar el pronóstico puedes agarrar y tal vez buscar un benchmark con la competencia alguna otra de la compañía. Algo que te permita hacer una comparativa. Si no tienes si no existe un competidor o nadie más así lo que tú haces bueno algún tipo de estándar norma hizo no de mil algo tiene que cumplir. Pero nunca dejes un número solo siempre da una comparativa para que puedas convencer a alguien porque fiel en cuentas esta comparativa va a permitir que tan bueno que tan malo es el número que están buscando. No porque tengas un número puede ser significativo. Siempre acompaña con alguien más. Otro caso para que logres provocar convencimiento a una persona es muestra el beneficio. Pregúntate cuando estás tú presentando información bueno que con esto. ¿Por qué razón estoy viendo esta información? En esta lámina estoy presentando tres plantas diferentes y en las tres plantas me están dando resultados reales y al final las tres plantas me están presentando un pronóstico de cierre de mes. Entonces ¿por qué estoy viendo esta información? Si a mí me están presentando pronósticos una forma que puedo ver o tratar valor en mi información es hablar bueno y qué tan confiable es este pronóstico. Por un lado voy a mostrar las plantas pero con una gráfica que me muestre con mayor claridad cómo está la tendencia. Donde puedo ver que esta tendencia está al piso están pero en el hoyo y luego me está mostrando un pronóstico que mágicamente van a recuperar. Mi grado de confianza es bajo. La siguiente planta con la misma gráfica tiene una tendencia positiva por lo cual su pronóstico es confiable y finalmente la última planta ha estado cambiando mucho esa tendencia por lo cual el pronóstico es medio reservado. Y a eso me refiero con que demos valor a la información. Cuando me refiero muestra el beneficio hay mucha confusión porque la mayor parte de nosotros pensamos es que tenemos que mostrar el beneficio económico y no va tanto para ahí que tú te esfuerces a mostrar el beneficio económico. Cuando me refiero a que muestra el beneficio para convencer a una persona lo que estoy buscando es muestra el beneficio de la información. Esta información en que me está agregando valor a mi toma de decisiones. Esto que a fin de cuentas no entendía que está pasando muestra una forma que yo pueda entender con claridad qué es lo que está sucediendo. Hemos estado trabajando en esta parte de convencer y el siguiente paso es un poquito más complejo porque es provocar. Aquí si me interesa influir en tu decisión, aquí me interesa persuadirte. Me interesa que cambies de opinión respecto a un tema para convencer con que muestres buena información, bien construida, bien graficada, con colores adecuados, tú vas a lograr convencer. Pero para provocar, ahí estamos hablando de otro animal, requieres tú manejar emociones. El cerebro tiene dos hemisferios, un hemisferio que ve la parte analítica, pero hay otro hemisferio que ve el arte, ve la espiritualidad. A la parte analítica tú la vas a convencer, pero si quieres provocar tienes que llegarle a la parte emocional. ¿Cómo? Le hago para meter emociones a mis datos. Entonces como estamos hablando de emociones vamos a hablar del paso 5 que es hazlo colorido. El color es algo muy emocional. Definitivamente te pueden apoyar mucho en tus datos a seres emocionales. Por ejemplo, si estoy hablando de ciertos elementos donde estoy involucrando marca, no basta cometer alguna imagen, cometer algunos colorcitos. Cuando me refiero hazlo colorido es que realmente metas pero acelerador al color. Cuando tú involucras elementos de marca a la compañía, elementos de marca al cliente, logras mucho levantar ese aspecto emocional. Si aquí estoy mostrando usuarios activos de la red de Snapchat, tal vez ya con esto, con la limpieza en esta gráfica, con su claridad muy bien construida, estoy logrando convencer que están creciendo los usuarios activos. Pero si quiero emocionar tengo que meterle algo más y hacerla colorida me ayudará mucho a ese propósito. El siguiente punto de hacerla emocional para provocar, te puedo hablar de usar imágenes. Entonces aquí está la información desde el aspecto de convencer, a lo mejor está bien construida y puedo ver información con claridad. Pero emocionar difícilmente. Me interesa provocar a la persona. Y ahí involucrar imágenes siempre me va a ayudar con esta foto panorámica de la noche, en este caso de Asia y México, puedo provocar esto de generar ese apego emocional. Tendré muchos reportes como en este caso estaré hablando de el uso de productos de higiene. Algo que te funciona muy bien es involucrar imágenes y contenido de tu cliente dentro de la presentación. Para de alguna manera hacer ese apego emocional de mi interés involucrarme. Y esto es otro buen tip de cómo puedes involucrar imágenes. Con los datos de convenso, con las emociones te provoco. Y eso es lo que estamos logrando con estas cosas. Mi siguiente punto es haz lo inesperado. Haz cosas diferentes. Porque para el encuentro recuerden que las gráficas es un plano. En este plano tengo coordenadas y puedo graficar muchos puntos. Tengo coordenadas y así construyo mis gráficas. En una gráfica de expresión puedo hacer diferentes cosas. Por ejemplo, imagínense que yo puedo graficar unos puntos y estos puntos los conecto con una línea entre sí y esta línea le saco líneas perpendiculares esa línea con otras coordenadas. Todo esto son valores matemáticos que están graficados. Esta cosa rara que ves en pantalla para qué te sirve, como te decía para presentarlo y hacerlo inesperado. Esta es una gráfica de barras que está puesta en espiral. Pero a fin de cuentas cumple con esas coordenadas que te mostré. Es difícil hacer esto en Excel, pero esto me refiero con algo inesperado. Puedes irte a casos mucho más sencillos como este que está mostrando una gráfica de barras apiladas. Pero a fin de cuentas qué tan relleno está el vaso. Esta te representa el dato. Arriba tienes etiquetas, pero esto es muy sencillo de hacer. Es tu gráfica de barras muy corriente y lo que tienes que poner es una máscara, que en este caso un vasito, pero es algo que tú creas y le pones encima la gráfica para lograr ese efecto. Aquí tenemos otro caso como un diagrama de Sanke que está hablando de en la India los tiempos que se quedan sin internet, cuando hay apagones del internet y cómo a lo largo de los años cada vez hay más apagones. Esto es la parte de datos. Si quisiera hablar, bueno, no me interesa parte convencer, puedo agregarle esto. Es decir, yo tenía no más esta parte que es mi parte racional y le agrego este como cablecito que te está hablando a tu parte emocional. Oye Rodrigo, ¿tengo que presentar los resultados así? No. Líneas, barras, anillos o tortas son bastante eficientes, son fáciles de entender. Todos las conocemos, pero te digo que si involucras dentro de reporte estas cositas, estamos llegando a esa parte. Esto está interesante, esto es diferente, esto es inesperado. Y mi último paso será hacerles el pastel que es meter historias. Todo mundo quiere meter historias y está bien porque las historias tienen su propósito, pero aquí estamos hablando de datos y cuando hablamos de datos lo que queremos ver es que esto demostra historias con datos, eso es data storytelling. Y data storytelling requiere un enfoque con mayor estructura. Para hacer un buen trabajo de data storytelling tú requieres tener bien manejado tus datos, crea buena base de datos. Recuerda que los datos son los ingredientes del guisado, buenos ingredientes hacen buenos guisos, ten buenos datos. En segundo lugar, es algo importante que tengas una adecuada narrativa. La narrativa es lo que realmente quieres decir, pero debe estar soportada por tus datos. No es una narrativa como un cuento, no, es resultado de foco de análisis. Y finalmente todo esto conectado con visuales. Esas gráficas muy bien construidas, muy bien seleccionadas, con buenos colores que atraen la atención y aquí juegan toda esta parte, meterle imágenes, meterle formas, meterle colores, para llegar a lograr a tocar esas figuras emocionales. Meter historias es algo que vemos todos los días. Este es un poster de una campaña para recaudar un antes, para juntar dinero, para hacer institutos que ayuden a gente con discapacidad. Pero fíjate en la forma en que cuentan la historia. No te hablan es por favor, donale dinero. Te están hablando de Diego y te están hablando de Diego. Diego es terco. ¿Por qué es terco? Porque creyó en su discapacidad y hoy lidera su equipo de básquetbol. Entonces la historia de Diego. No te están diciendo por favor, dona, la historia es oye, ¿por qué no le echas una mano a Diego? Porque a fin de cuentas, 63% de la audiencia recuerda historias y un 5% recuerda estadísticas. Tengo este ejemplo de una historia publicada en el periódico El País. Está hablando de la cantidad de refugiados que están saliendo de Ucrania debido a la guerra con Rusia. Pues está marcando el flujo de personas que se van de Ucrania a diferentes países que tienen vecinos. Cada punto que ves en el mapa equivale a 300 ucranianos. Esta es la parte más visual, más bonita y luego ya entra una parte de gráfica. Y aquí está mostrando cómo a lo largo de los días más y más y más refugiados se van anexando a las personas que salgan de Ucrania. Después volvemos con gráficas. En esta parte es una gráfica de barras apiladas y te está mostrando la cantidad de refugiados que hay hoy y la cantidad de la proyección de refugiados que puede haber después. Estos pueden llegar de un millón y medio a cuatro millones de personas y así está dividida. Posteriormente regresa a usar un mapa y el mensaje es y los puntos donde salen los trenes en Ucrania están llenos de personas. Se está marcando en el mapa las diferentes estaciones más importantes y eso ya no cabe nadie. Después para conectar el reportaje con esta parte emocional va a imágenes y ese grado de desesperación que tienen de salir de ese lugar. Y nuevamente mostrando imágenes que en este caso son el atasque que hay de vehículos en los puertos de frontera. Finalmente, remata es y todo esto, el final de la historia es que va a ser un conflicto para la Unión Europea. Esto es una manera de contar una historia. Te está involucrando imágenes, está siendo inesperada, está manejando colores, está poniendo gráficas muy convincentes de alguna manera que tú del dato no dudas. Eso es una forma interesante de cómo contar mi historia con datos. Entonces, a manera de resumen, datos tenemos de todo. Tenemos ya todas estas láminas llenas de información atascadas de filas y columnas, entonces no tenemos que preocuparnos por datos. Si queremos convencer, pues tenemos que hacer que el dato hable por sí solo, pero tenemos que tener técnicas de cómo hacer esto mucho mejor. Si quiero realmente convencer con esta lámina, pues demanda mucho tiempo y esto es lo que estoy logrando con convencer. Sin embargo, con esto estamos logrando nomás una parte de nuestro foco que es la parte convencer. Si quiero provocar, pues tengo que también irme hacia el lado emocional. No basta con tener láminas con mucha construcción en cuanto a elementos gráficos, imágenes, información. Hay que hacer algo más. Esta misma lámina que vamos en pantalla la puedo dividir en más láminas. En primer lugar, generar una imagen que de alguna manera dé entrada al tema que vamos a hablar. Después, sigo recorriendo imágenes, dando el espacio y empiezo a mostrar información que viene otra lámina, pero lo que viene con datos me apella con gráficas. Gráficas muy sencillas que no estén sobrecargadas de información y finalmente siguiendo trabajando en información con mi audiencia, ahora sí, manteniendo imágenes, manteniendo colores atractivos, hablar de este plan de acción que vamos a hacer. Toda esta información que ves en estas láminas ya estaba aquí. Lo que estamos haciendo es tratar de involucrar todo lo que vimos de historias, cosas inesperadas, darle color, meter cosas nuevas para tratar de hacer eso inesperado, cosas que te sorprendan para hacer cuentas sin dejar de perder el foco en qué es lo importante. ¿Por qué es importante aprender de estas cosas? Y aquí mi reflexión es cuando tú presentas información estás tú anclado a la información. Si presentas información que no es legible, que no se entiende, que no me convence, en automáticamente no te entiendo a ti y tú no me convences. Y al contrario del otro lado de la moneda, si haces algo muy claro, algo muy convincente, entonces tú también eres convincente. Queremos convencer, provocar, pero usar tus datos. Hoy que no sabe manejar eso, tiene una desventaja competitiva contra alguien que si la sabe manejar. Por esa razón, sí los animo a que se capaciten y que en fin de cuentas inviertan en ustedes. Nosotros ayudamos a que la gente represente sus ideas con datos. Mi nombre es Rodrigo Márquez, te deseo mucho éxito, que les vaya muy bien y hasta luego.
DOC0116|Data Storytelling|Hoy todo es presentaciones y vamos a la junta de resultados y es la presentación, vamos a ser un cliente, vamos a hacer presentación y tenemos el PowerPoint para todos lados. Es el ejercicio más importante de comunicación en MES, donde realmente decimos mira así están las cosas, donde estoy jugando con comunicación visual más comunicación verbal. Y entre las dos, lo que yo quiero es que mi audiencia resuena el mensaje que estoy transmitiendo. Sin embargo, lo que yo estoy haciendo es presentando mis resultados. La famosa presentación de resultados. Este son el ejercicio donde presenta a otros colegas en la empresa que lo que hago, lo que tengo a cargo, los análisis que realizó, pues cómo salieron. ¿Qué porcentaje de las presentaciones que tú asiste son aburridas? Estos ejercicios efectivamente van a ser bastante aburridos, pero son los resultados, no? Al final de cuentas son lo que está pasando en la empresa, es una lástima que sea de los peores momentos que pasen en MES, donde realmente voy y es donde me vuelvo más productivo en contestar mis mensajes atrasados porque no estoy poniendo atención a lo que me están dando. Y generalmente esta parte de que las presentaciones son aburridas, hay dos razones en particular que encuentro por las cuales iban a ser estos ejercicios tan aburridos. Por un lado es porque hay poca claridad en el mensaje que queremos dar, o sea, se pierde esa cuál es el punto de que yo esté aquí. Y también el otro punto es que la audiencia, no me interesa, la audiencia o el presentador no se preocupa por la audiencia, que se aburrieron, que están en celulares, que no tienen preguntas y realmente no me importa. Y eso es algo que también aporta mucho, por lo cual estos ejercicios se vuelven tan aburridos. Hablábate del primer caso, de cuando hay poca claridad en el mensaje, pues efectivamente, no? Si presento entre la audiencia láminas como esta, pues estoy aburriendo todo el mundo, o sea, hay demasiados datos, demasiada información, estamos hablando que es una comunicación visual y verbal y aquí ya la comunicación visual es tan saturada que ya dejo de poner atención a lo que la persona me está diciendo. Entonces, oigo un bla bla bla y estoy viendo y tratando de leer que dice la lámina, pero fíjate en cuentas esto es demasiado distractores y al tener toda esta sobrecarga de información pierdes foco en qué es lo que tú quieres comentar y obviamente esta audiencia se siente como que no sé cuál es el motivo, ¿no? ¿A dónde quiere llegar esta persona? Esto honestamente tiene muy poco que ver con la herramienta, no importa la herramienta que usas, casi todas son iguales, son diapositivas en blanco donde tú las rellenas de información y esa diapositiva va actuando en secuencias muy parecida al ejercicio que estoy presentando en ese momento. No es tanto tu habilidad para usar la herramienta, es más porque tenemos otros preconceptos como que metióse la cabeza. Tenemos ideas como que si metes más es mejor, si se ve complejo es que está muy bien hecho, es una persona astuta o si ve excesivo la cantidad de datos es que hay mucha análisis detrás de eso y la verdad es que las tres cosas son totalmente mentira, ninguna de ellas te va a llevar a hacer un claro mensaje en tu presentación de resultados. El siguiente punto es la irrelevancia de la audiencia, que realmente no nos interesan estas personas, que cuando estamos presentando es todo acerca de mí, todo es de mis datos, mis metas, mis resultados, mi equipo, mi todo, yo, yo, yo, yo, yo para todo y estoy llenando mis láminas con informaciones acerca de nosotros, nuestra empresa, nuestro mercado, hablando de nuestros productos, lo que hacemos y lo que queremos hacer y a fin de cuentas es, pues me pregunto si todas las informaciones que estás poniendo ahí realmente son de interés para las personas que te están escuchando y es si yo me intereso pero que estas personas quieren oír y muchas veces estoy más orientado en lo que yo quiero decir más que en lo que las personas desean escuchar y ahí es donde falla mucho, es buena aburridad porque realmente la persona que me presenta información no se está preocupando por lo que a mí me interesa escuchar y es porque si les pongo esta imagen todos nos vemos aquí como el héroe, no desde yo, yo soy la estrella y la verdad es que es una mala postura de nosotros como presentadores a suminos en este rol, nuestro rol no está en el héroe, nuestro rol es la persona que está colgada atrás, el Yoda, nosotros somos el guía de la audiencia, la audiencia debe ser nuestro héroe, la audiencia es lo que nosotros estamos buscando que las personas sientan que tuvieron valor de la presentación que se puede hacer, entonces yo debo guiarlos y debo estarles comunicando esa información para que sientan cuando acabe esa junta las personas que me presentan que encontraron algo de valor en la información que presenté. En sí que está buscando lo que estamos buscando en cualquier presentación de resultados, buscamos varias cosas dentro de lo que es una presentación de resultados, por un lado estamos buscando tener un lenguaje común de comunicación, queremos que a la hora que te comunico a ti información y otras personas también me comunican su información, tengamos un lenguaje común, pero el punto es que no nos hemos educado en cómo comunicar todo esto, y cada persona presenta diferente y cada quien tiene una visión distinta de cómo debe ser comunicar esa información, entonces veo todo tipo de cosas bien creativas que no me están ayudando a tener, pues así que una lectura de cómo leer estos datos. Otra cosa que buscamos en la presentación de resultados es que hagamos las cosas simples, son complejas pero trata de hacer algo que sea simple de entender, entonces yo puedo tener muchos números pero me tengo que preocupar por añadirle simplicidad a cómo les los datos y entonces tengo que pensar en cuál es la mejor forma de que te llegue más claro el mensaje sin que te satúre toda la información. Algo muy importante que buscamos en la presentación de resultados es por qué pasan las cosas, por qué este número que estoy viendo sucedió como se dio. Tengo que decirte bueno este resultado que ves es así por estas causas o qué va a pasar después entonces si esto no lo corregimos, eso se esperaría en la junta de resultados más allá de sentarnos a ver gráficas, las juntas de resultados se han vuelto juntas para ver reportes y el reporte es algo que te lo puedo enviar por correo electrónico y no lo quiero yo estar aquí presente para ver un reporte pero si requiero estar presente para platicar y discutir qué tenemos que hacer, entonces muéstrame el por qué de las cosas. Y otra cosa que buscamos en la presentación de resultados es cautivar a la audiencia que se sienta que realmente encontró algo de valor y motivarla a actuar, que las personas vean oye deberíamos hacer algo eso me interesa mucho que mi audiencia se sienta pues es así que que le toque fibras y que ya sabes que esto no puede estar como está, tiene que tener algún tipo de cambio. ¿Cuál es el objetivo de una presentación de resultados? ¿Cuál debería ser lo que tú quisieras lograr esa audiencia? Pon claridad en el mensaje que queda mucha simplicidad que me quise decir y preocupate por la persona, preocupate por las personas realmente ¿entendieron? ¿no entendieron? Si está claro el mensaje, si consiguieron lo que ustedes buscaban en mi presentación, si están viendo valor en lo que estoy presentando y si no hay que volverlo a hacer. En algunas ocasiones pasa que en la junta de resultados oye a ver ponlo por mes, a ver ponlo por producto, ahora muestra por cliente, ahora muéstralo por proveedor, entonces esa información es muy dinámica y ahí conviene más a un dashboard pero es una herramienta distinta. El que entonces se resumió reunido en un meeting no significa que estás en una presentación, pues estamos viendo el reporte y eso es lo que está pasando en la presentación. Cuando vas a una presentación no solo son las láminas también te valúan a ti, tú también sales embarrado dentro de esa evaluación de qué también está hecho esto, si está mal hecho tú también te llevas la mala nota, si está bien hecho tú te llevas las buenas notas y no es que sea algo diferente, es porque presentaste los números de una manera con claridad y preocupándote para la audiencia. Nosotros somos comunicación América, estamos en comunicacionamerica.com.mx, ahí nos puedes encontrar, te deseo lo mejor y mucho éxito en todas las presentaciones de resultados que llegues a hacer.
DOC0117|Data Storytelling|¿Cuál es una forma típica que todo mundo usa para crear animaciones? pues uno cuando puede animar puedes poner animas en la serie y tú puedes poner que se anime por pedazos. Animar por serie lo que hacemos es puedo hacer, si es una gráfica de líneas, puedo hacer que esta lámina se mueva de esta forma y esta otra línea se mueva también de esa otra forma. Vas a encontrar 5.000 tutoriales en youtube como hacer esto y finalmente puedo poner notas de qué está pasando. La animación es para soportar tu historia, en lugar de mostrarte esto, que son tres fases, una fase de arranque, una fase de reposicionamiento y una fase de pérdida, puedo hablarte mejor en láminas distintas. Arranco con el pedazo de la gráfica, con la fase de arranque, luego sigo trabajando con la gráfica, con la fase de reposicionamiento y la primera parte pasa a transparencia y finalmente la tercera pase, las otras dos pasan a transparencia y esto es una mejor forma de cómo ir acompañando mi historia. Otra forma es complementar tus gráficas con formas y aquí hay un comando que tiene Powerpoint que se llama transformación o morph, dependiendo del idioma, un cuadrado te puede ver círculo, un círculo te vale un cuadrado y eso la hace software, la hace un automático. ¿Pa qué me sirve eso? Por ejemplo, en esta gráfica yo estoy tratando de vender la idea de que al no ser capaz de instalada se va a ver por debajo de lo que demanda el mercado, pero el punto es que sigamos produciendo para que ese exceso de capacidad podamos atender el pico de demanda que tenemos a fin de año. Entonces una forma que puedo hacer es trabajar con animaciones, voy planteando el caso, en esta parte roja nos vamos a sobreinventar y ar para después con animación transformar con este comando y vamos a atender eso. Entonces esto causa mucho mayor engagement en mi audiencia y las personas se sienten mucho más atraída a la información que estoy presentando simplemente porque maneje las formas. Finalmente te quiero presentar que también en Powerpoint puede ser animaciones mucho más avanzadas con este comando de transformación. Por ejemplo, yo tengo aquí una gráfica, voy a hablar de dos décadas que ha pasado del valor del peso mexicano contra el dólar. Entonces tengo aquí que 2001-2006 este fue el comportamiento, arrancó más o menos menos de 10 pesos y se rompió casi un poquito arriba de 10 pesos. Sin embargo, para los siguientes cinco años que viene la animación, ya esto se empieza a disparar el precio. Para los siguientes cinco años que viene la animación y empieza ahora sí el precio ya aquí la paridad cambiaria se fue al cielo y en los últimos cinco años y viene la animación y sale ahí cuál fue el siguiente punto. Para lograr presentar de una forma creativa toda tu información en números, sí pasa con que potencialices más cómo vas a tu herramienta. Pero es muy importante que te pongas a estudiar todas las herramientas de formato gráfico que te va a permitir ocultar series, cambiarle de color, ponerle degradados, cosas que te ayuden a cómo presentar información mucho más clara y de una forma diferenciada.
DOC0118|Data Storytelling|Vamos a iniciar con hacer una infografía. Primero empecemos definiendo qué es una infografía. Las infografías son elementos con mucha carga visual que nos están explicando alguna historia, algún detalle, algún evento en particular. Son muy recargados de imágenes, a veces son muy cargados de datos, pero definitivamente son elementos de mucha densidad visual. No son elementos ágiles de leer, son muy atractivos y son muy enfocados a un tema muy particular. Déjame hablarte entonces de elementos que traen todo tipo de infografías. Hablando en especial del uso de textos en infografías, los textos son muy usados para comunicar información en números. Simplemente esta nota periodística ve la cantidad de información en números que está expresando. Sin embargo, al hablar de infografías queremos el texto desde un punto de vista mucho más visual. Combinando lo que son textos con datos, tenemos también esta parte que son las nubes de palabras y esto lo que hace es, entre más grande la palabra es que tuvo más repeticiones. Ve este ejemplo de un discurso que hizo Obama, donde analizaron todas las palabras que hizo el presidente de Estados Unidos y cuál es la palabra más repetida dentro de su discurso. Nube de palabras es una forma interesante de cómo involucrar textos en infografías. Algo también que usa mucho las infografías en lugar de meter gráficas, simplemente con textos hacemos mucho más ágil la lectura de la información que en lugar de llenar todo ese espacio lleno de gráficas. En el siguiente punto vamos a hablar de tablas en infografías. Las tablas son elementos en los cuales analizamos una gran cantidad de información. Sin embargo, hay que distinguir al momento presenta una tabla entre una tabla para analizar información y una tabla para presentar información. La tabla para presentación información es una tabla mucho más ligera y con elementos visuales mucho más atractivos. Estas son tablas usadas en infografías que no te sirven para hacer análisis porque su objetivo no es ese, es presentarte información muy limpia, atractiva y muy ágil de leer. Hablamos de gráficas para infografías, hay que entender que la gráfica a fin de cuentas es un espacio donde estamos graficando puntos en un plano de dos dimensiones y tenemos cualquier punto de esto requiere una dimensión en X y una dimensión en Y. Pero en el caso de infografías puedo colocar ahí puntos con distintas coordenadas, puede ser tan creativo y tan novedoso como yo quisiera. Por ejemplo, en este espacio yo puedo estar graficando puntos y esos puntos al final conectarlos por medio de una línea y esa línea que conecta esos puntos puedo también sacar líneas que son perpendiculares a esa línea. ¿Para qué me sirve esto? Pues para hacer cosas como esta. Esto es exactamente esa misma configuración de cómo estoy graficando todos esos puntos en ese plano de dos dimensiones. Y en infografías entonces puedo usar todo este sistema de coordenadas para crear elementos mucho más robustos y que permitan a la audiencia cautivarla e interesarla con gráficas mucho más novedosas. Ahora sí, combinando todos los elementos, en una infografía puedo usar textos, tablas, gráficas y no te he mencionado uno hasta ahorita que son el uso de imágenes. Hablando de infografías muy cargadas en imágenes, aquí tenemos una que habla del coronavirus, donde con imágenes está explicando el origen que puede tener el coronavirus y cómo puede llegar a distintos animales hasta llegar al ser humano. Podemos tener infografías de mucha lectura, no son elementos ágiles de leer, pero tiene mucho cargadas de imágenes, iconos, figuras y nos causan interés en la lectura que tenemos que hacer. También podemos tener infografías del otro a la moneda, muy cargadas en gráficas y aquí lo que estaremos buscando es algo que dé un poquito de balance. Este es una infografía interesante de cómo podemos hacer una infografía donde los datos juegue una parte importante dentro del contenido de la infografía. Vemos en la parte superior que tenemos unas gráficas de barras apiladas con unas imágenes de billetes y algunas figuritas dependiendo que estén hablando. Si a esta infografía vamos avanzando, pues efectivamente seguimos viendo gráficas, siguiendo figuras, colores muy uniformes para que no contaminen y tenemos mucha carga de textos que me está explicando cómo entender cada uno de los elementos que hay dentro de esta infografía. En el mundo de los negocios tiene mucho más sentido este tipo de infografías que son mucho más claras, no tan robustas, que te pueden ayudar para transmitir con mucha claridad qué está pasando. Infografías en los negocios puede ser mucho para lo que son líneas de tiempo y explicar en algún tipo de proyecto cómo van avanzando en distintas etapas y dejar muy claro cómo se van desempeñando distintas acciones. Infografías en negocios también se puede hacer muy bien para lo que son diagramas de proceso y las distintas fases y ahí las infografías son elementos que nos generan mucho más interés en la información que estamos viendo. Déjame hablarte de algunos recursos que te van a llevar a crear muy buenos elementos para involucrar en tus infografías o en tus presentaciones resultados. Todos los portales con saber son elementos de infografías hechos en herramientas de office en especial en powerpoint. El primero de ellos se llama Awesome Presentations. Es un portal de vídeos que te va llevando mucha información de muchos tips de cómo crear elementos que realmente le den una vuelta en la forma que haces tú tus infografías. El siguiente portal es de Andrew Patch. Andrew Patch también tiene muchos tips de cómo crear animaciones, cómo crear infografías para negocios, cómo crear muchos pequeños elementos que puedan llevar a potencializar mucho más tu uso de la herramienta powerpoint y a crear cosas realmente espectaculares. Tenemos C Graphics, también un canal para crear distintos elementos gráficos y aquí este canal me gusta porque te plantean cosas que son muy sencillas de hacer y que pueden dar un contenido emocional mucho más interesante a tus presentaciones. Tenemos Powerpoint School, que este portal también muy enfocado en la parte negocios y me gusta porque aquí tiene muchos slides que te pueden servir para dar introducción a ciertos temas o armar un índice. Es muy interesante. Tienes aquí Power Up con Powerpoint y también tenemos muchas de estas diagramas de proceso que te lleva llevando paso a paso cómo hacerlas y también tienen algunos templates que son descargables. Tenemos The Point, me gusta porque tiene mucha elegancia a la hora de construir sus presentaciones y puedes encontrar información también muy valiosa y que puede ayudarte a destacar la forma en que construyes tus reportes. Y finalmente este es mi favorito, Creative Venus. Tiene mucha información de diagramas y cosas que puedes usar dentro de tus reportes y está con mucha claridad cuál es el paso a paso de construir estos elementos. Te lo recomiendo muchísimo, especialmente también que te veas todo el canal incluyendo los vídeos más antiguos que a veces esos te pueden ayudar mucho como ir creciendo poco a poco con el canal como fue construyendo cada vez información mucho más compleja. Uno no aprende viendo vídeos, uno aprende practicando. Y aquí te quiero dejar una propuesta de hacer una de las siguientes cuatro infografías. Haz la que más te guste, elige por tu gusto y no elige por complejidad. Si puedes hacer las cuatro mucho mejor, pero cada de estas infografías te va a hacer un vídeo que te va a llevar paso a paso a cómo generar cada uno de estos elementos para que tú puedas hacer unas réplicas posteriormente dentro de tus reportes. Pero que te sirva a ti para ir aflojando la muñeca y puedas ahora sí crear este tipo de elementos dentro de tus presentaciones. Este tipo de conocimiento requiere práctica, entonces entre más los hagas mucho más experiencia vas a adquirir. Nosotros somos Comunicación Médica, me da muchísimo gusto que te hayas tomado el tiempo de ver este vídeo, sin embargo recuerda que lo importante es que practices. Hasta luego.
DOC0119|Data Storytelling|vamos a hacer un pequeño ejemplo en vivo de una transformación simple de que podría lograr de transformar esta gráfica que vamos aquí y meterle algo que tenga que ver con la parte de branding esta es mi gráfica simple como un corriente voy a usar esta red social de snapshot y voy a usar estos colores para meterle branding a esta gráfica está acá arriba nuevamente voy a hacer un duplicado de la lámina y que es lo primero que tal vez me gustaría hacer a mí cuando presento la parte branding en particular me gusta hacer que se vea que predomine el color de la marca estoy haciendo tengo este logotipo en amarillo y lo que quiero hacer es que toda la imagen de la lámina se ve amarillo porque me gusta que tenga esa presencia muy clara entonces voy aquí y formato de fondo como no tengo el color del amarillo ni sé su código me voy a esta parte del gotrito y le pongo este amarillo para que toda la lámina sea amarillo voy a ponerle aquí el logotipo más cerca tengo aquí la imagen es una imagen no son letras entonces voy a ponerlo aquí para que esté bien alineado como todo logotipo está en negro pues todos los datos están aquí lo va a poner en color negro no que se note esa diferencia voy a usar que estas barras también las pueda colocar en negro y en negro y las voy a hacer que estén un poco más más anchas algo para que aprovechar ese color negro sobre el amarillo voy a usar pues voy a poner un poquito más de formato al eje vamos a darle algo de limpieza vamos a ponerle algo de qué de cómo están las diferentes líneas de separación líneas primarias secundarias aquí voy a poner que saque bueno está la línea secundaria está malas a que entonces estas líneas las líneas primarias las va a poner en color negro vas a ponerle un poco más de grosor y las otras vas a ponerle también en color negro pero aquí voy a hacer que estas líneas como son muy pesadas va a ponerles algo de transparencia voy a usar una letra como snatch es una letra medio regordeta voy a usar algo así algún tipo de letra que se vea un poquito más grueso que se vea que pinta un poquito más y voy a usar que esta gráfica va a poner algunos números que se vean un poquito más estilizados tiene el tipo de letras que es la corporación velo poniendo pero aquí tuve poniendo los tipos de letras que ya trabajes bien para que puedas irlo colocando la información que tú quieres como está este fantasmita del snapchat voy a poner otro cambio voy a hacer que las barras tengan fondo blanco pero un contorno negro como el logotipo de snapshot y este contorno negro va a poner muy muy grueso para que se vea esta parte de los tipos nasa aquí voy a ponerle también el eje de las x una línea gruesa porque así está el logotipo entonces vamos creando esta parte de ponerle brandon finalmente aquí encontré un ícono de esta parte del fantasmita que puedo irlo cambiando el color no puedo por lo que me permite a mí cambiarle de color ponerle relleno puede hacerle varias cosas entonces cómo puedo hacer esto pues voy a hacer algo interesante voy a poner aquí algo que le agrega un poquito un detalle adicional voy a usar esta imagen la voy a poner aquí la nota que sea como que el fantasmita está jugando en la gráfica para darle a un poquito de movilidad no voy a hacer que esto sea muy ancho y más ancho todavía y nuevamente vamos a ponerle que sea algo de transparencia y la voy a mandar al fondo pasé de esta gráfica común y corriente sin chiste y le puse un branding de snapshot puedes hacer otra cosa distinta si si pensamos más distinto sigamos poniendo usando este fondo amarillo me voy a traer este logotipo que me encontré por aquí esto nuevamente donde lo consigue es que está buscando el internet constantemente va a poner este fondo blanco y va a poner esta línea gruesa que viene el logotipo y está mi gráfica me la traigo para acá con su título finalmente lo que voy a hacer es voy a meterla dentro del fantasmita se ve interesante para meter la gráfica en el fantasmita me deshago este eje me deshago de estas líneas esto si lo voy a conservar y lo a poner una imagen más gruesa voy a poner estas barras también bastante gruesas que se vean casi pegadas unas con otras voy a ponerle negro porque se vea mucho más claro estos números creo que es mejor ponerlos dentro de las barras voy a cambiar las etiquetas que se vean en el extremo extremo interno y que tengan un fondo blanco esto que tengo aquí voy a hacerlo más pequeño que alguna manera sea se pueda ver y voy a ponerle que la dirección del texto la pongan así para que quede más clara la lectura dentro de mi gráfica realmente en color negro estoy colocando mi gráfica dentro fantasmita puedo hacer un poquito más grande el fantasmita pero en fin de cuentas es todo estoy colocando estos elementos puedo hacer un poquito más pequeño para que se pueda meter y ampliar un poco más solamente aquí como son los usuarios promedio por millones y cambio el tipo de letra algo un poquito más grueso porque así así me gustaría verlo voy a mover el fantasmita hacia un costado y voy a traerme esta parte de snapchat nuevamente y la voy a poner en el fantasmita porque se ve interesante aquí a fin de cuentas es tengo esta gráfica y ya puede meter esta opción para hacerle de branding o bien me fui algo diferente y pongo esta opción para meterle brand finalmente estoy metiendo branding a mis presentaciones resulta
DOC0120|Data Storytelling|de la historia. Por qué ahora todos quieren meter historias. Yo soy Rodrigo Marquez y soy un especialista en visualización de datos decía la escritora Maya ángelo las personas van a olvidar lo que dijiste pero nunca van a olvidar cómo las sientes. Acá de pasar. Y ahora yo estoy en la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la noticia de la Cuando metemos datos con Cuando metemos datos con Cuando metemos datos con historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, historias, Y algo que también necesito para hacer un buen trabajo de data storytelling es manejar muy bien mis visuales. Cuando mostramos información, mostramos información de forma ilustrada, manejo de color, adecuado manejo de espacios, selección de gráfica, todas estas cosas es importante manejarlas bien al momento de comunicar una historia. Teniendo buenos datos, algo importante contar el resultado de un trabajo de análisis profundo de tus datos y buenos visuales que sean fáciles de entender, uno puede alcanzar a tener un buen trabajo de data historica. Si yo mezclo, tengo algo que contar y mis datos, lo que estoy haciendo es explicándole a las personas qué está pasando con la información. Si tengo mis datos y los estoy traduciendo visuales, lo que estoy haciendo es ilustrar a la gente qué está pasando. Cuando estoy mezclando algo que contar y tengo visuales, lo que estoy haciendo es creando mayor compromiso, mucho más engagement con mi audiencia. Si logro estas tres cosas, que serían crear engagement con mi audiencia, explicando la información y de alguna manera ilustrándola para que sea fácil entender, entonces tengo una buena combinación y ahí puedo hablar de que estoy haciendo un buen trabajo de data storytelling. Pues déjenme ponerles, platicarles de un ejemplo de un muy buen trabajo de data storytelling que es bastante actual. Este es un artículo que sacó el Washington Post que curiosamente es la visualización de datos más vista en ese portal, es la más vista todos los tiempos del Washington Post y habla de por qué los brotes como el coronavirus crecen exponencialmente y cómo aplanar la curva. Entonces empieza el artículo primero presentando unos datos, esta es una historia contada con datos. En primer lugar, empieza empezando con el argumento de la curva exponencial, que esto crece súbitamente y a dos meses en el primer caso en Estados Unidos ya esta curva empieza a crecer de una manera impresionante, pero después para hacerle entender a la gente lo que habla es vamos a utilizar una simulación para entender es qué significa esta parte de la curva. Vamos a explicar antecedentes en mi simulación una persona sana puede entrar con otra persona y puede quedar enferma y así quedan todo el tiempo enfermos, pero esto no es lo que ocurre en la realidad. En la realidad podemos asumir que una persona ya que entra en contacto con una persona enferma con una persona sana se recupere eventualmente, entonces ya no vuelve a estar enferma. Entonces son condiciones para empezar a plantear la simulación. Entonces empieza primero vamos a simular una población de 200 habitantes y en el primer caso vamos a hacer que dos personas puedan poner dos personas enfermas y que se propaguen ahí con todos los demás y vemos eso como se hace un contagiadero y arriba tenemos una gráfica de áreas que nos está diciendo la cantidad de contagiados y la cantidad de gente recuperada y vemos que aquí como está este pico impresionante en la pandemia. Después vendría un caso de que vamos a hacer una cuarentena, entonces del lado izquierdo vamos a poner a todos los enfermos y los encerramos. Entonces empiezan a contagiarlos enfermos, algunos se empiezan a recuperar y luego lo soltamos, abrimos esa cuarentena y esperemos a que pase. Y lo que estamos haciendo aquí es que estamos, si se fijan, en la curva que se ve en la parte superior se ven las dos olas que están pasando. Entonces no saturamos tanto los hospitales y esto fue la ocasión en China en la provincia de Hubei. Luego después tenemos hacia adelante tenemos esta que se llaman los gobiernos de la parte distancia social y dice que un cuarto por ciento de la población continuamiento y tres cuartos encierra y a partir de ahí vemos como la velocidad de contagios es mucho menor y entonces la curva pues ahora se queda mucho más planita. Eventualmente todo el mundo se va a contagiar pero es mucho más lento y los hospitales no se aturan. Finalmente en último caso es el distanciamiento extremo, sólo un octavo por ciento de aprobación puede moverse y el otro siete octavos se quedan en sus casas y ahí lo que nos vemos es que prácticamente es el escenario donde la curva es mucho más plana. El artículo de Washington Post os tiene que explicar a la gente hay cuatro escenarios, el escenario sin ningún control, que hay un pico enorme de contagios, el escenario con una cuarentena y disminuyó la cantidad de contagios con distanciamiento moderado y con distanciamiento sonptivo que tal vez no sea posible. Y ahí termina la historia y fue muy vista porque en el cuenta se conectó muy bien con las personas. Estamos hablando que son simulaciones matemáticas, probablemente las personas que se han visto primero llegaron a este tipo de conclusiones. De aquí tienen todo estos análisis que están diciendo las simulaciones y cómo se debe ver las cosas, pero esto no sirve, esto no te sirve para conectar con las personas. Esto no más va a provocar puros espantos en la audiencia. Nadie va a querer estar viendo esta historia. Por eso es importante traducirla en términos simples donde todo mundo puede entender.
DOC0121|Data Storytelling|Dentro de mi historia quiero hacer los datos impactantes a la hora de que tú estás viendo por primera vez la información. Yo soy Rodrigo Márquez y soy un especialista en visualización de datos. Algo importante para causar impacto es entender que cuando llevamos nuestras tablas, nuestras gráficas, todos nuestros textos, pues a fin de cuentas sólo le estamos hablando a una parte del server. Le estamos hablando a la parte del cerebro que ve aquello del análisis, las matemáticas, la lógica, pero no le estamos hablando a la parte del cerebro, el hemisferio derecho, que ve la parte emocional, el espíritu, la música, el arte. Y a la hora de causar impacto tenemos que hablarle a los dos hemisferios. Sin embargo es difícil. Aquí estoy hablando de una correlación del beneficio de producir mayor atrocidad y es una correlación entre el piper cápita de México con la producción de electricidad total. Lo que estamos viendo aquí es que ahí están las estadísticas. Estos son los datos, pero a fin de cuentas no logro impacto. Y si quiero hablar al otra parte del cerebro, que ve la parte emocional, no puedo lograrlo con esto. Pero puedo usar otro tipo de elementos que sí me apoyen a lograr llegar a esa parte emocional y ahí puedo usar la parte de imágenes. Entonces estoy hablando de electricidad de México. Aquí estoy sacando una panorámica de la Ciudad de México con mejor tipo de letra y en ese gran cielo estoy mostrando los datos. Y aquí estoy entonces hablando de las dos. Que estoy por un lado mostrando las estadísticas, pero otro lado me estoy apoyando en mostrar esa parte emocional. Vamos a ver otro caso para nuevamente crear impacto con nuestros datos. Tengo aquí un estudio que está hablando de países que están reclamando territorio en la Antártida. Y ahí tengo un pie de cuántos kilómetros cuadrados está reclamando cada uno de estos países como parte de su propiedad. Y si quiero hablar la parte emocional, en este caso en lugar de usar una imagen, voy a usar formas. Y qué estoy haciendo aquí, estoy mostrando la misma información, pero al colocar esta máscara arriba de mi gráfica de pie, que es la forma de la Antártida, de alguna manera estoy llegando a esa parte emocional que todos tenemos y ahí conecto mucho más con la audiencia y logro un impacto. Slides como estas, donde tenemos, pues estamos mostrando aquí los resultados de medios digitales, mostrando el actual contepresupuesto, mostrando el crecimiento cuarto a cuarto, mostrando las conversiones también cuarto a cuarto, pues no es suficiente. Tengo que hacer un trabajo de mejorar la forma de ilustrar mi información y podría cambiar estos datos y los puedo presentar de esta forma. Aquí están mucho más pulidos, una mejor selección de gráfica y buscando más claridad a la hora de comunicar con mi audiencia. Pero todavía no logro causar ese impacto y puedo irme entonces a manejar más elementos, más imágenes y aquí estoy mostrando la misma información con colores más llamativos, con fondos de color en lugar de blancos y metiendo iconos que a fin de cuentas están haciendo más ese apego emocional. Solo tablas, solo gráficas, solo textos, no le van a hablar a la parte, a la hemisferia derecho que ve la emoción. Tenemos que involucrar otros elementos que hagan ese trabajo. Esta es una infografía muy interesante, muy profesional, hecha por la revista Time y habla de por qué en Estados Unidos están caros los servicios salud. Pero vamos a deslozar lo que estamos viendo en esta lámina y sobre todo todos aquellos elementos que están usando el diseñador de información para atraer esos elementos emocionales. Por una parte, este botella donde ponen los sueros efectivamente lo están usando como una gráfica, entonces la cantidad de milímetros corresponde con las cifras que está mostrando dentro de los 750 billones y los 2.8 trillones. Entonces eso está con cierta relativa escala que en lugar de poner una gráfica te está poniendo este elemento totalmente visual. Luego ve cómo te redacta las cosas con los textos, dice, bueno, dónde está el problema y a fin de cuentas está usando imágenes, donde aquí aparece unos, hasta un isótopo donde está mostrando los datos, está mostrado aquí en un segundo punto, bueno, qué es lo que realmente hace caro al sistema de salud y ahí en lugar de poner una gráfica te está usando pastillas. Entonces, dice, cuánto vale la medicina en los Estados Unidos, cuánto vale la Argentina, cuánto vale España, cuánto vale en Francia y en lugar de ponerte gráfica te está poniendo las medicinas. También sigue avanzando igual, en lugar de ponerte pás, te pone formas con los algodones o inclusive te está colocando una gráfica de barras hecha con curitas. Todos esos elementos apoyan la parte emocional. Finalmente, el punto número tres, ve el uso de textos. Bueno, entonces, qué podemos hacer respecto, dice el texto. Y ahí te vienen varias soluciones y te está poniendo solución 1, solución 2, bueno, las diferentes ahorros que puedes entender dependiendo de cada tipo de sumisión. Todo esto puede haber quedado como una infografía llena de gráficas y de tablas, pero eso no conecta. Entonces, para hacerlo y crear ese impacto, lo que está mostrando aquí es, pues, esos elementos que no son datos, que a fin de cuentas sí están ayudando a crear mucho más impacto.
DOC0122|Data Storytelling|tengo que hacer los datos que sean personales. Yo soy Rodrigo Márquez y soy un especialista en visualización de datos y aquí hay algo que la palabra personal me refiero a que le llegue a la gente. Cuando estás presentando información y gente de la audiencia está en su correo electrónico y no te mira, pues de alguna manera es porque lo que está viendo en pantalla no es importante, no está conectando conmigo, por eso dejé de poner atención. Cuando estás hablando de mi historia, en mi historia existe un conflicto, hay algo que contar, pues es importante que yo redarte entonces de qué tamaño ese problema. Trate de orientar exactamente con algo que pueda comparar la persona de qué tamaño ese problema para serla personal. Estoy hablando de distancias, estoy hablando de áreas, estoy hablando de volúmenes, pues puedo yo auxiliarme de otros elementos que las personas puedan relacionar con esto, al igual hablando de tiempo. Si algo es muy rápido o es muy corto o se hace relativamente fácil, pues de alguna manera tratar de buscar alguna referencia donde las personas puedan aterrizar a su día a día. Cuando Steve Jobs presentó al mundo esta la MacBook Air, el tipo salió con un sobre amarillo con el objetivo de que viéramos qué tan pequeña era esa esa laptop. Al sacarla del sobre amarillo, lo que está haciendo es ponernos en referencia con algo que todos conocemos. A lo mejor si muestras solo la máquina, no tenemos ni idea de qué tan pequeña es, pero al momento de sacarla dentro de un sobre amarillo, a todos nos queda una vida muy clara de qué está hablando y ahí conecto con la historia, se me hace personal porque he visto ese sobre amarillo antes. Entonces si están con todos mis datos, los trato de referenciar, si estoy hablando de áreas con tantos campos de fútbol o estoy hablando de volúmenes como un avión o del tamaño de tu dedo o del tamaño de un cuerpo espacial, lo que estoy haciendo es, en lugar de hablar todos estos datos, no más soltar los porcentajes, lo que estoy haciendo es volver la historia más personal al tratar de conectarlo con algo que la audiencia conozca. Un caso, este es una historia que sacó el portal de Reuters hablando de un problema de migración de musulmanes que estaban saliendo de mi amar y estaban llegando hasta la angatesia y empieza a hablarte de toda esta serie de datos, pero a final de cuentas con las referencias. Entonces está hablando que todo empezó más o menos inicio el principio de agosto y los primeros cinco días llegaron cerca de 15 refugiados de salida de mi amar. Después de estos cinco días, al siguiente día llegaron 255 refugiados. Sigue avanzado el tiempo y los siguientes cinco días después de esa fecha llegan 2055 refugiados. Y fíjense que no nos está mostrando las gráficas de refugiados, estamos viendo ahí cada una de las personas, que vemos la cantidad de personas, la cantidad de munizxos. Ya entre el 25 y 29 de agosto ya habían llegado entonces 18.445 refugiados. Y recuerden que empezamos nomás con solo 15. Si estos 18.445 lo veíamos como campos de fútbol o como cuadros de, aquí tenemos de cerca de 1000 refugiados, significa un cuadrito de esos, para septiembre de 2016 ya había llegado esta cantidad de refugiados y cada cuadrito que ves en pantalla son cerca de 1000 refugiados. Y te dice que más de la mitad de todos ellos eran gente que tenía menos de ocho años y como ya previo al conflicto ya habían migrado cerca de 197.000 personas, esta es la cantidad total de refugiados que están saliendo a mi alma. Los datos nos pueden decir con esto, mejor te pongo la típica gráfica y ahí están los refugiados y ahí están las fechas, ya estás viendo cómo crece la cantidad de gente que va a estar migrando hacia Bangladesh a lo largo del tiempo. Pero nuevamente es no conecta. En la otra forma cuando estamos haciendo referencia a tamaños en ese momento es, eso sí lo puedo ubicar, eso sí me es fácil de aterrizarlo y relacionarlo con alguna vivencia que tuve. También en nuestra historia algo importante también para hacerla personal es hablar bien de quiénes son los personajes de mi historia. Que en mi historia hay héroes y hay villanos pero tal vez en el mundo de los datos no hay tantos villanos y no hay más adversarios. Tengo que identificar dentro de mi historia, dentro de mis datos quiénes son mis héroes o mi héroe y quiénes son los villanos o el villano. Y aquí es más simple, cuando tú tengas que identificar para que puedas hacer más personal la historia y conectar más con las personas, tu héroe es todo aquello que está moviendo el dato hasta la dirección que tú estás buscando. Todo lo que ocasione que el dato vaya en la dirección que quieras esos son tus héroes. Y los adversarios van a hacer lo que haga lo contrario, todo aquello que está obstaculizando que llegues a tus metas, todo aquello que tengas que resolver para hacia adelante y llegar a los objetivos que estás buscando.
DOC0123|Data Storytelling|Esto es como le hago para contar mi historia con mis datos. Yo soy Rodrigo Márquez y soy un especialista en visualización de datos. Tienes que tener, combinar, algo que contar con datos bien construidos y con visuales muy bien hechos. Entonces esto significa un conjunto de disciplinas que tú tienes que manejar muy bien para que logres una buena combinación. Tu data story telling va a funcionar mucho mejor. Entretengas mejor aterrizado, cómo trabajar con cada uno de estos tres elementos que ves en pantalla. Hablando más del primero, si tú estás usando las herramientas de Office, yo te diría que ya déjate de las tablas dinámicas y clávete más al ecosistema Power que ha creado Microsoft. Tenemos Power Query, que es de carga, transformación e información. Tenemos Power Pivot, que imagínate lo que una tabla dinámica con esteroides. Y tenemos a Power View para graficar de forma sencilla. Lo que es Power Query y Power Pivot lo comparten entre Excel y Power BI. Power View, nomás existe en Power BI. Si tú manejas unas herramientas distintas de esto, simplemente tienes que buscar formas de hacer más con las mismas herramientas. Generalmente todos usamos un porcentaje muy poco de las herramientas que usamos para analizar información. Entonces tenemos que hacernos más expertos y esas herramientas que más nos pueden dar. El siguiente punto sería la narrativa. Aquí es muy claro, estamos hablando de historias con datos. Si no hay datos, esa es tu opinión. Si tú vas a expresar algo, es porque hay un dato atrás que los desaporta. Pero muchas personas no saben cómo entrar a los datos. Aquí hay siete formas de cómo encontrar historias de nuestros datos. Cambiar el tiempo, profundizar en información, alejarme de una variable pequeña, ver el todo, contrastar los extremos, buscar intersecciones en tendencias, factorizar una variable de qué hace que esa variable crezca, o sea, que otras variables impactan, o buscar puntos fuera de la curva, los outliers, para que tengas en cuenta que está pasando. Para buscar historias en mis datos, hacer una adecuada labor de exploración en mi información con análisis de una variable, dos variables y multivariables, sacando distribuciones, sacando análisis de cambio, sacando análisis de relación o análisis de comparación. Cada una de esas cosas, uno debe de mejorar su pericia para que yo pueda contar historias interesantes dentro de la información que yo posee. La última habilidad que hay que conseguir es la parte de manejo de los visuales. Hoy, para presentar información, estas son las herramientas más relevantes que tú tienes. La verdad es que todas ellas son buenas. El chiste es que tú puedas potencializarla más. En mi punto de vista, Powerpoint sigue siendo la herramienta más poderosa al momento de presentar información, y es involucrarte más en manejo de animaciones, pero no animaciones, no más porque salga la letra random. La animación está ahí para que soporte tu historia. Si la animación no contribuye a que la historia le haga más fácil de explicar, entonces no la uses. Aprende a usar mucho este comando de transformación que te puede transformar un cubo en un círculo, y eso te va a permitir generar animaciones que tú no te das cuenta que es Powerpoint, pero está metido ahí. Está es fabulosa la herramienta. Y empezar a manejar esta parte, a modificar formas, porque es importante involucrar muchos iconos o algunas infografías que te pueda hacer, y este programa es muy bueno para hacer eso, pero ya tienes que meter ahora sí a alterar los cubos, los rectángulos, los círculos, y darle la forma que tú deseas, pero logras presentaciones de muy alto nivel. Y está la otra parte de enfocar la parte de manejo de gráficas. Y ahí es importante de que logres seleccionar la gráfica correcta. Cada gráfica tiene distintos propósitos. Una vez que yo selecciono qué quiero hacer con mis datos, tengo gráficas que me van a ayudar más a cumplir ese propósito. Si ustedes piensan que el entorno de Microsoft, tanto Excel como Powerpoint, son chafas para graficar, yo te puedo decir que todas las gráficas que estás viendo en esta pantalla son ejecutables dentro de Excel y Powerpoint. Tengo un buen curso lleno de tutoriales de cómo hacer estas gráficas que están en pantalla. Te lo sugiero muchísimo. De todas maneras, lo que quiero decir es que no requieres comprar otra herramienta diferente para tratar de hacer slides impresionantes a tu audiencia. Analizar información no solo es datos y gráficas, es que tú sepas encontrar y que sepas cómo contarla. Una historia hace más memorable las cosas, una historia genera mucho más impacto, una historia la hace mucho más personal. Meter historias a los datos también va a servir mucho a lograr estos tres objetivos. Recuerda entonces Data Storytelling, es un conjunto de disciplinas donde requieres datos bien construidos, bien armados, una narrativa resultado de un análisis profundo de información y visuales muy bien construidos con técnicas de visualización de información. Lo que haces es lograr explicar la información, ilustrar a otras personas y crear ese nivel de compromiso. Te invito a nuestro curso Data Storytelling y vamos a ver todas las metodologías que tenemos listas aquí para que tú aterrices esto en casos de negocio. Porque fíjate en cuentas, ese es el entorno donde uno se muere. Créeme que en una presentación las historias venden. Cuando estamos en un entorno laboral queremos vender ideas. Si vamos con un cliente queremos vender el producto y nada funciona mejor. Con datos para que fíjate en cuentas es, es la realidad, no la estoy inventando, pero con una buena historia para que tú puedas ahora sí comprenderla y analizarla y transmitir exactamente que está pasando a la audiencia que está viendo. Nosotros somos Comunicación América, somos una academia en analgía de datos especializada en negocios.
DOC0124|Data Storytelling|Porque no todo sale como uno desea y muchas veces estamos presentando juntas resultados que no son favorables y tenemos reacciones de los líderes que tenemos que saber cómo anticiparnos a lidiar con esas reacciones. Resultados como el objetivo era lograr un aumento del 30% de los ingresos, pero el equipo sólo entregó 1.8%. ¿Cómo reacciona el líder ante esos datos? Nuestra aparición de mercado se redujo 1.5% y eso produjo una pérdida de 3.6 millones de dólares. Primero es ¿por qué existe la sensación de que hay más noticias negativas? Antes había mucho menor cantidad de datos disponibles. Las empresas trabajaban más por sí los cada área tenía sus datos en particular. El día de hoy tenemos sistemas que colectan una gran cantidad de información y las herramientas que hay para analizarlos son mucho mayores y mucho más fáciles de usar. Ahora se miden mucho más cantidad de indicadores y lo difícil es que en ocasiones la cultura de la empresa no está abierta a escuchar algo negativo y esta cultura de las empresas provoca algo muy parecido al cuento de Hans Christian Andersson del traje Este historia nos cuenta de un emperador que le gustaba todos los días diciéndole un traje nuevo y un día llegaron a reino unos embarcadores que le prometieron hacer un traje tan especial que sólo las personas más inteligentes podían verlo. Cuando vio el traje, el emperador, él no podía verlo, pero no dijo nada porque no quería que las personas pensaran que él no era inteligente. La corte no dijo nada porque no quería que pensaran que ellos no era inteligente. Cuando salió el emperador a un desfile en el pueblo, el pueblo no dijo nada porque no quería que pensaran que ellos no eran inteligentes. Hasta que un niño grita el emperador está desnudo y es cuando todos empiezan a abrirse y decir sí, efectivamente está desnudo. Y esto pasa en las empresas. Muchas veces se ocultan informaciones porque el líder no quiere escuchar resultados negativos. Vamos a hablar de cuatro arquetipos diferentes de líderes y cómo reaccionan ante resultados negativos. El primero de ellos le llamamos el rey o reina burbuja. De este arquetipo vamos a ver cómo reacciona ante resultados negativos y qué implicaciones tiene eso para la cultura empresarial de la empresa y cómo se lidia con ellos. Ellos reaccionan que efectivamente viven dentro de una burbuja y su reacción ante datos negativos es desacreditarlos y haciendo preguntas que realmente ameléticamente no tienen ningún sentido. Hacia la cultura de la empresa, en pequeñas y medias empresas realmente tienen reinados cortos, pero en grandes empresas o en equipos con grandes presupuestos realmente tienen largos reinados porque no se identifica que ellos son la fuente del problema tan fácilmente. Como los sentimientos son más importantes que los datos, el comportamiento de los coladores es saludador y muchas veces se fomenta que digas cosas buenas al jefe porque es lo que quiere escuchar. Para estos líderes realmente los datos no van a jugar nunca un papel importante en la estrategia. A menudo puedes influir con tus datos en niveles más bajos en organización para tratar de conseguir soporte y poder influenciar en el líder. Nuestro siguiente arquetipo le llamamos el atacante. Ante los datos negativos relacionados con sus decisiones van a contraatacarte y van a tratar de ahogar tus análisis entre detalles inmundicia sin sentido. Impulsen la creación de una cultura donde nunca se filtran las malas noticias. Cuando el rendimiento del negocio no es positivo, cada empleado trabajará para encontrar alguna buena noticia en algún lugar. ¿Cómo se lidia con este tipo de líderes? Si eres una persona de datos actualiza tu currículum y sal de ahí encuentro una salida. Si no te es posible, centra tus mayores esfuerzos analíticos en los mayores temores del atacante, lo que más le preocupa y probablemente bajo estas situaciones es el líder estar abierto a escucharte porque realmente le preocupan muchos los temas. Nuestro siguiente arquetipo es el racionalizador. Este tipo de líder su instinto ante datos negativos es poner excusas. Sutilmente va a ir sembrando dudas y va a diluir el análisis con no hechos. Para la organización todo lo que la gente de datos haga para resultar la realidad será descontado y enterrado. Opiniones y puntos de vista que tengan tendrán el mismo peso que el análisis más labrado, más práctico e inteligente que se haga. Con este tipo de líderes si tienes el coraje de determinación encuentra el elemento más importante de la estrategia de negocio y usa tu poder análisis estratégico para presentar datos reales sobre esas estrategias. Un líder racionalizador nunca dejará de intentar resionalizar cada dato negativo y entonces la persistencia va a ser tu mejor virtud ante estos líderes. Finalmente el último arquetipo el líder curioso. El curioso frente a los datos negativos hace preguntas para comprender el por qué de lo que estás presentando. Demuestra una mentalidad abierta frente a datos negativos y su postura no va a ser de culpar instintivamente. Todo lo contrario, indidentificar y corregir el problema. Su impacto en la cultura de la empresa es que tiene una mentalidad abierta. Pero ese tipo de líderes tiene una gran confianza que da como resultado una mayor revelación de la verdad. Se dicen más las cosas tal como son. ¿Cómo se lia con ellos? Pues realmente si eres una persona de datos, pellíscate para ver si no estás soñando. Es importante que no pones tu posición por sentado. Continúa invirtiendo en tu aprendizaje. Organizaciones elegidas por el líder curioso van a ser los mejores lugares para crear un impacto y te habrá muchas oportunidades en tu carrera. Recuerda que ningún individuo es blanco y negro. Sin embargo, al menos profesionalmente, los líderes tienen a mostrar uno de esos arquetipos como dominante, porque se sienten ahí mucho más cómodos. ¿Qué tipo de plan de acción puedes hacer? Si tú estás en el papel de líder, te invito a que reflexiones en cuál es tu arquetipo dominante. Considera el impacto que tu postura está teniendo en tu equipo cuando te muestran datos negativos. Recuerda que la meta máxima es que consideres un cambio personal hacia los beneficios de evolucionar para convertirte en el arquetipo curioso, si es que aún tú no eres uno de ellos. Si tú no eres el líder, al contrario, es la persona que está frente al líder, dedícate un momento de silencio para que puedas reflexionar cómo las reacciones e implicaciones culturales demostradas por ese líder influyen en tu trabajo personal. ¿Cuál es tu comportamiento en respuesta a ese líder dominante? ¿Qué hará falta para que cambies tu comportamiento para lidiar de manera óptima con la estación en la que te encuentras? Siempre es mejor estar en un camino que tú elijas desde una cuidadosa reflexión y planificación, incluso si te encuentras en una estación ineseable.
DOC0125|Data Storytelling|¿Y por qué los datos requieren de una historia? ¿Por qué necesitamos contar una historia con los datos? Todos empezamos que tenemos mucha información, cada vez hay mucha información, es mucho más disponible y tenemos herramientas mucho más avanzadas para tratarlos. Con esos datos se nos pide hacer un trabajo de exploración, de investigación, de qué está pasando con los números y tratar de encontrar algo importante que comunicar respecto a esos números. Y lo que hacemos una vez que ya está esta información, encontraste este insight, vamos y lo comunicamos directamente a la audiencia y ahí está el reporte. ¿Y qué es lo que pasa cuando hacemos este camino? Acabamos en presentaciones aburridas, pasamos muchos minutos de una presentación viendo slides que realmente no me comunican nada y que sólo estoy deseando en qué momento acaban para ya poder salirme de esta junta. Esta forma de presentar los números es incorrecta. Para lograr que la gente vea lo que nosotros estamos viendo, entienda los números como nosotros lo vemos, necesitamos empacarlo dentro de una historia para a partir de ahí transmitir esa historia a la audiencia. ¿Por qué los datos necesitan de una historia? Porque buscamos que la historia sea más memorable. Después de una presentación, 63% de los asistentes recuerdan las historias, pero sólo un 5% va a recordar las estadísticas. La gente no recuerda que quedamos 7.6% por debajo del presupuesto, simplemente va a recordar que no lo cumplimos. Buscamos que los datos tengan historias porque queremos hacerlas más impactantes, porque fíjale cuentas con textos, con tablas y con gráficas, yo sólo estoy hablando a una parte del cerebro, le estoy hablando a la parte que ve lo racional, la lógica, el análisis, las matemáticas, las ciencias, pero no le estoy hablando con los números a la otra parte del cerebro del derecho, que ve la parte de la emoción, la espiritualidad, el arte, la música. Cuando yo conecto estos dos datos al cerebro es cuando la gente se queda impactada con lo que les estoy mostrando y me interesa que los datos tengan historias para hacerlos más personales, porque a mí me interesa que la gente conecte con lo que les estoy mostrando. Yo necesito saber qué le interesa a mi audiencia para a partir de ahí mostrarles cosas que realmente van a captar su atención con esta información que voy a mostrar. Porque al final de cuentas, analizar datos no es de que tú tengas muchos datos y hagas muchas gráficas, lo importante es que tú transmitas el significado de esos datos, que encuentres y que sepas contar una historia con datos.
DOC0126|Metodologías agiles|Hola, mucho gusto, yo soy Cris Rua y hoy les voy a hablar sobre la agilidad, que es eso y para que nos sirve. Si usted va a realizar un proyecto, cualquier tipo de proyecto entra dentro de una de dos categorías. Proyectos para generar productos repetitivos y proyectos para generar productos únicos que nunca en la vida se van a volver a repetir. Usted, para realizar un producto repetitivo puede aprender del pasado, cómo nos fue, qué tiempo nos duró, cómo mejoró el proceso y cómo hago que la próxima vez lo haga en menos tiempo. De esa manera yo puedo trabajar bajo la triple restricción, alcance, tiempo y costo. Así, yo puedo en el futuro ir mejorando el proceso e ir disminuyendo los tiempos y los costos de la generación de cada producto. Por lo cual, si a usted le piden 100 productos del mismo que tú realizas, entonces tú multiplicas y dices, si mi alcance va a ser 100, yo te puedo dar el tiempo y el costo, sin ningún problema. Bueno, y si todo está bien, ¿dónde está el problema? El problema está cuando tratamos de realizar proyectos que hacen un producto único bajo estas mismas condiciones. Usted, cuando va a hacer un producto único, no tiene idea de qué va a realizar ni qué cambios necesita y pueden pasar mil cosas en el transcurso del tiempo. Entonces, un ejemplo de un producto único, los proyectos de software, donde un equipo tiene que crear un programa y nunca más en la vida lo va a volver a crear. Con las metodologías tradicionales, trataron como pudieron, o sea, trataron de hacerlo de mil y una manera y todo se resumía en esto, en poder fijar alcance, tiempo y costo para un proyecto de software. Es más, llega la persona que necesita el proyecto del equipo de desarrollo y le dice, necesito que me hagan un proyecto para yo registrar mis usuarios de la compañía y la primera pregunta es, ¿para cuándo? ¿Para cuándo está? ¿Para cuándo está? Y el equipo muchas veces, hey, pero pues ni siquiera me has dicho qué hay que hacer, no me has dicho qué hay que hacer, ya me está pidiendo fecha. Ese es el inicio de casi todos los problemas. El equipo da una fecha o da una fecha, ¿cierto? Y empiezan a trabajar sobre eso, a tratar de hacer ese alcance que ellos definen durante un tiempo, en ese mismo tiempo y en ese mismo costo. Durante todo ese tiempo el cliente está tranquilo porque le dieron una fecha con el agravante de que más del 80% de los proyectos nunca cumplen esa fecha y solamente se enteran al final del tiempo cuando ya no hay qué decir y no se puede hacer nada, solamente dar más tiempo o a veces cancelar el proyecto o a veces se perdió simplemente todo el tiempo que se trabajó. Los proyectos que trabajaban en la metodología tradicional para hacer un proyecto de software lo hacían de esta manera y era o sea impecable, esto era lo que iba a suceder y era la forma más feliz de ver las cosas, donde tenemos unos requisitos iniciales que se van a realizar en un tiempo, luego se analizan, se diseñan, se construyen, se prueban, se sacan a producción que es poner a funcionar las cosas y con eso cumplíamos la fecha fin requerida, genial, ¿cierto? Usted le entrega eso a una persona que le pidió un proyecto y esa persona se va feliz porque ya ahora solamente espera la fecha del fin donde le van a entregar todas sus cosas que necesita. Pero pasaban algunos problemas con esto y no solamente en una empresa ni en una ciudad ni en un país pasaba a nivel mundial, él se demoró más tiempo levantar requisitos de lo esperado, cada vez el tiempo era menos ellos que eran las personas que realmente van a construir el producto, no tenían tiempo para hacer nada y se comían el resto del tiempo, cuando llegábamos acá si acaso y estábamos de buenas estas personas están terminando de hacer las cosas, no hay tiempo para pruebas y todo sale mucho después de la fecha y con muchos errores. Otro de los miles de problemas, el cliente sólo padece al inicio y al final del proyecto por lo cual lo que aquí pidió seguramente mientras pasa el transcurso de estos seis meses o un año ya necesita otra cosa que es muy difícil entrar porque eso es un contrato firmado con sangre, si la empresa requiere otra cosa diferente a la que aquí se requería en este momento estas personas le van a decir con mucho gusto que no. Otro problema muy común, un problema simplemente de comunicación, esta persona es la encargada de los requerimientos cuando él termina le pasa a él la antorcha para que él la lleve con el análisis, luego él a él con el diseño, mientras estas personas se comunican se va perdiendo la información y al final cuando algo pasaba que no era lo que el cliente esperaba él le echaba la culpa a él, le echaba la culpa a él, le echaba la culpa a él, le echaba la culpa a él hasta que llenó la culpa aquí, cierto porque aquí nadie es un equipo cada quien está encargado de lo suyo y como este miles de problemas y esta es la forma más fácil de usted pensar en un proyecto sin pensar en las realidades que realmente ocurren si usted pone a una persona que no sepa sobre metodología y lo pone a liderar un proyecto seguramente le va a llegar a hacer esto y le va a entregar el plan inicialmente hermoso que usted lo va a escuchar como música para sus oídos porque le va a parecer divino el tema de dinero, el alcance, el tiempo y el costo fijos y que al final no va a pasar ningún problema porque le va a decir yo le garantizo que al final no va a haber ningún problema ojalá no te esté pasando en tu proyecto si no tienes que hacer cambios. Después de esto hubo unas personas que se cansaron de que sucediera y empezaron a crear una metodología diferente que tuviera en cuenta todos estos inconvenientes que pasaban con esta metodología anterior solamente que trae un pequeño detalle tienes que cambiar la forma de pensar y de hacer las cosas y tener muy claro que quieres y tener estrategia desde el principio del proyecto por lo cual trabajar con ágiles seguramente debe ser mucho más difícil que trabajar con un proyecto tradicional solamente que vas a ver resultados desde el principio y para decirlo haciendo referencia a esta forma de decir las cosas claras y sencillas ahora sí entonces que es la agilidad las metodologías ágiles lo que hacen es visualizar el proyecto sin detallarlo todo encontrar lo más importante que tiene este proyecto ahí viene una cosa que se llama la ley de pareto tenemos que paretizar todo la ley de pareto dice que el 20% de lo que tú realices te va a generar el 80% de los resultados por lo cual si todo es importante nada es importante tú tienes que encontrar la parte más importante de tu proyecto y esa es la que usted va a detallar y construir luego la pones a funcionar y así te la pasas esto es trabajar con metodologías ágiles un proyecto sin tratar de conocerlo todo es difícil sí es difícil tienes que pensar tienes que priorizar tienes que estar pendiente tienes que trabajar bajo la incertidumbre que es lo que se pretende en el anterior no realizar pero esta es una forma muy realista de trabajar un proyecto entonces hay una pregunta muy común entonces ágiles que es arrancar como sea y empezar a construir y listo no en esta parte de visualizar el proyecto usted tira un estimado de fecha final pero ojo es un estimado esto es un estimado y de acuerdo a esto usted si necesita realmente que yo sé que muchos proyectos lo necesitan no todos porque conozco los que no lo necesitan pero la mayoría necesitan tener un tiempo donde yo no hice yo en ese tiempo ya voy a tener la parte funcional de mi proyecto entonces uno visualiza el proyecto y más o menos de acuerdo a eso empieza a decir este proyecto podría estar en este tiempo podría ir hasta allá vamos a dar un año para realizarlo vamos a dar seis meses podría hacer así por lo cual en ágiles usted lo que fija es el tiempo y el costo del proyecto el alcance va a ser variable la cabeza de muchos explotó en este momento cierto pero esa es la realidad de los proyectos realmente usted fija estas dos y empieza a hacer esto a paletizar a encontrar lo más importante detallarlo construirlo ponerlo a funcionar ya aquí tiene algo funcionando en el proyecto y así se va que puede pasar con el alcance entonces pueden pasar tres cosas dos muy probables y una muy poco probable una se cumple el tiempo y podemos tener más alcance del que esperábamos inicialmente pudimos alcanzar a hacer más cosas realmente pudimos ponerle muchos moñitos a nuestra aplicación y hacerla muy bonita dos podemos hacer menos alcance del esperado pero realmente ya lo tenemos funcionando muchas cosas realmente no eran importantes no eran necesarias eran algo que simplemente si estuviera chévere cierto pero aquí puede que hagas menos pero garantizarías que lo más importante ya está realizado y mucho no solamente un pequeño pedazo mucho de lo más importante lo que quedó faltando fue realmente lo que no importaba 3 puede pasar que tengas el mismo alcance identificado al principio esta es la menos probable que es la que fijan en la metodología tradicional pero realmente si te da el mismo alcance en el mismo tiempo y en el mismo costo es pura casualidad no podemos pretender conocer estas tres cosas eso es trabajar bajo en un mundo idealista donde usted está tranquilo porque fijó las tres cosas realmente usted tiene que ser consciente desde un principio que eso no lo va a fijar entonces que necesita hacer lo mejor posible en este tiempo que definan trabajar conociendo la realidad de un proyecto y sacándole el mejor provecho esto es agilidad las metodologías agiles piensan en dos cosas en entregar los resultados de esta manera que acabamos de ver y en las personas eso es un descubrimiento que se hizo en esta metodología que antes no tenían donde si nos faltaba tiempo para realizar el alcance entonces la gente mate se para cumplir porque partimos de que ustedes son unas personas que no lograron hacer lo que dijeron en un principio y ustedes las pagan eso es metodología tradicional metodología agiles que es ya trabajando desde esta otra manera sabemos que las personas dan lo que más pueden dar es más trabajamos con personas que están motivadas porque hay un descubrimiento grande si usted tiene unas personas motivadas dentro de un equipo van a hacer lo mejor por ese proyecto es más si pueden dar más van a dar más para eso necesitamos motivar las personas y también dentro de los marcos de trabajo ágil están esos espacios para que las personas puedan decir que los motivan que necesitan para que sean un equipo cada vez más productivo y puedan hacer cosas muy bacanas entonces piensa en las dos cosas de la agilidad entregar resultados constantemente y desde el principio y trabajar con personas que sean motivadas y que quieran hacerlo mejor para este proyecto y para esto existen unos marcos de trabajo que no vienen dentro de este vídeo que son los marcos de trabajo ágiles scrum kanban estos te los pongo aquí abajo en los comentarios las metodologías que ya están pensadas en generar esos resultados si te gustó el vídeo dale like y si quieres que tu celular te avise cuando haya otro vídeo en este canal suscríbete aquí abajo cualquier duda me la pueden dejar en los comentarios espero que les guste bastante realmente las metodologías ágiles es para cambiar la vida de la gente que construye proyectos y el resultado de los proyectos así que de verdad pónganse las pilas para que lo hagan de esta manera que estén muy bien
DOC0127|Metodologías agiles|Hola, bienvenidos nuevamente a mi canal Agil es por Cris Rua y hoy les voy a hablar sobre Scrum. Scrum es el marco más usado de las metodologías ágiles a nivel mundial. Las metodologías ágiles hacen que tú y tu equipo realicen los proyectos de la mejor forma posible. Cómo se deben trabajar ahora basándose en estos cuatro valores principales. Los 12 principios es lo más importante que tú debes lograr con tu equipo y son estos. La forma como las metodologías ágiles nos ayudan a lograrlo es por medio de unos marcos de trabajo. Uno es Scrum, pero recuerden que esto te ayuda a lograr los 12 principios. Si de la forma que yo te voy a enseñar Scrum no lo logras tienes que irte modificando y acoplando con tu equipo hasta que lo logres, si no Scrum por defecto no te va a servir de nada. ¿Por qué se llama Scrum? Hay una formación muy particular que es donde todo un equipo está contra todo un equipo y ellos empiezan a empujar al otro y hacer estrategias para lograr llegar hasta el objetivo. Lo que quiere decir que ahora debemos empezar a pensar en un equipo como un todo o todos ganamos o todos perdemos. Comencemos. Scrum como tal se compone de ceremonias, artefactos y roles y eso es todo lo que vamos a ver hoy. Para iniciar con Scrum debes tener un backlog muy bien definido. El backlog se compone de historias de usuario, unas más detalladas que otras, arriba están las más detalladitas y abajo están cada vez más grandes porque desde arriba va a estar lo más prioritario y abajo lo que viene para adelante en el proyecto. Un backlog bien definido es la magia que logrará que tu equipo siempre que produzca algo la saque del estadio, siempre va a ser lo más importante en cada momento del tiempo. Aquí aparece un rol supremamente importante, nuestro Product Owner es la persona que siempre está encargado de que este backlog esté perfecto y para ello él escribe muy buenas historias de usuario, detalla los criterios de aceptación de cada una de las historias, conversa y aclara las historias con el equipo durante la planeación que es lo que sigue, prioriza siempre el backlog, define el mínimo producto viable de cada proyecto y sobre todo debe sentir que es parte del equipo, él no es el de allá del negocio que sabe mucho y que no tiene tiempo y lo ideal es que esté dedicado al proyecto. Iniciamos muy mal cuando definimos un Product Owner por lo que sabe, porque es la persona que sabe mucho del proyecto pero es una persona que no tiene tiempo para darle al equipo, por lo cual muy chévere que sepa bastante pero allá se queda ese conocimiento y nunca tiene tiempo para hacer esto bien hecho. En ese caso es mejor que uno tenga un Product Owner externo, afuera, que sea el que sí está encargado de sacarle a él esa información que conoce, a él o a ellos sí son varios que le piden al equipo. Otra cosa supremamente importante, un equipo tiene un Product Owner, un Product Owner, si a ti te pasa eso de que mi equipo tiene cuatro Product Owners y entre ellos se pelean para meter acá la información del backlog, entonces estamos muy mal, hay que cambiar algo, listo, hay que cambiar algo. La primera ceremonia o reunión que hacemos en Scrum se llama la planeación, en la planeación el Product Owner trae el backlog perfecto y teniendo arriba lo que realmente es más importante para el proyecto, el equipo tiene un deber, decidir cuántas historias son capaces de hacer durante el sprint y para ellos se valen de algo llamado la estimación, es donde comparan y ponen números a las historias diciendo cuál es más difícil de hacer que otras, ellos previamente han tenido un proceso donde han encontrado esa velocidad que tienen, velocidad que es, nosotros el equipo por sprint realizamos x cantidad de puntos de historia y eso es lo que normalmente pueden empezar a comprometerse durante el sprint. Aclaro, de cada uno de estos términos se puede hacer un vídeo completo, cosas como estimación, cosas como Product Owner, Scrum Master, entonces en la medida que yo vaya haciendo vídeos los voy a ir agregando aquí abajo para que ustedes puedan ir detallando más cada una de las cosas. El Product Owner durante la planeación aclara cada una de las historias, o sea, él tiene que estar ahí, tiene que estar ahí, nada de que yo se los mandé por correo que eso va súper claro que eso no tengo que hacer yo Product Owner, saca cada una de las historias, la explica, explica los criterios y entre todos deciden si la historia está muy bien, si necesita algún cambio en caliente lo hacen, pero ojalá no tengan que hacerlo ahí. Cuando el equipo decidió qué es lo que puede hacer durante el sprint lo agregan a una parte llamada el sprint backlog, de ahora en adelante durante los 15 días del sprint, si lo tiraron de 15 días, el equipo va a estar pendiente de terminar estas cosas con las que se comprometieron y el Product Owner se va a seguir puliendo el backlog para la próxima planeación, para un sprint de 15 días la planeación tiende a durar más o menos cuatro horas. Ah, y por si no lo han visto, aquí hay un nuevo rol, los tres roles de Scrum son Product Owner, Scrum Master y Team Member que son los muchachos del equipo, el Scrum Master es una persona más estilo coach, algo parecido, que ayuda mucho a que todo esto salga bien hecho, uno dirá no, pues fácil, uno les explica y ya, y que lo hagan, no, necesitan de verdad una compañía que sepa sacar provecho de todo esto, porque usted por ejemplo acá puede planear, pero puede estar planeando cosas que no son importantes, el Product Owner pudo haber llegado con historias de usuario, pero pudo haber llegado con una historia de usuario nefastas, sin criterios de aceptación, sin haberlas hecho bien hechas y para esto está el Scrum Master, es el que siempre está observando y ayudando a que todo se haga muy bien hecho, el Scrum Master es una persona que comprende muy bien Scrum, nos ayuda a implementarlo, pero sobre todo es una persona que comprende más allá de Scrum que hay, entonces al seleccionar un Scrum Master usted no puede llegar y hacer lo siguiente, hey Juanita, de ahora en adelante usted va a ser la Scrum Master. ¿Yo? ¿Y yo por qué? Si yo no sé que es eso. Pues porque si, porque usted es la que puede o no quiere trabajar aquí. Yo lo hago, yo lo hago, tranquila, yo lo hago. Un Scrum Master debe saber muy bien su rol o si quieres poner a alguien primero, enséñale muy bien y capacítalo muy bien para que de verdad pueda lograr cosas muy importantes. La siguiente ceremonia o reunión después de la planeación es el refinamiento, el refinamiento inicialmente no había sido contemplado dentro de las ceremonias de Scrum, pero solía pasar mucho que las planeaciones se extendían bastante, ¿por qué? Porque había que hacer un cambio, de pronto una historia de usuario estaba muy grande, tocaba dividirla y tocaba hacerlo en caliente, por ese motivo salió el refinamiento, en el refinamiento lo que hacemos es pulir las historias que vienen para los demás Sprint, no es para este, es para otros que vienen después de esto, se hace más o menos a la mitad del Sprint para que el Product Owner haya tenido tiempo de empezar a pulir las siguientes historias y que las lleve ya creyendo que todas están listas y el equipo le va diciendo si están bien, hay que partir esta, dividimos esta historia en esta parte, aquí solamente pulen. Para un Sprint de 15 días lo recomendado es dos horas de duración de este refinamiento y es la única reunión que el equipo tiene durante el Sprint externa a lo que tiene que realizar, hay un espacio de equipo que se llama la Daily, en la Daily cada miembro del equipo responde tres preguntas, ¿qué hice ayer?, ¿qué voy a hacer hoy? y ¿qué impedimentos tengo? ¿esto para qué sirve?, para que el equipo se pueda colaborar entre ellos, solamente es para esto, esto no es para rendirle cuentas a nadie, esto no es yo me voy a entrar el equipo como está, si alguien externo al propio equipo asiste debe asistir calladito, porque si empieza a hablar extiende la reunión y después termina la gente aburrida y dicen no la Daily no sirve, no la volvamos a hacer, por ahí he escuchado hasta de equipos que optimizan el tiempo y dicen no, no hacemos Daily, hacemos Weekly, porque así evitamos todo lo de diario y ya nada más en semana nos reunimos y decimos ¿qué hicimos? realmente ahí no se pueden ayudar que es para lo que está hecha esta reunión, ya eso es una reunión normal que están teniendo por aparte. Ya al finalizar el sprint venimos a una reunión que se llama review, es una reunión que para un sprint de 15 días se debe hacer solamente de una hora, no tiene que ser algo supremamente preparado, pero es donde el equipo va a mostrar al Product Owner y a los stakeholders si quieren ser invitados el producto ya funcionando, entonces ya simplemente se sientan una horita muestran las cosas, mira esto fue lo que hicimos tuvimos estos inconvenientes, tiende a ser una reunión supremamente relajada y por último tenemos la retrospectiva, el espacio donde el equipo se reúne y empieza a analizar todo lo que pasó en el sprint, si entregaron si no entregaron, porque les fue así, si les fue súper bien, todo lo que tuvieron que ver mientras estuvieron pasando por todo esto. La retrospectiva es la que nos ayuda a mejorar sprint tras sprint, por lo cual es muy importante sacarle este espacio, la retrospectiva tiende a durar dos horas para un sprint de 15 días, es más muchos equipos que quieren iniciar a trabajar por metodologías ágiles arrancan por la retrospectiva que es la que empieza a darle ese orden al equipo, hay muchas técnicas de retrospectiva y hay formas eficientes y no eficientes de realizarlas, por lo cual aquí abajo les pongo un videito sobre 10 pasos para realizar una retrospectiva bien hecha, normalmente hay una pregunta el Product Owner debe estar o no debe estar, la recomendación es sí desde que se sienta parte del equipo, porque puede pasar que el Product Owner no es un buen Product Owner, sino que es una persona externa que simplemente está mandando y mandando cosas por hacer y al esta persona formar parte de esta reunión hace que el resto de las personas no lo puedan expresar, ahí tienen que mirar si lo van a invitar o no lo van a invitar, lo mejor y lo ideal es que si él hace parte del equipo siempre esté ahí y que siempre vayan mejorando juntos, hay una gráfica muy importante donde usted puede empezar a mirar cómo va avanzando el equipo día a día y si se van a cumplir los puntos que se comprometieron a hacer en el tiempo que está pactado, esta le sirve mucho. Y luego de la retrospectiva lo que viene es volver a la planeación donde el Product Owner trae todo, el backlog listo y comienza nuevamente el nuevo ciclo del nuevo sprint donde vamos a hacer otra entrega diferente de lo que es más importante ya para lo que sigue. Hemos terminado nuestro vídeo de Scrum, espero les sirva bastante de cada una de estas cosas, se puede hacer un vídeo completo porque de verdad que esto contiene muchas cosas, recuerden que este es el medio para llegar a realmente cumplir con los valores y los principios ágiles que es lo que es de verdad define un equipo ágil, esa es la forma para llegar allá, no vayan a hacer como los equipos que dicen que tienen un Stormaster definido, un Product Owner y trabajan cada 15 días ya están siendo un equipo ágil, realmente lo que importa es entregar continuamente y usted siempre esté entregando partes del proyecto ya funcionando, eso es lo que realmente importa. Si quieren conocer bastante sobre alguno de estos temas que consideren que quedó muy flojito pueden pedirme aquí abajo en los comentarios y yo estaré pendiente para irle dando prioridad a ciertos temas que voy a ir sacando en el tiempo. Bueno señores espero que haya sido supremamente claro que les sirva bastante, cualquier duda ya saben me la hacen saber, si les gustó denle like al vídeo y si quieren saber cuando vienen más vídeos de los que yo voy sacando pueden suscribirse a mi canal, que estén muy bien. Chao!
DOC0128|Metodologías agiles|Hola, mucho gusto. Yo soy Cristina Arruba, trabajo con metodologías ángiles ya hace algún tiempo. Hoy les traigo Kanban, una herramienta que te va a servir para organizar tu trabajo para que seas cada vez más óptimo, productivo y eficiente. Kanban es un método que se inventaron en Toyota para que sus procesos de producción de vehículos fueran mucho más eficientes y significa tablero visual o tarjetas visuales. Kan es visual y ban es tarjeta o tablero. En este vídeo te voy a mostrar estados y para qué sirven. Te voy a mostrar las filas y cómo se pueden diferenciar objetivos simétricas y los motivadores. Comencemos. Primero, identifica cuáles son los estados por los cuales pasa lo que se produce en tu caso y plásmalos en un tablero. Arranquen con cosas básicas que si en el futuro necesitan otro estado, se lo agregan. Segundo, identifica cuáles serán tus filas. Las filas es la forma como vas a organizar la información. Puede ser que inicie sin filas, ya cuando tengas bastante información y empieces a ver necesidades para tenerla visual, las organizas. Por ejemplo, aquí ponemos lo que hace Juan, aquí ponemos lo que hace Pedro y aquí abajo ponemos lo que hace Lisa. Esa es una forma. Cuando tú necesitas tener visibilidad por persona. Otra forma que podrías organizarlo por filas es de acuerdo a la prioridad. Entonces arriba lo que es juego, o sea lo que hay que hacer ya sea quien sea que lo coja. Medio y bajo. Y por lo que se te ocurra, por áreas, por personas a quien se les debe entregar, por lo que tú necesitas puedes hacer estas filas. La cosa es que siempre se estén cuestionando sobre qué es lo que necesitan. Kanban hace parte de Kaizen, que es mejora continua. Esto es, siempre estás cuestionando cómo estás trabajando ahorita y cuál puede ser la mejor forma para hacerlo de una manera más óptima. Y simplemente agregas un cambio. Hay una cosa pequeña, sencilla, pero que marca la diferencia de una manera enorme. Los mínimos y los máximos por columna. ¿Acaso te ha llegado a ocurrir esto? ¿Qué es esto? Esto es un cuello de botella. ¿Cómo podemos hacer para que este cuello de botella no siga sucediendo? Si tú no tienes esto visual es muy difícil encontrar esto o es muy difícil que te pese, pero si tú no tienes visual, basta un segundo que tú mires este tablero para ustedes decir aquí hay problemas. Por lo cual vamos a ponerle un máximo a esta columna. El equipo puede reunirse y decir ¿qué hacemos con este problema? Llegar a esto. O puede ser que un jefe o un gerente lo vio, miran este problema, llegan a esto. ¿Esto qué es? Un acuerdo que hagan con el equipo o que hagan dentro del equipo. Cuando aquí empieza a superar un máximo de cinco, o sea de seis para adelante, toda la gente se frena en estos lados y empiezan a ayudar a las personas que están en esta. La prioridad del equipo y de la empresa es acabar con esto para poder cerrar. Estos máximos tienen su ventaja dependiendo de dónde esté y puede cubrir muchos problemas diferentes. Ejemplo, se ha detectado que el equipo es capaz de hacer más o menos tres cosas por día, más o menos, a veces hace tres, a veces una, a veces cinco, no sé, pero anda rondando el tres. Y aquí entran diez por día. Entonces, ¿dónde va a estar el acumulado? ¿Dónde está el cuello de beta? Desde el inicio, ¿cierto? Y aquí estamos dando algunos problemas de equipo que puede ser que le están sobreexigiendo, están dejando que entre cualquier cosa, no están priorizando. Hay muchas cosas que pueden pasar, por lo cual vamos a ponerle un máximo a esta columna inicial, donde se va a restringir la entrada a las peticiones del equipo. ¿Ahí qué tiene que pasar? Dependiendo. Si son varias personas los que le piden a este mismo equipo cosas, entonces estas personas van a tener que hablar entre ellas para decir, voy a meter lo mío porque es más importante que lo tuyo, o que prioricen de alguna manera. Y si es una sola persona, entonces esa persona tendrá que pensarlo muy bien antes de pedir otra cosa, si sí es más importante que todo lo anterior. Ejemplo, puede que también los mínimos dentro de las columnas te ayuden a mejorar muchísimas cosas, es decir, tú tienes que estar mirando qué es lo que está pasando siempre en esta entrega de valor para que tú mires cómo se puede optimizar. Ejemplo de mínimo, son cinco personas y hay dos cosas haciéndose. Estas personas que están probando, no tienen nada por probar, entonces ahí es donde el equipo hace un acuerdo y dice, venga que por lo menos, somos cinco, que por lo menos cuatro cosas estén haciendo, uno para que se ayudan al resto supongamos, pero menos de cuatro, pues ya está pasando algo, por lo cual pondrían un mínimo. Ven cómo estas pequeñas cositas nos pueden ayudar tanto a disminuir esos cuchos de botella que tanto nos hacen mal para que seamos un equipo óptimo. Miren aquí, les recuerdo el objetivo de Kanban, optimización, productividad y eficiencia para el equipo. ¿Y cómo nos puede ayudar a esto? Disminuyendo el lead time, que es el tiempo en que una petición se demora desde que llegó al equipo hasta que terminó en hecho. Es algo que si a ti te encantan las métricas, mide eso, mide cuánto se están demorando las cosas desde que llegan hasta que sale y siempre enfócate en que esto reduzca al mínimo posible para el equipo. El sequel time es otra cosa que puedes medir, que ya viene desde aquí para acá, o sea desde que se inicia hasta que se cierra. Aunque si tú te centras en el cliente, para el cliente ¿qué es lo más importante? Es lead time. Y obviamente disminuye siempre los cuchos de botella. Para esto puedes basarte en una gráfica que se llama la gráfica del flujo acumulado, es la gráfica propia de Kanban. De eso te haré otro videíto y lo pondré por acá en los comentarios para que aprendas a sacarle mucho provecho a esta gráfica de flujo acumulado. Ahora, por último, y realmente algo que a mí me encanta, es cómo hacer que esto sea motivador para el equipo. Hay cosas pequeñitas y muy creativas que pueden hacer que el equipo se sienta contentos, felices, hay infinidad de cosas que puedes hacer. Te voy a proponer algunas, invéntate muchas más y lo que te inventes si puedes, ponlo aquí abajito en los comentarios. Supongamos que el equipo de alguna manera pudo haber sido en una retrospectiva que hicieron en el equipo o en un momento de equipo. Identificaron que Carlos, su perfil es que siempre es una persona que es ese amigo fiel que está siempre a tu lado hasta el fin del mundo y lo dibujaron con crimen, por lo cual de aquí en adelante todo lo que esté haciendo Carlos va a estar con un puntico rojo que significa crimen y lo tenemos aquí al lado del tablero. Daniel, Daniel es el chico con superpoderes, así el más teso de todos, que todo lo hace en un momentico, que todo lo aprende así rapidito, rapidito y que siempre se vuelve el mejor. Goku, ¿sí o qué? Goku entonces, que es Daniel en este caso, Daniel siempre tiene los puntos verdes y acá siempre vas a ver. Si tú miras este tablero, sencillamente puedes saber qué tema está haciendo cada uno de ellos en cualquier momento del tiempo y solo con tenerlo visual. Así como este hay muchos, he visto equipos que lo representan como con los gordos de botero, entonces acá todos dibujaditos como si fueran gordos, gordotes, pero con alguna característica física que sí es propia de los chicos y aquí van marcando las cosas y como estas hay miles de cosas que ustedes pueden inventarse, es decir, esto puede hacerse un trabajo divertido, visual, óptimo, rápido y sin mucha complicación. Bueno señores, esto es Kanban, les traeré otros videoditos luego, cualquier cosa piden acá los temas, hay muchos temas que hay como equipos de ágiles, de metodologías ágiles, se enseñan y se viven. Yo soy Cristina Ruá desde Medellín, Colombia. Chau!
DOC0129|Metodologías agiles|Hola, yo soy Cris Rua y bienvenidos a mi canal. Hoy vamos a hablar sobre Scrum vs. Kanban. Y esto pasó en el Ágiles Colombia 2019 que fue en Cali y que quedó genial. Felicitaciones a los organizadores, nos hicieron pasar un evento maravilloso. En este ágil les di la charla de Scrum vs. Kanban que hoy les traigo también acá como video y que al final termina mostrándonos que es Scrum. Y en este video vamos a ver cuando usar Scrum, cuando usar Kanban, por qué usarlos, qué busca cada uno de ellos, en qué se parecen y en qué se diferencian y finalmente cómo se puede hacer una mezcla responsable de ellos dos. Usamos Scrum cuando tenemos un proyecto que podemos visualizar y podemos ir programando entregas de a poquito, por lo menos entre una semana y máximo un mes. Y usamos Kanban cuando no podemos planear ni una semana y necesitamos disminuir el tiempo de entrega a nuestros usuarios. Porque quiero hablarles yo de Scrum y Kanban comparados. Porque de Scrum conozco muchas implementaciones que se están haciendo al respecto y mucho compromiso y están tratando siempre de mejorar, pero de Kanban hay muy pocas. He conocido muchos que creen estar trabajando Kanban y simplemente tienen un tablero y ya, eso es Kanban para ellos. No, realmente cada uno tiene su foco y son muy diferentes los resultados que pueden llegar a tener. Entonces necesitamos saber cuándo usar el uno y cuándo usar el otro. Y los dos son excelentes. Pero este no es bueno cuando realmente necesitabas utilizar este y Scrum no es bueno cuando realmente necesitabas utilizar Kanban. Por eso conocelos muy bien para que puedas tomar unas decisiones que realmente ayuden a tu equipo. ¿Qué es lo que busca Scrum? Primero, que tu visualices tu proyecto y empiecen a pensar entre todos más o menos que si quieres lograr, que es el proyecto. Y luego entre todos van entregando cada vez lo más importante y que ya sea usable. Van entregando, van entregando, acá lo mejoran y ya está listo. Ahí es Scrum y ahí sirve cuando puedes hacer esto. ¿Y qué busca Kanban? Primero, que tu conozcas e identifiques los estados por los que pasa lo que hace tu equipo para que puedas poner visible la información de lo que se está realizando. Porque aquí es donde vamos a empezar a mejorar para que este proceso de entrega sea cada vez mucho más fluido. ¿Y qué se parecen y en qué se diferencian? En Scrum se dice que tenemos equipos multifuncionales, es decir, que un equipo tiene las cualidades necesarias entre todos sus integrantes para resolver todo sin tener dependencias de otras o teniendo las mínimas mínimas posibles. En Kanban no. En Kanban no tienes que tener un equipo ni siquiera. Puede ser una sola persona, dos, tres. Aquí para cualquier cantidad de personas aplica. En Scrum trabajamos entregando por espacios de tiempos fijos, es decir, por sprint. Planeamos, entregamos al final, planeamos, entregamos al final. En Kanban no. Sabemos que acá no se puede planear, entonces acá nos enfocaremos en otro tipo de cosas. En Scrum se prescriben roles. Si queremos lograr esto que tenemos acá, necesitamos un rol, el Product Owner se encarga de que se haga cada vez lo más importante, mientras que el Scrum Master se encarga de que toda esa máquina de Scrum funcione muy bien y que esté en una mejora constante, ayudando en todas las partes de Scrum. En Kanban no se tienen roles definidos. Hay una similitud en ambos y es que cada uno tiene una forma de manejar el trabajo que se está realizando en el equipo. En Scrum lo manejamos por sprint. Él dice, definamos lo que vamos a hacer por sprint y no nos salgamos de ahí. Esto es lo que vamos a entregar. Y en cambio en Kanban, él se concentra en los puntos donde tenemos muchas cosas pendientes que nos están impidiendo que las cosas que se comiencen a hacer terminen rápido. Entonces él empieza a decir, por favor limitemos lo que estamos haciendo en cada columna o en las columnas precisamente que tenemos cuello de botella. Entonces aquí es donde él se concentra para limitar el trabajo y Scrum lo hace por sprint. Por esto mismo, Scrum se resiste a los cambios dentro de un sprint. Cuando entra un cambio viene en el backlog para un siguiente. Mientras que en Kanban no se puede manejar los cambios que te van ingresando de acuerdo a la priorización que se va dando. Tenemos dos tipos de gráficas diferentes. Esta, como trabajamos por sprint, nos va diciendo de lo que estábamos planeando en el sprint que realmente se está realizando para saber si vamos a lograr el objetivo por sprint. Y esta gráfica nos va mostrando qué tanto se nos va acumulando en cada uno de estos estados porque eso es lo que acá queremos reducir. Scrum es más prescriptivo, es decir, te dice más cosas por hacer, como más reglas y Kanban es más adaptativo. Kanban parte de unas poquiticas, poquiticas reglas para que de ahí para arriba tú lo hayas creciendo hasta donde necesites. En cambio Scrum hay que utilizarlo como muy completico porque él ya viene en combo, ya viene todo muy bien pensado. Ahora la parte que menos siento yo que se conoce, el poder que tiene Kanban. Muchos equipos creen que saben y conocen Kanban y que lo están utilizando porque tienen un tablero donde tienen las cosas expuestas. Pero realmente Kanban tiene muchísimo, muchísimo poder. ¿Y Kanban en qué enfoca todo su poder? En reducir el lead time, que es el tiempo que se demora algo, desde que llega hasta que sale. Y para eso Kanban tiene las pequeñas reglas propias. Entonces cómo debes utilizarlo? Debes llevar este tiempo cuando llegue acá y cómo sé cuando terminó porque es lo que queremos reducir con esto. Ya que no vamos a ir entregando un montón de cosas por poquitos, vamos entonces a enfocarnos en que lo que se inicie a ser vamos siempre sacándolo rápido y que no se nos quede en el camino. Ese es el foco de Kanban. Para ello tenemos que tener la capacidad, el permiso o el poder de modificar estos estados. Entonces tú en Kanban qué utilizas que no se utiliza en Scrum, que van a definir el flujo por donde pasa todo lo que hacen y que se va a empezar a modificar para poderlo optimizar, que es lo que te hace visualizar a simple vista cómo está el proyecto dependiendo de lo que tú quieras visualizar. Si la necesidad de tu equipo es visualizar las prioridades del proyecto, entonces empezarán a ponerlo por prioridades, es decir carriles por prioridades. Cada equipo empieza a tener sus carriles diferentes, pero supongamos que en este la prioridad se definió por lo que es urgente, lo que es importante y lo que es no importante. Entonces son acuerdos que se van dando en el equipo, que los hacen previos y empiezan a definir entre ellos, venga cuando es urgente, cuando algo entre acá en el carril de urgente se le va a dar prioridad, pero ya, así toque frenar lo que se estaba haciendo antes, esto con el fin de sacar lo más importante cada vez y así van teniendo sus acuerdos que pueden ir cambiando en el futuro. Algunos otros equipos necesitan tener los carriles pero por personas, porque digamos que en este caso una misma persona lo va pasando de estados, entonces queremos saber qué tanto tiene cada persona de carga para después permitir que le ayude a otra persona, entonces uno puede poner que en cada carril lo tenemos por persona y esto nos cambiaría totalmente el tablero otra vez y daría una visibilidad muy diferente. Otra cosa que trae Kanban y que no trae Scrum son los mínimos y los máximos por columna y que es a partir de que tú vayas identificando que en una de estas columnas se te está llenando mucho, entonces haces un acuerdo con el equipo de un máximo o un mínimo por columna y qué es esto, es un acuerdo de qué va a pasar cuando en una columna haya máximo x cantidad de cosas, por ejemplo acá vemos que el proceso es para crear patines y que debería ser fluido para crear la mayor cantidad de patines constantemente, entonces supongamos que esto es crear las llantas, esto es pegarle el zapato, esto es poner los cordones y este es pintar, en este caso tendríamos que tener otro que ya fuera cerrado. Supongamos que vemos que la parte de pintar patines se está llenando, en este momento el tablero Kanban les estamos dando el que acá está el problema, en esta parte que es pintar patines, entonces el equipo ya debe decir qué cantidad está permitida acá que sea algo tranquilo, pueden decidir que pueden haber máximo 6 cosas acumuladas y entonces poner un máximo de 6, el tablero en lo que les ayuda es tener esto visible, que te mande la alerta de que esto está sucediendo, ahora los acuerdos del equipo son los que logran las cosas, si ponen un máximo 6 en esta entonces el equipo tiene que saber qué va a pasar cuando hayan más de 6, puede que hayan hecho un arreglo donde él venga y le ayude a él a pintar o a preparar las pinturas o algo por el estilo que ya ayude a que esta parte se libere más fácil, puede ser, pero ya dependerá de qué estrategia vaya utilizando el equipo para ir sacando esto adelante, entonces Kanban cómo los va a ayudar? Ayudándoles a reflejar los problemas que tienen en el tablero porque lo van a tener muy bien hecho y empezarán entonces a identificar problemas y solucionarlo, identificar problemas, solucionarlo para que al final siempre sea fluido, entonces en Kanban que tenemos en restricciones? Sencillo, visualiza en el tablero todos tus estados, identifica carriles que te puedan ayudar a tener una mejor visualización de las cosas que pueda estar necesitando, ponle mínimos y máximos por columna que te ayuden a controlar el flujo para que pueda salir mucho más rápido, en Cambio en Scrum tenemos más reglas que hacer pero son las que nos van a ayudar si nuestra necesidad es tener un proyecto e irlo entregando de a poquitos, ahí sí estás de este lado, el Scrum Master, el Product Owner, el equipo multifuncional, la planning, la retrospectiva, la review, la daily, entre otras cosas, ahora hablamos mucho sobre Scrum Ban, cierto? y qué es esto? es mezclar uno con el otro pero de una manera muy responsable, no es simplemente mercar cositas porque sí y no tener el sentido de ello, antes es más difícil porque tú tienes que entender muy bien qué es Scrum y para qué son cada uno de esos elementos y muy bien qué es Kanban y para qué son cada uno de esos elementos, es mucho más difícil hacer Scrum Ban que hacer Scrum o Kanban, cuál es mi consejo si vas a usar Scrum Ban? parta de Kanban, o sea lo más básico y empieza a agregar elementos de Scrum pero sólo cuando ya los necesites, por ejemplo usamos Kanban porque no podemos planear pero si queremos irle mostrando avances a nuestro cliente, ¿cómo podemos hacerlo? agreguémosle la review de Scrum a Kanban, entonces cada x tiempo, una semana, 15 días, tú dices le vamos a mostrar al cliente lo que ya está en hecho, lo que está acá en esta columna hecho y aunque no lo hayan planeado le van mostrando mire lo que ya hicimos en esta, vea lo acá funcionando, mire cómo estaba esto de bonito, eso sería agregar la review, ¿para qué le agregarías por ejemplo un Product Owner? supongamos que tú, si tú acá tienes la forma de ya saber cuando algo entra, dónde entra, ejemplo si en los carriles ya sabemos qué es lo que entra a urgente, a importante, a no importante y no hay discusión con ello, no necesitaríamos un Product Owner, pero si en este Kanban nosotros no tenemos la tranquilidad de que estemos enfocados haciendo lo más importante cada vez podríamos decir que ponemos un Product Owner acá, ¿qué haría un Product Owner en la parte de Kanban? hacer que cada vez el equipo tenga priorizado lo que hay por hacer para que ya cuando la persona vaya a trabajar sepa qué es lo que debe tomar en cada momento, ¿para qué mecaríamos por ejemplo una retrospectiva? la retrospectiva es para mejorar, ¿cierto? para que haya una mejora continua que nos ayude a hacer cada vez mejor las cosas, entonces si la utilizamos en esta parte podríamos simplemente definir una periodicidad, cada 15 días va a haber retrospectiva y entonces la realizamos, evaluamos cómo estamos haciendo las cosas que podemos hacer mejor, identificamos dolores y elementos a probar que nos ayuden a mejorar, agregar por ejemplo una planeación no tendría sentido, ¿cierto? porque sabemos que en este punto es precisamente donde no podemos planear y por eso estamos trabajando con esto, así que ya sabes que es Scrum Band, es una mezcla muy inteligente de estas dos cosas que tu proyecto no se puede planear pero que entonces van a manejar la parte de Kanban con algunos elementos de acá, esto es trabajar Scrum Band, les cuento una historia personal que les puede servir, me llamaron de una empresa para implementar Scrum que por ellos decían que necesitaban más agilidad en sus procesos y que eso Scrum estaba como chevere, es una curaduría donde te dan los permisos para construir, resulta que en la parte inicial noté que realmente no era Scrum, que a ellos les llegan las solicitudes día a día, ellos no pueden simplemente llegar y planear, ¿qué hicimos? entonces trabajar con Kanban y los tableros que han creado han sido muy chéveres y la forma como les ha visualizado dónde tienen todos los flujos acumulados y entonces qué vamos a hacer para que eso empiece a fluir, qué parte de estos flujos son las partes más urgentes de que tengan que pasar y que tengan la forma de visualizar el estado actual sea bonito o sea feo para poderlo mejorar es una excelente forma con ese, realmente es el que ellos necesitaban y ahí estamos en este momento buscando las oportunidades de mejora y empezando a hacer pruebas con este tablet, un saludito para si llegaron hasta aquí en este vídeo porque creo que es el vídeo que va a quedar como más larguito de todos pero es que realmente el tema es bastante por ambos lados la comparación y la parte de Scrum, espero que este vídeo les sirva, que les aclare algunas dudas que tengan y sobre todo quisiera que este vídeo te ayude a ti y a tu equipo a trabajar de una manera mucho más tranquila y generando unos resultados enormes. Acá les dejo mi vídeo de Scrum y de Kanban donde van a poder verlos con mayor detalle cada uno para que conozcas y acuérdate para que lo sepas aplicar muy bien. Bueno recuerden que yo soy Cris Rua y nos vemos en un próximo vídeo. ¡Chao!
DOC0130|Metodologías agiles|Hola, yo soy Cris Rua y hoy vamos a hablar sobre qué hace el Scrum Master, cuáles son sus actividades, responsabilidades, cuál es su forma de actuar, con quiénes tiene que actuar, qué conocimientos debe tener. Espero que les guste, lo hago con mucho cariño para todos ustedes. Supongamos que tú tienes un equipo de obreros que siempre manejan todo con pala y tú te enteraste que están vendiendo una máquina gigante que es capaz de hacer un montón de trabajo en un momentico y mostrar resultados muy poderosos. Entonces tú piensas y haces cuentas y dices, sí, yo lo necesito, cubre todas nuestras problemáticas actuales y decides que vas a trabajar con ella de ahora en adelante. Tú sigues teniendo a las personas que están ahí trabajando y obtuviste esta maquinota, pero ahora, ¿quién la maneja? Es una herramienta tan poderosa que necesita a alguien que sepa conducirla y guiarla para que de verdad les de ese rendimiento que están esperando. Y eso es lo que hace un Scrum Master en un equipo, ser la persona que guía esta máquina tan poderosa que es Scrum para generar resultados, pero es la persona que sabe sacarle provecho y que es capaz de guiar a este equipo que tiene tantos conocimientos diferentes para que de verdad puedan usar esta gran máquina y sacar un producto funcionando correctamente. Nuestro Scrum Master influye en el equipo que desarrolla el producto, en el Product Owner y la forma como maneja el backlog, en la comunicación que tiene el Product Owner con los stakeholders, que son las personas que necesitan el proyecto, las personas que saben cómo se debe realizar pero que se mantienen muy ocupadas en sus asuntos. Por eso tenemos acá nuestro Product Owner con sus stakeholders. En el Sprint mismo, aquí tiene mucha influencia. Y en la empresa, como ven, el Scrum Master tiene mucha parte para influenciar, así que tiene bastante trabajo que hacer. Pero ahora, ¿cómo influencia a cada parte? Ahora les voy a hablar de cosas que hace el Scrum Master y a qué parte va influenciando en cada una de estas. Bueno, hay una cosa que yo quisiera aclarar mucho en este video y es que siempre decimos el Scrum Master remueve impedimentos porque es una de las cosas que hace, ¿cierto? Remueve impedimentos que tenga el equipo para poder que el equipo avance como debe avanzar y entregue los resultados que debe entregar. Pero ¿qué es resolver impedimentos? Los impedimentos pueden estar aquí, aquí, aquí, aquí, aquí y aquí en un montón de partes, por acá también en el backlog, pueden estar en muchísimas partes. Entonces, ¿cómo hace él para identificarlos? Es una persona muy observadora y que junto con el equipo, que son esas dos partes, junto con todo el equipo, ellos van identificando este tipo de impedimentos y cómo se pueden solucionando. No es que él tenga que saberse todas las soluciones. Es esa persona que es capaz de, con preguntas muy importantes y muy bien hechas, encontrar junto con ellos la solución. Mejor dicho, que ellos encuentren sus propias soluciones, propongan, intentamos y si no, pues bueno, ya estamos otra cosa. Pero hay que estar pensando continuamente en la mejora. El Scrum Master está pendiente de hacer surgir esos problemas por los cuales el equipo no está sacando a producción. Y entonces, ¿cómo vamos a hacer para resolverlos? Porque la idea no es, en el caso de Teddy, crear software y dejarlo ahí guardadito, y de a poquitos dejarlo ahí guardadito, sino que esos obstáculos son los que hay que remover para que al final del equipo lo que haga lo saque a producción y ahí vamos viendo cómo va el avance en el equipo. El Scrum Master está pensando constantemente en la motivación del equipo, porque un equipo motivado trabaja mucho mejor, aparte de que trabaja más feliz, es capaz de generar mayores resultados. Entonces, para eso tiene muchas herramientas que él conoce o va investigando sobre cómo hacer que un equipo trabaje motivado. Aparte es una persona que tiene esa capacidad innata de motivar. No cualquier persona puede ser un Scrum Master. Nunca debe ser una persona conflictiva. Así que si en tu equipo el problema es el Scrum Master, ya de ahí para arriba no hay. Si tiene una personalidad negativa, si tiene una forma de comunicación que no es aseptiva, entonces esa persona en ese rol no cabe muy bien. El Scrum Master se vale de las retrospectivas, esa es su arma, el momento donde él se comunica con el equipo totalmente y es capaz de hacer que las cosas fluyan y aquí identificar a riesgos y también cómo cubrirlos. Ejemplo de un riesgo. Hay un equipo donde nada más una sola persona conoce de un tema y es el tema más importante, entonces todo depende de él. Ahí, aunque el equipo no lo note, él tiene que estar pendiente de eso y empezar a hacer una estrategia para transferencia de conocimiento de este integrante del equipo a los otros integrantes para que no tengamos el riesgo de que si se nos va esa persona, entonces el equipo se frena. Para el Scrum Master realmente no deben haber personas malas, es esa persona que es capaz de potenciar las cosas que tiene cada uno porque cada persona tiene sus bondades, conocimientos y su forma de hacer las cosas, entonces él tiene un estilo para hacer que esas personas fluyan como equipo y que empecemos a aprovechar todo lo que cada uno puede aportar, nada más eso es un trabajo arduo. Él está pendiente de que este equipo permanezca unido y que sean muy amigos, que trabajen muy contentos juntos porque algo sencillo, conflicto entre dos personas pero necesitan trabajar juntos y la una depende de la otra. El Scrum Master tiene que empezar a actuar ahí, por eso él tiene cierto nivel de coaching o cierto nivel de coach para poder ayudar a enfrentar y resolver estos momentos. Él es capaz de resolver problemas uno a uno, si tiene que hablar con él, tiene que hablar con él o también tener esas técnicas para resolver problemas en equipos. Llevas un espacio muy bien planeado para que esos problemas surjan y le busquemos unas formas de mejorarlos y probemos hasta que lo solucionemos. Una cosa importante del Scrum Master con el equipo es una Scrum Master en un equipo, si tú quieres que sea un mismo Scrum Master para muchos equipos, ¿qué haces? Le estás quitando el tiempo al Scrum Master de poder profundizar y encontrar los verdaderos problemas que tiene cada uno y termina siendo un Scrum Master que nada más va a las reuniones porque realmente no tiene ese tiempo para construir y pensar. El Scrum Master con el Product Owner tiene que estar muy de la mano y tiene que validar que realmente es un Product Owner y está trabajando dedicándole el tiempo necesario que el equipo requiere para poder sacar su producto adelante. Si ve que no tiene tiempo, que eso es un problema muy común para solucionar, tiene que empezar a mover fichas. Con quién abre la empresa, necesitamos más tiempo de esta persona o realmente necesitamos otra persona que sí pueda ser el Product Owner. Y ya cuando tiene la persona correcta, con el perfil correcto, que también conoce cómo hace ese perfil, entonces le empezar a ayudar para que este backlog se mantenga impecable. Cómo crear las historias de usuario, cómo crear los criterios de aceptación, cómo es la comunicación del Product Owner con el equipo. Entonces él mira que de verdad sea de la forma correcta para que Scrum funcione bien. Asimismo el Scrum Master está pendiente de esta comunicación porque es que realmente el Product Owner no necesariamente sabe todo lo más importante para el proyecto, son ellos. Entonces, ¿qué hace el Product Owner sacar esta información desde aquí? Para ello también debe haber muy buena comunicación entre estas dos partes y él está pendiente de que eso suceda porque eso garantiza que el equipo realmente esté haciendo lo más importante para este proyecto. Les ayuda a priorizar, le muestra cuál es la mejor técnica para realizarlo y empieza a hacer que esta comunicación fluya muy bien entre todas estas partes. El Scrum Master o maestro Scrum es la persona que sabe utilizar muy muy bien Scrum, ¿cierto? Sabe para qué es cada cosa. Entonces interfiere totalmente en cómo se realiza el sprint, para que el sprint se realice de verdad bajo las normas que se deben hacer. Cómo realizar una planning. Si aquí en la planning se estima cómo estimamos, cuál es el método que nos funciona, cómo que el equipo comprenda ese método de estimación para poderse comprometer durante cada sprint y que sea un compromiso que de verdad lo pueda hacer en el tiempo laboral. Si tenemos 8 horas al día, él tiene que lograr que las personas no se gasten un minuto más en la empresa, un minuto más. Él tiene que garantizar que de verdad tengamos un ritmo constante en este sprint para que las personas puedan trabajar generando mucho valor en el tiempo por el cual la empresa se está pagando, no en su tiempo de vida personal. Soy muy estricta en esto de verdad porque necesitamos que las personas tengan su tiempo libre para que pasen con su familia, para que ellas salúden a sus hijos, para que se diviertan, para que hagan deporte, para que se desestresen, para que lleguen a la empresa con energías, para seguir trabajando y empezar a hacer un producto chevere. No que sea un producto emotivo de un estrés total que tenga el equipo porque no va a ser un producto bonito. Y hay muchos estudios que comprueban que si tu trabajas más del tiempo laboral no es necesariamente porque trabajes mucho, porque tu seas una persona muy comprometida, sino que realmente el tiempo que trabajes de más es por falta de estrategia o porque te estás comprometiendo con más de lo que puedes hacer y eso no lo debemos permitir en nuestros desarrollos de software ni en nuestros proyectos. Entonces para eso tenemos el Scrum Master, para que en mucho menos tiempo saquemos mayor Él comprende cada una de estas reuniones, cuál es el objetivo, cuáles son las formas de hacerlo, qué nos está fallando y cómo podemos tener un mejor resultado porque es el maestro Scrum y conoce Scrum en su totalidad, así mismo con la review y con la retrospectiva. La retrospectiva como lo dije anteriormente es su arma. Les cuento que yo he visto a Scrum Master, que llegan a mis entrenamientos y dicen si yo ejerzo el rol de Scrum Master y en algún momento escucho que dicen no, no hacemos retrospectivas y yo digo ¿y por qué? y las Scrum Master me dicen no, es que realmente no hay tiempo. No hay tiempo para hacer retrospectiva, entonces pasamos de planeación a review y a planeación, entonces ahí uno se cuestiona muchísimo sobre ese Scrum Master precisamente, porque él es el encargado de sacar ese tiempo, de mostrar que eso es muy importante y de verdad en las retrospectivas lograr que se mejore, no solamente sacarle el espacio al equipo para que piense qué hicieron mal, sino que de verdad ser capaz de crear en esa retrospectiva una forma de mejora para un problema que están teniendo que es el problema más grande que encontraron en ese momento, para eso es la retro. Es decir que ese es el espacio para generar la mejora continua que es para lo que está encargado el Scrum Master. El Scrum Master también está pendiente de que con la Daily se estén dando los resultados que se necesitan, que es que de verdad el equipo se colabore y se converse, se escuche ¿qué problemas tenemos que nos van a impedir sacar adelante ese compromiso del sprint? El equipo, el equipo se le Scrum Master cuida que la Daily no se convierta en el tema de mostrarles a él o a él lo que el equipo ha hecho, cuida en que no caigan en ese error de ser la reunión donde mostramos el avance porque eso no es una Daily y él tiene que conocerlo. En la empresa, él está pendiente, otros Scrum Master que están haciendo, ¿cómo podemos hacer que la empresa mejore? ¿cuáles son los problemas que hay a nivel de empresa y de los equipos que se están dando ahí que nos están impidiendo tener funcionando lo más importante a nivel de empresa? También está pendiente de este tipo de cosas. El Scrum Master es un líder, sí, puede decirse que es un líder, pero es un estilo de liderazgo muy diferente a lo que estamos acostumbrados con otros roles, es un líder servicial, no es la persona que manda, que ordena, que dirige de una forma abrupta, no es una persona controladora, es una persona mediadora. Si yo lo comparara con el entrenador de un equipo de fútbol, vemos que él de cierta manera es un líder, ¿cierto? Pero si el equipo viera que este líder no los está llevando a sacar su máximo potencial, el equipo podría decir, señor, cambiamos de entrenador. Entonces, eso es un líder al servicio, que realmente todos tienen el mismo rango, solamente que él está encargado de guiar todo este marco de trabajo que se llama Scrum para que de verdad potencialicemos todo lo que podemos hacer. Él está encargado de que para que no haya problemas en el equipo, el equipo se comunique de una manera asertiva, entonces tiene que enseñarles esta forma de conversar para que no empecemos a hablar y a tener problemas por haber dicho una frase mal dicha. El Scrum Master no necesariamente tiene que saber desarrollar o realizar el producto como tal, a veces que sepa desarrollar nos trae problemas porque empieza él también a hacer el trabajo del equipo y ya no hay esa persona que está mirando por encima a ver qué problemas está pasando, se empieza a meter en los mismos problemas y ya no hay Scrum Master. El resultado del Scrum Master es realmente el resultado del equipo porque él es el que guía, él está entregado a que ellos sean los que generen y saquen adelante todo, entonces realmente si tú lo quieres evaluar, evalúa cómo está el equipo. Si nunca puede sacar a producción nada, si siempre está apegado con cosas, si siempre está desmotivado y si tienen problemas de comunicación, ahí es donde debería estar el Scrum Master, entonces ¿qué está pasando? Si tú ves que el equipo está bien sacando resultados constantemente, si es en caso de tecnología subiendo a producción al final de cada sprint o por qué no varias veces en un sprint como ya lo han logrado muchas empresas, entonces ahí sabrías que tienen un buen Scrum Master y que está haciendo muy bien su trabajo. Espero que les haya gustado este vídeo que le sirva bastante, si algo me faltó por decir ponlo en los comentarios para que las personas no se pierdan de esa información tan importante sobre el Scrum Master y si tú estás empezando a montar en tu empresa Scrum o quieres empezar a hacerlo y no sabes cómo arrancar, puedes solicitarme un entrenamiento para Scrum Masters, entrenamiento para Product Owners o un entrenamiento en agilidad en general para que toda tu gente sepa cómo comenzar y arranque mucho más rápido a montar todo este tema de Scrum y de agilidad que es tan bonito y que genera tanto resultado en las empresas. Te dejo mi correo en los comentarios y si lo que quieres es asesoría online también podemos realizarla. Aquí te dejo mi correo donde podemos conversar al respecto. Recuerda suscribirte a mi canal y si te gustó dale like. Les agradezco a todos por esos mensajes tan lindos que me dejan en mis vídeos. Bueno que tengas un lindo día y hasta el beco.
DOC0131|Metodologías agiles|Hola, tiempos invernos, yo soy Cris Rua y hoy vamos a hablar sobre el rol del Product Owner y vamos a hablar cuatro cosas importantes, por qué se necesita su objetivo en el equipo, su perfil y sus responsabilidades. En mi experiencia el rol del Product Owner es el menos entendido de Scrum y tomárselo a la ligera trae serias implicaciones en el equipo, ¿por qué? Porque suelen poner a personas no indicadas o personas que serían indicadas pero no tienen tiempo, o personas que no saben hacerlo y creen saber hacerlo, o personas que no tienen el perfil, etc. ¿Por qué se necesitan? ¿Qué me dirían ustedes si yo les digo que es posible que una empresa tenga un equipo de tecnología o de cualquier tipo de proyecto? Personas que le cuestan bastante a la empresa en sentido monetario y usted con ese equipo ahí bien costoso, que es bien costoso y el equipo está trabajando en una cosa que no es la más importante para hacer en este momento y fuera de eso está a toda marcha, pero a toda marcha en algo que puede ser lo no más importante del proyecto o a veces hasta caprichos de las personas que le piden al equipo. ¿Qué me dirían ustedes? ¿Irreal? ¿Equipo costoso trabajando en algo que no es lo más importante? ¿O caprichos? ¿Equipo aburrido porque sabe que realmente no está generando el valor que podría estar realizando con ese potencial que tiene? ¿Será que eso podría ser verdad? Pues suele ocurrir mucho y por eso Scrum trae al Product Owner para que resuelva este problema. Algunas empresas dicen no, pero una persona dedicada a ser Product Owner, no, no, pongamos este un 20% del tiempo y que lo resuelva y termina el equipo con los mismos problemas que venía porque necesitamos una persona dedicada 100% a realizarlo. Entonces lo necesitamos porque sin un rol dedicado los equipos pueden enfocarse en cosas no importantes. Y de aquí sale el objetivo del Product Owner en el equipo Scrum. El Product Owner es el que logra que el equipo realmente se concentre a realizar lo más importante en cada sprint, es decir, en cada periodo de tiempo, que el equipo realice lo que es más importante para el proyecto y lograr identificar eso no es fácil. Y como no es fácil, por eso necesitamos una persona con el perfil adecuado. Una persona muy estratégica, que sea capaz de lograr que se visualice el proyecto a lo lejos y de eso identificarlo realmente más importante y de eso estructurarlo para que se vaya teniendo listo para que el equipo lo coja en cada sprint. Que sepa decidir qué va y qué no va en el proyecto, esto es muy importante. Porque decir qué va puede ser tan sencillo como meter cosas, decirle que sí a la gente, algo sencillo. Pero decir no a una petición de un usuario. Decirse que no cuando uno cree que es lo que se tiene que hacer. Decirle no a un gerente, a un director, a un dueño de otra empresa. Y ese saber decir que no es el que evita usted tener un equipo ahí trasnochando, haciendo un montón de cosas que muchas veces son caprichos. Y si usted los deja meter caprichos, le llenan un sprint de caprichos. Entonces por eso necesitamos que de verdad está esta persona que garantice que lo que se hace es porque se tiene que hacer y no porque una persona de un cargo alto lo pidió o porque se le ocurrió que sí se podría hacer. Importantísimo que tenga habilidades comunicativas porque le va a tocar ser la persona que recibe los requerimientos, que vaya y pregunta, evalúa qué cosas se necesitan hacer en el proyecto y qué le va a pasar esa información al equipo. Entonces necesitamos una persona que de verdad sepa entender las ideas y comunicarlas, expresarlas, adecuarlas. Y sobre todo que se disfrute de lo que va a realizar. Porque usted pone a una persona, a una persona que no sepa hablar, que no sepa decir que no, que no sea muy estratégica, esa persona se te quema. En este punto se te quema y te renuncia. Entonces busque una que de verdad se disfrute de esto. Conozco product owners que les encanta coger la gente y decirle, usted primero, usted viene a la segunda, usted tercero, usted todavía no viene, qué pena mismo, sigamos con el otro y se lo disfrutan. Les encanta saber que ellos son esa persona que guía al equipo a realizar lo más importante. A saber ordenar, saber liderar y saber decir que no, les llamamos tener pantalones muy grandes o tener muchos pantalones. Sobre las responsabilidades encontrarán mucha información en internet. Acá les listaré lo que debe hacer un product owner para lograr este objetivo. 1. Crear las historias de usuario con los criterios de aceptación claros. No es que él está trabajando en lo suyo y cuando el equipo va a iniciar sprint, va y corre y pone cualquier título en cualquier parte, eso no. Es hacerlos muy bien, darles el tiempo que se merecen para que queden perfectas y que sean las correctas. Entonces no es solo escribir historias de usuario, es de verdad haberse tomado el tiempo de identificar las que son y empezar a plasmarlas priorizadamente. Y con los criterios de aceptación claros, aquí abajo te voy a dejar el videito de cómo crear historias de usuario. Decidir que no entra en el proyecto es parte de su día a día, saber qué es lo que entra, qué se convertirá en historias de usuario y saber que no. El Release Plan es esa visión a futuro que se tiene del proyecto, se realiza por medio de la Inception. Acá también les voy a dejar los videitos que tengo sobre cómo iniciar un proyecto, que es con un Impact Mapping y un User Story Map. Acá también les dejaré los videitos. Pero él es el encargado de que esto sí exista, que lo sabe manejar. El Release Plan es su guía para ir llevando el proyecto a una feliz entrega y entregas constantes que nos van llevando a entregar los objetivos de los releases y que nos llevan a tener un proyecto entregado en el término que necesitamos. Así que este es su día a día. Tener claro el mínimo producto viable. El mínimo producto viable es lo mínimo que tiene que salir del proyecto para que sea usable. No lo más lindo, no lo más tierno, con los mayores moños que existen, sino algo que y que es lo más importante. Ahí leer sobre la ley de Pareto, el 80-20, que se dice que el 20% de un proyecto es lo que se usará el 80% de las veces. Así que tiene que ser un duro para ubicar eso y tenerlo como el mínimo producto viable del proyecto. Esto último no lo identifica él solo. Él realiza las reuniones con los equipos, las que se tienen que realizar, trae los stakeholders, que son las personas interesadas en el proyecto y el equipo y con ellos identifica todo esto, el release plan, el mínimo producto viable. No creas que se tiene que sentar solito a hacerlo, no es debido. Utilizar técnicas de priorización de los elementos que hay. No es lo que a ella le ocurrió o lo que cree que viene primero, no. De verdad tener muy claro esto por qué viene primero, qué pasaría si esto no se hace, y para eso hay diferentes técnicas que lo ayudarán a tomar estas decisiones. Tener total disponibilidad para aclarar dudas durante el sprint. No es, ve, yo fui un raticón del equipo, pregunté si necesitaba algo, pues realmente no dijeron nada, no. Es ir, estar presente, que el equipo sienta, que le puede preguntar, que haya feeling entre ellos, que vaya constantemente. Una duda que él aclare, cuando el equipo está desarrollando, les evita a futuros errores, malentendidos, entonces es estar presente. Jamás el tema de no, pues yo lo dejé súper bien documentado, mi historia son como de tres o cuatro páginas, eso ahí está todo. Recuerda que las historias son pequeñitas, lo mínimo necesario, pero un product owner que quiera escribir todo en su historia para evitar conversación, vea. Se los lleva al diablo. Porque aquí prima la conversación, sí, es, ay no, lo novedoso, la conversación es importante, sí, muchísimo, y esto lo debe tener súper claro el product owner y estar disponible y de verdad ejercer mucho esto con ellos, voy a hablar, voy a preguntar, que me cuenten, que me pregunten, aquí estoy. Voy a decir una obvia que ni siquiera la voy a notar, o sea, está así. Solo lo digo porque he visto que en algunas ocasiones hacen algo que no es, y es que tiene que estar presente en las reuniones, en la planeación, en el refinamiento, en la review, porque sin él estas tres reuniones no se dan, no falta el que dice no, que vaya el Esquema Master, que vaya no sé qué, eso no es tener un product owner, he visto casos de equipos embalados en una planning que porque no ha llegado, pues, no, eso sí, no, eso sí es lo mínimo necesario. Si él está presente en la retrospectiva, si el product owner es tan buen product owner que es parte del equipo, el equipo son Esquema Master Product Owner y el equipo de desarrollo, entonces si él es de los que siente que si ganaron ganamos, o sea, ganamos, entonces él va a una retrospectiva porque es un espacio de mejora y va a estar presente para de verdad mejorar. Si es un product owner nombrado, donde se va a una retrospectiva y sucede que al él estar presente en la retro la gente no habla sobre sus errores porque no le tienen confianza, entonces no debe estar en la retro, pero ahí la invitación es que sé parte del equipo, no seas la persona que le pide al equipo, ellos son allá, yo les pido, soy el product owner, usted es parte del equipo, si el equipo le va mal, a usted le fue mal, si el equipo le va mal nunca diga, ay son ellos, no, puede ser que estas escribiendo malas historias de usuario, que no son claras, los criterios no son claros, no estas conversándolo bien, entonces realmente tú eres parte de todo eso. Y en la daily normalmente no, porque en la daily es donde el equipo que está desarrollando dice todos los problemas que tiene para lograr cumplir todo en el sprint, entonces a veces les da pena con la persona, ahí vuelve y repito lo mismo, si eres parte del equipo esté ahí pero calladito, escucha nada más, no es un momento para estar mirando cómo va el avance, jamás ese es una daily y eso mucha gente lo lleva allá, entonces no, si es posible que el equipo se reúna solito mejor, y si el equipo te pide que estés presente, perfecto, vaya, si te necesitan ve. Bueno y este fue mi video sobre product owner, un rol que bien puesto, bien elegido y que de verdad tenga el tiempo y esté comprometido, puede sacar tu proyecto adelante, como jamás has tenido un proyecto, selecciona muy bien a esa persona y tú si me estás viendo y eres un product owner, si ves que tú no tienes tiempo para hacerlo y que esto es demasiado para ti, selecciona otra persona que sí tenga el tiempo para hacerlo, el hecho de que tú des nada más, un poquito de tu tiempo para realizar todo ese montón de cosas es permitir que se haga mal hecho este rol, entonces busca quién sí puede ser el adecuado y que tú seas un stakeholder para él, que él llegue a ti, te pregunte porque tú eres la persona que conoce. Y bueno, espero les haya servido mi video, les hago con muchísimo gusto que con este video realicen muchos cambios en el equipo, luego viene el video de el anti product owner y otro video de qué es lo que realiza el product owner mientras el sprint está corriendo, para que también los busques acá cuando ya los tengan listos. Saludos desde Medellín, Colombia, te estés muy bien, chao.
DOC0132|Metodologías agiles|Hola, mucho gusto. Yo soy Cris Rua y en este video vamos a hablar del Sprint Review. El Sprint Review hace parte de Scrum. Scrum es uno de los marcos de trabajo ágiles y es con el que vamos a poder entregar un proyecto grande en pedacitos pequeñitos llamados Sprint. Es una reunión de personas del mismo tiempo fijo y esta reunión se hace al final, es la penúltima reunión de Scrum, donde le mostramos al cliente nuestro avance del proyecto. Ojalá que ya el cliente pueda tomar eso y empezarlo a usar. Así sea, desde el inicio del proyecto que ya él pueda sacarle valor a su proyecto. El cliente al conocer cómo va avanzando su proyecto nos dará feedback y este es el punto más importante de Scrum. Que mientras vamos avanzando el cliente vaya observando cómo va el proyecto y puede decir, sí vamos súper bien o puede decir no, por aquí no es. Y entonces en ese caso reencaminamos el proyecto, asegurándonos de irnos de la mano del cliente para lograr el objetivo que él tiene con este proyecto. ¿Quiénes están presentes en la Sprint Review? En la Sprint Review está el Scrum Team completito y están los stakeholders o personas interesadas en este proyecto, que son los que van a validar cómo está el avance del proyecto. El Scrum Team muestra los resultados de lo que hizo. En la Sprint Review, si detectamos que debemos hacer algún cambio, estos pueden ser agregados al backlog, que son las cosas que están pendientes por hacer en este producto o en este proyecto. Así que pilas, no vamos a tener un backlog todo ya listico y todo súper planeado y todo detallado, porque recuerden que en Scrum vamos adaptando el proyecto según el feedback que nos va dando el cliente. Así que de verdad que podamos recibir esto y que pueda ser parte del backlog normalmente. Así que este es un punto de adaptación. Duración. ¿Cuánto dura la Sprint Review? Nosotros trabajamos por Sprint, recuerden, y los Sprints van a ser siempre tiempos fijos de una semana, 15 días, 3 semanas o máximo un mes. Si tenemos Sprint de un mes, una Sprint Review podría durar aproximadamente 4 horas, o mejor dicho, máximo 4 horas o lo correspondiente en tiempo. Ejemplo, para un Sprint de 15 días, que es lo que normalmente duran los Sprints, como el 80% de los equipos más o menos trabajan con Sprint de 15 días, para estos equipos dura 2 horas. ¿Qué no es una Review? La Review no es una presentación oficial en PowerPoint mostrando qué es lo que hemos avanzado del proyecto y mostrando que ya avanzamos en bases de datos o que ya tenemos el equipo listo que va a arrancar con esto. Eso no es una Sprint Review. La Sprint Review es donde mostramos tranquilamente, ojalá con la mínima presentación posible, porque la idea no es acá generar documentación y sentir que con eso estamos avanzando, ¡ay, ese papeleo! ¿no sería mejor mandarlo a volar? sino de verdad encaminar nuestro proyecto a recibir resultados. O sea que aquí lo que el equipo hace es decir, Vea, señor cliente, usted ya le puede dar click en este botón y mire cómo nos carga la información del cliente. Entonces, en este punto usted podría modificar esto, modificar esto o otro. Esa es la intención de Scrum, ir viendo avances y avances reales y concretos que sean evaluables por el cliente. Por eso es que hablamos de historias de usuario, porque todo lo que construye el equipo, Aunque sea pequeñito, ya es una funcionalidad que le sirve al usuario final del proyecto. Miren que ahí está todo el cambio cultural que trae esto. ¿Por qué el equipo lo presenta? Y no por ejemplo solo el Product OwnerPay le muestra al cliente la avancia del equipo. Porque el equipo está empoderado de lo que se comprometió a hacer. Y como el mismo equipo, pilas con esto que de verdad se hace en los equipos que tenemos en nuestras empresas, Nuestro equipo se comprometió en la planeación, que era la primera parte del sprint. Y el mismo equipo dijo, podemos hacer esto, esto, esto y esto y hasta aquí. Fue el mismo equipo quien se comprometió. No fue algo impuesto por una persona externa a los desarrolladores. Que les dijeron, vea ustedes tienen que hacer estas siete cosas en este sprint y ya. No, como el equipo mismo se comprometió y dijo hasta donde podía, el mismo equipo lo entrega en la review. Para que ahí tenga que decir y mostrar y si tuve algún problema ellos mismos dar esa cara. No va a haber alguien que esté dando la cara por ellos. Así que todos ellos van a encargarse de decir, vea nos comprometimos a hacer estas cosas en la planeación. Y vealas aquí funcionando. Y esta otra no alcanzamos o si alcanzamos pues tienen que decir en ese momento la realidad. Es parte de la transparencia que tenemos en Scrum. Otro consejo clave, no lo unas con la retrospectiva. He visto equipos que dicen, ah sí, o sea tres horas y ahí va a ser review y retrospectiva al mismo tiempo. Son cosas totalmente diferentes con objetivos totalmente diferentes. Así que cumple el objetivo de lo que es la review. Ya, genéale un espacio después de esta al equipo pero da un descansito en la mitad. O por ejemplo prepara la review antes del almuerzo y luego después del almuerzo que venga la retrospectiva. O la review al final del día y al otro día en la mañana que venga la retrospectiva. Porque en la retrospectiva otra vez tomamos energías, repensamos cuáles eran nuestros problemas. Y hacemos que todo sea diferente en el próximo sprint, que cada vez sea mejor. Por eso no deben estar unidas. Y ahora hablemos de si podemos aplazar o no aplazar la review. Cuando iniciamos el sprint ya sabemos que son de 15 días. O sea que ya sabemos la fecha en que va a ser la review. Por eso hacemos unas dailies para ir sabiendo que todo se va a hacer hasta este punto. Pero ¿qué pasa? Dijimos que vamos a hacer un alcance en un tiempo definido, ¿cierto? Dijimos por ejemplo, ocho cosas, ocho historias de usuario las vamos a entregar en esta fecha que hay aquí. ¿Qué puede pasar? Que se llegue la fecha y no tengamos el alcance listo. ¿Este todo comenzado? ¿O hemos avanzado y cerrado unas cositas otras no? Entonces en ese punto algunas equipos se preguntan si no han terminado el compromiso del sprint. Y van a hacer la review. Si mejores pegan y mueven la review hasta que acaben el compromiso. Y dicen, oh sí, entonces este sprint se alarga unos viditas más porque nos faltan un poquito de cositas y lo alargan. Eso no se hace. ¿Por qué? Porque estamos trabajando en Scrum y en Scrum es acomodarnos a entregas continuas. Si simplemente el equipo se compromete en una planeación a entregar cuando lo termine. Porque dice, ah pues súper ahí está la review planeada. Pero si no terminamos tampoco pasa nada. Entonces no va a haber esa presióncita de entrega y ese ritmo constante que se quiere con Scrum. Así que si tenemos la sprint review programada acá y no se ha terminado todo. Normal, se hace ese mismo día y el equipo da la cara y dice, no alcanzamos. ¿Y qué pasa después de eso? Puede que ese sprint nos fue mal en la entrega. Pero qué pasa cuando tenemos que dar esa carita al cliente y mostrar que no alcanzamos el compromiso. Para eso va a haber una retrospectiva después donde haremos esa mejora. Pero es mejor salir así porque en el próximo sprint seguramente lo haremos mucho mejor. Y no cometeremos el mismo error anterior. Algo parecido si de pronto el stakeholder o cliente no puede asistir a la review. Entonces, ¿qué pasaría? En ese caso lo ideal es estar conectando con ellos para que ellos asistan y puedan dar feedback del proyecto. Cierto, cada sprint feedback. Que de pronto en algún sprint no pudieron asistir. ¿Qué hace? El equipo no puede entonces frenar. Si el equipo ya sabe que termina sprint y vuelve a arrancar otro sprint de una y quiere frenarla acá porque no puede venir el cliente. Entonces, ¿qué va a pasar? Se frena el siguiente sprint, queda el equipo sin programación. No podrían. El equipo se compromete a trabajar ahora por sprint fijos. Entonces, en ese caso el equipo le muestra al product owner, valida en criterio, la aceptación de las historias de usuario, cómo estuvo todo, muestra todo funcionando. Y ya el product owner, que es esa persona en medio de el equipo y el cliente, se encargará de mostrarle a la gente el avance y que le de feedback al respecto. Pero el equipo mantiene su ritmo constante. Así que hagamos muy bien nuestra review que de verdad nos sirvan para esos tres pilares de scrum que son Transparencia, Inspección y Adaptación. Estos son los pilares del empirismo que es en lo que se basa scrum. Así que, ¿cómo se ve reflejada aquí? Transparencia, mostramos todo tal cual va. Inspección, este es uno de los espacios donde inspeccionamos cómo va todo. Y adaptación es un punto donde depende de cómo esté todo, adaptamos para que el proyecto siga el rumbo que debe seguir. Y esta es la review, donde revisamos el avance de nuestro sprint. Vamos a hacerlo muy bien en esos equipos. Vamos a conocer mucho esta cultura scrum, que es todo un cambio de mentalidad y un cambio de formas de hacer las cosas. De verdad que cuando uno llega a un equipo y le enseña esto, como que uno trata de hacer todo como ha venido haciendo. Pero scrum es una forma de obtener resultados tempranos y continuos. Así que vamos a cambiar esa cultura. Aparte de pensar en nuestras personas que trabajen muy motivadas, esto de verdad es un cambio de pensamiento que debemos hacer en nuestras empresas. Para ello, si quieres conocer más sobre scrum, yo puedo acompañarte. Si eres una persona, puedes asistir a uno de mis cursos abiertos. Aquí abajo te dejo los links para que puedas acceder. Y si eres una empresa, puedo capacitar tu empresa directamente con cursos para ustedes. O podemos hacer paquetes para que mandes algunas personas de tu empresa a mis cursos abiertos y conozcan que es scrum. Así que yo puedo enseñarles scrum para gestionar sus proyectos, kanban para gestionar ese día a día y reducir tiempos de entrega. Y giga, que es la herramienta donde se manejan estos marcos de trabajo ágiles. Espero que estén muy bien, que hayan aprendido mucho en este video. Y nos vemos en otro video. Que estén muy bien. Chao. Suscríbete a mi canal. Yo estoy imparable. No te enojes con ellos, quiero. Ellos lo van a hacer bien. No. ¿Si ven? Lo hicieron enojar. No sigan haciendo eso. ¡Suscríbete!
DOC0133|Estadistica y Probabilidad|Estadística es la ciencia que se encarga de recoger, organizar e interpretar los datos para tomar mejores decisiones. Es decir, saber el significado de los datos. Algunas aplicaciones de la estadística se dan en el campo deportivo para dar seguimiento a los resultados de tu equipo, por ejemplo de fútbol, saber cuántos goles, cuántos partidos se han ganado, perdido, empatado, entre otros datos, sabrás si pasa o no, a otra ronda rumbo al campeonato. Otra aplicación es para conocer el número de vacunas que se necesitan abastecer en un centro de salud. No se puede mandar un número aleatorio porque faltarían o sobrarían y es muy importante optimizar recursos. Por eso es importante conocer la cantidad de personas, saber sus edades, saber sus enfermedades, entre otros datos. Otra aplicación es para proyectar construcciones en una localidad o ciudad, saber dónde se van a construir más casas, de qué dimensiones, parques, escuelas, plazas comerciales. Aquí se necesitan datos de cantidad de personas, nivel económico, edades, entre otros datos para poder proyectarlas. Otra aplicación cada vez más sonada es la inteligencia artificial que está cada vez más en nuestras vidas. Se necesitan tener más datos organizados dependiendo a ciertas categorías para poder crear nuevos algoritmos que permitan automatizar tareas. La estadística se divide en descriptiva e inferencial. La primera se caracteriza porque se estudia a una población y la segunda se caracteriza porque estudia a una muestra. Estadística descriptiva. Permita analizar todo un conjunto de datos de los cuales se extraen conclusiones. Algunos ejemplos de estadística descriptiva sería para elaborar reportes de ventas mensuales ya que con toda la información de cuántos artículos se vendieron, precio unitario, cantidad de venta por artículo, entre otros, se elaboran y pueden tomar decisiones futuras. Otro ejemplo sería el rendimiento de los estudiantes de una escuela. Si se profundiza en las tareas, cuántos días asistieron, calificaciones por materias, participaciones, entre otros datos, se puede tener un buen rendimiento de cada estudiante en una escuela. Otra aplicación sería tomar todos los datos de un equipo deportivo, cuánto cuesta comprar un jugador, cuánto venderlo, el equipo, cuántos partidos ha ganado, cuántos ha perdido, cuántos ha empatado, cuántos ha llegado a una final de torneo, entre otros datos se pueden tomar decisiones futuras. Y para reforzar el tema de la clasificación de la estadística, aquí tenemos unas situaciones que te anticipo que puedes poner pausa al video. En la sección de comentarios puedes poner tu razonamiento, tu respuesta de ellos porque yo la voy a hacer a continuación. Pero antes te invito a que te suscribas al canal MAGTUMi, por acá está el botón rojo para que recibas antes que nadie nuestros videos de algebra de estadística de probabilidad de cálculo, entre otros temas. Y siempre al final acá te dejamos videos de apoyo, en este caso de estadística. Ahí voy con la respuesta del primer ejercicio, el promedio de goles de un equipo en una temporada. Aquí como se conocen los datos, la frecuencia, es decir, cuántos goles anotó cada jugador, en qué o contra qué equipo, toda esta información se puede tabular, poner en tablas, en gráficos y poder tomar decisiones o conclusiones de ellos. Entonces es una estadística descriptiva. En la siguiente situación se tienen porcentajes por color de cabello en estudiantes de una escuela. También se tienen todos los datos para obtener porcentajes, se pueden hacer tablas, gráficos, entre otras interpretaciones para poder tomar conclusiones. Así que también es una estadística descriptiva. En la tercera situación conocer la cantidad de personas que saben leer en una ciudad, cómo es una gran cantidad de datos que se tendrían que recabar, en este caso de una ciudad, es mejor trabajar con una muestra. Así que en esta situación se recomienda una estadística inferencial. En la siguiente situación se tienen porcentajes por color de cabello en estudiantes de una escuela. Así que también es una estadística descriptiva. En la tercera situación conocer la cantidad de personas que saben leer en una escuela. Así que también es una estadística descriptiva. En la tercera situación conocer la cantidad de personas que saben leer en una escuela. Así que también es una estadística descriptiva.
DOC0134|Estadistica y Probabilidad|Hola a todos y bienvenidos a esta clase donde vamos a explicar qué es la estadística. La estadística es una colección de herramientas que se utilizan para obtener respuestas a preguntas importantes sobre los datos. El campo de la estadística es la ciencia de aprender de los datos. Saber de estadística te permite utilizar los métodos adecuados para recopilar datos, hacer análisis de forma correcta y presentar los datos de forma eficaz. Es un proceso crucial detrás de cómo hacemos descubrimientos en la ciencia, cómo tomamos decisiones basadas en datos y hacemos predicciones. La estadística se puede dividir en dos bloques básicos, la estadística descriptiva y la estadística inferencial. La estadística descriptiva se refiere a métodos utilizados para resumir nuestros datos y para aprender y compartir información sobre ellos. Una forma de pensar sobre la estadística descriptiva es la de hacer cálculos para resumir las propiedades de nuestros datos. La estadística descriptiva también puede incluir gráficos y visualizaciones que nos permiten entender nuestros datos de una forma más visual. La estadística inferencial es una forma de cuantificar las propiedades de una población a partir de una muestra y hacer inferencias y predicciones. Podemos testear hipótesis utilizando diferentes grados de confianza. Hay muchas razones por la cual deberías estudiar estadística. La primera es para poder hacer investigaciones. Si no conoces los métodos estadísticos necesarios, será muy difícil tomar decisiones basadas en tus datos. Tendrás que saber recopilar tus datos, escoger el test adecuado y analizar los resultados. Estudiar estadística también desarrolla tus habilidades críticas y analíticas. La estadística puede ser utilizada para engañar o para desinformar. Entonces, si entiendes de estadística, estarás en mucha mejor posición para poder evaluar las conclusiones que te comunican los demás. Si trabajas con datos de cualquier tipo, haces investigaciones o quieres meterte en el mundo de la ciencia de datos, aprender estadística puede ser muy valioso.
DOC0135|Estadistica y Probabilidad|Hola, y bienvenidos a esta clase donde hablaremos sobre la población y la muestra. Es un concepto bastante simple, pero es importante repasarlo y realmente entenderlo. Entonces, ¿qué es la población? La población, denotada normalmente con N mayúscula, se refiere al universo, conjunto o totalidad de los elementos sobre los que se investiga o se hacen estudios. Es toda la población, mientras que la muestra, denotada N minúscula, es una parte o un subconjunto de elementos que se seleccionan previamente de una población para realizar un estudio. Los parámetros suelen ser los datos de la población, mientras que los estadísticos son los datos de la muestra, de ahí el nombre estadística. La población es un poco difícil de definir. Imagínate, por ejemplo, la población de la Universidad de Barcelona. ¿Cuántos alumnos son? ¿Todos? ¿Los que están de vacaciones? ¿Los que están de intercambio en otros países? Pueden ser todos ellos la población. Por eso es difícil de definir. Una muestra, en cambio, es más fácil. Puedes entrar a la cafetería de la universidad y buscar a 50 alumnos. Esto sería una muestra de la población de los alumnos de la universidad. ¿Entiendes la diferencia entre población y muestra? La muestra debe ser siempre al azar y representativa, que debe reflejar los miembros de la población. En el ejemplo que acabamos de hablar, si entramos a la cafetería de la universidad, no estamos eligiendo los alumnos al azar, ya que estamos escogiendo aquellos que estaban en ese momento en la cafetería. No hemos escogido de todos los alumnos. Por eso, en este caso, esta muestra no está escogida al azar. ¿Y son representativos? Bueno, nuestra muestra sería de aquellos alumnos que comen o van a la cafetería. Pero ¿qué pasa con los alumnos que nunca van a la cafetería? Ellos también deberían ser parte de nuestra muestra. Por eso, esta muestra tampoco es representativa. ¿Y cómo aseguramos de que nuestra muestra sea escogida al azar y sea representativa? Una cosa que podríamos hacer es pedir acceso a la base de datos de todos los estudiantes de la universidad y escoger un número de estudiantes al azar, utilizando alguna técnica o algún programa para hacer esto. Entonces, espero que hayáis entendido la diferencia entre población y muestra y cómo escoger una muestra al azar y que sea representativa de nuestra población. ¿De acuerdo?
DOC0136|Estadistica y Probabilidad|Hola, y bienvenido a esta clase donde hablaremos de los tipos de datos que existen. Hay básicamente dos tipos de datos generales que existen. Los categóricos o los numéricos. Los categóricos, por ejemplo, podrían ser la marca de un producto, un hombre o mujer, sí o no. Estas serían respuestas a preguntas como ¿qué marca de producto utilizas más? ¿Eres estudiante de la universidad? Todas estas preguntas tienen respuestas categóricas, por ejemplo, la marca de ese producto, un sí o un no, como hemos dicho. Luego tenemos variables numéricas o datos numéricos y que están separados en discretos y continuos. Un dato numérico discreto sería, por ejemplo, el número de hijos que alguien tiene. Nadie tiene 1,5 hijos, un hijo y medio. Es un número entero, en este caso el número de hijos. Cualquier tipo de objeto que se cuenta realmente serían variables discretas. El otro tipo de dato numérico es el continuo. Es infinito e imposible de contar. Por ejemplo, tu peso. Puedes pesar 70 kilos exactos o puedes pesar 70,253, por ejemplo. Otros ejemplos de datos continuos serían la distancia y el tiempo, que puede ser cualquier valor. Entonces, los dos tipos de datos generales son categóricos y numéricos. Y los numéricos pueden ser separados en discretos y continuos. En la siguiente clase hablaremos sobre los niveles de medición.
DOC0137|Estadistica y Probabilidad|En esta clase vamos a hablar sobre los niveles de medición. Los niveles de medición en tipo de datos se pueden separar en primero cualitativos y cuantitativos. Los datos cualitativos se pueden separar en nominal y ordinar y los cuantitativos en discreto y continuo. Empecemos con los cualitativos nominal y ordinal. Nominal puede ser la marca de un producto, por ejemplo si hablamos de coches serían BMW, Mercedes, Audi, etcétera. Todos estos serían cualitativos nominales. En cambio ordinales podrían ser las respuestas a lo satisfecho que estás con un producto. Tienen un orden por eso ordinal. Si alguna vez habéis hecho un cuestionario seguro que os habéis encontrado con respuestas como estas. Nada satisfecho, poco satisfecho, satisfecho, muy satisfecho y totalmente satisfecho. Estos son valores cualitativos ordinales porque se le puede dar un orden. Muchas veces hasta se le asigna un número para poder sumarlo y coger una media. Hablemos de los valores cuantitativos. Hay dos principales. El ratio que a veces se llama razón que tiene un valor cero real y el intervalo que no tiene un valor cero. Vamos a mencionar un ejemplo. Si yo tengo tres manzanas y tú tienes seis, tú tienes tres manzanas más que yo. Esto sería un ratio y su valor cero indica la ausencia del atributo. Por ejemplo, si yo tengo cero manzanas es que hay cero manzanas. Pero esto no es el caso con el intervalo. En el intervalo, por ejemplo, de la temperatura si hoy hace 40 grados centígrados y hace seis meses hacía 20 grados, podría parecer que hoy hace dos veces más calor que hace seis meses. Pero eso no es el caso porque si lo medimos en Fahrenheit, el sistema americano no sería correcto. 40 grados centígrados son 104 Fahrenheit y 20 grados centígrados son 68 grados Fahrenheit. Entonces no es correcto decir que hoy hace dos veces más calor que hace seis meses. El problema viene que cero grados centígrados o cero grados Fahrenheit no son valores cero reales, sino una representación de la temperatura. Por contraste, cero grados Kelvin que sí tiene un valor cero real, ya que lo más frío que algo puede ser es cero grados Kelvin, donde los átomos dejan de moverse. Cero grados Kelvin es igual a menos 273,15 grados centígrados. Entonces espero que hayáis entendido la diferencia entre cuantitativo de ratio con un valor cero real, que puede ser por ejemplo cuando estás contando objetos. Y un valor cero indica la ausencia de ese atributo. Cero manzanas es que hay cero, mientras que el intervalo pues no tiene un valor cero real. Cero grados centígrados no es que no haya temperatura, simplemente es en una escala. Para repasar tenemos los datos cualitativos nominal, ordinal y los datos cuantitativos discreto y continuo.
DOC0138|Estadistica y Probabilidad|¿Has oído hablar de la frase una imagen vale más que mil palabras? Pues un gráfico vale más que mil números. Ahora que hemos visto los tipos de datos y niveles de medición que existen, podemos explorar los diferentes tipos de gráficos y tablas que nos permitirán resumir nuestros datos en visualizaciones. Empezaremos con la representación de datos categóricos. ¿Qué tipos de gráficos y tablas podemos utilizar? Existen cuatro tipos de representaciones que son las más utilizadas. Tenemos primero las tablas de frecuencia, donde podemos contar el número de cada categoría y también calcular la frecuencia relativa. No te preocupes que ahora haremos un ejemplo. El segundo tipo es el gráfico de barras, donde podemos ver visualmente el número de ocurrencias de cada categoría. Luego está el gráfico de tarta, donde podemos ver la proporción de cada categoría. Este tipo de gráfico es utilizado mucho en cuotas de mercado, por ejemplo, pero hay que tener mucho cuidado porque no nos da valores absolutos, sino una proporción. El último es un diagrama de pareto, que nos permite ver los valores con barras, pero también tiene la frecuencia acumulada visualizada en una línea. Este gráfico une las ventajas del gráfico de barras y el gráfico de tarta en uno. Ahora haremos un ejemplo para que lo veáis. Aquí tenéis una tabla de frecuencia con tres productos, producto A, producto B y producto C. Tenemos 250 del producto A, 150 del producto B y 100 del producto C, que suma a 500. Esto podemos imaginarnos que son ventas de un producto. Tenemos la frecuencia y también tenemos la frecuencia relativa. Veréis que la frecuencia relativa es el número de ocurrencias de ese producto dividido el total. Con esta tabla nos da una visualización de estos datos categóricos con frecuencia y frecuencia relativa. Ya podemos ver que el 50% de las ventas fueron producto A, el 30% fueron producto B y el 20% fueron producto C. Pero aparte de la tabla de frecuencia podemos crear un gráfico de barras. Como veis en este gráfico visualmente ya podemos ver el número de ventas de producto A y vemos que es mucho mayor que producto B y producto C. Cuando tenemos muchos productos a lo mejor crear una tabla de frecuencia tan grande pues nos es incomodo y realmente no se visualiza tan bien. Por eso el gráfico de barras es un gráfico muy utilizado cuando tienes que contar el número de ocurrencias en cada categoría. Luego tenemos el gráfico de tarta. Como veis aquí el gráfico de tarta separa en lo que parece cortes en una tarta el porcentaje en los diferentes productos. Este como he mencionado antes, este gráfico pues sirve para ver las proporciones. Como vemos la mitad de la tarta, 50% es el producto A, 30% es el producto B y el 20% es el producto C. Esto nos deja verlo de una forma más visual. El último que habíamos mencionado es el diagrama de pareto. Para hacer el diagrama de pareto tenemos que calcular la frecuencia acumulada. Como veis aquí primero tenemos que ordenar, en este caso ya estaban ordenados por 250, 150 y 100. Tenemos que crear aquí la frecuencia acumulada. Utilizando estos datos podemos crear este gráfico que parece un gráfico de líneas con un gráfico de barras encima. Esto es el diagrama de pareto. Cuando tienes muchos productos puedes ver la proporción de cada producto. Por ejemplo aquí ya directamente vemos que hay 250 y vemos también que es el 50% de los productos. Y utilizamos la acumulación para decir por ejemplo que los dos productos que más se venden, venden el 80% de los productos. ¿De acuerdo? ¿Y de dónde viene este gráfico de pareto? Pues el gráfico de pareto fue inventado por Vilfredo Pareto, que fue un ingeniero, sociólogo, economista y filósofo italiano. Es muy conocido por el principio de pareto, la regla del 80-20, que seguramente habréis escuchado hablar de él. 80% del efecto normalmente viene del 20% de las causas. Un ejemplo que contó Microsoft es que solucionando el 20% de sus bugs o fallos en su código, solucionaban el 80% de los problemas de sus clientes. Y este número del 80-20 es un número que se repite en un montón de diferentes ámbitos que sean de negocio, de tecnología y ciencia. Siempre o casi siempre el 80% del efecto normalmente viene del 20% de las causas. ¿De acuerdo? Así que para la representación de datos categóricos, acordaros que existe la tabla de frecuencia, el gráfico de barras, el gráfico de tarta y el diagrama de pareto.
DOC0139|Estadistica y Probabilidad|Ahora vamos a hablar de la desviación estándar. La desviación estándar es la medida más común de variabilidad de un conjunto de datos. También tiene dos fórmulas diferentes que en este caso son casi idénticos. Simplemente es coger la raíz cuadrada de la varianza. Se utiliza la desviación estándar para no tener datos al cuadrados y tener unidades normales. Por ejemplo, si estábamos hablando de dólares, la varianza sería en dólares cuadrados. Esto no significa nada para nosotros. Por eso utilizamos la desviación estándar para volver a convertirlo en unidades simples de dólares. Luego tenemos el coeficiente de variación. El coeficiente de variación es la medida que se utiliza para comparar la variación sin tener en cuenta la escala de los datos. El coeficiente de variación es la desviación estándar dividido por la media. Esto nos sirve para comparar conjuntos de datos diferentes, ya que comparar desviación estándar no tiene sentido en datos diferentes. Pero comparar el coeficiente de variación que no tiene escala de datos, que no tiene métrica, que no tiene unidades, tiene la ventaja de poder hacerlo. Ahora haremos un ejemplo para que lo veáis. Si nos fijamos en estos datos, que son las pizzas, el valor de las pizzas, podemos convertir el valor de estas pizzas a dólares. Entonces tendríamos un conjunto de datos en euros y un conjunto de datos en dólares, con la conversión de hoy de 1,18. Si calculamos la media, vemos que tenemos medias diferentes. El caso de euros es 6,33, el caso de dólares es 7,47. Si calculamos la varianza de la muestra, del sample S, tenemos 3,50 y 4,87. Hacemos la raíz cuadrada y vemos que tenemos 1,87 en euros y 2,21. La varianza nos devuelve valores al cuadrados. Entonces eso realmente no nos sirve, como hemos mencionado antes. Por eso utilizamos la desviación estándar como medida de variabilidad. Pero si queremos comparar estos dos conjuntos de datos, usar la desviación estándar no es buena idea, porque en este caso tenemos diferentes desviaciones estándar. Aquí es cuando entra el coeficiente de variación. Dividimos la desviación estándar por la media y acabamos con un número, en este caso es 0,295 para ambos euros y dólares, que no tiene unidades. Pasamos de tener dólares al cuadrado a dólares a un número, a un coeficiente sin unidad que nos sirve para comparar dos conjuntos de datos diferentes. En este caso podemos decir, como esperábamos, que los dos conjuntos de datos tienen la misma variabilidad. ¿De acuerdo? Entonces acordaros que la varianza es al cuadrado, por eso tenemos unidades al cuadrado. La desviación estándar es en valores, los mismos valores que estamos comparando, pero si estamos comparando dos conjuntos de datos diferentes, queremos utilizar el coeficiente de variación, porque esto es sin escala y esto nos permite comparar dos conjuntos de datos de forma más fácil.
DOC0140|Estadistica y Probabilidad|Hola y bienvenido a esta nueva sección donde introduciremos la estadística inferencial. Antes de empezar con los test de hipótesis y meternos más en detalle en la estadística inferencial, tenemos que explicar qué es una distribución. La distribución de probabilidad de una variable aleatoria es una función que asigna a cada suceso definido sobre la variable la probabilidad de que dicho suceso ocurra. Podemos imaginarnos el típico ejemplo del dado. Si tiramos un dado, ¿cuál es la probabilidad de que saquemos un 1, un 2, un 3, un 4, un 5 o un 6? Pues la probabilidad es un sexto porque hay seis lados que serían de 0,17 o 17%. La probabilidad de sacar cualquier otro número es cero. Podemos hacer una simulación para ver esto. Si tiramos un dado y tiramos un lanzamiento, otro lanzamiento, otro lanzamiento, podemos tirar hasta 100 lanzamientos o 1000 o hasta 10.000. Por ejemplo, la probabilidad de cada uno es la misma. Se acaba convirtiendo en una distribución uniforme. Como vemos aquí, la probabilidad de sacar un 1, 2, 3, 4, 5, 6 es la misma, 0,17. Es importante quedarse con esta idea cuando hablamos de distribuciones. La distribución es la probabilidad de que ocurra, no el gráfico. El gráfico es solamente una representación visual. Entonces, ¿qué pasa cuando tiramos dos dados? Cuando tiramos dos dados, podemos sacar un 1 y un 1, un 1 y un 2, un 2 y un 1. Pero la probabilidad de sacar un 2 es 1,36 o 0,03, mientras que todo lo demás es cero. Podemos hacer también la simulación. Podemos hacer la simulación con dos dados que podemos sacar hasta 12 si sacamos dos 6. Podemos hacer un lanzamiento, otro lanzamiento, etc. 100 lanzamientos, 1000 lanzamientos, 10.000 lanzamientos. Pero, por lo general, tenemos mucha mayor probabilidad de sacar un 7 que un 2 o un 12. Aquí empieza nuestra distribución normal. Como hemos visto, la probabilidad de sacar un 2 es 0,03, la de sacar un 3 es 0,06, etc. Y ahora, hasta llegar a un 7, que es nuestra máxima probabilidad de 0,17 y vuelve a bajar hasta el 12. Esto nos crea una distribución normal. Esta famosa curva de campana que hemos oído todos hablar de ella, la distribución normal. En la siguiente clase entraremos un poquito más en detalle en esta distribución normal.
DOC0141|Estadistica y Probabilidad|Vale, vamos a hablar sobre la distribución normal. Si nos fijamos en el gráfico que teníamos antes, podemos denotar esta distribución con n, que significa normal, tilde o virguilla, que significa que es una distribución, la media mu, sigma, que es la desviación estándar o al cuadrado que sería la varianza, y tenemos esta distribución. Como vemos, la media, la moda y la mediana son iguales. En este caso serían 7. También es simétrico. Podemos mostrarlo de otra forma, con el valor central y la dispersión o desviación típica. ¿De acuerdo? También podemos tener distribuciones normales de diferentes formas. Si nos fijamos en la verde, primero, esta de aquí, podemos ver que la media es de cero y la varianza es de uno. Mientras que si nos fijamos en la roja, por ejemplo, la media sigue siendo cero, pero la varianza es de cero con dos. Por eso se hace más fina la distribución. Mientras que nos fijamos en la azul, la media sigue siendo cero, pero la varianza es de cinco. Esto la aplana y la hace más ancha. En cambio, la rosa o lila tiene una media de menos dos. Esto significa que se mueve hacia la izquierda. La varianza de cero con cinco cambia la anchura y la altura. Como vemos, tenemos distribuciones normales con diferentes estimadores y parámetros. En este caso, eso determina la forma que va a tener nuestra distribución normal. Básicamente, es influenciado por mu, la media, y la desviación estándar al cuadrado o la varianza. ¿De acuerdo? Así que espero que hayáis entendido esta breve explicación de las distribuciones normales y cómo cambian sus formas.
DOC0142|Estadistica y Probabilidad|En esta clase vamos a hablar sobre el teorema del límite central. El teorema del límite central es considerado uno de los conceptos más importantes de la estadística, por lo útil que es para observar el mundo a nuestro alrededor. Vale, imaginémonos una población, n, con su media mu y su desviación estándar sigma, y imaginemos también una pequeña muestra, n, con su media barra x y su desviación estándar s. Si fuésemos a coger muchas muestras, y para cada muestra cogísemos la media y dibujásemos la distribución, esto se llamaría la distribución muestral o la distribución muestral de medias. ¿Y cómo sería esta distribución de medias? Sería una distribución normal, independientemente de la distribución de la población original. Lo voy a repetir. La distribución de medias sería una distribución normal. Esto es el teorema del límite central. Da igual qué tipo de distribución tiene la población, si tú sacas muchas muestras y coges la media, la distribución de esas medias va a ser siempre una distribución normal. Ahora voy a ir a un ejemplo para que veáis que este es el caso. Aquí tenemos nuestra población, una media 16, media 16, desviación 5, etc. Yo lo que puedo hacer es ir cogiendo 5, 5, 5, voy a coger 10.000. Las medias las bloqueamos aquí y vemos que tienen una distribución normal. Esto es bastante intuitivo, porque tenemos aquí una distribución normal, cogemos las medias, distribución normal. Pero aquí lo que podemos hacer es cambiar un poco la distribución. Aquí podemos dibujarlo. Y esto es la inversa a la distribución normal, no es una distribución normal. Voy a coger 10, aquí coge el 20 y la media. Y voy a ir flotando 5, 5, 5, 5, 5. Y parece que bastante rápido va creando esta distribución normal. Vamos a poner 10,000. Véis? Y ya rápidamente vemos que aunque la distribución de esta población es un poco extraña, si cogemos las medias y floteamos esas medias, siempre vamos a tener esta distribución normal. Si nuestras muestras son más grandes, esa distribución normal será más fina. Si yo cojo aquí 5 y vuelvo a empezar, pues veis que esta distribución normal es mucho más ancha. Cuanto más grande nuestro tamaño muestral, más fino será. Podéis probar cualquier tipo de distribución. Podemos hacer así, podemos hacer así. Vale, da igual el tipo de distribución. Siempre nos va a salir una distribución normal si usamos las medias. ¿De acuerdo? Y esto es básicamente el teorema del límite central, que es uno de los conceptos más importantes de la estadística. Así que quedaros con eso. Da igual la distribución de la población. Si cogemos las medias y las dibujamos en una distribución, esa distribución de las medias va a ser una distribución normal. ¿De acuerdo?
DOC0143|Estadistica y Probabilidad|Muy buenas a todos y todas nuevamente aquí estoy yo soy Fernitz y seguimos con otra entrega más del contenido de estadística y probabilidad en el canal aquí estamos ahora con nuevo tema, en este caso este tema se va a dividir en dos partes porque aquí vamos a tratar teoría y en el siguiente video vamos a hablar de un ejercicio resuelto de este tipo de temas que es un poco más extenso y requiere de explicarlo paso a paso, el tema es comprobación de hipótesis haciendo énfasis en la distribución o la prueba del chi cuadrado entonces vamos a comenzar primero que todo hay que especificar que es una hipótesis, esto ya lo habíamos mencionado como una pequeña nota en el primer video de todo este material y es mejor mencionarlo aquí nuevamente la hipótesis es una suposición planteada a partir de datos investigados u observados previamente y que espera ser comprobada es simplemente algo que nosotros suponemos, algo que pensamos que puede ser la causa determinada de algún efecto o alguna situación que nosotros queremos investigar entonces cada investigación sobre todo a las que ocupan método científico y las que son investigaciones estadísticas utilizan este principio ya que sin una hipótesis básicamente no hay nada que comprobar no hay nada que podamos hacer después de recopilar tanta información o tantos datos de algo real sino que simplemente se recopila y ya estuvo ya no se hace nada o no se resuelve nada en concreto características de la hipótesis, aquí tenemos una en un seado simple y otra que es de fácil comprensión, quiere decir que solamente es un párrafo corto en la medida de lo posible y dice fácil comprensión porque se espera que cualquier individuo que tenga la capacidad de leer y que tenga la investigación a la mano pueda guiarse por la hipótesis para entender de una sola vez que es aquello que se espera comprobar o aquello que queremos lograr con la investigación, claro la investigación se compone de muchas partes, introducción, justificación, antecedentes, objetivos y todo eso va antes de explicar la hipótesis o aquello que necesitamos comprobar, entonces todas esas partes anteriores si tratan de explicar algo pero son párrafos muy extensos o muchas páginas de contenido por leer entonces es un poco tedioso, lo que uno principalmente haría como el espectador o como aquella persona que quiera saber de nuestra investigación se iría por estos rumbo, por la hipótesis para ya entender que es lo que se quiere hacer ahora, es sustentada por teorías previas, quiere decir que esto se sustenta de cosas que han pasado anteriormente, algo de historia, algo de alguna definición o algún concepto dado por alguien más en el pasado pero son cosas que han ocurrido y a partir de eso podemos formar a preguntarnos nosotros mismos que cuál es la causa de tal situación, entonces de ahí se va formando la hipótesis enuncia el problema que se presenta y sus posibles causas y a la vez debe plantearse bien para procurar una investigación eficiente, una hipótesis mal planteada desde el principio eso nos va a hacer que nuestra investigación sea muy difícil de llevar a cabo o que realmente en algún punto de esta misma investigación nos confundamos o nos perdamos de el punto de vista que queremos alcanzar y esas cosas en pocas palabras si no tenemos una hipótesis correctamente planteada nuestra investigación tampoco va a estar correctamente planteada ni se puede manejar correctamente ni va a tener un resultado eficiente como dice aquí ahora bien hay varios tipos de hipótesis pero me quiero enfocar más que todo en estos que están acá, generales que son básicamente hipótesis que explican un montón de cuestiones que ya son que se dirige mucho más a la generalidad algo que nosotros no podemos delimitar o que no está delimitado de antemano toma en cuenta todo, las específicas ahí si se delimitan bastante porque solamente explican de cierta área de algún país o de cierto contenido de algún tema pero no estamos tomando en cuenta todo el ámbito de aquello que es el tema principal sino que nada más una parte y esto a la vez se puede definir en este tipo de hipótesis que son las operacionales y estas para explicarlas mejor se van a dividir en dos alternativas o de trabajo y las nulas, las alternativas explican básicamente la causa y el efecto pero de una forma positiva queriendo decir que algo está relacionado o asociado con lo otro y las nulas es el otro lado de la moneda que quieren explicar que algo no está relacionado con otra cosa una variable no está asociada con la otra hablando de variables vamos a hablar de las independiente y la dependiente, esto como dije lo explique en el video anterior pero aquí tenemos unos cuantos ejemplos como nosotros sabemos del primer video la variable independiente es esa que no se ve afectada por ningún otro ente, ningún otra variable que especificemos y que está ahí y que básicamente puede llegar a afectar a otras y la dependiente es aquella que se ve afectada, asociada o relacionada por la independiente aquí tenemos unos ejemplos, tenemos esta hipótesis que dice la preferencia de una marca de jabones depende de la situación económica de los habitantes de la ciudad A aquí podemos desglosarlo en las variables que estamos explicando acá, en este caso la preferencia de una marca de jabones viene siendo la variable dependiente porque justamente aquí lo dice esto depende de esto otro, de la situación económica de los habitantes de la ciudad A entonces en otro lado esta otra variable de acá, la de situación económica es la variable independiente aquí no se está viendo afectada por ningún otro valor ni nada por el estilo sino que está afectando o asociándose con otra de forma directa aquí está otro ejemplo, el tipo de evaluación influye en el rendimiento de los estudiantes en la asignatura de ciencias naturales con tipo de evaluación nos podemos referir a que pueda ser un examen presencial o un examen virtual por ejemplo entonces eso viene siendo la variable independiente porque en este caso esta misma está afectando a la otra variable que es el rendimiento de los estudiantes puede ser que sea más fácil una evaluación virtual que una evaluación presencial o que los estudiantes se sientan más cómodos yendo a su institución a un examen presencial que haciéndolo virtual desde sus casas con una computadora cerca, puede que eso pase pero aquí solo estamos suponiendo y estamos manteniendo aquí una relación entre esta variable y la otra ahora tenemos este otro ejemplo, la decisión de ir a cierta universidad es afectada por la distancia que existe desde la misma hasta el lugar de residencia de los habitantes del departamento de san vicente aquí podemos explicarlo de la forma más rápida, la decisión de ir a cierta universidad viene siendo la variable dependiente y la distancia que existe entre la misma universidad y el hogar del habitante es la otra variable, la independiente y ahora vamos con este siguiente contenido el cual es básicamente ya el tema principal, comprobación de hipótesis porque es importante esto, nos ayuda a determinar la posible solución al problema que investigamos mediante la comprobación de la hipótesis nosotros ya veremos si al final lo que nosotros hemos supuesto desde el principio es verdadero o es falso y ya tomando en cuenta alguno de esos resultados ya vemos que es lo que podemos hacer para solucionar el problema que estamos investigando si al final es verdadero entonces tomamos un camino y si es falso tomamos el otro resuelve nuestras dudas iniciales, cada hipótesis va surgiendo debido a una duda o una inquietud que nosotros tengamos respecto al tema de investigación entonces se viene relacionando con lo mismo que decíamos antes de la posible solución porque además de determinar como resolver el problema determinamos también algún otro tipo de inquietudes o algún otro tipo de consultas que podíamos tener tanto nosotros como el que va a recibir el trabajo final, los que van a ser nuestros objetos de estudio vamos resolviendo esas consultas ya de una sola vez prácticamente y también nos ayuda a delimitar lo que en realidad queremos conseguir por medio de nuestra investigación mediante la comprobación de hipótesis ya como lo que decía antes tomamos el camino correcto y de una sola vez sabremos que viene después sabremos el resto del contenido que va a incluir nuestra investigación luego de eso, conclusiones, alguna recomendación, algún otro tipo de contenido que venga después de eso ahora vamos a hablar de algo importante, el modelo estadístico, esto se explica como una secuencia de procedimientos y operaciones que utilizan los datos obtenidos durante la investigación con el fin de comprobarlos y llegar a una conclusión y esto permite a la vez comprobar hipótesis o establecer relaciones entre causas y efectos determinados en otras palabras, el modelo estadístico es simplemente una serie de operaciones que vamos a hacer haciendo uso de aquellos datos que hayamos obtenido de alguna encuesta, de algún censo o de alguna otra herramienta que hayamos utilizado y eso finalmente nos va a dar una conclusión, o sea nos va a determinar a qué hipótesis vamos a tomar si hemos planteado varias o la que tenemos nos va a decir si es verdad o es falsa y ese tipo de casos y existen varios tipos de modelos estadísticos para otro tipo de fines y no solamente para comprobar hipótesis pero como les dije al principio del video nos vamos a enfocar en uno en particular el cual será el método de la distribución de chi cuadrado muy bien ahora el chi cuadrado se puede explicar como lo siguiente una prueba de hipótesis que compara los resultados observados de los datos con los resultados esperados que quiere decir esto? que los resultados que nosotros hayamos obtenido mediante alguna herramienta o algún otro método de recolección de datos eso lo vamos a comparar con aquello que nosotros creímos que íbamos a obtener o que según el proceso que vamos a ver luego es lo que podríamos haber obtenido un aproximado de aquello que realmente observamos entonces queremos comparar eso para obtener un valor determinado el cual nos va a ayudar a comparar si nuestra hipótesis va a ser real o no esto se puede utilizar para determinar si una variable está asociada a la otra y para determinar si los datos observados de una variable se asocian con los observados de otra variable básicamente esto nos da a entender lo siguiente lo que les explicaba anteriormente de que para formar una hipótesis en este caso vamos a utilizar las dos, la alternativa y la nula debemos de tener una variable independiente y una dependiente y esas dos tenemos que observar con este método si realmente están asociadas una con la otra y a partir de eso vamos a determinar cuál hipótesis vamos a aceptar y cuál vamos a rechazar o sea con cuándo vamos a quedar sin la alternativa o la nula luego de esto tenemos este contenido de acá el nivel de significancia se va a utilizar en el método de la distribución de chi cuadrado porque nos permite determinar si el resultado de nuestro estudio se puede considerar significativo o no luego de las pruebas realizadas o sea si la diferencia que encontramos nosotros es realmente algo que se da por la relación entre las variables o algo que se da de casualidad o sea que no es tan significativo simplemente se dio porque por mea coincidencia podría decirse y no está sujeto a ninguna desviación y ninguna otra regla entonces eso es lo que se determina con el nivel de significancia este suele ser determinado como 0.05 o el 5% en otros casos puede ser otro tipo de valor ya depende de qué es lo que nosotros queremos para nuestra investigación o qué probabilidad queremos agregar para lo que viene siendo esto de aquí el nivel de significancia representa la posibilidad o la probabilidad de rechazar la hipótesis nula cuando esta misma es verdadera indica el porcentaje de riesgo de concluir que existen diferencias entre la hipótesis nula y los resultados obtenidos cuando en realidad no existe diferencia alguna entonces con esto básicamente determinamos un posible error de que al final si hemos rechazado la hipótesis nula en algún momento hay algunos casos en donde si hay una diferencia significativa para realmente no haber rechazado la hipótesis nula ustedes solamente tengan en cuenta que el valor determinado el valor por defecto para este nivel de significancia es este de aquí 0.05 o el 5% Gracias por haber visto este material si les gustó no olviden dejar su like en el vídeo algún comentario si tienen alguna duda alguna consulta o si simplemente quieren dejar algún agradecimiento y no olviden tampoco suscribirse a este canal para recibir alguna notificación active la campanita para recibir las notificaciones cada vez que suba nuevo material también pueden seguirme en mis redes sociales facebook y twitter aquí les dejaré los links en la descripción del vídeo para que puedan visitarme Bueno, no queda nada más que decir les deseo un feliz día, una feliz semana y espero poder subir el siguiente vídeo pronto para continuar con la segunda parte de esta temática me despido, yo soy Fernitz y hasta el siguiente vídeo hasta la próxima están construyendo allá afuera
DOC0144|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí estoy otra vez, yo soy Fernets y este es el siguiente material para estadística y probabilidad en el canal Anteriormente habíamos tratado la primera parte de este tema, comprobación de hipótesis utilizando la distribución del chi cuadrado Y en ese video les explique nada más la teoría, lo que viene siendo la hipótesis, porque es importante comprobarla, también el modelo estadístico, una definición de lo mismo Y ya explique un poco de lo que viene siendo este método, el chi cuadrado Les voy a dejar un link en la descripción del video, con lo que viene siendo el otro video, la primera parte Para que puedan darse la idea, si no lo han visto aún, les recomiendo encarecidamente que lo vean primero, antes de pasar a este, así entenderán mejor lo que vamos a hacer hoy Porque lo que vamos a hacer hoy es un ejemplo, así que comencemos Esto se divide en una serie de pasos, el chi cuadrado se va a dividir en una serie de pasos, eso ya depende del método que ustedes deseen utilizar, serán los pasos que se van a aplicar Pero para este caso tenemos, paso 1, planear la hipótesis alternativa y la nula Deben tener en cuenta que la alternativa es básicamente la relación existente entre una variable y la otra, y la nula es cuando no hay relación alguna entre esas dos variables Esto por defecto se puede dejar como 0.05 o puede variar dependiendo de las condiciones que tengan para su investigación Esto lo vamos a hacer mediante fórmulas, una para determinarlo al principio y otra para comprobación Este va a ser un valor distinto, pero siempre va a tomar un papel importante como chi cuadrado porque en el siguiente paso vamos a comparar los dos valores El calculado mediante fórmulas y el de tabla encontrado en la tabla de chi cuadrado que luego vamos a ver cómo es Y el último paso es la conclusión, si este viene siendo mayor que este o viene siendo menor, entonces vamos a tomar un camino determinado Pasemos al ejemplo, el ministerio de educación en el programa escuela saludable lleva a cabo un sondeo a dos escuelas, una de Soya Pango y otra de San Salvador Se le pregunta a las personas cuales son los tipos de cereales que compra o consume, pudiendo elegir entre las siguientes opciones Cornflakes, choco crispies y zucaritas, y aquí tenemos una tabla de datos con los resultados obtenidos de la encuesta o el sondeo que explica aquí Ahora el ministerio pone a prueba los datos obtenidos en la encuesta para determinar si la marca de cereales o el tipo de cereal preferida de un estudiante está relacionada con la ciudad donde vive Hay que establecer las hipótesis adecuadas y comprobarlas con un nivel de significancia del 5% Ahora bien, aquí a la de antemano nos da los datos que vamos a utilizar en esta tabla y el nivel de significancia que es sumamente importante Vamos al primer paso, aquí hay que desglosar las variables, según el enunciado del problema nos da dos variables La del lugar de residencia o el lugar donde vive el estudiante y el tipo o la marca de cereal que compra o consume Entonces, según los datos que se nos ha dado, la variable independiente es el lugar de residencia Ya que la otra, el tipo de cereal, estamos buscando si va a depender o si se va a ver afectado por el lugar de residencia O sea, queremos ver si el estudiante come un tipo de cereal determinado porque vive en otro lugar Digamos que el estudiante vive en San Salvador y le gusta comer azucaritas Queremos ver si le gusta ese cereal porque lo encuentra más fácilmente en donde vive o porque todos sus compañeros en ese mismo lugar donde vive lo comen también Queremos ver si eso se puede dar, queremos ver si es cierto que las dos variables están asociadas una con la otra Para ello tenemos dos hipótesis, la alternativa que explica lo siguiente El lugar de residencia influye en el consumo del tipo de cereal y la nula es al revés El lugar de residencia es indiferente, o sea, no influye en el consumo del tipo de cereal Entonces queremos ver mediante el método de chi cuadrado los siguientes pasos Pasos 2, cual de las dos hipótesis vamos a tomar y cual va a ser básicamente el resto de la base para lo que falta de la investigación O sea, con cual nos vamos a quedar Paso 2, ya está determinado, el nivel de significancia es el 5% y eso como les dije en el video anterior lo vamos a transformar en decimal Eso será el 0.05, se va a utilizar este formato más adelante Paso 3, aquí vamos a calcular el valor del chi cuadrado calculado, esto es lo que significa esta simbología Chi cuadrado calculado, ¿qué significan estos otros valores que están acá? Pues ahora les digo, fo quiere decir frecuencia observada y fe quiere decir frecuencia esperada Ese último no lo tenemos, lo único que tenemos son las frecuencias observadas que están en la tabla de aquí abajo Eso es lo que nosotros hemos obtenido en los resultados de la encuesta o el sondeo según el enunciado Pero no es lo que se espera según las operaciones que vamos a hacer aquí O sea, los valores que obtuvimos acá son mayores o menores, o sea son diferentes a lo que realmente se esperaba obtener O el valor probable podría decirse, ¿qué podría haberse obtenido en lugar de esto? Entonces esos valores los vamos a obtener mediante esta operación de aquí Total de filas por total de columnas entre el total de los datos Aquí si siguen este formato que preparé aquí en medio de la diapositiva para esta tabla Esta es la simbología que van a seguir para saber dónde van a ir estos valores y los que van a utilizar para las siguientes operaciones Aquí hay que tener en cuenta que esto va a ser una sumatoria Porque en este caso vamos a sacar más de un valor que vamos a terminar sumando con los demás Y vamos a obtener 6, porque aquí hay 6 celdas ya que las demás son solo totales Aquí ya están las operaciones terminadas, pero ustedes se preguntarán hasta este punto ¿De dónde salen todos estos valores? Pues vamos por lo sencillo primero El 315 es el total de datos y eso como es parte de la fórmula en el denominador va a ir aquí para todas Ahora esto es lo que va a diferir, aquí viene el espacio del total de la fila y aquí viene el total de la columna ¿Por qué lo saqué así? Pues por esto Este va a ser el primer valor, esta es la primera frecuencia observada que tengo Esta es la segunda, esta es la tercera, esta es la cuarta, esta es la quinta y esta es la sexta Ahora, me encuentro yo aquí, yo quiero sacar la frecuencia esperada número 1 La cual va a corresponder a este espacio de acá, donde está el 23 La primera celda de todo ese orden que les di Entonces, estoy en esta celda y el total que me corresponde en la misma fila en donde me encuentro ahorita es 160 Entonces el total de fila para esta operación va a ser 160 Por el otro lado, para la columna, obtengo 53 porque estoy en esta celda y en toda la columna en donde me encuentro ahora mismo el total es este Entonces por eso tomo el 53 Y luego tomo el valor de 315 que corresponde al total de datos Opero todo esto y obtengo este valor Quiere decir que podría haber obtenido 26.92 aquí, pero realmente observé y tengo 23 Entonces la frecuencia esperada es esta y la observada es esta de acá Ahora si me voy al siguiente dato, la frecuencia esperada número 2 Este va a ser este espacio de acá Y me encuentro en esta otra fila de abajo Estoy en la fila que dice soyapango y esta tiene otro tipo de datos en el total Entonces en la misma fila el total es 155, lo pongo aquí En esta misma celda, está en esta columna, el total es este, lo pongo aquí Y después el dato correspondiente del total de valores Opero y obtengo esto Quiere decir que en lugar de 30 que fue lo que observé en todo el resultado que obtuve de la encuesta y del sondeo según el enunciado Esto fue lo que se obtuvo, pero se esperaba que se obtuviera esto en lugar de esto Y obteniendo los demás valores y nos vamos aquí por ejemplo en el número 3 Aquí la fila es esta de acá, me encuentro ahora aquí en la fila de San Salvador y su total es 160, lo pongo acá Y en este caso la columna cambia, porque ahora me encuentro en esta celda y su columna es todo este espacio de acá Y su total es 140, entonces ahí lo pongo en este espacio Y su mismo total acá y opero y obtengo este valor de acá Quiere decir que en vez de 95 que fue lo que se observó en todo lo que se sondeo Pudo haber sido esto, se esperaba que fuera esto, pero en realidad se obtuvo esto Y así sucesivamente hasta obtener todos los demás valores, aquí tenemos este, este y este Luego de haber obtenido las frecuencias esperadas venimos nosotros y obtenemos esta tabla de valores Aquí lo tenemos por celdas, dijimos que esta era la 1, 2, 3, 4, 5 y 6 Y estos son los valores según el orden que les establecí al principio en la depositiva anterior Y luego les vamos a colocar las frecuencias correspondientes, esta era la 1, la 2, la 3 y así Luego lo que vamos a hacer es restar estos dos valores, la frecuencia observada se va a restar con su respectivo valor esperado Y así los demás, cada uno de estos va a tener su valor esperado, se van a restar también Y obtenemos estos valores de esta columna, salen en menos porque algunas frecuencias esperadas son mayores que las observadas Y eso no va a ser ningún problema porque el siguiente paso va a ser que esta resta se va a elevar al cuadrado O sea se va a hacer esta operación primero y se va a elevar al cuadrado para obtener estos valores de aquí Luego de eso, este valor resultante para cada una de las celdas, le vamos a dividir la frecuencia esperada En el caso de esta primera fila vamos a dividir esto entre esto Y en las siguientes filas, por ejemplo aquí, vamos a dividir esto entre esto No confundirlo con esto, porque les puede salir un valor negativo Entonces luego de haber obtenido todas esas divisiones, obtenemos valores mucho menores Algunos están por debajo del 1, pero no hay ningún problema Lo que se hace luego de todo esto, es sumar todos estos valores que hemos obtenido Y nos sale este total de acá abajo Entonces, básicamente lo que va a pasar es que el valor 30.54 aproximado claramente es el valor del chi cuadrado calculado Ahora lo que sigue es comprobar si el valor anterior es correcto mediante la siguiente fórmula Esta fórmula puede ser bastante confusa de entender, pero aquí se los voy a explicar mediante este formato y la misma tabla que tenemos Aquí básicamente nos dice el sub-índice que está por debajo de la N Nos quiere decir, el primer numerito que está, el de la izquierda, corresponde a la fila Y el siguiente numerito, el que está a la derecha, corresponde a la columna Entonces aquí nos está diciendo N, que es básicamente el total de datos, eso no hay ningún problema N, 1 punto, el 1 punto en este caso se refiere al total de la primera fila Si tiene un punto en el lado derecho, quiere decir que no voy a tomar la columna para nada, solo voy a tomar el total de la fila Caso contrario acá, que el punto está antes del valor, quiere decir que aquí voy a tomar nada más el total de la columna, sin involucrar ninguna fila Entonces, según este formato que está aquí, ustedes van a reemplazar los valores en sus respectivos espacios Aquí esto puede variar, la fórmula se puede hacer un poco más extensa si tenemos más filas y más columnas Si tenemos más filas, entonces toda esta cláusula de acá se va a agregar antes de esto Y si tenemos más columnas, quiere decir que dentro de estos corchetes se va a hacer una suma más o varias sumas más Además de esto que está aquí, si tuviéramos a 5 columnas por ejemplo, entonces serían 5 valores los que estaríamos sumando dentro de los corchetes Y si tuviéramos 4 filas, entonces estaríamos agregando 4 veces esto de aquí en total Entonces, luego de hacer toda esta sumatoria, toda esta operación, como les decía, a todo eso se le va a restar el total de los valores Y si obtenemos un valor igual o lo suficientemente cerca del valor que obtuvimos antes, quiere decir que el chi cuadrado calculado está correcto Y aquí lo podemos observar, vamos a quitar esta tabla de acá y nos vamos a ir a la resolución Aquí están reemplazados todos los valores según el formato que les había comentado Y al hacer toda la operación, miren detenidamente, miren la operación Si lo agregan de una sola vez a su calculadora o lo quieren hacer paso a paso a mano, van a obtener este resultado 30.54604, este es exactamente igual en los primeros dos decimales y en el entero Aquí no importa que varíe, porque si se fijan en la depositiva anterior, obtuvimos este valor de acá en lugar de esto Ustedes dirán, es diferente el valor, o sea, no creo que al final vaya a estar correcto porque el valor en decimal es diferente, eso no importa Siempre y cuando en los decimales más significativos y en el entero esté igual o se logra aproximar lo suficiente pero bastante cerca Entonces está correcto, el valor está correcto y ya no hay que hacer nada más, lo cual quiere decir que aquí está todo en orden Todo eso fue el paso 3, ahora vamos con el paso número 4, encontrar el chi cuadrado de tabla Aquí ya no vamos a tocar ningún cálculo que hicimos anteriormente, aquí ya es otro tipo de valor que vamos a obtener Entonces necesitamos el nivel de significancia que se estableció en el enunciado y necesitamos otra cosa, los grados de libertad Los grados de libertad se obtienen mediante esta fórmula, el total de filas menos 1 por el total de columnas menos 1 En la tabla tenemos 2 filas, le vamos a restar 1, en la misma tabla tenemos 3 columnas, le vamos a restar 1, lo cual nos da 1 por 2 que es 2 Entonces ya tenemos nuestros valores, ¿para qué nos va a servir todo esto? Pues es muy simple, los grados de libertad nos van a decir en qué fila de la tabla de chi cuadrado vamos a buscar Y el nivel de significancia nos va a decir en qué columna lo vamos a buscar, esto se encenderá mucho mejor cuando veamos la tabla del ariste bus en el chi cuadrado Aquí tenemos lo siguiente, esta tabla es muy extensa porque tiene bastantes valores y bastantes filas y columnas, entonces vamos a buscar solo uno de los valores Y ese valor se encuentra con la fila y la columna que obtuvimos al hacer las operaciones de grados de libertad y nivel de significancia Los grados de libertad los vamos a asociar con la fila y lo del nivel de significancia lo vamos a asociar con la columna Entonces en los grados de libertad obtuvimos el número 2, quiere decir que aquí es donde vamos a buscar Y en el nivel de significancia tenemos 0.05, quiere decir que vamos a buscar el valor de la fila que corresponde a esta columna Y el valor termina siendo este de aquí, se ve un poco pequeño, le voy a hacer zoom, es este de acá, 5.9915, eso está en la columna 0.05 y en la fila 2 En resumen tenemos esto, que el valor del chi cuadrado de tabla es 5.9915 Paso número 5, vamos a comparar los valores, tenemos que el chi cuadrado calculado es 30.5453 Y el chi cuadrado de tabla es 5.9915, claro aquí hemos hecho una aproximación porque si notaron en el chi cuadrado calculado tuvimos valores un poco distintos A la hora de hacer las comparaciones y todo, pero les recomiendo que, y esto va a ser muy esencial a la hora de que estén haciendo operaciones así, en temas futuros Que cuando les toca hacer una operación en donde involucren muchos decimales, les recomiendo que utilicen nada más los primeros cuatro decimales Así aseguran en la medida de lo posible que no se pierdan datos y a la vez tampoco están dejando otros datos aparte o tomando demás, verdad? Entonces están en un término medio si dejan ese número de decimales, ustedes lo pueden hacer de cualquier otra manera O sea si quieren tomar nada más dos decimales o quieren tomarlos todos, entonces eso ya corre por cuenta propia, pero lo más recomendable es utilizar cuatro Ahora bien, las condiciones que tenemos son las siguientes, el chi cuadrado de tabla si este es menor que el calculado, entonces va a pasar esto Que vamos a rechazar la hipótesis nula y vamos a aceptar la alternativa, caso contrario si al final el chi cuadrado de tabla es mayor que el calculado Quiere decir que vamos a hacer esto, vamos a rechazar la hipótesis alternativa y nos quedaremos con la nula Ahora, cual creen que puede ser el caso aquí? Según los valores que tenemos aquí arriba, cual es mayor que el otro? y cual opción vamos a tomar? Ok, ya lo pensaron? Ok, perfecto Si pensaron en lo que está aquí, pues están en lo correcto, dado que el valor del chi cuadrado calculado es mucho mayor que el del tabla Rechazaremos la hipótesis nula y nos quedamos con la alternativa, entonces en este caso la alternativa dice el lugar de residencia influye en el consumo del tipo de cereal Muy bien, esto ha sido todo por el día de hoy, les agradezco mucho por su atención, espero que les haya gustado el video Si es así pueden dejar su like y pueden dejar algún comentario con algún agradecimiento o si tienen alguna duda respecto al tema, si algo no quedó muy claro o quieren saber más respecto al tema Pueden dejarlo también en la caja de comentarios como alguna duda o alguna consulta y también les sugiero que se suscriban al canal porque así estarán pendientes de algún otro nuevo material que pueda subir Suscríbanse y activen la campanita para estar pendientes de eso y también pueden seguirme en mis redes sociales, Facebook y Twitter, los links estarán abajo en la descripción del video como siempre Bueno, aquí me despido, yo soy Fernets y hasta la próxima Hasta el rato
DOC0145|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí estoy nuevamente, yo soy Fernets y esta es la siguiente entrega de videos de estadística y probabilidad en el canal. Ahora, se preguntarán por qué hay una letra T en grande en la portada. Si ya han estudiado estadística antes o saben del tema, pues sabrán de qué se va a hablar en este video, si no, pues de igual manera ahorita mismo les digo. El tema de esta ocasión será la prueba T de Student. Seguimos con lo que es la comprobación de hipótesis mediante diferentes técnicas. Así que, vamos a continuar. Definición. La prueba T de Student se usa para determinar si existe una diferencia significativa entre las medias obtenidas de dos grupos o dos experimentos. Aquí, a diferencia de otras pruebas que se pueden aplicar para lo mismo, aquí no estamos tratando un solo resultado general, sino que dos por separado. Y esos dos se van a definir en promedios o medias. Y esos los vamos a comparar para básicamente llegar a una conclusión. Por medias nos referimos a la suma de las cantidades de los resultados obtenidos entre la cantidad de resultados. Es exactamente lo mismo cuando decimos promedio. Entonces, gracias a esta prueba, podemos decidir cuál hipótesis debemos aceptar y cuál debemos rechazar. Entonces, seguimos el mismo principio que la prueba de chi cuadrado, por ejemplo. Solo que vamos a seguir pasos un poco diferentes para llegar siempre a este mismo final, podría decirse. Ahora, una pequeña recomendación, y esto lo veremos en los ejemplos que se van a mostrar en los siguientes videos. Es preferible que la cantidad de las muestras estudiadas sea menor que 30. Ahora bien, vamos a continuar. Estas características son las más fundamentales que yo logré agrupar. Cabe destacar, y se me olvidó mencionar antes, que este material que les estoy mostrando hoy me he basado en una pequeña fuente de información en internet, la cual les voy a dejar el enlace en la descripción del video por si quieren profundizar un poco más del tema. Pero, esto que les traigo en el video hoy es lo más esencial y la forma en la que se los puede explicar más brevemente posible. Así que continuemos. En esta prueba tenemos una variable independiente y una dependiente. Lo mismo que habíamos hablado anteriormente. Si lo recuerdan, la variable independiente es aquella que no depende de ningún otro factor o no se ve afectada de alguna manera por un factor externo, a diferencia de la dependiente. Ahora, la variable independiente, esto hay que tenerlo en cuenta, debe tener solo dos niveles o medirse solo bajo dos condiciones, si es que tiene varios niveles de medición, claramente. Entonces, aquí esto procurará que obtengamos los dos resultados de los que hablamos antes, las dos medias y promedios que hay que comparar. Por eso deben ser solo dos niveles o condiciones. Ahora, si la diferencia entre las dos medias es mayor, también lo será la probabilidad de que realmente exista una diferencia estadísticamente significativa. Ahora bien, vamos a ver los tipos de prueba Tede Student. Esto en un momento se mencionó en la definición, pero aquí lo vamos a ver detalladamente. El caso 1 es la diferencia par o la prueba para muestras relacionadas. Aquí en este pequeño esquema lo pueden ver. Tenemos un grupo, un solo grupo de la muestra, de lo que estamos investigando, un solo grupo de individuos y a este mismo grupo se le aplicará primero una prueba y después otra con un ligero cambio. Ahora, el siguiente caso es el otro lado. Es para muestras independientes o no relacionadas. Aquí tenemos dos grupos y a estos mismos grupos se les va a aplicar la misma prueba. Primero un grupo y después al otro, para luego medir cuál de los grupos se ve más afectado en resultados con la prueba. Ahora bien, para terminar vamos a hablar de los pasos que se deben de seguir para la comprobación con esta prueba, la prueba T. Paso 1 vendría siendo definir las hipótesis nula y la alternativa. Aquí primeramente se define lo que es la nula. La nula en este caso vendría siendo que no existe ninguna diferencia entre el factor A y el factor B. Caso contrario de la hipótesis alternativa que puede decir que el factor A es mayor en diferencia que el factor B o viceversa o simplemente que son diferentes. Ahora el paso 2, establecer el nivel de significancia. Puede ser el 0.05 o 0.10 por ejemplo. El paso 3 es obtener el T calculado, el valor T calculado que ahí vamos a utilizar fórmulas distintas para cada caso. Y después el paso 4 será obtener T de tabla. Aquí nosotros haremos uso de la tabla de T de student y siguiendo el mismo principio que por ejemplo el chi cuadrado. Paso 5 comparar los valores obtenidos. Paso 5 comparar los valores obtenidos. Según el valor que se obtuvo en el calculado y en el de tabla vamos a ver cuál es mayor que el otro y entonces llegamos a una conclusión. Si un criterio se sigue entonces aceptamos una hipótesis y rechazamos la otra. Y eso conlleva al último paso que es la conclusión. Muy bien, antes de finalizar el video de hoy quiero aprovechar el momento para mandar unos saludos a las personas que han estado comentando en los videos anteriores de mi canal. Aquí estoy un poco atrasado porque básicamente ya habían comentado desde hace mucho pero los tengo en cuenta. Así que les mando un saludo muy especial a las siguientes personas. Brian Ezequiel Miranda Rodríguez Sergio Hernández Matrix524 DNJ Y también a Waldo Reyes Les agradezco mucho por su apoyo chicos, espero que todo esté muy bien. Muy bien, muchas gracias por ver el video de hoy. Si les gustó no olviden dejar su like y algún comentario sobre el tema o algo que también deseen ver de esta serie de videos. Y no olviden seguirme en mis redes sociales, Facebook y Twitter, sobre todo Facebook que ahí es donde estoy un poco más activo. Los links estarán en la descripción así como siempre los dejo. Así que bueno, ya no hay nada más que decir. Estén atentos al siguiente video que ya viene siendo el primer ejemplo de este mismo tema. Y todo está en orden. Muchas gracias por ver. Yo soy Fernets y nos vemos en una próxima entrega. Hasta la próxima. Bueno, ya está listo.
DOC0146|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí estoy presente otra vez, yo soy Fernets y bienvenidos a una entrega más de los videos de estadística y probabilidad en el canal. Ahora bien, vamos a pasar de una sola vez con el tema, porque es algo de teoría y hay que explicar bastante. El tema de esta ocasión es muestreo. Antes de comenzar hay que dar una pequeña definición a lo que es la muestra. Según recuerdan en el primer video que se subió sobre todo este material hablamos de la población y de la muestra. Entonces, la población es un todo, un universo, algo que se toma en general. Y luego tenemos la muestra que es simplemente una extracción de alguna porción de ese todo o de ese universo. Entonces, población es un todo y la muestra es una parte. Y en los mismos videos, en siguientes entregas hemos hablado de cómo obtener el tamaño, la cantidad adecuada de la muestra para alguna investigación o problema en particular. Ahora, si hablamos de muestreo, este es otro tipo de concepto. Porque sí, sabemos que muestra es una parte y sabemos cómo obtener la cantidad de individuos que formarán parte. Pero no tenemos idea todavía de cuáles van a ser en concreto esos miembros. O de cómo los vamos a enlistar o cómo los vamos a básicamente obtener uno por uno, ¿no? Entonces, ahí es donde entra el concepto de muestreo. Vamos a verlo. El muestreo, según lo que dice aquí, es una herramienta de la investigación que se encarga de determinar qué parte de una población o universo debe ser examinada con el fin de hacer inferencias sobre esa población. Entonces, es simplemente un proceso de selección. Un proceso de selección de todos esos elementos que van a entrar en la muestra. Ahora veamos lo siguiente. Tenemos dos tipos de muestreo a principales. El probabilístico y el no probabilístico. Vamos a ver cada uno en detalle. Ok, vamos con los muestreos probabilísticos. En este caso hablamos de aquellos tipos de muestreo en los que cada individuo de una población tiene la misma probabilidad de ser elegido para formar parte de la muestra. Es como querer decir que todos tienen la oportunidad de ganar una posición en la muestra que se va a tomar para una investigación. Además se puede definir como aquellos muestreos en los que todas las posibles muestras de tamaño tal, tamaño N, tienen las mismas oportunidades de ser seleccionadas. En este caso ya hablamos de grupitos. Podemos decir que el grupo A tiene igual oportunidad de ser seleccionado para investigar que el grupo B y lo mismo que el grupo C y lo mismo que el grupo D hasta llegar a cierta cantidad. Pero todos esos grupos forman parte de la población de la que se quiere sacar alguna conclusión en nuestros trabajos. Entonces veamos cuales son esos métodos de muestreo. Los probabilísticos claro. Tenemos el aleatorio simple, el sistemático, estratificado y por conglomeración. En el muestreo aleatorio simple se asigna un número a cada individuo que forma parte de la población. Luego de eso utilizando cualquier medio se eligen varios sujetos al azar hasta que completen la cantidad de la muestra. Naturalmente para eso se utiliza la tabla de números aleatorios. Esa tabla la vamos a ver en próximos videos. Aquí tenemos un pequeño ejemplo para que se entienda mejor. Aquí se asigna un número a todos los empleados de un sector de la empresa Z y se eligen 10 de estos números para obtener un grupo de 10 empleados. Quienes serán sometidos luego a una prueba especial de control de calidad con el fin de determinar el rendimiento de todo el sector. En pocas palabras aquí lo que se espera es que de la empresa Z solamente van a necesitar 10 personas que laboren ahí para hacer dicha prueba de control de calidad. Pero para elegirlos lo que hacen ahí es asignar un número a todos los demás. Digamos que hay un grupo de 500 empleados en total en la empresa. Entonces se van a ir enumerando del 1 al 500. Y van escogiendo 10 números. Digamos que eligen a la persona con el número 75. Entonces esa persona va a ser llamada para formar parte del grupo de 10. Luego vuelven a elegir otro número y el que se escoge es el número 7. Entonces la persona con el número 7 va a pasar a formar parte del grupo. Y así sucesivamente hasta llegar al décimo número. Que bien podría ser el 4 por ejemplo. Entonces esa persona ya sería la última en ser seleccionada. Ya teniendo el grupito de 10 se hace la prueba, se sacan conclusiones. Y luego se estima que el resultado que se obtuvo en esa prueba se va a obtener o puede obtenerse por medio de todos los empleados de la empresa Z. Ahora bien, vamos con el aleatorio sistemático. Se obtiene el valor K el cual se define como el resultado de dividir el tamaño de la población entre el tamaño de la muestra. Lo que decíamos antes a N mayúscula entre N minúscula. Y ese valor K va a ser una constante que a la hora de operar y de hacerlo paso a paso como se verá después. Se van a obtener como todos los números siempre de forma aleatoria pero siguiendo un patrón de intervalos. Quiere decir que se seguiría el mismo intervalo para cada selección. Luego como pueden ver aquí con la población previamente enumerada se elegirá un solo número aleatorio que será el inicio. Con la letra I minúscula. Y este valor debe estar comprendido entre 1 y el valor K. Digamos que mi valor K es 6. Entonces ese inicio I, ese valor I debe estar comprendido entre 1 y 6. O si en este caso K fuese 9 entonces sería comprendido entre 1 y 9. Y ese número que se seleccione se puede obtener con el muestreo aleatorio simple. A partir de ese valor I que se obtenga que va a ser el primer valor del primer individuo de la muestra. Los siguientes individuos a partir del segundo van a ocupar los siguientes lugares I más K, I más 2K, I más 3K. Hasta llegar a I más N menos 1K. ¿Qué quiere decir todo esto? Lo que quiere decir es que digamos que obtenemos como primer valor el número 2. Entonces el individuo con el número 2 va a entrar a la muestra, no hay problema. Pero ahora queremos saber los siguientes para una muestra total de 15 personas. Y nuestro valor K digamos que es 5. Entonces el segundo individuo de la muestra va a seguir el criterio de I más K. I es 2 como dijimos antes y K es 5, 2 más 5 es 7. Entonces el siguiente individuo va a ser el que tenga el número 7. Y luego lo que se hace para el tercer individuo es multiplicar K por 2. Entonces eso sería 5 por 2 es 10 más 2 son 12. Quiere decir que el siguiente individuo va a ser el del número 12. Para que se entienda mejor, una vez tengan la letra K definida, para que no se confundan con esto lo único que van a hacer es ir sumando K varias veces. Porque, por eso decíamos intervalos, porque todos los demás valores que obtengan van a ir de K en K en K en K y así. Entonces en el caso de que K es igual a 5, van a ir sumando 5 más 5 más 5. A medida que vayan obteniendo los demás individuos. El siguiente más 5, el siguiente más 5. Y entonces si se dan cuenta cuando hayan hecho eso, todos sus individuos obtenidos van a tener una separación entre ellos de un valor de 5. Entonces eso es lo único que harían, solo ir sumando K más K más K más K hasta llegar a su último individuo. Este de aquí es bastante confuso, el aleatorio estratificado. Vamos a leer detenidamente cada uno de estos. Se consideran categorías típicas diferentes entre sí que posean gran homogeneidad respecto a alguna característica. Por ejemplo, se puede stratificar según la ocupación, el lugar de residencia, el sexo, el estado civil, entre otros más. Cada estrato funciona de manera independiente, por lo que a cada uno de ellos se les puede aplicar un tipo de muestreo distinto. Para sacar los individuos necesarios que formaran parte de la muestra. En este caso estamos hablando de que una vez se hayan agrupado varias personas o varios individuos de la población. Según alguna de estas características, por decir un ejemplo. Entonces se va a obtener de cada grupo un tamaño distinto de muestra. Y todas esas muestritas que obtengamos de cada grupo se van a sumar para obtener la cantidad total de la muestra para toda la población. Y todos esos grupos iniciales, todos esos estratos iniciales y sus respectivas muestras se obtendrán a partir de la fijación. La fijación se denomina así a la distribución de la muestra en función de los diferentes estratos. Aquí es sencillo de entender, la fijación simple es que a cada grupo o cada estrato le corresponde un número igual de elementos de muestra. Entonces si tenemos 5 estratos de una población, entonces las muestritas que se obtengan van a ser iguales. O sea que cada muestra va a ser de 5 o cada muestra va a ser de 7. Quiere decir que de cada grupo se van a sacar 7 personas, sin importar el tamaño de cada grupo. Y en el proporcional la distribución se hace de acuerdo con el peso o el tamaño de la población en cada grupo o estrato. Quiere decir entre más grande sea el grupo o el estrato mayor será la muestra que se va a obtener. Y si es menor entonces la muestra va a ser menor obviamente. Y se suman y se obtiene el tamaño total de la muestra. Ahora bien habiendo explicado el estratificado vamos a ver el siguiente que ya es como bastante bastante parecido. Pero luego explicaré las diferencias. El aleatorio conglomerado. Las unidades muestrales se toman como colectivos en este caso. Ejemplo las escuelas, los hospitales, las empresas, etc. etc. Estos grupos se les llama conglomeraciones. Ahora si los conglomerados son pequeños entonces se toman todos sus elementos como parte de la muestra. Ahora si los conglomerados son grandes entonces hay que aplicar otro tipo de muestreo por cada uno de ellos. Para obtener su tamaño indicado y no tener que ocuparlos a todos. Pero ahora bien ustedes hasta este punto si se logró entender bien el concepto se preguntaran. Y el estratificado y el conglomerado no son lo mismo? Estamos agrupando, si es cierto. Cada una de esas poblaciones que entran en cualquiera de estos métodos de muestreo. Ahí se van a dividir en grupos. Pero aquí hay varias diferencias fundamentales. Eso lo veremos en esta siguiente diapositiva. La diferencia entre estratificado y conglomerado. Estrato es un grupo de personas dentro de una sociedad que se considera diferenciado del resto. Por tener un nivel socioeconómico semejante. Más alto o más bajo que otros grupos. Entonces aquí se explica de la siguiente manera. Que tenemos dos grupos de personas que pertenecen a una población o una sociedad. Pero cada uno de los individuos que están dentro del grupo en particular. Son similares, son semejantes en una característica. Entre todos son iguales. Pero entre todos los grupos hay una diferencia. Entonces todos que están dentro del grupo son iguales en algo. Pero todos los grupos que están dentro de la población son diferentes. Espero que se entienda de esta forma. Ahora en el muestreo estratificado se tomarán individuos de todos los estratos o grupos formados. En función de la fijación que se utilizará. Ya sea simple o proporcional. Pero se van a tomar en cuenta todos los grupos. Si hay cinco grupos se va a sacar muestra de cinco grupos. No de más, no de menos. Ahora bien, en el conglomerado ya es otra cosa. Es una mezcla confusa de personas o cosas de distinto origen o naturaleza. Y a menudo contrarias. Aquí ya se puede definir de otra manera. Puede ser que sean iguales o distintos. Pero siempre debe de haber un criterio de división de los grupos. Por ejemplo estamos hablando anteriormente de escuelas y de hospitales. A tomemos de ejemplo las escuelas. Si queremos dividir en un departamento de El Salvador. Poblaciones de todas las escuelas que hay en el departamento. Entonces se van a tomar así los grupos por cada escuela que exista. Ahí mismo. Y lo interesante es que en cada escuela hay alumnos. Pero hay una diferencia notable entre todos o la mayoría de los alumnos. Igual pasa en la otra escuela. Igual pasa en otra escuela que está en el departamento cerca. Igual pasa en otra, igual pasa en otra. Y así. Pero lo único en común es que lo que se está tomando para la investigación son escuelas. Entonces teniéndose en cuenta ya entra en la categoría de conglomeración. Como les digo esto es básicamente de analizar cual es el tipo de escuelas. Y el igual de los dos muestreos va muy bien. De acuerdo a como se relacionan los individuos y los grupos que se forman. Ahora en el muestreo conglomerado no se tomarán individuos de cada grupo. Sino que solo de unos cuantos grupos previamente seleccionados. Aquí los pasos cambian. Digamos que tenemos otra vez los cinco grupos que ya son conglomerados. Tenemos los cinco grupos y solamente queremos elegir dos o queremos elegir tres. Y de esos dos o de esos tres se van a tomar la muestra. Y los que queden pues ahí se dejan. Volviendo al ejemplo de las escuelas. Digamos que hay un total de 30 escuelas. Por decir algo es un número exagerado para una investigación. Hasta cierto punto. Pero de esas 30 escuelas solo queremos elegir diez. O solo queremos elegir ocho. Y de esas que se elijan de ahí se va a tomar la muestra. Y a las que resten las otras 20 o las otras 22. Se quedarán ahí. No tendrán oportunidad de a que se les seleccione algún alumno. Y por último para finalizar esta presentación. Voy a explicarles sobre los muestreos no probabilísticos. Aquí básicamente es lo opuesto de lo que se explicó al principio. Aquí no hay iguales oportunidades para los individuos que están en la población. Los que se seleccionen en ese momento. Son los que van a entrar. Y los demás ni siquiera sabrán de que se han seleccionado. A parte de su población para alguna investigación. Se entiende mejor con los siguientes métodos que están aquí. Estos son los principales. Puede que haya otros por ahí. Pero no siempre se suele utilizar este tipo de métodos. Veámonos bien. Accidental. Aquí el investigador elige a los individuos que se encuentren cerca. Sin atender ningún criterio ni principio previamente establecido. Por ejemplo cuando estamos hablando de un reportaje. Cuando hay reportajes en las noticias. Agarran a alguna persona al azar. De las cercanías de un lugar. Para poder hacerle unas preguntas. Y esas personas salen hablando. Dando su opinión sobre la noticia que se está realizando en ese momento. Esas personas cuentan como muestra. De la población que está involucrada en la noticia. Por decir un ejemplo. Ahora hablando de el tipo de muestreo no probabilístico. Por cuotas. Aquí se le facilita a un entrevistado. El perfil de las personas que deben ser entrevistadas. Y varias de ellas son seleccionadas y cumplen con el perfil buscado. Aquí mencioné entrevistador. Podemos tomar de ejemplo al departamento de recursos humanos de alguna empresa tal. Entonces. Lo que hacen ahí es recibir los currículums por ejemplo. O recibir los. Las hojas de vida de las personas que quieren aplicar a una base. O a una base de la gente que quiere aplicar. Las personas que quieren aplicar a una vacante que está en la empresa. Entonces las tienen que entrevistar y deben de pasar por un proceso de selección. Y al ser entrevistadas. Las personas a cargo de ello van a elegir a quienes les parezcan. Aptos o aptas para la vacante de empleo. Ahora pasemos con el intencionado. Se basa en una buena estrategia y buen juicio del investigador. O sea que queda a su entero criterio. Elegir a los individuos para su muestra. Aquí a este es como más rápido podría decirse. Porque tenemos la población verdad. Y lo que haría el investigador es simplemente decir. Bueno los reúne a todos. Reúne a buena parte de esas personas que van a investigar. Y lo que hace es simplemente decir. Tú vienes conmigo. Tú también. Y así verdad. O sea los va seleccionando así de una sola vez. Sin importar a quiénes agarró. Muy bien. Eso ha sido todo por el video de hoy. Les agradezco mucho por verlo. Si les gustó no olviden dejar su like y su comentario. Si tienen alguna duda o sugerencia. Y también suscribanse al canal. Para estar atentos a cualquier notificación de algo nuevo que suba. Para lo mismo también pueden seguirme en mis redes sociales. Facebook y Twitter. Sobre todo Facebook. Que ahí es donde paso más activo. Los links estarán siempre en la descripción. Así que bueno. Mucho gusto. Y como siempre. Hasta en otro video. Hasta la próxima. Adiosito.
DOC0147|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí les habla una vez más Fernetts y sorpresa, luego de más de un año y medio de hacer estos videos, finalmente regreso con los videos de estadística y probabilidad. En este caso vamos por el video número 9, en el anterior habíamos visto algo respecto a temas de muestreo y lo que viene siendo la teoría de los tipos de muestreo que existen, sobre todo los probabilísticos por ejemplo. Y en este video vamos a retomar a partir de ahí con lo que viene siendo un ejemplo práctico. Esto de la continuación de este tipo de videos tutoriales, ya lo había avisado anteriormente, hace creo que ya más de un mes por cierto, en lo que es mi página de Facebook. Y por si no han visto el anuncio al respecto o por si no han visitado mi página aún, les dejaré el link en la descripción del video, así como también el link de lo que es el playlist o la lista de reproducción de todos los videos que vaya subiendo de esta asignatura. Y también les invito a que si les gusta este tipo de contenido, así como las cosas que subo de juegos y cosas variadas, les invito también a que se suscriban al canal para estar pendientes de más trabajos como este. Así que bueno, sin nada más que agregar para el principio, vamos a comenzar. Como les mencionaba anteriormente, vimos lo que es el muestreo. Vamos a repasar un poco ese contenido y luego les mostraré, para entrar a lo que es la práctica, el enunciado de un ejercicio o un problema práctico que va a requerir de que hagamos varios muestreos. En este video en particular solamente quiero enfocarme en mostrar lo que es el ejercicio a resolver y los primeros pasos que son obtener el tamaño de la muestra indicada y de ahí aplicar el primero de los muestreos probabilísticos, en este caso el aleatorio simple. Para recordar, el muestreo es una herramienta de la investigación que se encarga de determinar qué parte de una población o universo debe ser examinada con el fin de hacer inferencias sobre esa misma población. Entonces, para resumirlo de un mejor modo, digamos que ya tenemos a la población que se va a estudiar, digamos unas mil personas, y de esas mil personas solamente hemos determinado que se van a estudiar unas 80, por ejemplo, 80 vendría siendo una muestra pequeña, una porción de esas mil personas que son la población total. Pero ahora, sabemos cuál es ese número, la cantidad de gente que se va a estudiar, pero ¿quiénes conformarán esa muestra? ¿Quiénes van a ser los 80 individuos en ese caso que van a formar parte de esa porción de gente? Eso es lo que nos ayuda a determinar el muestreo, y existen diferentes técnicas para muestrear, en este caso les quiero hablar de las probabilísticas, en el ejemplo que les mostraré, está el aleatorio simple, el aleatorio sistemático, el estertificado y por conglomerados. Este tema ya se había explicado bastante a detalle en lo que es el video anterior, les voy a dejar una tarjeta aquí arriba en la esquina para direccionarlos al video número 8, que habla sobre el muestreo, por si desean básicamente recordar un poco más de la teoría al respecto. Pero por ahora pasemos a la práctica. El enunciado dice así, dada la siguiente información referente a vendedores de repuestos de automóviles, obtener lo siguiente, el tamaño de la muestra en la cual nos pide establecer un error muestral del 10%, esto ya lo habíamos cubierto en videos anteriores, nivel de confianza del 90% y un 85% de probabilidad de éxito. Seleccionar los elementos de la muestra haciendo uso de todos los muestreos, cabe destacar aquí que son los probabilísticos y son los que ya habíamos mencionado antes. Y el último paso, a partir de la muestra obtenida con el muestreo aleatorio simple únicamente con uno de ellos, elaborar la tabla de distribución de frecuencias utilizando como dato el peso en gramos de los repuestos. Ok, así es como se les puede presentar un ejercicio de este estilo, digamos en un examen o en una simple tarea, una guía de ejercicio que se les pueda brindar si son estudiantes universitarios o si solamente están explorando esta asignatura para conocer más a fondo por cuenta propia. Pero en sí esta es una manera en como se les puede presentar, no se les presentan todos los datos por ejemplo en el caso del tamaño de la muestra, porque hay algunos que a partir de los que ya existen se pueden obtener, por ejemplo el caso de la probabilidad de éxito y de fracaso, aquí solo nos están dando la probabilidad de éxito y mediante esta misma y con una simple fórmula se puede obtener la probabilidad de fracaso. Pero eso es algo que veremos a detalle en las siguientes diapositivas. Esta es la tabla inicial, tenemos todos estos vendedores que en este caso están etiquetados por nombres de empresas conocidas, por ejemplo Didea, Impresa, Repuestos, A y A y también Super. Y tenemos a la vez el tipo de repuesto que se les está adjudicando a estos vendedores, que son repuesto eléctrico, repuesto de suspensión o también tenemos repuesto de motor y repuesto de carrocería. Y en la siguiente columna tenemos los pesos de cada uno de estos en gramos, que figura desde el 100 hasta el 150. Y si se fijan aquí están enumerados del 1 al 70, lo que nos indica que en total la población según esta tabla es de 70 vendedores. Primer paso, obtendremos el tamaño de la muestra, como lo hacemos ahora verán. Los datos con los que contamos son los siguientes. La población que ya vimos que es de 70 vendedores. El error muestral que el mismo enunciado nos indica que es del 10%, lo cual a su vez es lo mismo que decir 0.10. El nivel de confianza o de certeza que es lo mismo, 90%, que es lo mismo a decir 0.90. Probabilidad de éxito, 0.10. 0.85, lo mismo que decir 85%. ¿De dónde sacamos ahora la probabilidad de fracaso? El enunciado no nos los dio en ningún momento. Pero gracias a la fórmula de P más Q igual a 1, podemos obtener lo que es Q. Dado que la probabilidad de éxito más la probabilidad de fracaso es igual a 1, entonces 1 menos 0.85 es igual a 0.15. Ahora, lo que no tenemos y que de verdad tendríamos que sacar mediante las fórmulas y todo, es el valor de Z y lo que es el tamaño de la muestra que es lo que se nos pide realmente. Lo siguiente que vamos a hacer habiendo determinados los datos que se van a utilizar, es obtener el valor de Z. Dividimos lo que es el nivel de confianza, pero lo que es su representación decimal, entre 2. Y el valor resultante lo buscaremos en la tabla de distribución normal. En este caso la tabla es N, paréntesis, 0,0.5. Está también la otra que es la de 0,1, pero esa no la vamos a utilizar, sino que esta misma. Ahora bien, luego de dividir el nivel de confianza entre 2, tenemos este dato, 0.4500. Lo he dejado con 4 decimales porque en la tabla de distribución normal, los datos que vamos a ver tienen como máximo 4 decimales, 4 cifras decimales. Buscando en lo que es la tabla, obtendremos el valor de 1.64. Pero ¿cómo lo hacemos? Aquí tengo lo que es la porción de la tabla de distribución normal en donde se encontró el dato. Lo hacemos de este modo. Anteriormente nos dio 0.45 al hacer la división del nivel de confianza entre 2, ¿verdad? Entonces, agarramos nuestra tabla normal y buscamos ese mismo numerito 0.4500 en lo que es alguna de estas celdas de la tabla. Si no encontramos el valor exacto, exacto, pues vamos a buscar el que se aproxime muchísimo más a ese valor. Por eso es que tomé este valor de 0.44950, que está en la misma tabla. No es exactamente 0.45, pero se acerca muchísimo. Y si lo comparamos con este otro valor que dice 0.45053, también se aproxima. Pero si logran ver, hay una mínima diferencia que hace que básicamente se aproxima un poquito menos que el número que sí elegimos acá. Ahora, ¿cómo formé este 1.64 de acá? Tomo la fila a la que pertenece esa celda con el número y lo junto con la columna en donde aparece el número. En este caso, este numerito 0.44950 aparece en la fila 1.6. Es la primer parte del valor nuevo. Y la siguiente parte es el número de la columna. Si hacemos la suma 1.6 más 0.04, obtendremos 1.64. Luego de tener el valor de z, ya contamos con todos los valores que necesitaremos para operar la siguiente fórmula. La cual nos ayudará a obtener la n minúscula que corresponde al tamaño de la muestra. Haciendo la sustitución de los valores, obtenemos esta pequeña fórmula de acá. Y luego de hacer los cálculos, obtendremos que n es igual a 23.2395. Y aquí hay una nota importante. Lo vamos a aproximar al próximo valor sin importar la cantidad decimal obtenida. Siempre y cuando obtengamos un decimal acá, siempre se va a llevar a la siguiente cifra entera. En este caso nos queda que n es igual a 24. Lo que significa que de las 70 personas, que son los 70 vendedores, se van a investigar 24 de ellos. Podemos comprobar los resultados mediante una fórmula secundaria, la cual es esta. Entonces siempre llegamos a lo mismo, 24. Lo que nos da a entender de que si está correcto el cálculo que hicimos. Ahora bien, ya sabiendo de que de 70 vendedores vamos a tomar solo una muestra de 24, vamos al siguiente paso. Selección de individuos con muestreo aleatorio simple. El primer paso es que la población se tiene que enumerar. Y eso ya lo tenemos hecho. Si se fijaron en la tabla que las mostré anteriormente ya todos están debidamente enumerados del 1 al 70. Entonces ese paso ya está. Ahora el siguiente es el uso de la tabla de números aleatorios. Ojo, en este caso no sería la de distribución normal, sino que es una tabla totalmente diferente. Antes de pasar a la tabla de números aleatorios que les decía, hay que determinar cómo vamos a trabajar con esa tabla. Lo que vamos a hacer primero es digitar el número de la población y contar cuántas cifras tiene. Sabemos en este caso que el número de la población es 70, ¿no? Y el número 70 cuenta con dos cifras, 7 y 0. Ubicaremos el grupo de dígitos que queremos tomar de los números aleatorios de la tabla. En la tabla que les mostraré luego, los números que se presentan en todas las celdas de la tabla tienen un máximo de 5 cifras. Por lo cual podemos tener hasta números de 99,999 o hasta el número 1. Ahora lo que vamos a hacer es, de esos 5 dígitos posibles, vamos a seleccionar un grupito en el cual nos vamos a enfocar. En este caso yo elegí los primeros dos a la izquierda. Bien pueden seleccionar si gustan estos dos de aquí, o pueden seleccionar estos dos de acá, los últimos, o estos dos de acá, depende. Pero siempre se recomienda que los números en cuestión estén juntos. Si tuviéramos un número de población de 4 cifras, por ejemplo, entonces bien podríamos tomar este grupito de cifras, obviando este último, o podemos tomar este otro grupito de cifras, el de los últimos 4. Luego, nuestro siguiente paso será que, sin ver la tabla, vamos a elegir un número al azar. Y a partir de ese número comenzaremos a desplazarnos. Ok, esta es la tabla de números aleatorios que les mostraba anteriormente. En mi caso, el número que termine seleccionando está en la fila 10 y en la columna 5, y es este número de acá. 71.953. Ahora, según los pasos anteriores, ¿qué pasará a partir de aquí? De este número que yo seleccioné, lo voy a acercar un poco, de este número en cuestión, yo solamente decidí enfocarme en las primeras dos cifras a la izquierda. Por lo cual, lo único en lo que me voy a fijar, tanto en este número como en los siguientes, serán estas cifras de aquí, la primera y la segunda a la izquierda, sin tener en cuenta esto. Así que aquí lo que estoy viendo realmente es un 71. A partir de ahí, nuestro siguiente paso es elegir un sentido de desplazamiento para recorrer la tabla. En este caso tenemos que buscar dos sentidos. El primero tiene que ser o arriba o abajo, pero no los dos, y el segundo debo elegir o la izquierda o la derecha. OK, en este caso yo ya tengo los números que van a formar parte de mi muestra, los números que yo encontré, según el sentido en el que fui recorriendo la tabla, y aquí están ordenados de menor a mayor. Vamos a buscar los individuos los cuales contienen esos números y esos vamos a tomar como parte de la muestra. En este caso, por ejemplo, los números 4, 7 y 10 corresponderían al vendedor número 4, al vendedor número 7 y al vendedor número 10. Y luego volvemos y vemos que tenemos al vendedor número 11, el vendedor número 16, el vendedor 17, el 20, el 21, hasta llegar al número 70. Aquí tengo mi tabla ya con todos los resultados obtenidos del muestreo aleatorio simple. Como vieron, a como encontré el número 4 en la tabla de números aleatorios, entonces el vendedor número 4 forma parte de la muestra. Encontré también el número 7, entonces el vendedor número 7 forma parte. Y si vieron también encontré el 20, y el vendedor número 20 forma parte también. Y el número 70 también, y así según los números que yo encontré. Estos resultados pueden variar bastante. Si ustedes están practicando ahora mismo este ejercicio conmigo, o ya han hecho ejercicios anteriores así, van a fijarse de que no va a salir exactamente los mismos datos para la muestra. Bien puede ser que ustedes hayan encontrado el número 5 y el número 6, y entonces en su muestra tengan al vendedor número 5 y al vendedor número 6. Yo no los tengo porque no encontré esos números. A la hora de recorrer la tabla y seguir los pasos debidamente, no encontré eso en mi caso, en mi resolución. Pero puede ser que ustedes sí. Por ello, los resultados aquí presentados solamente son un pequeño ejemplo para explicar a realmente cómo se resuelve este tipo de problemas, qué pasos hay que seguir y todo. Muy bien, esto ha sido todo por el video de hoy. Espero que les haya gustado y que se haya logrado comprender lo que es el paso a paso de la resolución de esta parte del problema. Estén atentos por favor a la siguiente parte de este video para que sigamos resolviendo este ejercicio paso a paso. Y para eso les invito nuevamente a que se suscriban al canal, para que les aparezca la notificación pues activen también la campanita, y estarán informados de cuando suba este contenido otra vez. A la misma vez les invito a seguirme en Facebook, aparezco como Fernets, no como aquí que aparece FN48 pero es bastante similar. Para estar pendientes también de cuando suba algún meme o algo respecto a juegos de los que ya he jugado yo, o alguna ilustración o algo por el estilo. O simplemente una notificación de cuando ya haya subido uno de estos videos aquí al canal. Por ahora yo me despido, yo soy Fernets y será hasta en un próximo video. Hasta la próxima. Alguien se comió mis galletas.
DOC0148|Estadistica y Probabilidad|Muy buenas a todos y todas aquí les habla nuevamente Fernets y les doy la bienvenida a esta nueva entrega de vídeos de estadística y probabilidad para el canal. Estamos ahora en el vídeo número 10 y es la segunda parte del ejercicio de muestreo para llevarnos a la práctica. En este caso les voy a presentar nuevamente el enunciado del ejercicio e iremos al procedimiento de la aplicación del muestreo aleatorio sistemático. Si gustan ver vídeos anteriores sobre todo el último que fue la parte 1 en donde aplicamos el muestreo aleatorio simple y pues obtuvo antes de todo el tamaño de la muestra les voy a dejar en la descripción del vídeo lo que viene siendo el enlace del playlist o la lista de reproducción que incluye todos los vídeos de estadística probabilidad. El enunciado del ejercicio es este, nos dan la información una tabla de datos referente a vendedores de repuestos de automóviles y nos piden sacar cosas como el tamaño de la muestra con los parámetros que nos indican ahí y lo que viene siendo la selección de elementos de la muestra haciendo uso de todos los muestreos que están ahí enlistados y la creación de la tabla de distribución de frecuencias. Les voy a dejar también en la descripción del vídeo un enlace a un archivo PDF que yo mismo cree que incluye todo sobre este ejemplo de este ejercicio de muestreo que estamos tratando en este y en los demás vídeos que serán por lo menos unas cinco partes que se van a grabar de esto. Lo que vamos a hacer ahora es la selección de individuos con el muestreo aleatorio sistemático. El paso número 1 el cual vamos a obviar porque ya está hecho es que los elementos de la población deben de estar enumerados y en este caso va a ser del 1 al 70 por ser 70 el total de vendedores que hay. El paso número 2 sería obtener el valor de K, la constante K que viene siendo el resultado de la división entre el número de la población y el número de la muestra. Aquí como podemos ver K es igual a N y en nuestro caso es 70 entre 24 que viene dando 2.9 que se redondea a 3. Entonces K es igual a 3. El siguiente paso es obtener el valor de y minúscula el cual debe ser un valor comprendido entre 1 y K. En este caso 1 y 3. Y para encontrar ese valor vamos a usar la tabla de números aleatorios que utilizamos en el vídeo anterior para el muestreo simple. Aquí haciendo un breve resumen de los pasos que se siguen lo que se va a hacer es esto. Se va a seleccionar únicamente una cifra de las cinco posibles que hay en las cantidades de la tabla de números aleatorios. ¿Y por qué una sola cifra? Porque el valor de y debe estar comprendido entre 1 y 3 y los números entre 1 y 3. Y los números entre 1 y 3 son sólo de una cifra. 1, 2 y 3 son sólo una cifra cada uno. Entonces por eso buscamos una. Y en este caso yo elegí la última a la derecha. Seleccionando el punto de partida de la tabla en mi caso me tocó el número 11.781 que está en la fila 14 y en la columna 13 de la tabla. Y ese sería mi punto de partida y me comenzaría a desplazar ya sea digamos arriba o abajo y a la izquierda o derecha para obtener el valor de y. Pero en mi caso yo veo de que el valor que seleccioné al principio ya incluye un número 1 en la última cifra en la que yo decidí fijarme. Decidí fijarme en la cifra la última de la derecha y en este número este numerito 1 es la última cifra a la derecha y está entre 1 y 3. Entonces por eso ya lo seleccioné y quedé de que y en mi caso al resolver el ejercicio es 1. El paso número 4 es el siguiente. Teniendo las constantes y y la constante k comenzaremos a sumar k varias veces a la y para poder obtener los demás números que en sí representarán los elementos de la muestra. ¿A qué me refiero con esto? Les voy a poner el ejemplo. En mi caso como y me dio igual a 1 entonces voy a tomar ese 1 como el primer elemento de la muestra. Después a partir de aquí voy a sumarle la constante k una vez y más k o sea 1 más 3 es igual a 4 y aquí lo tengo. Aquí para resumirlo todo lo único que van a hacer es sumar k varias y varias y varias veces y por cada suma van a ir obteniendo los datos que van a formar parte de la muestra resultante. Ok y aquí está la tabla resultante. En este caso como se pudieron dar cuenta obtuve primero el número 1 entonces el vendedor número 1 entra acá después al sumar k la primera vez obtuve el número 4 entonces el vendedor número 4 entra en la muestra y así sucesivamente hasta tener todos los 24 elementos. Muy bien esto ha sido todo por el vídeo de hoy. Si les gustó no olviden compartirlo con sus demás amigos y amigas y conocidos y a la vez dejar su like y algún comentario por si tienen alguna duda sugerencia o algo que deseen comentar nada más para respondérselos lo más pronto posible. A la vez les invito a suscribirse al canal para estar pendientes de siguientes trabajos como este también subo cosas de videojuegos, blogs, reacciones y alguna otra cosa por ahí que se me ocurra y también les invito a seguirme en facebook estoy como fernets ahí comparto memes comparto otras cosas de juegos de desencuando y otras cosas más como ilustraciones por ejemplo yo soy fernets y será hasta en una próxima entrega hasta luego
DOC0149|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí está nuevamente Fernets y les doy la bienvenida a una nueva entrega de videos de estadística y probabilidad para el canal. Ahora vamos por el video número 11 y este va a ser la tercera parte del ejercicio de muestreo que estábamos tratando en videos anteriores. Si gustan regresar a esas entregas, pues les voy a dejar en lo que es la descripción del video un enlace a la lista de reproducción que contiene todos esos videos. Viendo rápidamente el enunciado del ejercicio nos dan una tabla de datos que contiene vendedores de repuestos de automóviles y el peso en gramos de estos mismos repuestos. Y nos piden sacar el tamaño de la muestra, la selección de elementos con los amuestreros que están aquí y la tabla de distribución de frecuencias que luego vamos a ver. Por el momento pasamos a el uso de este muestreo, el estratificado. Viendo rápidamente, esta es la tabla de datos con la que estamos trabajando. Estos son los individuos que son vendedores de repuestos de automóviles debidamente enumerados del 1 al 70. Les voy a dejar en la descripción del video además un enlace a lo que es un archivo PDF que contiene toda la resolución de este ejercicio. Entonces, el primer paso es el siguiente. Se cuentan los individuos de las poblaciones que fueron divididas previamente y en este caso se dividieron por vendedor. Aquí en el PDF que les decía ya tengo enlistados todos los elementos de la población por vendedor y enumerados debidamente, claro está. Aquí en esta tabla únicamente van a ver los que están en la clase de vendedor A y A y son 20 como pueden ver aquí. Y el siguiente estrato, la siguiente tabla resultante, los vendedores de idea. Y en la siguiente tabla los vendedores impresa y en la siguiente tabla los vendedores super. Y cada uno con su respectivo dato, no fueron manipulados. Para tener una mejor idea de lo que significa un estrato o un conglomerado, les invito a que vean el video anterior de estas entregas que tiene que ver con la teoría del muestreo. Les voy a dejar una etiqueta aquí arriba ahora mismo para que vayan al video si desean verlo ahora. Para que vayan al video si desean verlo ahora. El siguiente paso es determinar la fijación que se utilizará. Se entiende por fijación lo que es la distribución de la muestra en función de los diferentes estratos. O se hace como una forma de dividir o de repartir qué cantidad de individuos se va a tomar de cada estrato para completar la muestra que teníamos que sacar anteriormente, que es de 24. Para verlo de otro modo, como tenemos cuatro grupos de vendedores divididos en este caso por vendedor, por empresa como sea. De esos cuatro grupos hay que ver cuántas personas vamos a sacar de cada grupito para sumarlas todas y que hagan el valor de 24 individuos en total. Y hay que ver cómo vamos a hacer esa repartición, cómo vamos a hacer esa división. Tenemos que se puede ocupar la fijación simple o la fijación proporcional. La fijación simple indica de que se va a sacar igual número de individuos de cada grupo, de cada estrato. Sin importar la cantidad de individuos que tengan cada uno de los grupos. No importa si es un grupo de mayor o de menor cantidad de individuos. Se va a sacar igual cantidad para la muestra. Caso que no ocurra con la fijación proporcional. En la cual se va a sacar dependiendo del tamaño del grupo un número indicado de individuos para la muestra. Entre más grande sea el grupo, más individuos se sacarán. Y en el otro caso entre menos personas hay en el grupo, menos personas se sacarán. Ahora bien, aquí lo hice de ambas formas para que se pueda ver el uso de las fórmulas. En el caso de la fijación simple tenemos que se va a dividir N minúscula, o sea el número de la muestra, entre el número de estratos. 24 que es el tamaño de la muestra, entre 4 que es el número de estratos es 6. Quiere decir que de los cuatro grupos vamos a sacar seis individuos de cada uno, ni más ni menos. Y 6 por 4 son 24. ¿Verdad? Y en el caso de la fijación proporcional es diferente. La fórmula es así, tamaño de la muestra sobre tamaño de la población, todo eso multiplicado por el número de elementos que hay en ese estrato. En este caso no se va a hacer una única operación, como ocurre con la simple, sino que se va a hacer una por cada grupo. Y como son cuatro grupos se va a hacer cuatro operaciones. Aquí como podemos ver el estrato número 1, que era, si no más recuerdo, A y A, tiene 20 individuos. Por eso es que el número de elementos del estrato a la hora de sustituirlo le ponemos un 20. Y 24 sobre 70 muestran de población. Y obtenemos que del grupo 1 vamos a sacar 7 personas. Y en el caso del segundo grupo, que es el de Ridea, que tiene 16 individuos, al hacer la operación tenemos que se van a sacar 6 personas de lo que es ese grupo. Y del otro grupo se van a sacar 5 y del otro grupo se van a sacar 7. El paso número 3 ya es obtener los muestreos. Hasta este punto ya hemos identificado a cuantas personas o cuantos individuos vamos a sacar de los grupos según la fijación que elegimos. Entonces sabiendo cuantas personas van a salir de cada grupo para completar el tamaño de la muestra que sacamos inicialmente. Entonces ya recae en cada uno de nosotros que muestreo se va a aplicar para cada grupo. A cada grupo se le va a aplicar un muestreo diferente. Ya sea a unos le pueden aplicar el simple o a otros le pueden aplicar el sistemático. Y los resultados como les decía en entregas anteriores variará muchísimo. Por ende no les presento en este video lo que es un resultado final porque solamente va a ser una tabla con los individuos que se eligieron. Aparte esto ya lo había explicado en videos anteriores. En las últimas dos partes les explicaba lo del muestreo simple y el muestreo sistemático. Por ende los pasos, el procedimiento, la secuencia y todo ya están explicados. Muy bien hasta aquí llego por hoy. Muchas gracias por ver este video y pues si les ha quedado alguna duda o tienen alguna sugerencia pues les invito a dejar un comentario en la parte de abajo en este video. También les invito a suscribirse al canal para estar pendientes de nuevos trabajos como este y a seguirme en mi Facebook, EstoyComoFernitz. Y si les gustó el video no olviden dejar su like y compartirlo con tanta gente como puedan para que el conocimiento llegue a más personas. Y pues hasta aquí termino, yo me despido. Estén pendientes de la cuarta parte en la que hablaremos del muestreo por conglomerados. Yo soy Fernitz y hasta en un próximo video. Hasta luego.
DOC0150|Estadistica y Probabilidad|Muy buenas a todos y todas, aquí les habla una vez más Fernets y les doy la bienvenida a lo que es esta nueva entrega de videos de estadística y probabilidad para el canal. Vamos ahora en el video número 12, vamos ahora por la parte 4 del ejercicio de muestreo. Si gustan pasar a entregas anteriores antes de ver este siguiente procedimiento, les dejaré el playlist de todos los videos de esta asignatura, los que están en mi canal por cierto, se los dejaré en la descripción del video. Y a la vez les voy a dejar un archivo PDF, un enlace de un archivo PDF que contiene todo lo que hemos hecho para este ejercicio. Ok, dando un breve repaso en lo que es el enunciado del ejercicio, ya obtuvimos lo que es el tamaño de la muestra y los resultados a los elementos de la muestra con el muestreo simple, el sistemático y el estatificado. Ahora pasamos a este, por conglomerados, como les decía antes. Ahora bien, como el tamaño de la muestra se calculó con anterioridad, haremos lo siguiente. El paso número 1 es dividir la población en conglomerados. En este particular caso se van a dividir, o se van a categorizar, digámoslo así, por el tipo de repuesto que venden. Por ende, según las tablas resultantes las que les mostraré luego, tenemos que hay 16 que venden repuestos de carrocería, hay 14 que venden repuestos de motor, 26 que venden repuestos de suspensión y 14 que venden repuesto eléctrico. He aquí la tabla de los conglomerados. Como les dije, se categorizan por el tipo de repuesto y como pueden ver, el primero es de carrocería y en toda la columna que dice tipo de repuesto están únicamente los que venden repuesto de carrocería, y sobre respectivos a datos que les faltaban. Y luego tenemos los de motor, luego tenemos los de suspensión, que son varios, y luego tenemos los de repuesto eléctrico. El siguiente paso es encontrar los conglomerados y para ello se deben delistar en tablas separadas, cosa que ya les mostré hace un momento. El siguiente paso es encontrar el promedio de los conglomerados y para eso vamos a utilizar esta fórmula. Prom igual a sumatoria de cantidades de los conglomerados sobre el número de conglomerados que hay. En este caso los números anteriores 16, 14, 26 y 14 se van a sumar y se va a dividir entre 4 ya que son 4 conglomerados y obtenemos que el promedio es igual a 17.5. El siguiente paso es determinar la cantidad de conglomerados que se van a utilizar. A diferencia del muestreo estratificado aquí solamente vamos a tratar con unos cuantos grupos, no todos, solamente los que se nos indique en este paso. En el caso del estratificado utilizamos todos los grupos. Ahora bien, volviendo al tema, en este caso el número de conglomerados se determina por medio de esta fórmula. N sobre prom, N siendo el tamaño de la muestra y prom el promedio que obtuvimos anteriormente que es de 17.5. 24 entre eso viene siendo 1.3714 y si lo aproximamos al siguiente valor sin importar el decimal que está ahí, obtenemos el número 2. Lo que nos dé a entender que vamos a utilizar 2 conglomerados de 4 posibles opciones. El paso número 5 es seleccionar los conglomerados a utilizar. En este momento pues vamos a hacer uso nuevamente de la tabla de números aleatorios, cosa que ya se explicó en el video de la parte 1 de este ejercicio en el cual hablamos del muestreo aleatorio simple. Pero para dar un breve resumen de los resultados, siguiendo los pasos de cómo se utiliza la tabla de números aleatorios, en mi caso yo tengo el número 71953 en la fila 10 columna 5 y de ese número yo decido elegir el segundo dígito a la izquierda. Y seleccionando el sentido de movimiento hacia arriba y hacia derecha para encontrar los demás números que tendrían que estar entre 1 y 4 para saber si elijo o el primero, el segundo, el tercero o el cuarto de los conglomerados. Y hasta que encuentre 2 de esas cantidades, ahí me detengo, ya no utilizo más la tabla porque ya tendré las 2 cantidades que necesito para determinar cuales son los 2 conglomerados que utilizaré. En mi caso obtuve el número 1 y el número 4, buscando en la tabla esos son los números que me salieron. Lo que quiere decir que voy a terminar utilizando el primer conglomerado que es el de repuesto de carrocería y el número 4 que es el de repuesto eléctrico. Y el siguiente paso solo es la elaboración de la lista de los conglomerados seleccionados, cosa que ya les mostré hace un momento pero lo traigo en esta depositiva para que estemos al tanto, para que estemos hacia el corriente. Y tengo aquí los de repuesto de carrocería que son 16 y tengo aquí los de eléctrico que son 14. Y aquí solamente el último paso, así como en el estratificado, solamente vamos a determinar que mustreo se va a ocupar, si el simple o el sistemático. Hay que asegurarnos de que de todos los conglomerados seleccionados se logra obtener la cantidad de la muestra original. Así que aquí lo más recomendable para mi sería utilizar una fijación simple. En este caso tenemos que son 2 conglomerados de los cuales se van a obtener 24 personas en total. Entonces repartiendo bien entre uno y el otro tendría que sacar 12 individuos de cada tabla que formé aquí, de cada conglomerado, para hacer el total de 24. 12 individuos por 2 conglomerados es igual a 24. De nuevo les digo, esto depende de los resultados que ustedes obtengan porque esto puede variar muchísimo. No les va a salir exactamente el mismo resultado que a mi o que a la persona X o la persona Y. No, en este caso van a obtener algo totalmente distinto, pero siempre y cuando sigan los pasos que se les han indicado en estos videos, entonces todo estará en orden. Muy bien, hasta aquí por el video de hoy. Muchas gracias por verlo, espero que les haya gustado y que se haya entendido el contenido. Todavía nos falta un paso más que es el de la obtención y la creación de la tabla de distribución de frecuencias. Pero antes de continuar con este ejemplo le voy a dedicar un video teórico a este tema, la distribución de frecuencias. Y luego pasamos a la finalización de este ejercicio. Por el momento todo está en orden, si hay alguna duda o sugerencia me la pueden indicar en los comentarios de este video o en los últimos que se han subido. Y también les invito a dejar su like, a compartir este video con más personas y suscribirse al canal para estar pendientes de cualquier otro video que suba de este contenido o de algún otro de los que subo normalmente. Y a la vez les invito a seguirme en mi página en Facebook, la cual es Fernets, ahí subo ilustraciones, memes, más memes que otra cosa y algunas cosas de juegos. Y bueno, hasta aquí me despido por hoy, yo soy Fernets y hasta en una próxima entrega. Hasta luego.
