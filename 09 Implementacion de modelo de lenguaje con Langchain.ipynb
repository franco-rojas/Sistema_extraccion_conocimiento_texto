{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "722e530a",
   "metadata": {},
   "source": [
    "### Master oficial en Big Data & Data Science - Universidad Internacional de Valencia (VIU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff25bfaf",
   "metadata": {},
   "source": [
    "<font color=\"darkblue\">\n",
    "\n",
    "-   **Alumno:** Franco Rojas Yucra\n",
    "-   **TFM:** Sistema para la busqueda de conocimientos en textos de documentos multimedia\n",
    "-   **Email:** franco.rojasyucra@alumnos.viu.es\n",
    "-   **Fecha:** Octubre de 2023\n",
    "<font color=\"black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8669b20",
   "metadata": {},
   "source": [
    "## 09 Implementacion de modelo de lenguaje con Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de53580",
   "metadata": {},
   "source": [
    "Librerías utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4579b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "import re\n",
    "from dash import Dash, dcc, html, Input, Output, State, callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d541cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None) #motrar todas las columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded39e0",
   "metadata": {},
   "source": [
    "Lectura de modelo con HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700b8a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franco\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.0.1+cpu)\n",
      "    Python  3.9.13 (you have 3.9.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(model_id=\"franco-rojas/bloom-1b1-finetuned-tfmviu\",\n",
    "                                        task=\"text-generation\",\n",
    "                                        model_kwargs={\"temperature\": 0.2, \"max_length\": 200})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a576c462",
   "metadata": {},
   "source": [
    "Template que ayuda a generar pregunta y respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b6d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<|prompter|>{question}<|endoftext|><|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8d2fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para retirar codigos de template\n",
    "def answer_str(question,txt_lg):\n",
    "    txt1=re.sub(question, '', txt_lg)\n",
    "    txt2=re.sub(f'<|assistant|>', '', txt1)\n",
    "    txt3=re.sub(f'<|prompter|>', '', txt2)\n",
    "    txt_resp=re.sub('[%s]' % re.escape('?¿|'), '', txt3)\n",
    "    return txt_resp.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fce292",
   "metadata": {},
   "source": [
    "Configuración Dash para interfaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72f87cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "app = Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "app.layout = html.Div([\n",
    "    dcc.Input(id='input-1-state',style={'width': '500px'}, type='text', value=''),\n",
    "    html.Button(id='submit-button-state', n_clicks=0, children='CONSULTAR'),\n",
    "    html.Div(id='output-state')\n",
    "])\n",
    "@callback(Output('output-state', 'children'),\n",
    "              Input('submit-button-state', 'n_clicks'),\n",
    "              State('input-1-state', 'value'))\n",
    "def update_output(n_clicks, input1):\n",
    "    answ=llm_chain.run(input1)\n",
    "    txt_resp=answer_str(input1,answ)\n",
    "    return f'''\n",
    "        Respuesta: \"{str(txt_resp)}\"\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd796a",
   "metadata": {},
   "source": [
    "###### Interfaz de respuesta para modelo BLOOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855439da",
   "metadata": {},
   "source": [
    "Escribe una consulta en el cuadro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cadbc10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1b9c27e5fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franco\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1270: UserWarning:\n",
      "\n",
      "You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "\n",
      "C:\\Users\\Franco\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning:\n",
      "\n",
      "Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec3890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d64549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a6e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c48f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
