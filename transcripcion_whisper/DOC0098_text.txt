 Cuando resolvemos un problema de clasificación con Machine Learning, muchas veces nos debemos enfrentar a la decisión de determinar cuál de los múltiples modelos entrenados resulta más adecuado. Y comúnmente la métrica más usada es la exactitud, que es la proporción entre los datos clasificados correctamente y la totalidad de los datos. Sin embargo esta métrica a veces no es una fiel representación del desempeño real del modelo. Así que en este vídeo hablaremos de la matriz de confusión, una herramienta muy útil para evaluar el desempeño de modelos de Machine Learning. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Bits, en donde encontrarán los cursos que les permitirán construir su carrera en ciencia de datos y Machine Learning. Este mes estamos con el curso Python Nivel Avanzado, en donde aprenderemos a realizar el manejo de archivos y de excepciones y todos los detalles de la programación orientada a objetos en Python. Así que los invito a suscribirse a la Academia por un valor mensual de tan solo 10 dólares. Y ahora sí, comencemos. Supongamos que queremos crear un modelo para determinar si un sujeto tiene o no una enfermedad cardiaca, a partir del análisis de su electrocardiograma. Así el modelo clasificará al sujeto como normal o anormal. Y supongamos que para resolver este problema entrenamos varios modelos. Una red neuronal, un bosque aleatorio, una red convolucional y una máquina de soporte vectorial. Pero entonces ¿cómo sabemos cuál es el mejor clasificador? Para responder a esta pregunta necesitamos alguna herramienta que nos permita cuantificar este desempeño. Así podemos aplicar esta herramienta a cada modelo y luego realizar una comparación objetiva para escoger el mejor clasificador. Una de las métricas más usadas en estos casos es la exactitud, que es simplemente la proporción entre el número de aciertos del clasificador y la cantidad total de datos. Si por ejemplo entrenamos la red neuronal y luego la validamos con un set de 10 pacientes y encontramos que 8 de ellos han sido clasificados correctamente, podemos decir que el modelo tiene una exactitud del 80%. Y si repetimos este procedimiento con los otros tres modelos y encontramos que por ejemplo tienen exactitudes del 90, 70 y 60%, podríamos entonces concluir que el bosque aleatorio es el más adecuado para este problema, porque simplemente tiene un mayor número de aciertos, es decir una mayor exactitud. Hasta acá no hay ningún inconveniente. La exactitud parece ser la métrica más adecuada, aunque tiene una gran limitación. En el ejemplo anterior hemos asumido una condición ideal. Los sets de entrenamiento y prueba estaban balanceados, es decir que tenían prácticamente la misma proporción de normales y anormales. Pero en la práctica no siempre podremos tener sets de datos balanceados. Por ejemplo, en un sistema anti-spam, generalmente habrá más correos normales que spam. O en un sistema de detección de fraudes en transacciones bancarias, generalmente habrá más transacciones normales que fraudulentas. Así que muchas veces los sets de datos estarán desbalanceados, es decir con más datos de una categoría que de otra. Y si entrenamos nuestro modelo con este set de datos tendrá un sesgo, es decir que clasificará mejor la categoría con más datos, pero no lo hará también para la otra categoría. Y si luego validamos este modelo y calculamos su exactitud, no tendremos una medida fiable de su desempeño. Para entender esto volvamos al caso de los cuatro modelos y supongamos que ahora los entrenamos y validamos con sets desbalanceados. En el caso del set de prueba tendremos 100 datos, 90 normales y tan solo 10 anormales. Al clasificar estos datos con por ejemplo el bosque aleatorio, encontramos que 89 de los 90 normales fueron clasificados correctamente, mientras que en el caso de los anormales tan solo hubo un acierto. Si calculamos el desempeño obtendremos una exactitud del 90%, lo que nos llevaría a concluir de manera errónea que en el 90% de los casos nuestro modelo realiza una clasificación correcta. Pero si miramos en detalle este resultado, encontraremos que la exactitud está enmascarando el comportamiento real del modelo, pues la proporción de normales y anormales clasificados correctamente no es la misma. En el primer caso tendremos 89 de 90 aciertos, mientras que en el segundo tan solo 1 de 10. Así que en realidad el modelo lo está haciendo muy bien con los normales, pero tremendamente mal con los anormales. En conclusión podemos decir que la exactitud no es la forma más adecuada de medir el desempeño de un modelo cuando tenemos sets de datos desbalanceados, así que tenemos que buscar otra alternativa, y una manera de hacerlo es precisamente usando la matriz de confusión. Como vimos en el ejemplo anterior no basta simplemente con determinar el número de aciertos, pues debemos ser capaces de diferenciar el desempeño entre una categoría y la otra, y la matriz de confusión nos permite lograr precisamente este objetivo. Esta matriz de confusión es simplemente una tabla que nos permite ver qué tan confundido está nuestro modelo al momento de la clasificación, mostrándonos tanto los aciertos como desaciertos cometidos para cada una de las categorías. Para entender cómo construir esta matriz debemos tener claros cuatro conceptos básicos, los verdaderos y falsos positivos y los verdaderos y falsos negativos. Volvamos al caso del clasificador de enfermedades cardíacas y supongamos que nos interesa usarlo para detectar casos normales. A estos casos les asignaremos la etiqueta positivo y a los anormales la etiqueta negativo. Entonces al momento de la clasificación se podrán presentar estas situaciones. Un dato normal es clasificado correctamente como normal, a esto lo llamaremos verdadero positivo. Un dato normal es clasificado incorrectamente como anormal, a esto lo llamaremos un falso negativo porque realmente no es un caso negativo. Un dato anormal es clasificado correctamente como anormal, a esto lo llamaremos un verdadero negativo. Y un dato anormal es clasificado indecisivo como normal, a esto lo llamaremos un falso positivo porque realmente no es un caso positivo. Es decir que los verdaderos positivos y los verdaderos negativos serán simplemente los aciertos, mientras que los falsos positivos y los falsos negativos serán los desaciertos. Y con estas definiciones ya estamos listos para construir nuestra matriz de confusión. Para construir la matriz de confusión seguimos estos pasos. Primero tomamos el set de prueba y lo clasificamos con el modelo entrenado y realizamos el conteo de los aciertos y desaciertos por cada categoría. Luego organizamos este conteo de aciertos y desaciertos en una tabla donde las columnas representan las categorías a las que realmente pertenece cada dato y las filas representan las categorías predichas por el modelo. Y por último ubicamos los aciertos y desaciertos en la celda correspondiente. Así la primera celda contendrá la cantidad de normales que fueron clasificados como normales, la segunda la cantidad de normales clasificados como anormales, la tercera la cantidad de anormales clasificados como normales y la cuarta la cantidad de anormales clasificados como anormales. Y podemos ver que esta matriz de confusión será de 2x2 porque tenemos precisamente dos categorías. Además en la diagonal principal tendremos los aciertos, verdaderos positivos y verdaderos negativos y en las celdas restantes la cantidad de desaciertos, falsos positivos y falsos negativos. Con la matriz de confusión ya construida podemos ver en detalle el desempeño del modelo para las diferentes categorías y acá resulta evidente que lo hace mucho mejor para los datos normales y que lo hace muy mal con los anormales. Y si obtenemos la matriz de confusión para los modelos restantes podremos concluir que la red neuronal es la que tiene el mejor desempeño, pues el número de aciertos para cada categoría es superior a todos los demás modelos. Y podemos extender esta idea de la matriz de confusión para la clasificación binaria para casos en los cuales tengamos más de dos categorías. Volvamos a nuestro ejemplo y supongamos que en lugar de dos tendremos ahora cinco categorías, una para los sujetos normales y cuatro para los anormales. En este caso ya no podremos hablar de positivos y negativos pues hay más de dos categorías, así que simplemente hablaremos de aciertos y desaciertos. Entonces al construir la matriz obtendremos un arreglo de 5x5, pues tenemos cinco categorías y en la diagonal principal tendremos el conteo de aciertos mientras que por fuera de ella estarán los desaciertos o clasificaciones erróneas. Por ejemplo la celda de la fila 3 columna 1 nos indica la cantidad de sujetos anormales tipo 2 que fueron clasificados incorrectamente como normales, mientras que la fila 4 columna 4 nos indica cuántos sujetos anormales tipo 3 fueron clasificados correctamente en esta categoría. Muy bien ya sabemos cómo construir una matriz de confusión para un modelo de clasificación binaria o multiclase y cómo usar esta información para determinar el modelo con el mejor desempeño para el problema que estemos resolviendo, pero en ocasiones no resulta fácil determinar la ventaja de uno u otro modelo tan sólo con la información proporcionada por esta matriz. Por ejemplo volvamos al caso de la clasificación binaria y supongamos que obtenemos las matrices de confusión para tres modelos diferentes. Al comparar las matrices es evidente que el modelo 2 tiene el peor desempeño, pero no resulta fácil determinar cuál de los dos modelos restantes es el mejor, si el 1 o el 3. El modelo 1 tiene una mayor tasa de verdaderos positivos mientras que el 3 tiene una mayor tasa de verdaderos negativos. En este caso la decisión de cuál es el mejor modelo dependerá por un lado del uso final que esperemos darle, es decir si lo que nos interesa es la detección de casos normales o anormales, pero además de esto tendremos que recurrir a métricas adicionales que nos permitan escoger el mejor modelo de manera objetiva. De estas métricas que se pueden calcular precisamente a partir de la matriz de confusión hablaremos en detalle en el próximo vídeo. Muy bien, acabamos de ver qué es y cómo usar la matriz de confusión para comparar el desempeño de diferentes modelos de Machine Learning y determinar cuál es el más adecuado para la aplicación que estemos desarrollando. Sin embargo vimos que es difícil diferenciar un modelo de otro cuando se tienen tasas de aciertos y desaciertos similares, así que tendremos que recurrir a herramientas como el precision o el recall de las cuales hablaremos en detalle en el próximo vídeo. No olviden dejar sus dudas o comentarios acá abajo y tampoco olviden darle un pulgar hacia arriba de me gusta al vídeo y suscribirse al canal porque esto me ayudará a llegar cada vez a más personas. Por ahora esto es todo les envío un saludo y nos vemos en el próximo vídeo.
