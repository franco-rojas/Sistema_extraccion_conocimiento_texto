 Buenas tardes. Me propongo hoy compartir con ustedes una reflexión sobre lo que está ocurriendo con los almacenes de datos, los lagos de datos y una propuesta nueva que es el data mesh. Un poco lo que vamos a hacer es ver bien qué son, qué tienen en común y qué valor tratan de agregar uno sobre otros. Muy bien, se acordaron ustedes que en el principio, cuando éramos jóvenes, las aplicaciones vivían aisladas entre sí, conjuntos de datos que no se hablaban. Quizás los nombres de las calles, de las provincias eran distintos en distintas aplicaciones y si uno quería combinar datos que venían de distinto lado, y era un esfuerzo brutal. Muy bien, la otra cosa que teníamos en este contexto era una importante dificultad para realizar cambios en lo que era la estructura de los datos. Muy bien. Bueno, y entonces, ¿qué pasaba con todo esto? Que era muy difícil que la información resultara disponible para usuarios finales. Requería un procesamiento importante por parte de los equipos de sistemas. Ah, bien. Con lo cual, muchas cosas se demoraban porque entraban en la temida cola de prioridades que siempre crecía. Muy bien. El gran desafío en este momento era normalizar las cosas. La idea era que si yo normalizaba los datos, entonces, sumando a eso interfaces de consulta, iba a poder dejar que los usuarios finales se sirvieran los datos y sacaba estos problemas crecientes, abundantes de la cola de sistemas. Muy bien. Para normalizarla, la idea era, hacemos un almacén de datos en lugar de los datos ya limpios. Pueden ser consultados sin cargar las aplicaciones transaccionales, con lo cual tengo más libertad para dárselas a los usuarios finales. Bien. Pero había un problemita. Necesitábamos que la estructura estuviera bien de una vez y para siempre. Y si algo cambiaba en toda la cañería, porque cambiaba el negocio, porque cambiaban las necesidades, bueno, era muy rígido todo el proceso. Había muchos problemas. Bueno, lo bueno del caso, con lo que sí se cumplió, es con hacer disponible la información mediante interfaces gráficas que permitían hacer consultas sin entender demasiado el base de datos relacionales, SQL y compañía. O sea, algo realmente útil para usuarios finales. Y lo que sí era claro, es que el ETL es un esfuerzo considerable y que cambios llevan mucho tiempo. Ah, bien. Acá el desafío era avanzar con que esto fuera más libre, con que se pudieran aceptar los cambios más rápido. Bueno, muy bien. Nos fuimos entonces al lago de datos. Lago de datos ya lo construimos con una mentalidad de Big Data que pueda crecer todo lo que querramos. Para liberalizar, la idea fue que el formateo de la información se hace en la extracción, no en la carga. Entonces, adentro uno puede guardar cualquier cosa al postergar el momento en el cual le damos estructura a lo que estamos buscando. Entonces, no nos atamos a un formato dado. Podemos tener más alta velocidad de respuesta. Ajá. La gran tentación, entonces, ahora es una gran ingestión de datos. Guardamos por si nos va a hacer falta en el futuro. Dato que pasa, se pierde y no lo guardé, se ve como una desgracia. Y tampoco nos preocupamos demasiado, dado lo barato que está haciendo el espacio de almacenamiento, en ver cómo es el retorno de la inversión de cada dato que nos guardamos por las dudas. O sea, si ustedes quieren, nos convertimos en data hoarders, sí, en este lugar. Muy bien. Pero claro, si tengo que formatear los datos en la extracción, ya eso sí se le escapa a los usuarios finales. Entonces, nos encontramos con un equipo de ingeniero de datos centralizado que tiene su lista de prioridades para cumplir con las necesidades de datos de los usuarios finales. Estamos en un problema. Bien. Entonces, ¿sí? ¿Qué podemos hacer al respecto para justamente no quedar atrapados y torturados por esta circunstancia? Bueno, la solución pasa por productizar. Tenemos que, para empezar a partir este lago de datos, en un montón de dominios de negocio cada uno con su lagunita, si ustedes quieren, que va a tener un equipo mixto de profesionales que se va a encargar de crear productos de datos que van a poder ser consumidos por varias áreas de la organización. A ver, ¿sí? Esta es la visión entonces del data mesh. El data mesh separa ese lago de datos por dominios de negocios, o sea, por grupos de usuarios que están atrás de un proceso punta a punta y sus necesidades de datos y sus necesidades de infesta. Y va a tener eso. Muy bien. Entonces, van a tener propiedad de sus datos y propiedad de sus procesos y van a mantener sus productos. Con lo cual, el equipo está alrededor de ese problema de negocios y, por lo tanto, va a asumir un montón de cosas para ese contexto, que no va a ser el contexto general de la corporación. Con lo cual, ya ahí tenemos una ganancia interpretativa importante. Bien, cuando uno trata de hacer cosas muy en general, lo que le suele pasar es que tiene que poner mucha complejidad para convivir con esa generalidad. Y esa complejidad que agrega hace las cosas más duras de cambiar después. Si algo es sensible al contexto, es más particular, puede ser más simple y, por lo tanto, terminar siendo más flexible y a un costo de propiedad menor. Bien. Sin embargo, alguien podría decir, pero pará, pará, pará, pará. ¿Qué pasa con la consistencia? Porque si vuelvo al modelo en el cual las cosas de distintos dominios van a tener inconsistencias que no les voy a poder juntar, perdí. Bueno, eso no lo tengo que perder. Tengo que mantener maestros de datos generales que sean de aplicación mandatoria para todos los dominios. Ah, bueno, las calles van a tener que llamarse todas iguales. Listo. Además de eso, los equipos van a generar estos productos. Estos productos van a tener que tener algunas características que es interesante señalar. Descubribles. Vamos a necesitar algún catálogo centralizado donde la gente pueda ver qué hay. Tiene que ser direccionable, tiene que estar siempre disponible en un lugar y ese lugar ser alcanzable. Tiene que haber garantías de que tiene un nivel de confiabilidad el dato que está ahí, que cumple con algún estándar que se publica. Hay un compromiso. Muy bien. Que refleja verdaderamente la realidad del negocio que está detrás. Bien. Después tiene que ser algo que se autodescriba. Tiene que ser una interfase que la vemos y la podemos usar. Que no necesitamos irle llevando al usuario de la mano para que avance con eso. Piensen como paradigmas de las cosas que se describen a sí mismas en muchos juegos que hoy están en el celular. La gente no toma un curso para operar el juego, sino que el juego mismo lo va llevando y le va enseñando a jugar. Si no, el juego muere. Muy bien. Interoperable con todas las plataformas que maneje la organización. Si no, te ahuman el horno. Y así como el descubrimiento tenía que venir a partir de un repositorio, un catálogo centralizado, también la seguridad tiene que poderse administrar de manera central. Ellos van a decir quién puede ver qué, quién puede usar qué. Fantástico. Estos equipos por dominio de negocio van a necesitar contener varios roles. Esto está alineado con lo que sugiere de Vox de tener equipos mixtos atrás de un objetivo y no tener sí, los ingenieros de datos por acá, los ingenieros de software por acá. Porque eso genera lentitud en la transmisión de información, pérdida de contexto y multiplicación de las trabas burocráticas. Bien, vamos a necesitar un dueño de producto que va a ser el responsable del crecimiento de ese producto en el tiempo. Vamos a necesitar ingenieros de datos, vamos a necesitar ingenieros de software y en particular vamos a necesitar expertos en confiabilidad que se encarguen de ver que punta a punta este producto entrega lo que se espera de ellos y que identifique dónde están los puntos posibles de falla. Muy bien, muchas gracias por haberse interesado por este tema. Nos seguiremos encontrando en el futuro por este canal y si dejan preguntas sobre lo que les gustaría que les contáramos, maravilloso, lo miramos para ver qué cosas trata. Muchas gracias.
