 El despliegue de un modelo de Machine Learning consiste en llevarlo de la etapa de desarrollo a la etapa de producción para que esté a disposición de un usuario final y este aspecto ha cobrado mucha importancia en los últimos años para el desarrollo de aplicaciones a nivel industrial, así que en este vídeo nos enfocaremos en este despliegue y en particular veremos los aspectos a tener en cuenta cuando queremos llevar nuestro modelo a producción, así como las principales herramientas existentes en la actualidad. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Bits en donde encontrarán cursos de formación en ciencia de datos y machine learning y por una suscripción mensual de tan solo 10 dólares y ahora sí comencemos. Bien, comencemos viendo una definición más formal de lo que es el despliegue y de algunos conceptos asociados. El Machine Learning Engineering o Machine Learning Operations del cual hablamos en un vídeo anterior es un conjunto de prácticas que busca lograr el despliegue y mantenimiento de modelos de Machine Learning de manera confiable y eficiente y el despliegue es una parte central de este proceso que consiste en tomar el modelo entrenado y hacerlo accesible a un usuario final. Por ejemplo, en la etapa de desarrollo lo que hacemos usualmente es crear un primer prototipo del modelo y generalmente esto lo hacemos en un computador local a través de algunos scripts de python o usando notebooks de Jupiter o en la nube usando por ejemplo servicios como Google colap con el modelo ya entrenado y bien sea que se trate de un proyecto personal académico o que estemos trabajando para una empresa lo ideal es llevarlo a un entorno de producción para que sea accesible a un usuario final y esto es precisamente el despliegue. Y la elección del tipo de despliegue dependerá no solo del uso que queramos darle al modelo sino de varios requerimientos de diseño así que hablemos en detalle de estos requerimientos. El primer requerimiento a tener en cuenta es el tipo de predicción que queremos realizar que puede ser en tiempo real o por lotes. En tiempo real las predicciones son generadas y devueltas al usuario en el menor tiempo posible después de recibida la solicitud. Un ejemplo es Google Translate el usuario introduce el texto en el idioma original y casi de inmediato recibirá el texto traducido. Por su parte en las predicciones por lotes se procesa una gran cantidad de datos de entrada y el modelo genera las predicciones pero de forma asíncrona es decir que la predicción no es inmediata. Un ejemplo es el sistema de recomendación de Netflix que toma el historial de contenido visto por el usuario y aproximadamente cada cuatro horas genera nuevas sugerencias de contenido y otros dos requerimientos a tener en cuenta al momento de realizar el despliegue son la latencia y el rendimiento. La latencia es simplemente el tiempo de respuesta requerido desde que se envía la solicitud al modelo en producción hasta que se recibe la predicción y la idea es que esta latencia sea lo más pequeña posible máximo de unos pocos cientos de milisegundos. Por ejemplo si estamos usando un modelo de reconocimiento facial para permitir el ingreso a los trabajadores de una empresa queremos que las predicciones se generen lo más pronto posible y el rendimiento es el número de solicitudes por segundo que puede soportar el sistema donde está alojado el modelo y es un aspecto importante al hacer predicciones por lotes. Volviendo al ejemplo de Netflix que genera por cada usuario predicciones cada cuatro horas lo que buscaríamos sería un rendimiento alto teniendo en cuenta que el servicio tiene varios cientos de millones de usuarios. Y por último el tipo de despliegue también dependerá de la complejidad del modelo que queramos llevar a producción. No es lo mismo un modelo del tamaño de GPT-3 que un pequeño modelo como MobileNet. El primero requiere más capacidad de cómputo y de almacenamiento mientras que el segundo podría incluso correr en un dispositivo móvil. Bien teniendo claros estos requerimientos ahora sí podemos enfocarnos en las diferentes alternativas de despliegue que son esencialmente dos grandes grupos en la nube y lo que se conoce como on the edge. En la nube quiere decir que el cómputo requerido para las predicciones se realizarán servidores remotos alojados en la nube por lo cual los datos y las predicciones se transfieren a través de internet. Esta es la alternativa a usar cuando se requieren muchos recursos computacionales es decir cuando usamos modelos complejos o cuando la latencia no es un problema porque podemos aceptar retardos en el envío de los datos y la recepción de las predicciones o porque las predicciones se hacen por lotes. Al desplegar en la nube usualmente podemos acceder al modelo de dos maneras una es almacenando los datos de entrada al modelo en una base de datos y programando el sistema para que cada cierto tiempo tome esos datos y genere las predicciones. Las predicciones resultantes son también almacenadas en una base de datos y periódicamente son entregadas a la aplicación cliente encargada de llevarlas al usuario final. Otra forma muy usada es empaquetando nuestro modelo en una API que le permitirá recibir solicitudes hechas por el usuario final así como entregarle el resultado de la predicción. Esta última forma es usada cuando queremos hacer predicciones con baja latencia o cuando no son por lotes como por ejemplo el servicio de google translate y la otra forma de despliegue es la que se conoce como on the edge es decir que el modelo no va a estar alojado en la nube sino en el mismo dispositivo encargado de recibir las solicitudes que generalmente es un dispositivo con limitadas capacidades de cómputo como por ejemplo un teléfono móvil, una smartwatch o una tablet. En este caso la totalidad del cómputo se realiza en el dispositivo local on the edge así que esta alternativa se usa cuando no se requieren muchos recursos computacionales es decir cuando no se usan modelos muy complejos cuando se requieren bajos niveles de latencia como en los vehículos autónomos donde nos interesan tiempos de respuesta casi que de inmediato o cuando por temas de seguridad no resulta conveniente enviar información a través de internet como por ejemplo en los sistemas de reconocimiento biométrico. En este tipo de despliegue la predicción sólo puede ser en tiempo real ya que en dispositivos móviles tenemos recursos limitados que dificultan el procesamiento por lotes. Muy bien teniendo claros los tipos de despliegue que existen vamos a ver ahora un poco más en detalle las herramientas que se usan en cada caso aunque vale la pena aclarar que no es un listado exhaustivo sino más bien las que considero son las más usadas en la actualidad y además veremos estas herramientas organizadas por nivel de dificultad al momento de la implementación yendo de las más sencillas hasta las más complejas. En primer lugar están las herramientas de despliegue local que rigurosamente no permiten llevar el modelo a producción pero sí permiten desplegarlo en nuestros propios computadores es decir localmente sin llegar a un ala nube o a un dispositivo edge aunque pueden ser un primer paso para luego llevarlas a la etapa de producción. Dos de las más usadas son Flask y Fast API que son librerías de python que permiten empaquetar el modelo como una API y acceder a este localmente desde nuestro navegador de internet. La ventaja es que el modelo puede haber sido creado con cualquiera de las librerías usadas comúnmente en machine learning. También tenemos tensorflow serving y tors-serve que están optimizadas para tomar modelos entrenados con tensorflow o pytorch respectivamente y que con muy poca línea de código permiten desplegarlos localmente. Ambas permiten también hacer inferencias es decir predicciones en tiempo real o por lotes. Y finalmente si queremos hacer despliegue on the edge podemos usar tensorflow lite o pytorch mobile para desplegar modelos de tensorflow o pytorch. Luego tenemos las que he llamado las herramientas out of the box con las que podemos desplegar modelos como aplicativos web y sin necesidad de muchos recursos computacionales e incluso muchas de ellas son gratuitas. La más usada es tal vez streamlit que tiene un servicio gratuito en la nube para alojar nuestra aplicación y donde en unos pocos pasos podemos pasar de nuestro código en python a alojar el aplicativo en los servidores de streamlit. Este enfoque lo recomiendo para quienes estén interesados en desplegar en la nube proyectos personales o académicos de machine learning y que no necesiten predicción por lotes sobre todo porque la curva de aprendizaje es baja y en pocos minutos podemos tener el modelo disponible. La grandes ventajas es que no funcionan cuando queremos desplegar modelos complejos bien sea para hacer inferencia en tiempo real o por lotes. Esto debido a que por ser gratuitos tendremos nuestro modelo alojado en servidores con restricciones de memoria y de capacidad de cómputo. Y finalmente tenemos los servicios en la nube en los casos en los cuales se requiera mayor capacidad de cómputo o la posibilidad de inferencia tanto en tiempo real como por lotes así como la posibilidad de llevar nuestro modelo a múltiples usuarios finales. En la nube quiere decir que el cómputo de las predicciones se hace totalmente en servidores remotos donde los principales competidores son amazon web services google cloud o microsoft azure. Este enfoque es útil cuando tenemos modelos complejos que requieren muchos recursos computacionales o cuando la latencia no es un problema. Para este despliegue podemos incluso aprovechar las herramientas que habíamos usado para el despliegue local para así alojar estos aplicativos en cualquiera de estos servicios en la nube usando la misma lógica de empaquetamiento y acceso a través de solicitudes en la nube. La gran ventaja es que estos servicios permiten no sólo el despliegue del modelo sino la implementación de todo el ciclo del machine learning operations incluyendo el entrenamiento y el monitoreo. Las desventajas son una curva de aprendizaje muy alta y obviamente el costo de estos servicios. Y bien esto es todo por ahora déjenme abajo sus comentarios si quieren que en un próximo vídeo veamos en detalle cualquiera de estas herramientas que les he mencionado o un tutorial aplicado sobre estos métodos de despliegue y si aún no lo han hecho no olviden suscribirse al canal y darle un pulgar hacia arriba de me gusta a este vídeo y compartirlo con sus amigos y conocidos porque esto me va a ayudar un montón a llegar a más y más gente y seguir creando este tipo de contenidos. Así que les envío un saludo y nos vemos en el próximo vídeo.
