 Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán todas acerca de la inteligencia artificial, el deep learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com. Hoy vamos a hablar del método, el algoritmo de la regresión lineal, vamos a explicar todos los conceptos asociados a este algoritmo que es bien importante para entender más adelante cómo se implementarán diferentes modelos de deep learning. Entonces comencemos. ¿Qué es lo que hace este algoritmo de la regresión lineal? Aquí tenemos un ejemplo de una serie de datos, cada uno de esos datos está representado con un punto acá dentro de esta gráfica y la idea de este algoritmo ustedes observan que los datos tienen un comportamiento o una relación lineal ven por ejemplo que a medida que x la variable que está en el eje horizontal se incrementa y la variable y en el eje vertical crece o se incrementa también en la misma proporción. La idea del algoritmo de la regresión lineal es encontrar una línea recta que me permita matemáticamente representar estos datos aquí vemos ese concepto, hay diferentes líneas rectas que pueden existir pero de estas tres líneas rectas que tenemos en este ejemplo ustedes observan que sólo una de ellas, la de color blanco, es la que realmente se ajusta mejor a esos datos, es la que está digamos en promedio más cercana a la totalidad de los datos la que lo representa de forma más adecuada. La idea del algoritmo de la regresión lineal es que de forma automática el algoritmo sea capaz de encontrar precisamente una línea recta como la línea blanca que estamos viendo que estamos viendo ahí. Para entender cómo funciona el algoritmo recordemos inicialmente cómo es la ecuación o cuál es la ecuación de una línea recta en este caso tenemos una variable en la gráfica sobre el eje y, una variable que depende de una variable llamada x en el eje ubicada en el eje horizontal si yo hago una representación de y en términos de x a través de una línea recta la ecuación que observamos en la que está aquí arriba wx más b eso va a ser y donde w lo que representa es la inclinación que tiene esa línea recta eso se conoce como la pendiente acá en la gráfica les muestro el significado de esa pendiente entonces entre más inclinada es decir entre más hacia arriba esté la línea recta más alto es el valor de la pendiente y dice entre más abajo más entre más cerca hacia el eje horizontal se encuentra esa línea recta pues menos pendiente tendrá y el parámetro b en este caso tiene un valor de 20 y lo que indica es simplemente cuando la variable x es igual a 0 esa línea recta donde cortaría o cruzaría el eje y en este caso es un valor igual a 20 entonces el objetivo del método de la regresión lineal es encontrar estos dos parámetros w y b de forma totalmente automática partiendo únicamente de los datos o de la nueve de puntos que les mostraba yo anteriormente cómo se logra esto entonces estamos diciendo que el objetivo es encontrar una línea recta pero no cualquier línea línea recta es encontrar la línea recta que mejor se ajuste a la totalidad de los datos entonces tenemos que definir de alguna manera una cantidad numérica que mida que también o que tan mal está ese ajuste o esa línea recta que estamos obteniendo en el caso de la regresión lineal aplicaba algoritmos de machine learning y de deep learning la función que más se usa o la métrica que más se usa se conoce como el error cuadrático medio el sm esta función no sólo el error cuadrático medio sino esa métrica que usamos para determinar qué tan bien o qué tan mal está hecha la aproximación se conoce como función de costo o pérdida este es un término bien importante sobre el que más adelante en otros tutoriales seguiremos hablando la función de costo función de pérdida entonces cómo se define ese error cuadrático medio en este caso la ecuación la tenemos aquí del lado derecho pero entendamos un poco el significado de ese término que aparece elevado al cuadrado dentro de la sumatoria veámoslo acá en la gráfica del lado izquierdo ahí es señalado dos puntos un punto de color azul acá en la parte inferior este punto de color azul que es el punto que conocemos el dato que tenemos de entrada lo vamos a llamar y porque y porque tenemos una cierta cantidad de puntos en total esos puntos azules a eso se refiere este término n entonces para cada uno de esos puntos le vamos a dar este nombre y el punto correspondiente a esa línea recta que estamos tratando de determinar lo vamos a llamar y y gorro si entonces lo que lo que mide esta diferencia es que tanto se acerca el punto que estamos obteniendo de forma automática con el algoritmo que tanto se acerca al punto original y ustedes ven que para ciertos puntos esa diferencia es pequeña para otros puntos como el que estamos viendo acá la diferencia relativamente grande entonces lo que hace la el error cuadrático medio es promediar todos esos valores todas esas diferencias y eso da una cantidad numérica que es el error el objetivo es que ese error sea lo más pequeño posible y para que ese error sea pequeño pues precisamente estos esta línea recta tiene que ajustarse en promedio bastante bien a la mayoría de los datos esa es la idea de el error cuadrático medio porque se eleva al cuadrado porque estas diferencias a veces pueden ser positivas a veces pueden ser negativas si si no la elevo al cuadrado esos signos positivos y negativos pues al sumarse me van a dar una cantidad que probablemente es pequeña pero que realmente cuando hago el ajuste el error total es bastante grande entonces para evitar esa cancelación debida a los signos se eleva al cuadrado bien ahora entonces sabiendo que lo que queremos es minimizar este error cuadrático medio que aquí ya lo estoy reescribiendo en términos de los parámetros que nos interesan w y b como calculamos automáticamente esos dos parámetros entonces fíjense ustedes acá que tenemos la función de error cuadrático medio pero esa función depende de estas dos variables de w y b si yo escojo esas dos variables habrá alguna serie de valores para los cuales el error es más grande habrá una serie de valores para los cuales el error es relativamente pequeño que lo que ocurre que si yo hago la gráfica de esta función tiene un comportamiento cuadrático y depende de dos variables que tenemos acá en este w y b entonces cuando hacemos la gráfica eso tiene una forma como de un tazón esto matemáticamente o geométricamente se conoce como un paraboloide y acá en este punto de color rojo yo les he señalado el mínimo de esa función es decir fíjense que aquí esta altura me define el valor del error y este punto está en la altura más baja es decir corresponde al mínimo de ese error entonces el objetivo de la regresión lineal es obtener la ubicación de este punto es decir el valor de b y de w que hacen que este error alcance este valor mínimo y eso cómo se logra con el método del gradiente descendente que lo vimos anteriormente en otro tutorial y esos enlaces los van a encontrar en la descripción del vídeo para para que lo puedan revisar más adelante con el gradiente descendente encontramos ese mínimo y lo que vemos es que entonces cuando se aplica ese método del gradiente descendente se toman los datos originales y se hace iterar el algoritmo una cierta cantidad de veces ustedes ven acá del lado izquierdo que progresivamente es error cuadrático medio a medida que avanzan las iteraciones se va haciendo más pequeño estoy minimizando el error me estoy moviendo hacia el fondo de ese tazón y a su vez ven acá del lado derecho el resultado que se va obteniendo en cada iteración entonces fíjense que la línea recta cada vez se ajusta mejor a los datos a medida que el error va disminuyendo entonces esta es la idea principal del algoritmo de la regresión lineal bien esto ha sido todo gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com hasta luego
