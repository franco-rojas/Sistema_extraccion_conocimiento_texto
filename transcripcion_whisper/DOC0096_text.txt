 En el video anterior en donde hablamos del Machine Learning Engineering vimos todas las etapas involucradas en el desarrollo de un proyecto de Machine Learning y además vimos que la etapa de preparación de los datos es una etapa fundamental y que requiere usualmente entre el 60 y el 70% del tiempo de desarrollo y una de esas tareas fundamentales es el análisis exploratorio de los datos que es precisamente el tema de este video. Y aunque en internet se encuentran muchos recursos y tutoriales sobre ese tema, realmente no existe una guía clara de cómo hacer este análisis exploratorio de los datos. Así que en este video, en lugar de un tutorial, veremos una guía paso a paso de cómo hacer precisamente este análisis exploratorio de los datos en Machine Learning y Ciencia de los Datos. Vamos a ver en qué consiste el análisis exploratorio, cuáles son los tipos de datos y las herramientas estadísticas para describirlos. Hablaremos de las herramientas de visualización, del análisis bivariado y multivariado y de la sumarización. En la descripción del video les voy a dejar el enlace para que puedan descargar esta guía. Pero antes de todo esto, les quiero contar que el próximo 10 de julio haremos el lanzamiento de la Academia Online de Cursos de Machine Learning y Ciencia de Datos. Así que si están interesados, pueden ingresar a codificandovids.com, diligenciar el formulario con sus datos y a vuelta de correo les voy a enviar más detalles. Bien, primero tengamos en cuenta que todo lo que les voy a contar de aquí en adelante aplica únicamente para datos estructurados, es decir, que vienen en formato tabular. Para datos no estructurados o para series de tiempo, el análisis exploratorio es totalmente diferente. Así que si les interesa, me pueden dejar un comentario abajo para preparar más adelante otros videos sobre esos temas. Ok, el principal propósito del análisis exploratorio de datos es tener una idea completa de cómo son nuestros datos antes de decidirnos por una técnica en particular de ciencia de datos o por un modelo de machine learning. Y como en la práctica los datos no son ideales, debemos organizarlos, entender su contenido, entender cuáles son sus variables y cómo están posiblemente relacionadas. Comenzar a ver algunos patrones, determinar qué hacer con los datos faltantes y con los datos atípicos y finalmente extraer conclusiones acerca de todo este análisis. Y todo esto es precisamente el análisis exploratorio de datos, que en últimas es una forma de entender, de analizar los datos, de visualizarlos, de extraer información relevante para posteriormente elegir cuál es la ruta o técnica más adecuada para su posterior procesamiento. Y este es siempre el paso cero en cualquier proyecto de machine learning o ciencia de datos. Siempre tenemos que comenzar por acá. Bien, teniendo esto claro, podemos resumir las fases del análisis exploratorio en siete pasos. El primero es tener clara la pregunta que queremos responder. El segundo es tener una idea general de nuestro data set. Después debemos definir los tipos de datos que tenemos y luego elegir el tipo de estadística descriptiva y la visualización a utilizar. En el sexto paso debemos analizar las posibles interacciones entre las variables del data set y finalmente debemos extraer algunas conclusiones de todo este análisis. Para entender todas estas fases usaremos un data set clásico de Kaggle, el del Titanic, un set de datos que contiene información de los pasajeros como nombres, edades, género y obviamente la categoría a la que pertenece, es decir, si sobrevivió o no al hundimiento. El primer paso, la pregunta que queremos responder en este caso es qué tipo de personas tenían la probabilidad más alta de sobrevivir al hundimiento del Titanic. Y para responder esta pregunta debemos echar primero un vistazo al data set, mirar su tamaño, determinar cuáles son las características o variables, es decir, las columnas de la tabla y dar un primer barrido a los registros u observaciones, es decir, las filas del data set. Y con esto nos haremos una idea general de los datos, viendo que por ejemplo cada pasajero estará caracterizado por variables como el nombre, la edad, el género, etcétera. Bien, después de esto podemos empezar a analizar en detalle el data set, así que el paso tres es definir a qué tipo de variable pertenece en nuestros datos y aquí tenemos dos opciones, las variables numéricas y las variables categóricas. Los numéricos pueden ser discretos cuando toman solo valores enteros, como por ejemplo la edad de cada pasajero o continuos cuando pueden tomar cualquier valor dentro de un intervalo, como por ejemplo la tarifa del tiquete. Los datos categóricos pueden ser nominales, binarios u ordinales. Los nominales se usan para etiquetar el dato, pero no pueden ser ordenados ni medidos, como por ejemplo el género de los pasajeros, que puede ser hombre o mujer. Los datos binarios indican una de dos posibles categorías, como por ejemplo sobreviviente o no sobreviviente. Y finalmente están los datos ordinales, que como su nombre lo indica, corresponden al orden en el que vienen representados los datos, como por ejemplo la categoría del tiquete. Uno, dos o tres. El cuarto paso es iniciar con la descripción estadística, que depende precisamente del tipo de dato que tengamos en cada una de las variables. Y para esto usamos dos grandes tipos de medidas, las de tendencia central y las de variabilidad. Las de tendencia central nos dan una idea general del valor típico que pueden tener nuestros datos, y las principales son la media y la mediana. La media es simplemente el promedio de los datos y por tanto se puede aplicar a datos discretos, como por ejemplo la edad de los pasajeros, o continuos como por ejemplo el valor de los tiquetes. La desventaja de la media es que es muy sensible a valores atípicos. Si por ejemplo la mayor parte de los tiquetes tenían precios bajos, pero sólo unos cuantos tenían unos precios muy altos, al calcular el promedio daría la impresión de que la mayoría de los pasajeros compraron tiquetes un poco más costosos. La mediana resuelve este inconveniente, y es simplemente el valor que divide los datos en dos mitades, y se puede aplicar para datos ordinales o discretos, como la categoría del tiquete o la edad. Para calcularla debemos primero organizar los datos de manera ascendente y luego encontrar el valor tal que la mitad de los datos estarán por debajo de dicho valor y la otra mitad por encima. Pero resulta que no es suficiente con conocer la media o la mediana de la distribución, porque también debemos tener una idea de qué tan agrupados o dispersos se encuentran los datos. Para determinar esto usamos las medidas de variabilidad, donde las principales son la desviación estándar y el rango intercuartiles, y nos indican que tanto se alejan los datos del valor medio o de la mediana respectivamente. La desviación estándar se puede calcular para cualquier tipo de dato numérico, entre más bajo sea su valor tendremos datos más agrupados y viceversa. La desventaja de la desviación estándar es la misma de la media, es muy sensible a los valores atípicos. Una alternativa es usar el rango intercuartiles, que es la diferencia entre el percentil 75 y el percentil 25. Si la mediana es el punto medio de los valores observados, el percentil 75 es el valor por debajo del cual se encuentra el 75% de los valores, mientras que el percentil 25 corresponderá al 25% de dichos valores. Al igual que la mediana esta diferencia intercuartiles también es menos sensible a valores atípicos en comparación con la desviación estándar. Así, en nuestro dataset el percentil 75 es 38 años y el 25 es 20 años, y por tanto el rango intercuartiles será de 18 años, y entre más grande sea este rango, más dispersos estarán los datos. Los percentiles 25, 50, es decir la mediana y 75, dividen la distribución exactamente en cuatro partes llamadas cuartiles. El primer cuartil cubre del 0 al 25% de la distribución, el segundo del 25 al 50%, el tercero del 50 al 75% y el cuarto del 75 al 100%. Tengan en cuenta esta definición porque la usaremos más adelante. La limitación de las medidas centrales y de variabilidad es que son solo un número, entonces nos pueden dar solo una idea muy general de cómo se están comportando nuestros datos, Así que el quinto paso de este análisis exploratorio es precisamente visualizar los datos para poder empezar a encontrar más detalles. Para datos continuos y discretos podemos calcular y dibujar el histograma, que se obtiene tras organizar los datos en diferentes grupos o bins y realizar el conteo del número de datos en cada uno. Con el histograma podemos verificar que la distribución es normal, es decir que tiene forma como de campana, como por ejemplo la edad, o si está sesgada, como una campana pero asimétrica, como por ejemplo la tarifa. La desventaja del histograma es que no permite ver los valores atípicos porque quedan enmascarados cuando los datos se introducen en uno de sus bins, así que la alternativa en este caso, o cuando los datos están sesgados, es usar lo que se conocen como los diagramas de caja o los boxplots, que se pueden usar para datos tanto continuos como discretos. En un boxplot se dibujan los presentiles, las barras superior e inferior corresponden a los presentiles 75 y 25, mientras que la línea en medio de la caja es la mediana. Por fuera de la caja hay dos líneas, conectadas por líneas punteadas que se llaman whiskers o bigotes en español, y cada una de ellas es igual al presentil 75 o 25, más o menos 1.5 veces el rango intercuartil. Si por ejemplo dibujamos el boxplot para la edad y la tarifa y superponemos los datos originales, podemos fácilmente interpretar estas variables. Vemos que el rango de edades es uniforme entre 0 y 68 años, mientras que la mayor parte de los pasajeros tenía etiquetes económicos entre 0 y 30. También podemos ver los outliers que están más allá de las líneas de los extremos. El tratamiento de los outliers lo veremos en detalle en otro video, pero por ahora recordemos que es un elemento fundamental de este análisis exploratorio de los datos. Y bien, pero qué pasa cuando tenemos datos categóricos? Pues en este caso podemos visualizar un gráfico de barras y mostrar, por ejemplo, el conteo de ocurrencias en cada categoría o el porcentaje que estas categorías representan del total de datos. Por ejemplo, el gráfico de barras para la variable supervivencia nos muestra que fueron más los no sobrevivientes que los sobrevivientes. Esto nos da una pista del posible esquema utilizar en la predicción. El set de datos está desbalanceado y probablemente si usamos un clasificador convencional tendremos problemas para entrenarlo. Hasta el momento hemos analizado y visualizado una sola variable, lo que se conoce precisamente como el análisis univariado, pero también podemos empezar a mirar interacciones y posibles relaciones entre dos o más variables, lo que se conoce como el análisis bivariado y multivariado. El análisis bivariado consiste en comparar pares de variables y aquí podemos aprovechar los tipos de gráficas que vimos anteriormente para analizar esas interacciones. Por ejemplo, si queremos comparar dos variables numéricas como la tarifa y la edad del pasajero, podemos usar una gráfica de dispersión. Donde cada punto es representado por un dato y podemos verificar si existe alguna tendencia lineal, es decir, si el aumento de una variable genera un aumento o disminución de la otra o podemos calcular el índice de correlación entre estas dos variables, donde un valor cercano a uno nos indica una relación lineal, uno cercano a menos uno una relación lineal inversa y un valor cercano a cero indica que no hay correlación lineal entre los datos, que es precisamente lo que ocurre en este ejemplo en particular. También podemos comparar una variable numérica como la tarifa con una variable categórica como la variable supervivencia y usar, por ejemplo, un gráfico de barras para determinar si la tarifa está relacionada con la probabilidad de supervivencia o para la misma comparación podemos usar una gráfica de violín que es similar a un boxplot, pero además de mostrar la mediana y los límites de los cuartiles, incluye una gráfica de densidad de la distribución, que es como una gráfica continua del histograma. También podemos comparar dos variables categóricas, como por ejemplo el título del pasajero y la variable supervivencia, usando gráficos de barras apiladas, lo que nos permite ver que en este caso la mayor parte de las pasajeras con categoría señorita sobrevivieron al hundimiento. Por otra parte, en el análisis multivariado comparamos simultáneamente todos los posibles pares de variables para encontrar algún tipo de relación. Para cada comparación calculamos el índice de correlación entre diferentes pares de variables y dibujamos los resultados en una matriz de correlación. En la diagonal principal de esta matriz tendremos valores iguales a uno, porque estamos comparando una variable consigo misma. Pero lo que nos interesa es lo que está por fuera de esta diagonal. Por ejemplo, para el caso de nuestro dataset podemos ver que no existe relación alguna entre la clase del pasajero y la probabilidad de supervivencia y podemos analizar en detalle diferentes pares de variables para ver si hay relaciones más relevantes que otras. Y la última fase de este análisis exploratorio consiste en sumarizar las observaciones, es decir, en extraer las conclusiones más importantes de todo este análisis que hemos venido haciendo. En este caso, lo que les sugiero es simplemente escribirlas como frases muy cortas. Esto nos servirá para identificar, por ejemplo, las variables o características que están correlacionadas o cuáles de ellas de pronto son más relevantes. Y esto es fundamental para las etapas posteriores del proyecto, como por ejemplo el preprocesamiento de datos, la extracción de características o el desarrollo mismo del modelo en el caso de que tengamos un proyecto de machine learning. Bien, con esto ya tenemos las principales fases del análisis exploratorio de los datos. Recuerden que todos estos pasos que les acabo de contarlos podrán descargar en la guía que van a encontrar el enlace que les dejo aquí abajo en la descripción. Pero también recuerden que se nos quedan por fuera dos etapas importantísimas de este análisis exploratorio, de las cuales vamos a hablar en los próximos videos, que son el manejo de datos faltantes, es decir, cuando nuestro dataset está incompleto y el manejo de los Outliers. Bien, por el momento esto es todo. No olviden darle un pulgar hacia arriba de me gusta al video, compartirlo con sus amigos y conocidos y dejar sus comentarios acá abajo. Les envío un saludo y nos vemos en el próximo video.
