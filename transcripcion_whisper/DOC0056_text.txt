 Meta lo ha vuelto a hacer. A ver, la cosa es la siguiente. Quiero grabar un vídeo, pero la voz no me acompaña, no... No tengo voz, no puedo grabar dos horas de metraje para el siguiente vídeo, así que os propongo un experimento. ¿Y si dejamos para este vídeo que tanto mi voz, como mi presencia, como todo yo, se encarga una inteligencia artificial? No te preocupes, tío, que a partir de aquí me encargo yo. Meta lo ha vuelto a hacer. Y es que hace unas semanas ya os hablé de cómo esta compañía nos sorprendía al liberar una tecnología como SAM, capaz de segmentar cualquier objeto de una imagen que se le pusiera por delante. Y ahora, unas semanas después, nos encontramos con nuevos trabajos que siguen haciendo avanzar el mundo de la inteligencia artificial y en concreto en el campo de la visión por computador, donde Meta está haciendo una aportación enorme liberando tecnologías como la que vamos a estar hablando hoy. Tecnología que bueno. Entre otras, muchas cosas nos va a permitir hacer cosas tan locas como sumar un audio con una imagen para obtener otra imagen correspondiente. O también usar prompts acústicos para generar imágenes con stable diffusion. Ya llegaremos a eso, mola bastante, ya verás. Hoy vamos a hablar de ImageBind. De la multimodalidad ya hablamos hace no mucho en este vídeo de aquí, y es la idea de no entrenar a la inteligencia artificial con un único tipo de dato, por ejemplo solo texto o solo imágenes, sino de combinar varios tipos de datos en el entrenamiento para conseguir aplicaciones más interesantes. El ejemplo más famoso de esto son las IAS generativas de imágenes, donde las capacidades multimodales te permiten expresarte con un texto. El prompt para generar una imagen visual. Y también hemos visto recientemente el caso inverso, donde a partir de una imagen dada como input podemos usarla para generar texto, como próximamente veremos en modelos como GPT-4. Pues bueno, Meta ha cogido esta idea de la multimodalidad y la ha llevado más allá. Y para que lo podáis entender bien, primero tenéis que entender un poquito mejor qué es esto de aquí. No que es un pangolin, no, sino cómo funciona internamente esta idea de la multimodalidad. Mirad, imaginad que tenemos una red neuronal entrenada para reconocer imágenes, solo imágenes, es decir, un único tipo de dato, unimodal. En su capacidad de entender el contenido visual de las imágenes, la red podrá codificar cada una de ellas en un vector o embedding. Y estos vectores, lo que representarán, son coordenadas en el espacio construido por la red, donde conceptos visualmente similares se encontrarán en un punto cercano del espacio. Y si, por ejemplo, codificas una imagen con un contenido totalmente diferente, pues su lugar en este espacio vectorial estará lejos de los puntos de los pangolines. Veis? Así podemos saber que la IA ha aprendido bien su tarea, sabe percibir que estas imágenes visualmente representan algo similar y que esta de aquí no. Bien, ha aprendido a diferenciar pangolines de coches. Ahora, y si quisiéramos ser multimodal, y si ahora queremos que la red percibiera como el mismo concepto, tanto las imágenes del pangolín como un texto en el que se menciona el pangolín. Es decir, lo que queremos ahora es que independientemente de lo que le demos a la IA sea una imagen o un texto, ésta sea capaz de procesarla y generar vectores de embeddings cercanos tanto para la imagen como el texto, puesto que al final son el mismo concepto, ¿no? Esta idea es la que en el paper se menciona como un espacio de embedding conjunto, una característica que como veremos luego nos ofrecerá un montón de posibilidades. Pero, ¿cómo conseguimos esto? ¿Cómo conseguimos explicarle a la red neuronal que esta imagen y este texto de alguna forma están relacionados? ¿Qué representan la misma cosa? Pues, esto no es tan complicado. ¿Sabéis la cantidad de imágenes en Internet que cuentan con títulos? Descripciones de lo que representan. Contexto alternativo que nos explican con texto lo que se puede ver en una imagen. Éste es el dataset perfecto de imágenes y texto ideal para nuestra tarea de entrenar a un sistema multimodal. Pero, ¿multimodal es sólo imágenes y texto? Pues no. Y aquí es donde entra el nuevo trabajo de meta, donde atentos han querido entrenar a la inteligencia artificial para conjuntamente ser capaces de codificar texto, audio, mapas de profundidad, imágenes térmicas y datos de unidades de medición inercial que nos da información sobre la velocidad, orientación y fuerzas de un objeto. Todo esto aprendido por un único modelo. En palabras de ellos, un espacio de embeddings para unirlos a todos. Claro, ¿cómo consigues esto? Antes hablábamos de que podíamos unir imágenes y texto porque naturalmente podemos encontrar en Internet pares de datos de cada tipo. Pero, ¿y si quisiéramos unir texto y audio? ¿Tenemos datos de esto? Bueno, sí. Ahora que contamos con otras IAs como Whisper que nos puede transcribir horas y horas de audio en texto, vale. Pero, ¿y mapas de profundidad con audio? ¿Encontrar pares de datos así es raro? ¿O mapas de profundidad con datos de mediciones inerciales? Aún más raro. Pues aquí es donde brillantemente meta ha tenido la siguiente idea, y es conectar todos estos tipos de datos usando las imágenes y vídeos como punto central de conexión. De ahí el nombre de este proyecto, ImageBind. Pensadlo bien. Hemos dicho que en Internet podemos encontrar muchos ejemplos de imágenes y textos conjuntos. ¡Perfecto! Ya tenemos texto. Además de imágenes, también existen numerosos datasets de imágenes reales emparejadas con sus mapas de profundidad. Y también datasets de imágenes reales con sus mapas de temperatura. ¡Perfecto! Además, a través de los millones y millones de horas de vídeos publicados en Internet, también tenemos un dataset enorme de fotogramas, imágenes asociadas con un audio. Ahí de nuevo tenemos otra conexión, y también si necesitamos relacionar datos de velocidad y aceleración con vídeos, por suerte meta cuenta con un dataset enorme de vídeos grabados en primera persona que cuenta también con estas medidas. Con lo que sí, es usando imágenes y vídeo, que meta ha conseguido emparejar varios datos de imágenes y conectar varios tipos de datos diferentes, y más interesante de forma transitiva, ha conseguido conectar tipos de datos que no suelen aparecer emparejados de forma natural, como por ejemplo audio e imágenes térmicas. ¡Mola, eh! Claro, la pregunta es, ¿y todo esto para qué? ¿De qué nos sirve ahora que una red neuronal pueda codificar tanto el sonido de un pangolín, como su texto escrito, como el sonido que hace, o su mapa de calor tomado por una cámara térmica? ¿Para qué todo esto? Pues aquí viene lo divertido. Contar con un espacio latente conjunto tiene unas cuantas cualidades que podemos aprovechar, y una de ellas sería lo que en inglés se conoce como Information Retrieval, y que en español podríamos traducir como búsqueda de información, y lo vais a entender muy bien. Si yo por ejemplo quiero hacer una búsqueda en una base de datos de imágenes similares a esta, con el sistema que hemos desarrollado antes, tengo el mecanismo perfecto. Si codifico mi imagen en un vector de embeddings, obtendré un punto en este espacio, donde por cercanía a otros puntos podré encontrar imágenes que sean parecidas. ¿Lo ves? Es una forma muy inteligente de poder hacer búsquedas de imágenes conceptualmente semejantes. Pero claro, con ImageBind ya no estamos hablando solo de imágenes, sino que en este espacio ahora estarán cerca aquellas imágenes, textos, audios, imágenes térmicas, que representen la misma cosa, dejándonos con un montón de posibilidades muy locas. Meta ha enseñado ejemplos donde consiguen encontrar imágenes a partir de un audio que les representa. Por ejemplo, mirad en este caso, como este audio devuelve imágenes de perros ladrando, pero además atentos al detalle de cómo posiblemente por el eco de la habitación donde se oye, las imágenes de vueltas son de perros en interior. Cierra los ojos, escucha este audio, y dime qué imagen te viene a la mente. Pues este ejercicio de conectar diferentes fuentes de información, en este caso audio con imágenes, es lo que ahora también puede hacer la inteligencia artificial. Y para el caso contrario también hay ejemplos donde para una imagen dada se puede obtener un audio que lo represente. Esto mola porque combinado con Sam podemos coger en una imagen o un vídeo y automáticamente estar seleccionando diferentes objetos para escuchar cómo suena. ¿Cómo sonaría este elemento de aquí? O este elemento de aquí. O este. Esto es algo que podría tener utilidad en videojuegos procedurales o para producción audiovisual donde quieras sonorizar lo que se ve en una imagen. Y pensad que esta estrategia la podemos aplicar usando cualquier tipo de pares de datos. Por ejemplo, una búsqueda con texto nos podría devolver el sonido y la imagen más cercana de lo que hemos escrito. ¿Veis qué interesante es esto? Pues si esto os gusta, esperad porque la cosa se puede volver aún más loca. Y es que cuando dejamos que una red neuronal construya y ordene este espacio matemático, lo que ocurre es que en él se conservan ciertas propiedades matemáticas. Al final, cada uno de estos puntos no deja de ser un vector. Vectores con los que podemos operar matemáticamente. Por ejemplo, tú puedes tener un vector que represente a una imagen en este espacio y también puedes tener un vector que represente a un sonido como este. Y con estos dos vectores tú puedes sumarlos, al final no dejan de ser vectores, para obtener un nuevo vector resultado. ¿Y qué crees que va a pasar aquí? ¿Qué crees que tipo de datos representará este nuevo vector? Pues esto es lo interesante. Y es algo que ya se sabía que ocurría con espacios vectoriales como estos bien construidos. Y es que si tú tomas, por ejemplo, el vector que representa a imágenes de pangolines y lo sumas con el vector que representa a imágenes de coches, el resultado de esta operación es otro vector, que apuntará a otra zona de este espacio donde encontraremos también un vector que podría representar conceptualmente la suma de los elementos anteriores. A lo mejor aquí es donde estarían localizadas las imágenes de coches en forma de pangolines, si es que eso existe. ¿Lo entendéis? Pues volviendo al ejemplo de antes, sí, este álgebra de vectores también se cumple, pudiendo sumar una imagen de una playa con el audio de un perro para obtener como resultado un vector que estará cerca de una imagen de un perro en una playa. ¿Veis qué loco es todo esto? En su web, Meta ha enseñado más resultados de este tipo, donde se puede comprobar que efectivamente la IA está comprendiendo los conceptos con los que está trabajando y los ha ordenado correctamente en su representación interna. Como podrás imaginar, este espacio vectorial que hemos armado no solo sirve para buscar información ya existente en nuestro dataset, como hemos hecho antes, sino que también estos vectores los podemos usar conjuntamente con otros modelos de inteligencia artificial, como las IAs generadoras de imágenes. Hasta ahora, muchas de estas redes se condicionan en vectores que representan información textual dado por un prompt para luego convertirlas en imágenes, pero imaginad ahora, poder condicionar lo que genera la red a un prompt de audio por ejemplo. Y esto es lo que ha querido explorar Meta aquí, el concepto de multimodalidad llevado más allá, y de nuevo este es un modelo que han puesto a disposición de la comunidad, y que han puesto a experimentar con cosas como usar prompts de audio para stable diffusion. ¿Y cuál es el límite de todo esto? Pues no lo sé. Pero lo que comenta Meta en este trabajo es que visionan un futuro cerebro artificial multimodal capaz de procesar toda esta información y nuevas fuentes más como el tacto, el habla, olores o señales de resonancia magnética. O bueno, eso dicen. Espero que este vídeo os haya servido para entender mucho mejor los fundamentos de mucho de lo nuevo que va saliendo en inteligencia artificial y si queréis apoyar todo este contenido, ya sabéis que podéis hacerlo a través de mi Patreon con una aportación mensual. Chicos, chicas, muchas gracias por verme y nos vemos en el
