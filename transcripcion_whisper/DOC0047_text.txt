 Este vídeo está patrocinado por la Universitat Politécnica de València. Imagínate que eres un ingeniero al que se le pide que para un texto dado de un tweet, por ejemplo, pues tienes que reconocer si ese texto tiene un sentimiento positivo o un sentimiento negativo. Esto es un problema de análisis de sentimiento y es un problema que se puede reducir al final a un problema de clasificación. Clasificame para esta secuencia de palabras si el sentimiento identificado es positivo o negativo. ¿Cómo lo resolverías? Pues si tu respuesta es que siendo un texto de Twitter siempre va a ser negativo, jaja, alabo tu humor, pero fuera de mi clase, ¿vale? Y es que te lo pregunto en serio, ¿cómo lo resolverías? Pues mira, si estuviéramos a comienzos de siglo allá por los dos miles, seguramente esto lo resolveríamos a través del análisis de la polaridad de las palabras. Buscaríamos un vocabulario de buenas palabras, otro de malas palabras, y asignando puntuaciones a cada palabra de un texto, pues acabaríamos con una puntuación total del sentimiento de la frase. ¿Lo ves? Es una solución bastante sencilla, pero no es óptima, ya que frases como esta compra es muy recomendable, si quieres tirar el dinero, podría apuntar una frase positiva cuando en realidad no lo es. Y rápidamente lo vamos a ver en código. Mira, en este caso, pues yo puedo coger una frase, procesarla con esta librería y con esto podemos extraer rápidamente cuál es su sentimiento. Podemos ver que una frase como me encanta comer en este restaurante, la ejecutamos, pues tiene un sentimiento positivo. Y si esto lo cambio por odio comer en este restaurante y lo ejecutamos, pues esto se analiza con sentimiento negativo. Pero claro, este análisis se queda insuficiente cuando usamos la ironía, por ejemplo, y para frases como me encantaría ir a este restaurante si quiero tirar el dinero, pues esta librería podemos ver que lo va a analizar también con sentimiento positivo. Hay forma de hacer este análisis más avanzado, teniendo en cuenta, por ejemplo, el contexto de las palabras. Pero ¿para qué? Si ya estábamos en 2012 y el boom de las redes neuronales acababa de comenzar. Y es que con la llegada del deep learning y las redes neuronales allá por 2012, la cosa se volvió más sofisticada. Ahora podríamos usar redes neuronales especializadas para el análisis de texto, como las redes neuronales recurrentes, para que éstas fueran aprendiendo cuál era la relación entre una secuencia de palabras dada como input y su sentimiento, dado como output. Esto era aprendizaje supervisado. Y claro, lo que nos obligaba era tener un dataset específico para este tipo de problema, es decir, un montón de ejemplos de tweets donde un humano se hubiera sentado a marcar manualmente si el sentimiento de ese texto era positivo o negativo. Ya sabéis, el típico esquema de entrenamiento supervisado, pero cambio de planes. Y si quisiéramos ahora que el sentimiento no se clasifique de positivo o negativo, sino con una review de 1 a 5 estrellas. Ah, vale, bueno, pues esto te obliga de nuevo a sentar a tus humanos y pagarles, en el mejor de los casos, para que vuelvan a hacer este proceso de etiquetado, pero en este caso valorando cada texto de 1 a 5 estrellas. Un dataset que ahora utilizaremos para hacer de nuevo nuestro entrenamiento. Y aquí tenemos algo importante que quiero que entendáis, y es que por aquel entonces, entrenar a una red neuronal para una tarea diferente nos exigía tener un dataset concreto, da igual si esa tarea se parecía o no se parecía a lo que habíamos entrenado previamente. Una nueva tarea era un nuevo dataset y un nuevo entrenamiento. Algo que en 2017 empezaría a cambiar. Y es que en el campo de la visión por ordenador, que ya llevaba unos cuantos años de ventaja al campo del procesamiento del lenguaje natural, ya se conocían estrategias para poder reducir la cantidad de datos necesarios para entrenar a un sistema. Una buena estrategia era la de tomar como punto de partida a un modelo que ya hubiera sido entrenado con anterioridad. Un modelo preentrenado que ya supiera reconocer los patrones que conforman a coches, mascotas, vehículos, paisajes, un montón de imágenes que ahora podríamos aprovechar en nuestro propio beneficio para reentrenarlo. Eso sí, utilizando una menor cantidad de datos que le dieran ese ajuste final al modelo para que este pudiera hacer bien la tarea que nosotros quisiéramos. Y tiene sentido, ¿no? Al final es mucho más fácil aprender a reconocer patrones en una radiografía cuando sabes ver que tener que aprender a ver desde cero. Pues esta misma idea, que sabíamos que funcionaba bien en el ámbito de la visión por ordenador, fue la que se quiso llevar al procesamiento del lenguaje. A partir de 2017, y coincidiendo con la llegada de los transformers, que eran potentísimos modelos capaces de procesar secuencias de texto y comprender el lenguaje mejor que cualquier otra red neuronal anterior, pues empezamos a entrenar modelos de inteligencia artificial para aprender a hacer exactamente eso, a entender el lenguaje. Así, tal cual, toma tarea difícil. ¿Cómo podemos hacer que una inteligencia artificial pueda aprender el lenguaje? Bueno, pues de forma similar a cómo lo hacemos nosotros en el colegio cuando queremos aprender un lenguaje, y es sacando el workbook y practicando. Por ejemplo, para algunos modelos se cogían frases y se enmascaraban algunas palabras, y el objetivo de la inteligencia artificial era aprender a rellenar dichos huecos, encontrar la palabra más probable. U otros modelos, pues estaban especializados en tomar una frase y aprender a cómo finalizarla. Ejercicios sencillos, pero claro que repetidos millones y millones de veces durante días y días de entrenamiento, pues conseguían que estos transformers acabaran por aprender cuál era la estructura de nuestro lenguaje. Estábamos entrenando los primeros modelos del lenguaje basados en transformers. Y el cambio de paradigma aquí es importantísimo, porque si recordáis, antes, para nuestra tarea, pues necesitábamos de muchos humanos esclavizados que fueran etiquetando todo nuestro dataset para poder hacer así el entrenamiento. Pero fijaos que ahora nuestro esfuerzo se reduce a prácticamente cero. Fíjate que para estas actividades que hemos fijado, lo único que necesitamos es coger frases de internet y de forma automática ir borrando palabras o separar entre lo que sería el input y la siguiente palabra predecir. Todo esto gratis. El cambio de paradigma aquí es que ya no necesitábamos dedicar tantísimos recursos para etiquetar manualmente a nuestros datos. Ahora podíamos establecer algún tipo de proceso donde automáticamente estas etiquetas fueran generadas, además sobre cualquier tipo de texto que quisiéramos. Estábamos pasando del aprendizaje supervisado al aprendizaje autosupervisado. Y como conseguir datos etiquetados ya no era un problema, la tendencia a partir de entonces fue la de ir aumentando los dataset de entrenamientos en órdenes de magnitud. Claro, ya no estábamos limitados por el etiquetado manual. Y así vio comienzo la era de los enormes modelos del lenguaje. Modelos entrenados con grandes corpus de textos tomados de todo internet para que de forma general aprendieran a entender cómo nos comunicamos, el significado de las frases, la estructura de nuestro lenguaje, todo. Y donde además se hizo habitual que estas grandes organizaciones que entrenaban a estos enormes modelos, pues gastando computación, infraestructura, talento en investigación, pues acabaran compartiéndolos gratis. Modelos gratuitos que tanto tú como yo podemos descargar y empezar a utilizar. Y aquí toca hacer reconocimiento de la comunidad de Hackingface, que se ha convertido en un portal donde se están recopilando todos estos grandes modelos que las grandes compañías están liberando, y que los han integrado todos en un repositorio donde fácilmente puede buscarlo, probarlos e incluso descargarlos y utilizarlos a través de su librería Transformers. Se puede ver aquí en su documentación cómo tienen un montón de modelos disponibles, de los más famosos de los últimos años tienen Ver, tienen Clip, Diverta, Distilbert, Image, GPT, T5, bueno, un montón de modelos que podéis utilizar automáticamente. ¿Cómo? Pues instalando su librería Transformers vamos a poder configurar rápidamente un pipeline donde se va a configurar todo para tokenizar nuestras secuencias, para descargar los modelos todo automáticamente, en este caso configurado para hacer análisis de sentimiento. Vamos a ejecutarlo y veremos cómo rápidamente la librería va a empezar a descargar por defecto un modelo de tipo Distilbert, y una vez se descarga pues podemos ver que cada una de estas frases directamente pasa a ser analizado. Está súper bien, sentimiento positivo, espero que no sea tan pesado como tener que programar todo desde cero negativo y negativo también la última. Fijaos que aquí estamos utilizando un modelo pre-entrenado, yo no lo estoy entrenando, sino que me estoy aprovechando del entrenamiento que otra organización ha hecho previamente para esta tarea de análisis de sentimientos, y de hecho si quisiera sacarle más rendimiento a este modelo pues podría aprovechar algún dataset etiquetado y darle un reentrenamiento, hacer fine tuning, para poder así adaptar mejor el modelo a la naturaleza de mi datos, algo que notaría más rendimiento y sin necesidad de tener que utilizar tanto datos como si fuéramos a hacer un entrenamiento desde cero. Mola. Y parecería que la historia se acaba aquí, ¿verdad? Ya hemos entrenado nuestro sistema analizador de sentimientos con muy muy buen rendimiento, además hemos reducido la cantidad de datos a utilizar, ¿qué más pedir? Bueno, llegamos a 2020 y aquí se desata la locura. Por aquel entonces los transformers estaban demostrando un rendimiento sobresaliente al analizar texto, y además permitían paralelizar mejor su entrenamiento sobre GPUs y TPUs, haciendo factible entrenarlos cada vez con más datos. Y como acabamos de ver, contar cada vez con más datos pues ya no era un problema, el límite era el cielo. Bueno, Internet. Todo el volumen de texto recogido de nuestras interacciones en webs, redes sociales, enciclopedias, todo era susceptible de ser utilizado como datos para un entrenamiento masivo. Y una empresa quiso intentarlo. Una empresa que ya había estado probando previamente a entrenar a sus modelos generativos pre-entrenados basados en transformers, o dicho más breve, a sus GPTs. Así, OpenAI en 2020, tras una primera y segunda versión, se le va la cabeza y entrena a GPT-3, el modelo del lenguaje más grande entrenado hasta aquel momento, con 175.000 millones de parámetros y 117 veces más grande que GPT-2. Y lo que sucedió después, pues no te sorprenderá, porque lo hemos comentado muchas veces aquí en el canal. GPT-3 se convirtió en un modelo muy bueno a la hora de generar lenguaje humano. Por primera vez contábamos con una inteligencia artificial que podía imitar la forma en la que nosotros nos comunicábamos. Pero además de esto, cosas interesantes sucedieron. Y es que, como hemos dicho antes, a GPT-3 y a este tipo de modelos se les entrenó con una tarea sencilla de encontrar una palabra o autocompletar texto. Nada más. Pero lo que se comprobó es que a estos modelos, cuando los escalabas a tamaños tan, tan grandes, pues en su tarea de aprender a autocompletar texto, también aprendían otras cosas. Tú podías, por ejemplo, escribir una frase como esta y luego añadirle esta frase en inglés sería y darla a autocompletar. Y de repente, GPT-3 te iba a demostrar que sabía traducir de español al inglés. Pero ojo, nunca había sido entrenado para esto. Y lo mismo sucedía si lo hacíamos en chino o en python. Ya muchos sabéis a través de este canal que estos enormes modelos del lenguaje son los que hacen funcionar a modelos como Copilot Codex o Alpha Code. O también podía actuar en forma de chatbot si tú le ponías el formato adecuado a modo de diálogo. Esto era una locura, porque, repito, en ningún momento se le había entrenado explícitamente para resolver ninguna de estas tareas. Su única tarea era autocompletar. Y así en muy poquitos años pues habíamos pasado a entrenar desde cero a modelos para cada una de las tareas que quisiéramos, a acabar con modelos que directamente y por motos propios habían aprendido todas estas tareas automáticamente. Lo único que habíamos tenido que hacer era escalar el tamaño del modelo. Ahora sí, esta era la revolución de los enormes modelos del lenguaje. ¿Y el analizador de sentimientos qué? Pues aquí viene el ejemplo más loco que mejor explica toda esta evolución que ha vivido el campo del deep learning en los últimos años, porque vamos a intentar aprovecharnos de estas dinámicas que parece que ha aprendido GPT-3 para ver si podemos resolver nuestro problema. ¿Cómo lo haría? Pues en este caso lo que haría sería crear algún tipo de dinámica con este enorme modelo del lenguaje para que entienda que lo que quiero hacer es analizar el sentimiento de estas frases. Por ejemplo, le podría poner un caso inicial donde sabríamos que esta primera frase está súper bien esta librería pues tiene un sentimiento positivo y ahora lo que vamos a hacer es dejar que GPT-3 pues haga la tarea que también sabe hacer que es autocompletar texto. En este caso para la frase espero que esto no sea tan pesado como tener que programar todo desde cero. Vamos a ver qué hace. Sentimiento y nos genera automáticamente entiende que tiene que ser un emoji que es un sentimiento pues neutro medio tristón y si por ejemplo ahora le pongo la frase de testo tener que hacer esto, pues va a entender que el sentimiento de esta frase es negativo y esto mola muchísimo porque en ningún momento a GPT-3 le estamos entrenando para que haga análisis de sentimientos simplemente es un enorme modelo del lenguaje que hemos escalado en su tarea de autocompletar texto que acaba pues aprendiendo a hacer un análisis de sentimiento y si le ponemos sentimientos diferentes pues por ejemplo tengo miedo de tener que hacer esto sentimiento ahí está nos sale el miedo o por ejemplo si le cambiamos la dinámica del problema no a tener que hacer ahora clasificación de una a cinco estrellas no pues vamos a probar a ver si ahora lo entiende de texto tener que hacer esto nos marca una estrella efectivamente vamos a probar con una frase positiva esto podría ser una frase de 3 4 estrellas máximo vamos a darle a ejecutar ahí estaría 4 estrellas hacer esto a ver sentimiento pues 12 estrellas y en realidad esto es tan versátil como vosotros queráis podéis aprovechar de estos enormes modelos del lenguaje para hacer clasificación de frases entre harry potter y el señor de los anillos pues tenemos estas frases de aquí y con esto pues gpt 3 debería saber que 10 puntos para greefindo eres de harry potter vamos a ver si lo entiende efectivamente y con esto pues ya podríamos probar con cualquier frase que queramos no pues cuenta con mi hacha del señor de los anillos podemos coger otra vez por aquí la comarca está en peligro también debería ser el señor de los anillos dovi es libre pues debería ser harry potter y así veis como rápidamente hemos creado un clasificador esto lo podéis probar y es un ejemplo de uso de gpt 3 que es uno de los enormes modelos del lenguaje que poco a poco van saliendo si queréis saber más de cómo poder utilizarlo pues ya sabéis que aquí en el canal tenemos un vídeo dedicado a esto y ojo este vídeo no pretende decirte que si quieres crear un analizador de sentimientos lo tengas que hacer de esta forma y es que lo correcto para garantizarnos una buena robustez y calidad de las predicciones pues sería coger a nuestros grandes modelos y reentrenarlos con datos etiquetados como hemos comentado anteriormente no el objetivo de este vídeo es otro si hoy te he contado esta breve historia que creo que ilustra muy bien los cambios tan bestiales que se han vivido en el mundo del deep learning en los últimos años es porque creo que explica muy bien también el futuro que está por venir y comprendiendo esto pues ya estás listo para entender todo lo que viene después qué pasaría por ejemplo si cogemos a estos enormes modelos del lenguaje y seguimos haciéndolos más grandes en el próximo vídeo entre otras muchas cosas os estaré hablando de palm que es un modelo creado por google que cuatriplica en tamaño al modelo gpt 3 un modelo que de nuevo escala se vuelve mucho más grande y que de él emergen nuevas propiedades que nos han vuelto a sorprender a todos de todo esto y de muchas otras cosas súper interesantes de estos enormes modelos del lenguaje seguiremos hablando en el próximo vídeo chicos chicas muchas gracias como siempre y nos vemos con más inteligencia artificial tecnología y ciencia aquí en dot csv
