 En los últimos meses han aparecido sistemas de inteligencia artificial como ChatGPT, GPT-4 o BART que tienen una capacidad impresionante de interpretar y de generar lenguaje natural logrando una comunicación muy similar a como lo hacemos nosotros los seres humanos. Y en el fondo estos sistemas usan lo que se conocen como los grandes modelos de lenguaje o Large Language Models que es el tema precisamente que da comienzo hoy en el canal a una serie de videos en donde poco a poco iremos explorando diferentes aspectos de estos modelos. En esta lista de reproducción iré publicando videos en donde abordaremos no sólo los aspectos técnicos de estos modelos sino también herramientas de software y programación que nos permitirán desarrollar diferentes tipos de soluciones de inteligencia artificial usando esta tecnología y donde también analizaremos las diferentes implicaciones que este tipo de desarrollos pueden tener en nuestra vida cotidiana. Y en este primer video de la playlist comenzaremos con lo esencial que es precisamente entendiendo qué son los Large Language Models. Así que los invito a ver este primer video y a seguir la playlist que iré alimentando poco a poco con videos sobre este tema de los grandes modelos de lenguaje. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online concursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan sólo 10 dólares. Así que listo, comencemos. Comencemos aclarando algunos conceptos básicos que nos permitirán entender posteriormente qué son estos grandes modelos de lenguaje. El concepto más sencillo es el del lenguaje natural que es simplemente el lenguaje escrito o hablado que usamos los seres humanos cuando nos comunicamos. Y de aquí se desprenden los conceptos de procesamiento del lenguaje natural y generación del lenguaje natural que son dos campos de las ciencias computacionales y de la inteligencia artificial. El procesamiento del lenguaje natural busca dotar a los computadores de herramientas que les permitan comprender y procesar el lenguaje humano. Es decir, que a través del texto o la voz podamos comunicarnos con un computador. Pero además de esto es necesario que la comunicación se de en la otra dirección del computador a nosotros los humanos. Que es lo que busca precisamente la generación del lenguaje natural. Que permite dar herramientas a los computadores para que generen texto o incluso voz usando el lenguaje natural. Y la combinación del procesamiento y la generación del lenguaje natural permite desarrollar un amplio rango de aplicaciones como la traducción de voz y texto, el reconocimiento de voz, el análisis de texto y los chatbots entre otros. Y generalmente detrás del procesamiento y la generación del lenguaje natural está lo que se conoce como un modelo de lenguaje. Un modelo de lenguaje permite estimar la probabilidad de ocurrencia de un carácter. Una palabra, una frase o un símbolo dentro del texto. Y esta probabilidad es simplemente un número entre 0 y 1. Que nos indica que tan adecuada o inadecuada puede resultar una secuencia de elementos en el lenguaje natural. Por ejemplo, un modelo de lenguaje podría asignar diferentes probabilidades a diferentes combinaciones de las palabras comió, gato, queso, él y ratón. Por ejemplo, el modelo debería asignar una alta probabilidad a la frase el ratón comió queso, pues se trata de una secuencia de palabras que podríamos encontrar en una conversación entre humanos. Pero a su vez debería darle una baja probabilidad a la frase el queso comió gato, pues se trata de una frase que no tiene mucho sentido para nosotros los humanos. De hecho estos modelos de lenguaje básicos los podemos encontrar por ejemplo en las aplicaciones de chat, en los dispositivos móviles. Pues por ejemplo cuando escribimos uno o varios caracteres o algunas palabras inmediatamente la aplicación nos sugiere los siguientes caracteres o las siguientes palabras para completar esta secuencia de texto. Y para entender cómo funcionan estos grandes modelos es necesario también que entendamos la evolución que han tenido los diferentes modelos de lenguaje a lo largo del tiempo. Los primeros modelos de lenguaje fueron desarrollados hacia los años 90 y la idea era construir modelos capaces de predecir la siguiente palabra en una secuencia de texto, usando diferentes técnicas estadísticas. El problema de estos primeros modelos es que eran entrenados con sets de datos que se conocen como corpus que eran relativamente pequeños y además en esos momentos la capacidad de cómputo también era limitada. Y entonces esto hacía que estos modelos estadísticos no estuviesen en capacidad de capturar las diferentes relaciones que existían entre los elementos de ese texto escrito y por tanto su capacidad predictiva era limitada. Es decir que en la práctica no funcionaban muy bien. Luego a comienzos de los años 2000 se implementaron los primeros modelos de lenguaje basados en redes neuronales donde la idea era entrenar una red neuronal para que aprendiera a predecir la siguiente palabra en una secuencia de texto. Sin embargo este intento tuvo las mismas limitaciones de los modelos estadísticos es decir que al usar muy pocos datos de entrenamiento y tener unas capacidades de cómputo limitadas se obtuvieron modelos que no funcionaban muy bien en la práctica. Pero hacia el año 2010 hubo un cambio muy importante pues en ese momento ya se contaba con equipos de cómputo mucho más potentes y con muchísimos más datos disponibles precisamente en la nube. Así que en ese momento se comenzaron a usar las redes neuronales recurrentes y las redes LSTN que son tipos de redes neuronales especializadas en el procesamiento de secuencias como lo es precisamente el texto. Y con este tipo de redes fue posible entrenar modelos de lenguaje más robustos que mejoraron la capacidad de interpretación y generación del lenguaje natural. Sin embargo este tipo de redes tiene una gran limitación su reducida memoria de largo plazo. Esto quiere decir que funcionaban bien consecuencias de texto relativamente cortas pero cuando el texto era demasiado extenso los modelos no estaban en capacidad de recordar las primeras palabras del mismo y por tanto no eran capaces de procesarlo adecuadamente. Pero en el año 2017 surgieron las redes transformer un tipo de red neuronal que revolucionaría el campo de la generación y procesamiento del lenguaje natural y que es la base de los grandes modelos de lenguaje que conocemos hoy en día. En la descripción del vídeo les voy a dejar un enlace en donde explico en detalle cómo funcionan estas redes transformer pero por ahora lo que nos interesa tener claro es que estas redes a diferencia de las redes recurrentes y LSTN tienen una memoria de largo plazo muchísimo más grande. Así una red transformer puede analizar secuencias de texto mucho más extensas pero no solo eso pues por la forma como procesa los datos también está en capacidad de analizar y codificar numéricamente las relaciones que pueden existir entre los elementos del texto a diferentes niveles. Entonces la red transformer puede analizar relaciones entre las palabras de la secuencia pero también entre las diferentes frases que hacen parte del texto e incluso entre diferentes párrafos. Y esto en últimas permite que la red transformer interprete el texto de forma similar a como lo hacemos los humanos pues para dar significado a una palabra necesitamos tener claro el contexto dentro del cual se encuentra escrita. Las primeras redes transformer fueron entrenadas para realizar tareas muy específicas como por ejemplo la clasificación de sentimientos la traducción de texto o la generación de texto en una temática particular pero en el año 2018 se dio otro gran hito pues varios investigadores de Google desarrollaron BERT un modelo basado precisamente en redes transformer de este modelo también hablo en detalle en un video que encontrarán acá en el canal pero de momento lo que nos interesa es entender claramente cuáles fueron los principales aportes de este desarrollo. El primer aporte fue la idea del pre-entrenamiento que consiste en entrenar un modelo con cientos de millones de parámetros con un corpus gigantesco y para una tarea genérica que consistía en completar palabras faltantes y predecir la siguiente palabra en el texto. Esta fase requiere muchos equipos de cómputo con altísimas capacidades de procesamiento y con este pre-entrenamiento a gran escala fue posible crear un modelo de lenguaje mucho más robusto que los que existían hasta ese momento. El segundo aporte fue el aprendizaje autosupervisado. Al entrenar el modelo para que aprende a completar la palabra faltante o a predecir la siguiente palabra en el texto No es necesario que este set de datos sea etiquetado previamente por un ser humano y al no requerir este tipo de preparación resulta posible recolectar precisamente un set de datos gigantesco. Y el tercer aporte fue la afinación. Una vez pre-entrenado el modelo se puede usar un corpus más pequeño para re-entrenarlo y lograr especializarlo en una tarea determinada como la clasificación de sentimientos, una tarea de pregunta-respuesta o la sumarización por ejemplo. Y en esta fase de afinación se requieren menos equipos de cómputo y menos tiempo de entrenamiento en comparación con la etapa de pre-entrenamiento. Así que BERT fue el primer gran modelo de lenguaje creado después del cual vinieron GPT, versiones 1, 2, 3 y 4, ChatGPT, BART y muchos otros más. Entonces en este punto ya hemos visto implícitamente que es un gran modelo de lenguaje así que vamos un poco más en detalle esta definición. Entonces podemos definir un gran modelo de lenguaje como un modelo de lenguaje basado en redes transformer que contiene cientos o miles de millones de parámetros que ha sido entrenado con un corpus gigantesco y que es de propósito general pero que puede ser afinado para tareas específicas de procesamiento y generación de lenguaje natural. Y acá vale la pena desglosar varios elementos de esta definición. Al hablar de cientos o miles de millones de parámetros y de un corpus de entrenamiento gigantesco estamos diciendo que se requieren altas capacidades de cómputo y muchas horas, días e incluso semanas de entrenamiento. Esto quiere decir que no podemos entrenar uno de estos modelos en nuestro computador personal o ni siquiera contratando un servicio en la nube, pues el costo sería demasiado alto. El término propósito general hace referencia a que el modelo al contener muchos hiperparámetros y haber sido entrenado con un set de datos inmenso es un modelo de lenguaje lo suficientemente robusto que logra modelar adecuadamente los patrones y estructuras del lenguaje humano. Y todo esto le permite al modelo aprender relaciones estadísticas entre palabras, frases y textos enteros para procesar y generar texto de manera coherente y muy similar a como lo haría un ser humano. Y el término afinado hace referencia a que podemos tomar este modelo gigantesco pre-entrenado y entrenarlo con un corpus más pequeño para especializarlo en una tarea específica de procesamiento o generación del lenguaje natural. Muy bien, entonces para finalizar este video hablaremos de algunas habilidades que son únicas de estos grandes modelos de lenguaje. Y la primera de ellas es el aprendizaje en contexto que quiere decir que el modelo es capaz de generar un texto coherente a partir de tan solo una instrucción o de unos cuantos ejemplos. Por ejemplo, le podemos indicar al modelo que queremos que traduzca un texto de inglés a español, pero antes de que lo haga le mostramos unos cuantos ejemplos del resultado esperado. Y con esto el modelo ya logrará generar la respuesta esperada. La segunda habilidad que tienen estos modelos es el seguimiento de instrucciones. Es decir que podemos darle al modelo una instrucción simple y el modelo generará la respuesta esperada. Por ejemplo, podemos indicarle que escriba un poema corto y el modelo generará dicho poema. Y la tercera habilidad es el razonamiento paso a paso. Esto quiere decir que el modelo logra resolver tareas relativamente complejas que involucran múltiples fases de razonamiento. Como por ejemplo problemas matemáticos escritos en lenguaje natural. Por ejemplo, en este sencillo ejercicio le podemos indicar al modelo el número de bolas de tenis que Miguel tenía inicialmente y la cantidad de tubos que compró, indicando cuantas bolas hay en cada tubo. Y al final podemos pedirle al modelo que nos indique cuantas bolas de tenis habrá en total. Y en la respuesta entregada por el modelo se observa claramente el razonamiento paso a paso. Pues descompone el enunciado del problema en sus elementos esenciales y luego ejecuta las operaciones requeridas para obtener la respuesta correcta. Muy bien, en este video hemos entendido la evolución y el significado de lo que es un gran modelo de lenguaje. Así como muchas de sus características. En esencia estos modelos están construidos sobre una red transformer gigantesca que ha sido entrenada con un set de datos inmenso y que permite que se tenga entonces un modelo de lenguaje muy muy robusto. Y esto es precisamente lo que ha permitido desarrollar muchas de estas aplicaciones que hemos visto recientemente, que permiten procesar y generar lenguaje de forma muy similar a como lo hacemos nosotros los seres humanos. Recuerden que en esta nueva lista de reproducción encontrarán inicialmente este video, pero que poco a poco irá alimentándola con videos en donde exploraremos aspectos técnicos de estos grandes modelos de lenguaje, así como herramientas de programación y de software que nos permitirán desarrollar diferentes aplicaciones de inteligencia artificial para el procesamiento del lenguaje natural. Así que si tienen alguna sugerencia de alguna temática que quieren que incluyan esta lista de reproducción me pueden dejar la sugerencia acá abajo en los comentarios del video. Y también si les gustó el video no olviden darle un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos. Pues ya saben que esto me ayudará a llegar cada vez a más y más personas con este tipo de contenidos. Y desde luego si aún no lo han hecho los invito a suscribirse al canal y a activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Así que por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
