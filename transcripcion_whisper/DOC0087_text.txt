 Los grandes modelos de lenguaje contienen una serie de parámetros que permiten controlar la calidad y la aleatoriedad del texto generado. Estos valores son la temperatura, el muestreo TopK y el muestreo TopP y son valores que nosotros podemos definir al momento de solicitar al modelo que nos genere un determinado texto. Así que es clave conocer cómo funcionan estos parámetros al momento de construir una aplicación con estos grandes modelos de lenguaje. Entonces en este video vamos a entender precisamente que son la temperatura y los muestreos TopK y TopP y el efecto que los valores que escojamos tiene al momento de la generación del texto por parte de estos grandes modelos de lenguaje. Pero antes de comenzarlos invito a visitar CodificandoVids.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Para entender que son la temperatura y los muestreos TopK y TopP debemos entender el principio de generación de texto de estos grandes modelos de lenguaje de los cuales ya hablé en un video anterior. Estos modelos son entrenados entre otras cosas para generar texto en lenguaje natural y esta generación consiste simplemente en predecir de manera recursiva la siguiente palabra dentro de una secuencia de texto. Por ejemplo, supongamos que introducimos a uno de estos modelos la frase a los gatos les gusta. Y queremos que el modelo prediga la siguiente palabra. Tanto a la entrada como a la salida el modelo utiliza una representación intermedia del texto que se conoce como tokens y de los cuales ya hablé en un video anterior. Pero para no complicarnos con este detalle nos enfocaremos únicamente en el texto de entrada y el texto generado por el modelo, obviando este procesamiento intermedio a través de tokens. Cuando el modelo recibe el texto de entrada lo procesa a través de una red transformer y a la salida genera no solo una palabra sino realmente un conjunto de palabras que podríamos llamar palabras candidatas y cada una de estas palabras tiene asociado una probabilidad de ocurrencia. Por ejemplo, volviendo a la frase de entrada a los gatos les gusta, tras procesarla el modelo genera por ejemplo un total de 18 palabras candidatas. Comer, jugar, dormir, abrazar, ronronear, etcétera, etcétera. Cada una de las cuales tiene una probabilidad asociada. Estas probabilidades son simplemente valores numéricos entre 0 y 1 y si sumamos todas estas probabilidades obtendremos un resultado exactamente igual a 1. Cada uno de estos valores nos indica que tan probable es encontrar cada palabra candidata en el lenguaje natural. Y para calcular estas probabilidades el modelo tiene en cuenta el significado completo de la frase introducida por el usuario. Así, una probabilidad cercana a 1 nos indica que es altamente probable encontrar esa palabra en el lenguaje natural y junto a la frase de entrada. Mientras que una probabilidad muy cercana a 0 indica que es poco común encontrar esa palabra junto al texto de entrada. Y luego la idea es que el modelo debe seleccionar una de estas palabras candidatas la cual será precisamente la palabra predicha. Y en este último paso podríamos pensar que la palabra seleccionada sería simplemente aquella con la probabilidad más alta, que en nuestro caso sería la palabra comer. Sin embargo esto no es del todo cierto y aquí es donde entra en el juego la temperatura y los muestreos top-k y top-p que en últimas van a determinar cuál de todas estas palabras candidatas será finalmente seleccionada por el modelo y predicha como la siguiente palabra dentro de la secuencia. Así que teniendo esto en cuenta entendamos qué son estos parámetros y cómo afectan el proceso de generación de texto. La temperatura es simplemente un número que es mayor que 0 y menor que 1 y que permite escalar la distribución de probabilidades de la cual hablamos hace un momento. Un valor de temperatura cercano a 0 hace que la palabra con la probabilidad más alta en la distribución original tenga tras el escalamiento un valor muy cercano a 1 mientras que las palabras restantes tendrán valores cercanos a 0. Por ejemplo si volvemos a nuestro listado de 18 palabras candidatas originalmente la palabra comer era la más probable con una probabilidad de 0.135 pero ahora usando una temperatura muy cercana a 0 la probabilidad es de 0.9999 casi 1 y todas las demás palabras tienen una probabilidad prácticamente igual a 0. ¿Y qué quiere decir que una de las palabras candidatas tenga una probabilidad de ocurrencia tan alta es decir muy cercana a 1? Pues que al momento de escoger la palabra para completar el texto de entrada lo más probable es que el modelo seleccione precisamente la palabra comer. Todo esto quiere decir que con un valor de temperatura cercano a 0 tendremos un modelo más determinístico es decir menos aleatorio lo cual implica que el modelo no hará un uso tan creativo del lenguaje. Analicemos ahora la otra situación extrema. Supongamos que la temperatura tiene un valor muy cercano a 1. De nuevo al generar y escalar la distribución de probabilidades con este valor de temperatura tendremos las 18 palabras pero ahora observamos que sus probabilidades son muy similares. Así que al momento de determinar la siguiente palabra a predecir el modelo tendrá más opciones para escoger pues buena parte de las palabras candidatas tienen probabilidades muy cercanas. Esto quiere decir que con una temperatura cercana a 1 tendremos un modelo más determinístico y con una temperatura cercana a 1 tendremos un modelo que generará texto de forma un poco más aleatoria es decir será un poco más creativo al momento de utilizar el lenguaje. Y esto se debe a que al momento de predecir la siguiente palabra dentro del texto considerará un conjunto más amplio de palabras candidatas. Entonces en resumen con una temperatura cercana a 0 tendremos un modelo que genera texto de forma más determinística es decir un poco más predecible Mientras que con una temperatura cercana a 1 tendremos un modelo que genera texto de forma más aleatoria es decir más impredecible pero además este parámetro de la temperatura lo podemos mezclar por ejemplo con el muestreo top-k del cual vamos a hablar a continuación. Para entender en qué consiste el muestreo top-k volvamos a la distribución de probabilidades de las palabras candidatas que teníamos originalmente cuando ingresamos la frase A los gatos les gusta. En principio esta distribución contiene un total de 18 palabras así que el modelo tendría en principio la libertad de escoger cualquiera de estas pero resulta que podemos también limitar ese listado de palabras candidatas a solo unas cuantas. Por ejemplo podemos indicarle que en lugar de considerar las 18 palabras tenga en cuenta únicamente las 5 más probables al momento de la generación. ¿Y cómo podemos controlar este comportamiento? Pues precisamente usando el muestreo top-k que lo que hace es esencialmente es tomar las K palabras más probables dentro de la distribución de probabilidad original. Entonces cuando usamos el muestreo top-k lo que hace el modelo es primero generar la distribución de probabilidad para un valor específico de temperatura luego organiza de manera descendente cada palabra de acuerdo a su probabilidad de ocurrencia En el tercer paso selecciona las primeras K palabras de dicha distribución y finalmente selecciona aleatoriamente una de estas K palabras. Por ejemplo si fijamos el parámetro K con un valor de 5 el nuevo conjunto de palabras candidatas sería comer, jugar, dormir, abrazar y ronronear y el modelo hará la predicción de la siguiente palabra escogiendo aleatoriamente una de estas 5 palabras candidatas. Y acá vemos que aunque la selección de la palabra generada es aleatoria realmente estamos limitando esta selección a las 5 palabras más probables así que la generación de texto no es del todo aleatoria. Y con esto podemos ver que dependiendo del valor que escojamos para este parámetro K también podremos controlar la aleatoriedad del texto generado. Por ejemplo si escogemos un valor extremo de K igual a 1 el modelo siempre escogerá la palabra más probable dentro del conjunto de palabras candidatas y por tanto la generación de texto será totalmente determinística. Pero por otra parte si para el ejemplo que estamos analizando fijamos por ejemplo el valor de K en 18 el modelo podrá escoger cualquiera de las 18 palabras candidatas al momento de generar el texto es decir que con un valor de K relativamente alto la generación de texto será más aleatoria. Y una tercera forma de controlar la aleatoriedad del texto generado es usando el muestreo top P. En este caso lo que hacemos es definir un umbral que va a ser un valor entre 0 y 1 y que corresponde a lo que se conoce como una probabilidad acumulada. Y las palabras candidatas cuyas probabilidades sumen al menos esa probabilidad acumulada serán las únicas tenidas en cuenta al momento de generar la siguiente palabra. Entendamos esto con un ejemplo. Volvamos a la distribución original de probabilidades que teníamos para un valor determinado de temperatura. Y supongamos que definimos un umbral P de 0.3 con base en el cual haremos el muestreo top P. Esto quiere decir que para generar la siguiente palabra el modelo debe hacer lo siguiente. Primero debe tomar la distribución original y organizarla de manera descendente de la palabra más probable a la menos probable. Luego crea un nuevo listado de palabras candidatas que inicialmente estará vacío. Para llenar este listado se añaden iterativamente una a una de las palabras sumando en cada iteración el valor correspondiente de la probabilidad. Las iteraciones se detienen cuando se sobrepasa el valor de P definido anteriormente. Así por ejemplo, en la primera iteración se añade la palabra comer al nuevo listado de candidatas y la suma de probabilidades será en este caso 0.135. Como este valor es inferior a P igual a 0.3 continuarán las iteraciones. En la segunda iteración se añade la siguiente palabra del listado original, jugar, cuya probabilidad es de 0.125. Esto quiere decir que ahora la probabilidad acumulada es de 0.135 más 0.135. Es decir 0.26. Como este valor sigue siendo menor que 0.3 continuarán las iteraciones. En la tercera iteración se añade la palabra dormir que tiene una probabilidad de 0.115. Ahora la probabilidad acumulada será de 0.135 más 0.125 más 0.115. Es decir igual a 0.375. Como este valor sobrepasa el límite de P igual a 0.3 en este punto se detienen las iteraciones. Entonces como resultado de este muestreo top P tendremos ahora solo tres palabras candidatas comer, jugar y dormir. Y para finalizar la palabra generada se escoge aleatoriamente de este nuevo listado. Entonces vemos que el muestreo top P es otra manera de controlar la aleatoriedad del texto generado por el modelo. En esencia si el valor de P es pequeño, es decir es bastante cercano a 0, las palabras candidatas consideradas por el modelo al momento de la generación serán muy pocas. Y por tanto el modelo terminará escogiendo casi siempre las mismas palabras. Es decir que tendremos un modelo que genera texto de forma un poco más grande que el de la generación. Y por el contrario si el valor de P es relativamente grande, es decir muy cercano a 1, el modelo tendrá en cuenta prácticamente todas las palabras candidatas al momento de seleccionar la próxima palabra predicha. Y por tanto la generación de texto será mucho más aleatoria. Así que en este punto ya tenemos claro el significado y el impacto que tienen la temperatura y los muestreos top K y top P al momento de generar texto. Entonces en la práctica tenemos que tener en cuenta que el modelo genera texto de forma al momento de generar texto. Entonces en la práctica podemos jugar un poco con todos estos parámetros para controlar esa aleatoriedad al momento de la generación del texto. Como lo vamos a ver a continuación en el siguiente ejemplo. Tanto la temperatura como los valores de K y P los podemos definir bien sea al momento de usar una aplicación como por ejemplo ChatGPT o BART, es decir cuando estamos escribiendo la solicitud a esa aplicación, o también los podemos definir dentro del código mismo o en otros mismos estamos desarrollando la aplicación, usando alguno de estos grandes modelos de lenguaje. Por ejemplo supongamos que usamos BART de Google para generar una corta estrofa de una canción comenzando con la frase la luna oculta y que además de eso utilizamos diferentes valores de temperatura y de los parámetros K y P. En un primer caso le podemos decir por ejemplo al modelo. Escriba una corta estrofa de una canción comenzando con la frase la luna oculta. Use una temperatura de 0.2 con un muestreo TOP-K con un valor de K igual a 2 y un muestreo TOP-P con un valor de P igual a 0.1. Observemos que estamos usando valores relativamente bajos para estos tres parámetros, así que el texto generado será más determinista. Como resultado de esto obtendremos esta estrofa. La luna oculta tras las nubes, su luz brumosa ilumina la noche, un secreto que guarda la luna oculta, un secreto que guarda para nosotros un misterio que nos hace soñar. Vemos que la estrofa si tiene algo de lírica pero que el texto es bastante descriptivo y digamoslo así poco creativo. Y esto lo vemos en las frases la luna oculta tras las nubes o la luz de la luna que ilumina la noche. Ahora incrementemos un poco los valores de la temperatura y de los parámetros K y P para generar un texto un poco más aleatorio, es decir un poco más creativo. Y ahora creamos la siguiente solicitud. Ahora escriba una nueva estrofa corta de una canción comenzando con la frase la luna oculta pero en este caso use una temperatura de 0.9, un muestreo top K con un valor de K igual a 15 y un muestreo top P con un valor de P igual a 0.8. En este caso la estrofa generada por Barth es la siguiente. La luna oculta tras el velo de la noche, su luz tenue baña el mundo de sombras como un sueño que se desvanece como un deseo que nunca se cumple. Observemos que aparecen términos como el velo de la noche o el mundo de sombras o el sueño que se desvanece. Definitivamente se trata de un texto mucho más creativo. Bien, acabamos de entender estos tres sencillos conceptos de temperatura, muestreo top K y muestreo top P que como vimos nos permiten controlar la aleatoriedad del texto generado por estos grandes modelos de lenguaje. En últimas lo que vimos es que entre más pequeños sean estos valores, más determinístico o más predecible será el texto generado por el modelo. Mientras que entre más altos sean esos parámetros, el texto generado será más aleatorio o más impredecible. Y esos parámetros los podemos definir al momento de escribir la solicitud que hagamos sobre la aplicación, por ejemplo, sobre Barth o ChatGPT o al momento de programar el código si es que estamos desarrollando nosotros mismos la aplicación. Además, ya los valores que escojamos para esos parámetros pues dependerán precisamente de la aplicación que estemos desarrollando. Como siempre no olviden dejarme abajo sus dudas y comentarios y si les gustó el video no olviden compartirlo con todos sus amigos y conocidos y darle un pulgar hacia arriba de me gusta pues todo esto me ayudará a seguir llevando este contenido cada vez a más y más personas. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video. Subtítulos realizados por la comunidad de Amara.org
