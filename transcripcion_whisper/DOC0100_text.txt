 Imaginen que trabajan para una empresa en donde les interesa analizar el comportamiento histórico de las ventas y para eso cuentan con un dataset que contiene el monto total facturado cada día para un año en particular. La idea es lograr predecir el nivel de ventas para un mes determinado y para años posteriores. Pero ¿se podría usar el Machine Learning para este caso? ¿Y cómo sabemos si es realmente la alternativa más adecuada? Pues este sencillo ejemplo es un caso típico de lo que ocurre cuando nos enfrentamos a una situación en la cual tenemos que resolver un problema usando una gran cantidad de datos. Y como el Machine Learning es un área que en los últimos años ha tenido unos desarrollos impresionantes, a veces la tendencia es usarlo para resolver absolutamente cualquier problema que involucre el uso de datos. Pero esto no quiere decir que lo podamos usar prácticamente para cualquier situación. De hecho, si lo usamos en el caso donde realmente no es necesario, la empresa para la que trabajamos puede perder tiempo y dinero y podemos llegar a un modelo que no tiene un buen desempeño o que simplemente no resulta útil. Es decir, hay casos en donde el Machine Learning simplemente no es necesario. Pero entonces ¿cómo saber cuándo usarlo y cuándo no? Pues ahora voy a tratar de resolver esta pregunta. Y para hacerlo he organizado una hoja de ruta que nos guiará paso a paso en el proceso de determinar cuándo usar y cuándo no el Machine Learning. En la descripción del video les voy a dejar el enlace de descarga. Y para esto debemos partir de esta definición súper importante. El Machine Learning es un enfoque que permite aprender patrones complejos a partir de datos existentes y usar estos patrones para realizar predicciones sobre datos nunca antes analizados. Traten de recordar los términos aprender patrones complejos y realizar predicciones porque son esenciales para lo que vamos a hablar ahora. La primera condición que debemos verificar está precisamente relacionada con los datos que son el punto de partida de cualquier proyecto de Machine Learning. En este caso debemos verificar que se cumplan al menos dos condiciones. Que sean accesibles y que sean asequibles. Que sean accesibles quiere decir que los podemos recolectar fácilmente. Por ejemplo podemos intentar crear un modelo para predecir el volumen de ventas de una empresa pero es imposible construirlo a menos que tengamos acceso a datos de varios años atrás. Y que los datos sean asequibles significa que en muchos casos no serán de uso libre y probablemente será necesario comprarlos. Por ejemplo, imaginemos que queremos desarrollar un sistema para prevenir fraudes con tarjetas de crédito. Para poderlo construir necesitamos un dataset con datos reales que muy probablemente pertenecerán a una entidad bancaria. Lo más probable es que este dataset no sea de acceso libre y si queremos utilizar esos datos puede resultar necesario adquirirlos. Así que en resumen el paso cero es tener certeza de que contamos con los datos que necesitamos. Bien, si se cumple esta condición debemos analizar ahora otra situación que es menos de tipo técnico y más de tipo humano, el tema ético. Y este es un aspecto súper importante porque independientemente del problema que queramos resolver tenemos que asegurarnos de que a los datos y al modelo que vamos a entrenar se les dé un uso precisamente ético. Imaginemos por ejemplo el caso de los vehículos autónomos. ¿Qué pasa si uno de estos vehículos atropella a una persona? ¿A quién podemos culpar de este accidente? Es un dilema ético de fondo que realmente ha generado una discusión en torno a este tema y ha creado una barrera que ha dificultado el uso masificado de estos vehículos. Y dilemas como este se vienen dando en diferentes ámbitos de nuestra vida diaria. Desde el mismo uso de las redes sociales, pasando por la medicina y llegando incluso a temas militares. Así que no podemos confiar ciegamente ni en los datos, ni en los algoritmos, ni en el Machine Learning. Así como resulta benéfico en muchas situaciones, en otras como en el caso de los vehículos autónomos un error del modelo puede llevar a consecuencias catastróficas. Y en estos casos mi sugerencia es simple. Ante cualquier dilema ético que pueda tener consecuencias negativas sobre cualquier actor de nuestro entorno, es mejor no usar ni el Machine Learning y de hecho ningún otro enfoque. Resuelto el tema ético, el siguiente paso es tener claridad sobre el proceso que está detrás de la generación de los datos. Que puede ser determinístico o estocástico. Un proceso determinístico quiere decir que puedo predecir con una certeza del 100% un evento futuro. Es decir, es un proceso donde no tengo ningún tipo de aleatoriedad. Por ejemplo, para calcular el saldo de mi cuenta de ahorros a final de mes simplemente debo tomar los aportes hechos y sumarle lo que ha ganado por cuenta de la tasa de interés. Por otro lado, en un proceso estocástico no tendré una certeza del 100% y en su lugar tendré una estimación o una probabilidad de que ocurra el evento. Es decir, que en estos procesos tengo un cierto grado de aleatoriedad. En el caso anterior, un ejemplo de un proceso estocástico sería el comportamiento de la tasa de interés. Pues ésta depende de las condiciones del mercado, de dónde invierte el dinero el banco, de la volatilidad de las acciones, etcétera, etcétera. Y por tanto tendrá ese grado de aleatoriedad. Pero un proceso estocástico es diferente de un proceso aleatorio, como lanzar un dado. En este último caso es imposible predecir en cada lanzamiento en qué número caerá. Mientras que la tasa de interés tiene un mayor grado de predictibilidad. Por ejemplo, con un alto grado de confianza, puedo decir que prácticamente nunca tendremos una tasa de interés del 1000%. Así que si el proceso es determinístico no necesitamos recurrir al Machine Learning. Podemos usar el mismo modelo o las ecuaciones que tengamos para resolver el problema. Pero el Machine Learning resulta viable si el proceso es estocástico. Si los datos se generan a través de un proceso estocástico, el siguiente paso es definir qué es lo que queremos hacer con esos datos. Es decir, queremos hacer un análisis de comportamiento histórico o queremos hacer algún tipo de predicción. Volvamos al ejemplo inicial, el del volumen de ventas de la empresa. Si simplemente queremos saber en qué mes o día se tuvieron más ventas, basta con consultar el registro histórico que tenemos. Acá realmente no necesitamos el Machine Learning. Pero si por el contrario lo que queremos es tomar estos datos históricos y predecir lo que podría ocurrir en años posteriores, en este caso el Machine Learning podría funcionar. Y digo podría porque realizar predicciones no es realmente la única condición. Recordemos que podemos predecir el saldo de nuestra cuenta simplemente usando una ecuación. Y acá vale la pena retomar la definición que les mencionaba al comienzo. El Machine Learning es un enfoque que permite aprender patrones complejos a partir de datos existentes y usar estos patrones para realizar predicciones sobre datos nunca antes analizados. Entonces es evidente que una de las tareas del Machine Learning es realizar predicciones. En tareas como la clasificación o la regresión. Pero además debe haber algún tipo de patron en los datos. Y la idea es que con el Machine Learning el modelo pueda aprender a identificar este patron. Por ejemplo, aprender a predecir el número ganador de la lotería resulta imposible porque es un proceso totalmente aleatorio. No hay patrones. Pero aprender a predecir con cierta precisión la variación de la tasa de interés sí sería posible con el Machine Learning. Pero además estos patrones deben ser complejos. Predecir cuándo habrá un eclipse de sol requiere recurrir a la física para predecir las trayectorias de la tierra y del sol. Pero predecir el valor de un inmueble a partir de su área, del número de habitaciones, del año en que fue construido, de si hay o no escuelas o supermercados en el vecindario es posible con el Machine Learning porque allí tenemos un patrón más complejo. Bien estando seguros de que queremos hacer una predicción la siguiente condición a analizar sería la interpretabilidad que es uno de los grandes signos de interrogación del Machine Learning. La interpretabilidad es el grado con que un ser humano puede comprender las razones de una decisión tomada por un modelo de Machine Learning. Por ejemplo, si trabajamos en una entidad bancaria y estamos desarrollando un sistema de detección de fraudes allí resulta fundamental poder explicar los motivos por los cuales una transacción fue clasificada como fraudulenta. De esta forma la entidad puede tomar los correctivos necesarios para evitar este tipo de fraudes a futuro. Desafortunadamente la mayor parte de los modelos de Machine Learning no son fácilmente interpretables exceptuando posiblemente los árboles de decisión y los bosques aleatorios en algunos casos particulares. Estos modelos generalmente funcionan como una caja negra que logra altos niveles de precisión pero donde lo que ocurre en el interior de esa caja tiene todavía un cierto halo de misterio para nosotros los humanos. Aunque vale la pena aclarar que se están haciendo esfuerzos para lograr que estos modelos sean más interpretables. Así que en resumen, si requerimos interpretabilidad realmente la mayoría de las veces el Machine Learning no es la vía recomendada. Pero si la interpretabilidad no es un inconveniente el Machine Learning podría ser una alternativa. Así que poco a poco nos vamos acercando a lo que estamos buscando que es saber cuándo podemos usar el Machine Learning. Para llegar a este punto debemos determinar si la cantidad de datos que hemos recolectado es suficiente porque para que el modelo de Machine Learning aprenda a detectar esos patrones generalmente necesita una gran cantidad de datos. Aunque el número depende del problema particular que estemos resolviendo podemos usar tres reglas básicas para tener una primera aproximación del número de datos que se requeriría. Si es un problema de clasificación podemos definir un factor dependiendo del número de clases. Por ejemplo, podemos definir que por cada categoría se requieren al menos 100, 1000 o 10,000 ejemplos de entrenamiento. También se puede definir un factor dependiendo del número de características de cada dato. Es decir, si cada ejemplo de entrenamiento está representado como un vector de por ejemplo 9 características entonces el número de ejemplos puede ser de al menos 100, 1000 o 10,000 veces ese número. Por último se puede definir un factor dependiendo del número de parámetros del modelo. Por ejemplo, si estoy entrenando una red neuronal con 100,000 parámetros podríamos definir una cantidad de ejemplos de entrenamiento que puede ser 5 o 50 veces ese número. Pero esto siempre será una aproximación y es necesario entrenar el modelo para verificar si se requieren más datos o no. Desafortunadamente es imposible calcular de forma precisa y con antelación un valor exacto para el tamaño del dataset, pues en el Machine Learning este tamaño depende de muchas variables que son propias del problema a resolver, como el tipo de datos o el tipo de modelo a implementar. Para resumir, si encontramos que los datos no son suficientes la única opción que nos queda es recolectar más. Pero si la cantidad es adecuada entonces estamos mucho más cerca de confirmar que efectivamente el Machine Learning es la opción. Así que ya estamos llegando a lo que nos interesa, que es saber en qué situaciones podemos usar definitivamente el Machine Learning. Y para esto tenemos que verificar el tipo de datos que tenemos, si son estructurados o si son no estructurados. Los datos estructurados, como lo mencioné en el video sobre cómo usar SQL en el Machine Learning, vienen usualmente presentados en forma de tabla, es decir dentro de una estructura predefinida, de ahí su nombre. En este caso debemos verificar si se requiere efectivamente la complejidad de un sistema de Machine Learning o si podemos realizar algo más sencillo, por ejemplo un modelo con una serie de reglas para poder realizar la predicción. Un ejemplo sencillo, volvamos al caso de las ventas de la empresa. Para predecir el mes y el volumen de ventas a futuro, podríamos crear una regla como esta. Si el mes es mayo o diciembre, entonces se tendrán los mayores niveles de ventas. Y estos niveles serán iguales al valor del año anterior con un incremento porcentual igual a la inflación más un punto. Así que la sugerencia es, si el patrón es fácil de identificar y podemos fácilmente codificar las reglas necesarias para realizar la predicción, entonces optar por este método. Pero si los patrones no son tan obvios y no podemos codificar de manera explícita estas reglas, la alternativa es finalmente el Machine Learning. Bien, y esto para datos estructurados. Ahora vamos con el segundo grupo que son los datos no estructurados. Y acá tenemos ejemplos como el texto o datos que provienen de procesos perceptuales como el audio, las imágenes o el video. Por ejemplo, para desarrollar un sistema de análisis de sentimientos a partir del texto o un sistema de visión artificial o uno de reconocimiento de voz, los patrones son extremadamente complejos. Los caracteres conforman palabras que pueden estar relacionadas de diferentes formas. La imagen o el video contienen millones de pixeles o frames. Cada uno con diferentes colores y con escenas con múltiples objetos. La entonación y pronunciación depende de la persona que esté hablando y puede haber incluso ruido de fondo. Los patrones son tan complejos que resulta imposible codificar de forma explícita todas las reglas para resolverlo. Incluso podemos tener casos en los cuales combinamos datos no estructurados con datos estructurados, como por ejemplo el sistema de recomendación de YouTube que analiza tanto el contenido de los videos, que son datos no estructurados, como el comportamiento del usuario en la plataforma, que son datos estructurados. Así que en resumen, si los datos son estructurados pero para realizar la predicción no podemos codificar las reglas explícitamente, la solución más viable es el Machine Learning. Y si los datos son no estructurados o están combinados con datos estructurados, definitivamente la alternativa a usar es también el Machine Learning. Bien, espero que esta guía les resulte muy útil cuando tengan que enfrentarse a un proyecto donde tengan que usar datos y necesiten unas pautas para determinar si es viable o no utilizar precisamente el Machine Learning. Creo que esta guía se ajusta a la mayoría de los casos de aplicación en el mundo real, pero puede haber situaciones particulares en las cuales de pronto no funciona. Si es así, déjenme sus comentarios acá abajo. Recuerden que abajo en la descripción les voy a dejar el enlace para que puedan descargar este roadmap. Y eso es todo, si les gustó este video les agradecería un montón que lo compartan con sus amigos y conocidos, que le den un pulgar hacia arriba de me gusta y que dejen abajo sus comentarios. Por ahora les envío un saludo y nos vemos en el próximo video.
