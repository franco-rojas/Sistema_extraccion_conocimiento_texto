 Este vídeo está patrocinado por la Universitat Politécnica de València. Punto de partida. Queremos trabajar con texto, pero una red neuronal solo procesa números. ¿Cómo lo solucionamos? Pues ya lo sabemos del anterior vídeo. Tenemos que vectorizar el texto. Cambiar su representación a, por ejemplo, One Hot Encoding, que no es más que una forma de convertir a cada palabra en un vector que tiene tantas componentes como palabras tengamos en nuestro vocabulario. Y donde todas estarán a cero, excepto aquella que se corresponda con la palabra que queramos representar. Algo que, como ya vimos, geométricamente nos aporta algo muy beneficioso. Y es que cada palabra, cada vector, es independiente al resto de vectores. O dicho de otra manera, que cada palabra ocupa su propia dimensión. ¿Y qué es lo bueno de esto? Pues que al final cada palabra mantiene la misma distancia con el resto de palabras. Algo que, bueno, también tiene algo malo. Y es que cada palabra tiene la misma distancia con el resto de palabras. Y es que sí, asumir que cada palabra es equidistante al resto de palabras de tu vocabulario es un punto de partida correcto. Pero también es cierto que nosotros como humanos no funcionamos de esta manera. Nosotros entendemos que hay palabras que conceptualmente están más cerca, son más similares, que otras palabras que, bueno, pues, estarán más lejos conceptualmente. Planeta, galaxia o universo sabemos que son palabras que conceptualmente están más cerca que, por ejemplo, pangolin. ¡Sigo siendo pangolin! ¿Verdad? Esto nos habla de un tipo de información que sería interesante codificar en nuestros algoritmos de procesamiento del lenguaje natural. Pero, claro, ¿esto numéricamente cómo se consigue? Pues fácil. Compartando. Planteate la siguiente tarea. Imagínate que te muestro las siguientes imágenes de caras, de caras creadas artificialmente, y te pido que hagas la siguiente tarea. Tu tarea va a ser darle una ordenación. Imagínate que digo que los tienes que ordenar en función de dos atributos, dos propiedades que tú consideras relevantes, para resolver el problema que te propuesto. Ordenarlos. Por ejemplo, tú decides edad y color de piel. Pues venga, probemos. Te pones a ordenar las imágenes en un gradiente de pieles más oscuras y pieles más claras, de rostros más jóvenes a más mayores. ¡Y, boom! Terminado. Pues genial. ¡Mira! ¡Felicidades! Porque ahora, además de tener un bonito collage de caras, lo que has conseguido es reducir la dimensionalidad de los datos de tu problema. De partida, cada una de estas imágenes cuenta con miles y miles de píxeles. Intensidades lumínicas que podemos interpretar como variables de entrada, que tú, con tu intelecto, has sabido percibir, procesar, comprimir y ordenar, hasta armar este mosaico donde cada imagen tiene su posición en base a solo dos atributos. Es decir, tus imágenes han pasado de estar representadas por miles y miles de dimensiones a solo dos dimensiones. Has reducido la dimensionalidad de tus datos. Y en este caso son dos dimensiones porque yo te lo he pedido, pero te podría haber pedido ordenarlo en base a tres atributos, por tanto en tres dimensiones, o en una dimensión, consiguiendo así compactar mucho más la información y armando una taller list como si esto fuera de repente Twitch. Pero bueno, tampoco te creas tan especial porque esto es algo que una red neuronal artificial también sabe hacer bastante bien. De hecho, es un modus operandi habitual. Comprimir datos y ordenarlos de manera inteligente para así poder solucionar de la mejor manera la tarea que tenga que resolver. Fíjate, por ejemplo, cuando nosotros en una red neuronal convolucional tenemos una imagen representada por miles y miles de variables y la vamos pasando capa por capa, esta imagen poco a poco se va comprimiendo, por tanto su dimensión se va reduciendo hasta finalmente alcanzar un vector de predicciones cuyo tamaño pues será de tamaño 10 o tamaño 20. También hemos comprimido nuestros datos. Ahora, el punto a entender es que este proceso de compactación y ordenación que ocurre en una red neuronal no es exclusivo solo de trabajar con imágenes, sino que ocurre de manera general. Es decir, si yo tengo un vector de entrada de tamaño 10 y lo paso por una capa que tiene solamente tres neuronas, el resultado de este procesamiento va a ser un vector que va a tener tamaño 3, un vector tridimensional. Y claro, si podemos conseguir esta compactación y esta ordenación tan interesante, ¿qué pasaría si lo aplicáramos a nuestros vectores de texto? ¿No era eso lo que estábamos buscando en el vídeo anterior? Efectivamente, necesitamos que nuestra red aprenda a comprimir el texto. Y es que sí, trabajar con palabras representadas en One Hot Encoding, donde cada vector es del tamaño de tu vocabulario, pues la verdad que es una representación que apetece que tu red neuronal la comprima un poquito. Y ya te digo, así lo vamos a hacer. ¿Cómo? Pues mira, lo acabamos de ver. Asumamos que tu vocabulario tiene 10.000 palabras diferentes, y esto va a significar que tus palabras de entrada también serán vectores de tamaño 10.000. Ahora, si esto lo pasáramos como input a una red neuronal cuya primera capa tiene, por ejemplo, digamos, 300 neuronas, ¿cuál crees que será el tamaño del vector en este punto? Efectivamente, su tamaño ha cambiado, o dicho de otra manera, su dimensionalidad se ha reducido. Y ojo, porque en un comienzo realmente nuestra red neuronal se va a parecer más a esto. Es bastante estúpida, puesto que aún no ha sido entrenada, y por tanto las palabras tampoco van a estar bien representadas. Pero claro, según empiece el entrenamiento, la red poco a poco irá aprendiendo a saber cómo comprimir y ordenar todas estas palabras de la manera más óptima para resolver el problema que la hayamos planteado. Ojo, esto es importante. ¿Qué estamos prediciendo si el texto tiene un sentimiento positivo o negativo? Pues quizás en esta compactación la red aprenda a ordenar estas palabras que representan sentimientos similares. O si estamos analizando reviews de clientes del campo de la electrónica, a lo mejor este proceso de ordenación y compactación dará prioridad a aquellas palabras del campo semántico de la electrónica. ¿Lo entiendes? Es decir, esta nueva representación que aprende la red dependerá del tipo de tarea que quiera resolver, encontrando aquella codificación de lo tato de entrada que mejor le permitan acercarse a la solución óptima. ¿Lo entiendes? Este proceso que ocurre en la primera capa de la red neuronal, en el que necesariamente tiene que aprender a comprimir vectores de One Hot Encoding a una representación más compacta que le sirva para resolver su tarea, es lo que se conoce como embedding, el tema principal del vídeo de hoy. Pregunta. A ver, si tú por ejemplo quisieras enseñarle a alguien a analizar imágenes médicas, ¿qué preferirías? ¿Coger a una persona que sin saber de medicina, pues por ejemplo, sabe percibir el mundo que le rodea, sabe entender los objetos que está viendo, o coger un trozo de carne visualizado aleatoriamente que solo sabe gritar y llorar? Exactamente. Mejor será partir con algún tipo de conocimiento previo a priori que te aproxime a aprender mejor tu tarea. Esto en el contexto del deep learning significa que si tú por ejemplo tienes algún tipo de red que quieras entrenar para observar imágenes médicas, es mejor partir de una red que ya haya sido pre-entrenada para alguna tarea genérica de visión como sea clasificar un gran conjunto de imágenes variadas. Partir de una red pre-entrenada te dará sustancialmente un mejor rendimiento que comenzar con una red completamente nueva, puesto que parte del conocimiento ya aprendido podrá ser transferido a la nueva tarea. Estamos transfiriendo el aprendizaje. Vale, vale, ok. Otra pregunta. Si tú ahora por ejemplo quieres enseñar a una persona a analizar textos jurídicos, ¿qué preferirías? A una persona que sin saber del tema pueda entender al menos cuál es la relación de palabras dentro de ese vocabulario y más o menos tener un entendimiento del lenguaje o al trozo de carne y... creo que se entiende la respuesta, ¿no? Cogeremos aquel que realmente entienda el vocabulario. Y esta misma idea la encontramos en el campo del natural language processing, porque está muy bien dejar que tu red aprenda la capa de embedding a representar tus palabras en vectores para resolver tu problema, pero... ¿y si contaras ya con algún tipo de embedding pre-entrenado de partida? ¿No sería más interesante contar con algún tipo de modelo que pueda convertir tu texto a vectores comprimidos y que pueda ser aplicado de manera universal a múltiples problemas? Pues sí. Y esta es la tendencia que terminó de despegar en 2013 con la publicación de Word2B, uno de los sistemas de embeddings pre-entrenados más utilizados hasta la fecha desde que Google lo publicara. Y no es el único. La cosa ha evolucionado bastante en la última década, convirtiéndose los embeddings en una herramienta fundamental en este campo. Vale, voy a dar respuesta a esa pregunta que a lo mejor todavía ni siquiera te has planteado. Y es, ¿cómo podemos conseguir entrenar a un embedding que sea lo suficientemente universal como para poder aplicarlo a diferentes tipos de tareas? Bueno, los ingredientes son dos. Por una parte, mucho texto, y por el otro, pues una tarea que sea lo suficientemente genérica, como para no condicionar la estructura que el embedding aprenda. Si la tarea que planteemos es lo suficientemente genérica, la estructura que será aprendida pues también lo será, y este embedding podrá ser utilizado en múltiples tareas. Y esto es súper interesante, porque lo que conseguimos finalmente es construir una estructura de nuestro vocabulario donde aquellas palabras que conceptualmente son similares van a estar representadas de manera próxima, en un espacio vectorial que incluso podemos operar matemáticamente. Esto, por ejemplo, en el caso de Word2vec, lo que hace es transformarte vectores de un vocabulario de tamaño 10.000 a un vector de tamaño 300. Y a lo mejor te estarás preguntando qué tipo de estructuras se forman en estos espacios, cuál es la relación entre palabras. La buena noticia es que los podemos ver. Podemos coger, por ejemplo, estos vectores que Word2vec genera, que son de tamaño 300, y aplicar nuevamente un algoritmo de reducción de la dimensionalidad para convertirlo a vectores tridimensionales, que en este caso sí podríamos visualizar. Y el resultado es el siguiente. Lo que estamos viendo ahora es la estructura tridimensional que surge de haber reducido de nuevo la dimensionalidad sobre las palabras que provienen de Word2vec. Los vectores de tamaño 300 son reconvertidos a vectores tridimensionales que, por tanto, podemos visualizar y nos permiten observar un poco la estructura del texto que el embedding ha aprendido durante su entrenamiento. Podemos encontrarnos diferentes estructuras de palabras que estarían relacionadas por proximidad. Palabras como go, return, wake, run, stood, gone. Podemos, por ejemplo, encontrarnos clúster de palabras de números como 0, 5, 7, 4, 2, todos juntos y muy cerquita. Podemos ver también que hay un clúster que indica meses del año. Marzo, octubre, agosto, junio, diciembre. Recordemos que estos clústers han sido aprendidos de manera no supervisada. Simplemente el algoritmo ha analizado secuencias de texto y, en función de su contexto, ha aprendido que estas palabras tienen que ser palabras similares. Podemos también encontrarnos palabras que, por compartir su misma raíz, también están próximas. Por ejemplo, sugerencia, sugiriendo, sugerir, sugerir. Podemos hacer búsquedas en el mapa. Podemos poner que queremos encontrar la palabra cats y nos marque aquí palabras que podrían estar cerca. Y vemos que, por ejemplo, aquí está cat, sí estaría cerca de elephants, the breeds, the cat, the breed, rat, dogs. Todo lo que tiene que ver con el campo semántico de animales. Salvaje, cabra, alimentar. Incluso podemos buscar por palabras que a nosotros nos apetezca, por ejemplo, python. Y en este caso podemos hacer que nos marque cuáles serían sus vecinos cercanos en el espacio 300 dimensional, que sería el espacio original del embedding. Y nos estaría marcando aquí cuáles son estos puntos, que a lo mejor no se encuentran próximos después de hacer la segunda reducción de dimensionalidad. Pero que sí sabemos que el embedding al menos ha entendido que son palabras relacionadas. Y vemos cómo son, por la ambigüedad de la palabra, cosas como mozilla y deviant del campo de la informática. Pero al mismo tiempo también encontramos aventura, joke. Y esto está relacionado efectivamente también con la palabra monty de monty python. Y vemos film, episodes, sketch, credits. Y podemos encontrarnos con todo el campo de palabras relacionadas que el propio embedding, el propio algoritmo de inteligencia artificial, ha aprendido a encontrar como palabras próximas solamente a partir de analizar cuál es el contexto de dicha palabra. Si queréis jugar con esto, pues lo he dejado abajo en la descripción un enlace para que podáis jugar con esta misma herramienta. Final conclusions. Lo que tenemos que saber es que con un embedding finalmente lo que tenemos es una herramienta para poder representar, para poder codificar a nuestras palabras que inicialmente están representadas en One Hotend Coding. Y que por tanto, pues inicialmente son todas independientes unas con otras. Con un embedding podemos dejar que sea una red neuronal la que aprenda una codificación más inteligente para resolver la tarea que queramos. Y que incluso podemos aprovecharnos de preentrenamientos de embeddings ya preentrenados, que nos puedan dar ese rendimiento extra que buscamos en nuestros problemas de Natural Language Processing. Esta herramienta es fundamental y no es solamente exclusiva del campo del texto, sino que podemos utilizar un embedding para representar de mejor manera pues genes, jugadas de ajedrez, variables categóricas, cualquier tipo de variable que inicialmente viniera representada como un vector One Hotend Coding. De hecho, tienes que saber que esta representación que consiguen los embeddings es tan interesante, es tan interesante que incluso te puede construir un espacio matemático donde se pueden implementar diferentes álgebras. Te estás dando cuenta que no hay tanto tiempo, que el vídeo se va a acabar ya, o sea que esto se va a quedar pendiente para otro vídeo. Una álgebra que cuentan los papers que podríamos utilizar para coger la palabra rey, restarle la palabra hombre, sumarle la palabra mujer y que esto no te vuelve el vector reina. Bueno, hay bastantes matices, pero eso te lo contaré en el próximo vídeo. Y hasta aquí el vídeo de hoy. Muchas gracias por aguantar hasta el final. Continuamos con nuestra serie de Natural Language Processing y tendremos un vídeo muy próximamente. Y en este caso pues no voy a pedir likes, sino voy a pedir visualizaciones, ¿vale? Para liberar la siguiente parte de la serie tenemos que llegar a 75.000 visualizaciones. Y si también queréis ir a la primera parte que se quedó bastante mal porque el algoritmo de YouTube lo cogió por la parte mala, pues podéis volver a ver ese vídeo que nunca está mal repasar los contenidos y así pues duplicamos las visualizaciones. Y en cualquier caso, si queréis mostrar un apoyo más fuerte al contenido que estoy haciendo y para que pueda seguir trabajando en este proyecto, en .csv, recordad que podéis apoyar a través de Patreon, ¿vale? Tenéis el enlace en la descripción y ahora que ya estamos a punto de cerrar temporada, pues es un buen momento para valorar si todo lo que hemos visto este año, biobots, leer la mente, coches autónomos, Nerve, si todos estos vídeos te han valido para algo, si has aprendido cosillas, pues si quieres mostrar ese apoyo extra al canal lo podéis hacer a través de Patreon. Además tendréis acceso a un fantástico grupo donde se discuten cosas bastante chulas relacionadas con el Machine Learning. A veces sí, a veces no. Muchas gracias por ver el vídeo y nos vemos con más Inteligencia Artificial aquí en .csv. .
