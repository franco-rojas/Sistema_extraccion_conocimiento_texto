 Hace cosas de un año el panorama de la creación de contenido con inteligencia artificial vivió su gran tsunami con la salida de Stable Diffusion, el primer modelo de generación de imágenes a partir de texto que se liberaba en abierto para que cualquiera, para que tú, para que yo, lo podíamos descargar y utilizar en nuestro PC sin límites y sin control. Fue en agosto de 2022 cuando se inició la revolución de la creación de contenido con inteligencia artificial. Y ahora, cuando casi se cumple un año de la salida, Stability AI, la principal impulsora de todos estos modelos, ha dado un golpe sobre la mesa AU para publicar Stable Diffusion XL. De nuevo, un modelo open source que todos podemos descargar y utilizar en nuestros ordenadores y que ahora sí presenta una mejora sustancial en cuanto a la calidad de imágenes que genera, quedándose muy cerquita de otros modelos privados como Mid Journey, que creo que lo van a tener bastante complicados para no ser arrollados por el tsunami del open source. Así que en este vídeo vamos a responder las grandes preguntas. ¿Es Stable Diffusion XL una mejora tan importante respecto a los modelos anteriores? Y en ese caso, ¿cuáles son las mejoras? ¿Es solo calidad o hay algo más? Y bueno, la pregunta fundamental, ¿cómo podéis vosotros utilizar este modelo de forma gratuita y sin necesidad de un ordenador superpotente? Las respuestas a todas estas preguntas las tendréis en este vídeo para que vosotros también podáis surfear este tsunami que ya está aquí. Y crearás imágenes espectaculares que querrás mostrar al mundo en tu propia página web personal, en tu propio proyecto. ¿Cómo lo haces? Pues a través de Hostinger, que con todo su servicio va a ser muy sencillo crear en minutos, guiado paso a paso, tu propia página web. Ya que Hostinger pone a tu servicio un creador de sitio web superintuitivo donde podrás elegir entre una gran variedad de plantillas de calidad que te gusten. Pues si estás buscando algo profesional y moderno, esta sería perfecta. Y luego podrás ir modificando cada elemento para darle tu toque personal, simplemente clicando y arrastrando elementos. ¿Eh? Como dices, que en el siglo XXI esto es trabajo para una inteligencia artificial. Pues ojito que en su creador de webs también hay funcionalidades de inteligencia artificial para generación de texto, para la generación de los logos de la webs e incluso modelos predictivos que te dibujarán un mapa de atención de qué partes será más interesante para tus usuarios, para darlo en cuenta en el diseño. Y luego también te fostean la página web, que también está muy bien. Si queréis probar todas estas funcionalidades a muy buen precio, os dejo este código de descuento que tendréis abajo también en la cajita de descripción para que lo probéis. Aprovechadlo y ahora vamos a generar unas cuantas imágenes. Pues la primera respuesta es obvia, más calidad de imagen. Sacadito de la caja, podréis comprobar cómo los primeros prompts que probéis con el modelo XL os darán resultados muy superiores a las versiones anteriores de Stable Diffusion. Rápidamente podréis ver cómo los prompts que probabais en la versión 1.4, 1.5 y 2.1 llevados al modelo actual se convierten en imágenes mucho más espectaculares. Pues vamos a probar. Uno zoopanda, vestido de Gandalf. Fotografía de un patito de goma gigante en mitad de la ciudad de Madrid. Pintura al óleo de un robot metido en un tarro de cristal al estilo de Van Gogh. O por ejemplo mi prompt favorito, un pangolin surfeando una ola. Como veis estos son resultados que se acercan bastante a lo que otros modelos comerciales y privados, como Dalidos en sus últimas actualizaciones o Mid Journey, pueden ofrecer. Con la ventaja de que en este caso Stable Diffusion, amigos, es open source. Y en este caso la mejora en calidad de imagen no solo viene por la parte visual, sino también en la resolución, siendo en este caso las imágenes generadas imágenes de 1024x1024, lo cual aporta mucho más detalle y calidad a las imágenes generadas. Respecto a esto también hay mejoras que son más sutiles, pero que aportan muchísimo al resultado final, que se han introducido en la fase de entrenamiento. Pues por ejemplo durante el entrenamiento se ha tenido en cuenta el centrar correctamente el motivo importante de cada imagen utilizada para conseguir así que Stable Diffusion no genere resultados que estén mal centrados, ya que con los modelos anteriores era habitual ver resultados con partes importantes que caían fuera de la imagen y que ahora con el modelo XL pues ya no es un problema que ocurra en la mayoría de casos. Pero las mejoras en calidad no son las únicas mejoras que trae este modelo, y es que Stable Diffusion XL es más inteligente. Al final lo interesante de estos modelos no es que solo generan imágenes visualmente bonitas, sino también que tengan cierta lógica. Si yo le pido un pangolin surfeando lo que quiero es un pangolin subido en una tabla en el mar, y no otra cosa diferente. Y aquí sí se ha podido comprobar que Stable Diffusion XL responde mucho mejor a los prompts que escribimos, entiende mejor qué es lo que se le pide y esto añade mucho más control sobre esta herramienta. Y esto es algo difícil de comprobar hasta que pongamos a jugar con el modelo y veamos qué también responde, pero sí es cierto que ya se ha demostrado que Stable Diffusion XL es lo suficientemente versátil como para ejecutar numerosos estilos diferentes. Y hay gente que ha creado catálogos enteros probando diferentes estilos y técnicas con el modelo XL, y los resultados que se observan son muy prometedores. Además sus mejoras en inteligencia también vienen demostradas por sus mejoras en capacidades como saber entender mejor cómo distribuir los elementos en una imagen según se lo pidamos nosotros, pues quiero que aparezca un cubo rojo sobre un cubo azul. Esto lo puede hacer mejor. Mejoras también a la hora de entender la cardinalidad, pues quiero tres gatitos en una imagen y que no te genere ni dos ni cuatro. O mejoras también a la hora de generar texto que sea legible, que se pueda entender. Algo que no funciona siempre al 100%, todavía hay letras que se repiten, palabras que no salen correctamente, pero ciertamente este es el mejor modelo que lo ejecuta, tanto si lo comparamos con los modelos open source como con los modelos privados. Así que sí, StableDiffusion XL es más y es mejor. Pero ¿por qué XL? Pues es XL porque se trata de un modelo más grande, con tres veces más parámetros que el modelo de StableDiffusion original. De hecho, el modelo ni siquiera es un único modelo, sino que en realidad son dos. Por un lado tenemos el modelo base que se va a encargar de hacer una primera propuesta de generación, que podemos observar como una imagen final. Y luego hay un modelo refinador, un refiner, que a modo de image to image, pues va a tomar el resultado del modelo base y va a depurarlo añadiendo más detalle fino y haciendo que la imagen se vea con más calidad. Esto de aquí sería el antes de aplicar el refiner y esto sería el después. Y claro, teniendo no uno sino dos modelos y además diciendo que son más grandes, pues ya me veo a todos vosotros preguntando, pero Carlos, ¿esto cabe en mi GPU? Porque ya sabemos que es marca de la casa del equipo de Stability AI, pues todos los modelos que van liberando se puedan ejecutar en hardware convencional como el que tú tienes en tu ordenador. Pero este es un requisito que es complicado de mantener cuando al mismo tiempo se está intentando competir en calidad contra modelos como Mid Journey. Y aquí el equipo de Stability ha hecho un trabajo impresionante. Han comentado que este modelo podrá ejecutarlo si cuentas con una GPU que tenga 8 gigas de VRAM, que debería ser suficiente para ejecutar al menos el modelo base y que seguramente de esa cifra luego la comunidad Opensource cuando lo vaya optimizando pues consiga bajar esto a valores de 6 gigas o 4 gigas a costa de que el modelo vaya más lento. Entonces, ¿qué opciones tenéis si queréis probarlo? Pues tenéis varias. Como el modelo Opensource, pues ya muchos servicios lo han integrado en sus herramientas y páginas como ClipDrop o como Playground ya ofrecen, pues si te registras el poder utilizar el modelo un poquito de forma gratuita. Simplemente preocuparos de tener seleccionado el modelo de StableDiffusion XL, configurad lo que necesitéis y escribid vuestro prompt y testead que os devuelve el modelo. Esto está muy bien para sacar unas primeras impresiones de que también rinde StableDiffusion XL. Ahora, si queréis tener un control total, pues la mejor opción es tenerlo ejecutado en vuestro ordenador. Si contáis con el hardware suficiente para poder ejecutarlo, os recomiendo que echéis un vistazo a este tutorial de aquí, donde os va a guiar paso por paso sobre cómo instalarlo en vuestros equipos. Y ahora, para los que no contéis con el hardware suficiente en vuestros ordenadores, os voy a enseñar cómo podéis instalarlo y utilizarlo de forma gratuita a través de Google Collab. Vamos a estar trabajando con este Google Collab Jack. Ya sabéis que Google Collab nos ofrece hardware gratuito, no estamos trabajando con la GPU de vuestro ordenador, estamos trabajando con la GPU que Google nos ofrece de forma gratuita. Y lo primero que tenemos que verificar es que en entorno de ejecución, cambiar tipo de entorno de ejecución, aquí esté marcado Acelerador por Hardware GPU. Con eso ya sabemos que estamos trabajando con la GPU como corresponde. Vamos a clicar aquí, le vamos a decir ejecutar de todos modos. Vamos a esperar a que se ejecute esta celda de aquí. Cuando se haya ejecutado, luego vamos a ejecutar esta otra celda de acá. Este proceso puede tardar 1 o 2 minutos mientras se va instalando todo. Poco a poco se va instalando y ya estamos por la segunda celda. Y vamos a esperar a que aquí en el texto que está apareciendo pues nos aparezca el enlace que estamos buscando. Vemos que todo está funcionando sin errores y vamos a buscar en el texto pues esta cajita de aquí donde hay un enlace que nos va a llevar justo a la interfaz que vamos a estar utilizando. Y esta interfaz es la de StableSwarm UI, que es la interfaz que ha sacado el equipo de Stability junto a StableDiffusion XL, que todavía está en alfa, todavía está en una versión bastante prematura, pero que promete bastante. Porque, sobre todo, te permite, si tienes en tu ordenador, pues varias GPUs, poder coordinar todas ellas para ejecutar StableDiffusion, lo cual está bastante bien. Lo primero que tenemos que hacer aquí es hacer la instalación de esta interfaz. Pues vamos a aceptar la licencia, elegimos el color de interfaz que nos guste, en este caso oscuro. Vamos a elegir que lo vamos a ejecutar solo en este PC, en este caso en la máquina de Google Collab. Vamos a elegir la forma en la que se va a ejecutar StableDiffusion, en este caso con la interfaz de Config UI, bastante popular últimamente. Y aquí importante tenemos que seleccionar los modelos que queremos. Podéis elegir el modelo 1.5 de StableDiffusion, el del año pasado, el 2.1. Bueno, podéis activarlo si queréis hacer comparaciones. Pero en este caso lo que nos importa es el StableDiffusion XL, el base y el refriger. Con esto ya aquí nos dice que esta es nuestra configuración. Si estamos de acuerdo con ello, le damos a Instalar. Y claro, como estamos trabajando en Google Collab, pues cada vez que cerrais Internet, a la suerte vais a tener que repetir este proceso de instalación. ¿Por qué? Porque la máquina que Google os está dando se va a eliminar una vez terminéis de trabajar con ella. Con lo cual, esta instalación de la interfaz de StableDiffusion la tendréis que repetir en cada caso. No sucedería así si estuvierais trabajando en vuestro equipo en local, pero es lo que hay. Al final es hardware gratuito. No nos vamos a quejar. Y una vez instalado, ya tenéis aquí la interfaz. Esta es una interfaz visual similar al Automatic 1111, quien haya trabajado con él. También tenéis por aquí el editor basado en nodos, que está ganando bastante popularidad de Confi. Pero en este caso vamos a centrarnos en esto. Lo que tenemos que ver es que los modelos están bien instalados. Para eso vamos a venir aquí abajo a la pestaña de modelos. Vamos a clicar estos dos puntitos de aquí para que se nos abra el enlace donde está Official StableDiffusion y aquí deberían aparecer los modelos que hayamos instalado. En este caso, el StableDiffusion 1.0 y el StableDiffusion Refine. Como quiero trabajar con el StableDiffusion base para generar imágenes, vamos a clicarlo. Cuando lo clico vemos que aquí está seleccionado y vemos que automáticamente aquí se ha reconfigurado todo para que la resolución ya esté en 1024x1024. Podéis venir aquí si queréis cambiar el aspect ratio por la proporción de alto y ancho de la imagen. Podéis seleccionar diferentes aspect ratios, en este caso lo dejo 1x1 para que salgan imágenes cuadradas. Y ya solamente con esto podéis venir aquí al prompt y podéis escribir lo que sé. Vale, pues por ejemplo, un perro vestido de pirata bajo el agua. Fotografía para que saque aquí una fotografía. Y con esto vamos a darle a Generar Imagen y vamos a ver si todo funciona. Oye, Carlos, que me está tardando muchísimo. Bueno, es que la primera vez que le damos a ejecutar no solo se está generando la imagen, sino que podéis leer aquí que se está cargando el modelo, con lo cual va a tardar un poquito más, pero luego ya veréis que la media de generación va a estar entre 20-30 segundos, lo cual puede parar. Una opción que es gratuita no está nada mal. Podéis utilizar Stable Diffusion, el modelo más avanzado, el XL, generando imágenes cada 30 segundos. Bastante bien. Y lo vemos, lo vemos, lo vemos. ¡Boom! Primera imagen generada con Stable Diffusion XL. Y bastante, bastante bien. Fijaos que la imagen tiene muy buena calidad. Se cumple todo lo que le hemos pedido. Aquí hay un símbolo un poco raro de pirata. Y además, muy interesante también por debajo de la imagen, va a ir integrado en ella todos los metadatos sobre cómo la hemos generado, lo cual va a venir muy bien a la hora de intercambiar imágenes a través de internet. Fijaos además que el resultado está bastante bien siendo solo el modelo base el que ha entrado en juego. Aquí no hemos utilizado el refinador todavía. Esto es el resultado que te genera de Stable Diffusion base. Y ya con ello podéis generar imágenes bastante espectaculares. Aún así, si queréis utilizar el modelo refiner, ¿cómo lo vamos a hacer? Pues vamos a venirnos aquí abajo, vamos a activar el refiner y vamos a abrir este menú para seleccionar el modelo refiner. Vamos a activar esto aquí. Y aquí debería de aparecernos el modelo refiner. Yo me estoy encontrando que esta interfaz, todavía al estar en alfa, tiene bastantes bugs. Uno de ellos es este, que el listado de modelos a veces falla. Entonces el truco aquí es simplemente darle a F5, actualizamos. Y tras actualizar, si no ha fallado nada, fijaos que aquí ya sí que nos aparece el Stable Diffusion refiner. Entonces, si tenemos el modelo base seleccionado y tenemos el modelo refiner en el refiner model, vamos a darle a generar imagen de nuevo y vamos a ver qué sale. Y en este caso, pues obtenemos una nueva imagen de un perro vestido de pirata bajo el agua. Bastante espectacular. Hay algún error por aquí, pero vamos a ver aquí abajo los metadatos que todo está correcto. Y efectivamente podéis ver que esta imagen lo ha generado el modelo base, pero que luego el refiner model ha sido el Stable Diffusion refiner. Es decir, aquí están todos los modelos actuando como debería, todo ejecutado de nuevo en Google Collab. Y también ojo cuidado que hay gente que se equivoca y acaba activando el modelo refiner como modelo base, es decir, generan imágenes directamente con el modelo refiner y eso genera unos resultados. Vamos a decir curiosos. Quiero que lo veáis porque es bastante, bastante curioso. Vamos a darle a generar imagen ahora con el modelo refiner seleccionado y a ver qué pasa. Vale, tenemos la imagen, se ve la cabeza del perro bien. Qué está pasando? Ojo, cuidado que esta foto ya es un poco más rara. Y al final esto es lo que el refiner nos puede ofrecer. El refiner al final es un modelo que está entrenado para generar pues detalle fino, detalle más marcado, más rico en las diferentes partes de una imagen, pero no está pensado para generarte una imagen con una estructura global coherente. De eso ya se ocupa el base model, es decir, el base model hace como un primer boceto estructural de toda la imagen y luego el refiner refina. Entonces, si tú utilizas el refiner como modelo base, lo que va a pasar es que te va a generar una imagen. Sí, con los patrones de lo que le has pedido. Pero donde no hay ninguna estructura global y eso genera pues cosas bastante curiosas, bastante amorfa, donde se repiten muchos patrones de aquello que le has pedido tú como input. Así que ya sabéis que vamos a tener siempre el modelo base bien marcado. Y por aquí os recomiendo ajustar cosillas como el número de steps. Podemos subirlo un poquito más a 30, que eso está visto que es el punto dulce donde vamos a conseguir una buena cantidad de detalles. Podéis manejar el aspect ratio o si queréis tener más control, pues podéis utilizar una imagen de inicialización que te va a permitir pues yo que sé si tienes tu capacidad de dibujo nivel paint, pues puedes venirte con este dibujo, te lo llevas para la aplicación. Le damos aquí image, puedes cargar este archivo, cargamos aquí la imagen y con esto pues ya simplemente sería ajustar estos parámetros donde y image creativity te marca cuánto quieres que influencie tu imagen al resultado final. Si lo acercamos más a un valor de uno, significa que le estamos dando más posibilidad de ser creativo, estable diffusion, es decir, tu imagen no va a influir tanto si lo bajamos más cercano a cero, pues es todo lo contrario. Ola, tu imagen va a tener mucha más fuerza en el resultado final. Aquí recomiendo que si lo que ha generado en el paint es una especie de boceto donde tú estás simplemente ubicando los elementos, pues le pongas un valor alto de creatividad y con esto pues simplemente voy a ajustar aquí el aspect ratio porque el dibujo tiene esta proporción 16,9 y vamos a poner aquí arriba pues algún prompt que sea interesante, por ejemplo. Vale, en este caso puesto, pues frutas explotando, high quality estudio, fotógrafo y también podemos marcar algún negativo prompts que ya sabéis que esto te permite pedirle al modelo que no quieres que aparezca y esto suele servirnos para orientarlo a resultados más visualmente espectaculares. Por ejemplo, en este caso no quiero que sea digital art porque estoy buscando una fotografía, no quiero que sea ilustración, no quiero que sea low quality, no quiero que tenga artefactos JPEG. Es decir, aquí ponéis un listado de aquello que no queréis que aparezca. Esto lo podéis buscar en internet en un montón de de ejemplos de negativo prompts. De hecho, voy a dejar abajo la caja de descripción un ejemplo y vamos a ver qué sale de aquí. Y aquí tendríamos un ejemplo de fruta explotando, utilizando la imagen de inicialización que le hemos especificado. Vamos, por ejemplo, a cambiarlo. Vamos a pedirle que en vez de esto sea un pixel art a ver qué sale de aquí. Y ahí lo tendríamos, no? Pues el pixel art que le hemos pedido con nuestra imagen de inicialización, con nuestros prompts positivos, negativos, todo esto funcionando correctamente. Seguimos. Y qué esperar de un modelo con este potencial liberado en internet? Pues la respuesta es muy sencilla. Solo tenemos que mirar atrás al último año para ver todo el trabajo que ha desarrollado la comunidad open source a un ritmo frenético. Lo que podemos esperar es una comunidad entera trabajando por mejorar esta tecnología, optimizándola, integrándola con otras herramientas, creando mejores interfaces, creando más funcionalidades. Podemos esperar funcionalidades que están llegando ya como outpaintings customizaciones con Lora, modelos como control net que permitirán controlar mucho mejor que se genera con este modelo. El poder crear con mucha más calidad nuestros avatares más realistas con técnicas como DreamBoot o el poder aplicar estilos a partir de una imagen como vimos en este vídeo con style. Todo esto y mucho más va a llegar. Y esto sucede porque si os dais cuenta, si miramos atrás a los últimos meses, muchos de los trabajos más impresionantes que empresas como Google, como Microsoft han ido sacando en trabajo donde mostraban sus resultados, pero no notaban acceso a las herramientas, pues ellos lo lograban porque tenían acceso en sus empresas a modelos, generadores de imágenes de calidad y ahora la comunidad open source también tiene a un modelo de estas características. Esto va a suponer un impacto directo en las funcionalidades que estos modelos van a desplegar en los próximos meses y no solo en la generación de imágenes. Hemos visto trabajo de generación de modelos 3D que se basan en estos modelos de generación de imágenes para poder funcionar. Y ahora con StableDiffusion XL, pues la posibilidad de tener a uno de estos modelos open source se hace cada vez más real, algo similar a lo que ocurrirá con los modelos de generación de vídeo que ahora, al menos en el ámbito privado, están demostrando unos resultados que son verdaderamente impresionantes. Si a todo esto le sumamos la publicación de llama 2 en los modelos del lenguaje open source que estuvimos comentando en el vídeo de la semana pasada, está claro que estamos viviendo un momento que no se ha vivido con anterioridad, una época durada para el open source y la inteligencia artificial. Un enorme potencial donde ahora la comunidad, donde ahora vosotros podéis empezar a explorar e investigar, a jugar con todos estos modelos para sacarle todo su rendimiento y empezar a construir pues todas las herramientas que seguramente serán protagonistas en un futuro no tan lejano. Chicos, chicas, con este vídeo y con el de la semana pasada, creo que hacemos un cierre de arco argumental muy interesante, donde vamos a hacer coincidir también el final de la temporada de vídeos hasta septiembre. En agosto voy a tomarme un mes de descanso donde si hay novedades, pues nos veremos a través de algún directo puntual. Pero creo que es el momento perfecto para cerrar una temporada, un arco argumental que ha estado protagonizado por una revolución de la IA generativa 2022-2023, donde la actualidad nos ha tenido arrollado semanas tras semanas y donde ciertamente en este canal le hemos dedicado mucho hueco a vídeos como este donde explicamos las novedades. Pero sí noto que me ha faltado pues hablar más de fundamentos, hacer cosas más prácticas. Y sé que muchos de vosotros, pues también ese es el contenido que estáis esperando. Entonces para septiembre vamos a abrir una nueva temporada donde sí vamos a seguir hablando de actualidad, donde voy a intentar mantener un ritmo de publicación de vídeos más frecuente y donde también dedicaremos tiempo a entender los fundamentos de todo esto, de cómo funciona toda esta tecnología y donde vamos a estar practicando con ella también, pues de la forma que sé que a vosotros os gusta. Chicos, chicas, muchas gracias por estar aquí, por seguir apoyando el canal, por apoyar en Patreon los que sois Patreons de este contenido y lo hacéis posible. Muchas gracias a todos y nos vemos en septiembre con más Inteligencia Artificial.
