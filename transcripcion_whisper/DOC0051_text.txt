 Hablemos de Sam, pero no es de Sam, sino es de Sam. Segment Anything Model, las tijeras más potentes de la inteligencia artificial. Un nuevo regalo que nos ha facilitado el laboratorio de meta en forma de modelo open source, que todos nosotros podemos empezar a descargar y utilizar en nuestros proyectos. Una ya que busca impulsar una idea muy potente en el mundo de la visión por computador, de forma similar a como los modelos GPT han revolucionado por completo el mundo del procesamiento del lenguaje natural. Y un modelo que para muchos que lo han probado ya, pues está suponiendo un avance frente a los modelos anteriores con los que habían trabajado. ¿Por qué? ¿Por qué es tan importante un modelo que solo segmenta, que solo recorta imágenes? ¿Y qué supondrá este modelo para el futuro de la visión por computador, de la robótica, de la realidad aumentada que tanto promueven desde meta? Hablemos de Sam. Pero antes de avanzar, quiero agradecer al patrocinador de este vídeo, a Hostinger, por apoyar este contenido. Hostinger es uno de los mejores servicios de hosting y dominio disponible actualmente, donde además te van a ofrecer un montón de herramientas que te van a simplificar el proceso de crear tu página web, algunas de ellas integrando incluso inteligencia artificial. Con los servicios que te ofrece Hostinger podrás abrir tu página web a internet sin dificultad en pocos minutos, desde registrar con ellos tu propio dominio hasta configurar paso a paso cómo quieres que se vea tu página web final. En este caso voy a crear un nuevo sitio web, que será un porfolio personal, y aquí tengo la opción de elegir alguna de sus plantillas predefinidas de calidad y listas para ser personalizadas. Esta por ejemplo está muy chula, tiene un estilo que me gusta bastante, pero aún así he visto por aquí que también tienen un creador de webs con IA, así que lo vamos a probar. Y poniendo unos pocos datos y una descripción de a qué me dedico, automáticamente se va a crear una web con todos los textos e imágenes listos y con un diseño que si quiero puedo modificar y personalizar. El editor además te permite fácilmente cambiar imágenes, arrastrar elementos, muy fácil todo, e incluso tiene algunas herramientas más avanzadas como esta que me ha gustado, donde usando IA te va a estimar el mapa de atención de a dónde mirará la mayoría de usuarios cuando vean tu página web. Muy muy chulo. Tenéis todas estas funcionalidades a muy buen precio usando mi código de descuento. Esta es una oferta por tiempo limitado, utilizando mi código .csv vais a tener acceso a un descuento exclusivo. Tenéis los enlaces y el código de descuento abajo en la caja de descripción y de nuevo muchas gracias a Hostinger por patrocinar este vídeo. Sam es el Segment Anything Model, o modelo que segmenta cualquier cosa. Un modelo de segmentación capaz de... segmentar cualquier cosa. Buen naming. La tarea de segmentación en Deep Learning existe desde hace años y es una tarea en visión por computador donde embete tomar una imagen como input y intentar predecir qué hay en toda esa imagen. Por ejemplo, en esta imagen hay un perro o aquí hay un gato, pues aquí la predicción de la clase se va a hacer a nivel de píxeles. Es decir, la red neuronal va a predecir, hey, en este píxel hay un trozo de perro y aquí también y aquí también. Y en estos píxeles de aquí, de la misma imagen, pues hay un trozo de gato, gato, gato, gato y así hasta completar la predicción de todos los píxeles de la imagen. Cuando has hecho esto, lo que obtienes como resultado es un mapa de segmentación. Un mapa donde vas a tener identificado los diferentes elementos que hay en tu imagen. Y cuando tienes la masa de píxeles ya identificada y localizada, pues esto te puede servir para múltiples tareas. Pues por ejemplo, para borrar el fondo de una imagen y así sacar un nuevo sticker para WhatsApp o usar esa máscara para hacer un inpainting y borrar justo ese objeto. En generación de imágenes también hemos visto lo útil que son los mapas de segmentación en proyectos como Gauguin, donde a partir de un mapa de segmentación se obtenía una imagen realista. En audio, por ejemplo, la segmentación de imágenes se ha utilizado sobre los espectrogramas de audio para eliminar ruidos molestos que pueden estar afectando a tu grabación. O en medicina, desde hace ya unos años, esto se viene utilizando para poder segmentar y diferenciar muchos de aquellos patrones que por no estar acostumbrado el ojo humano no puede detectar tan fácilmente. Así que sí, segmentar imágenes bien es una tarea muy útil para un montón de tareas. Y ahora llega Meta y nos dice, hey, pues con esta tecnología esto lo podéis hacer mucho, mucho mejor. ¿Y por qué mucho, mucho mejor? Bueno, pues esta es la idea con la que quiero que os quedéis en este vídeo, y es que lo interesante del segment anything model no es tanto el segment, sino el anything. Y es que este modelo es capaz de segmentar cualquier cosa, cualquier cosa que se le ponga por delante. Aquí hay una diferencia fundamental con los ejemplos de modelos que hemos comentado antes, donde tú a priori ya le estás indicando a la IA qué tiene que segmentar. Por ejemplo, tú la entrenas para segmentar perros o segmentar gatos, entrenándolas solo para esa tarea como un problema de aprendizaje supervisado. Sin embargo, con Sam la cosa es diferente y es que este modelo es capaz de segmentar cualquier objeto que sea segmentable. Es decir, si yo, por ejemplo, te enseño esta imagen y te pido que identifiques algún patrón interesante, alguna forma u objeto, no vas a encontrar nada. Pero si, por ejemplo, te enseño una imagen como esta, aunque nunca haya visto este objeto, sí sabrás identificar que este es un objeto diferente a este y que sus partes también diferenciables son esta y esta. Y esto lo sabes porque eres capaz de entender diferencias entre patrones, texturas, la continuidad de las formas. Eres capaz de generalizar la información del mundo que has visto con tus ojos. Y esto es lo que ahora inteligencias artificiales como Sam también pueden hacer. Y es impresionante. Esta capacidad de la IA de poder segmentar bien un objeto que nunca ha visto antes durante su entrenamiento es lo que en deep learning llamamos zero-shot learning. La capacidad de una IA de poder resolver una tarea a pesar de, bueno, para ese caso concreto no haber recibido ningún ejemplo de entrenamiento. Algo que nosotros los humanos solemos hacer con mucha frecuencia. Y ahora Sam, pues siguiendo la misma estrategia de entrenar un modelo a lo bestia con una gran cantidad de datos, pues nos ofrece un nuevo modelo fundacional capaz de generalizar la noción de objeto. Siendo capaz de desplegar sus habilidades en contextos que nunca ha visto durante su entrenamiento. Siendo capaz, por ejemplo, de poder segmentar la forma de un teléfono pintado con un estilo concreto aun cuando nunca ha sido entrenado para hacer esta tarea de segmentación en este estilo de cuadros. Ah, que no me crees. Vamos a probarlo. Os voy a dejar abajo el enlace a la página web para que vosotros también lo probéis de Sam, que es esta página de aquí donde está toda la información que estamos comentando y donde también podréis encontrar una demo para probar toda esta herramienta. Cuando entramos vamos a ver un mensaje como este que nos indicará que este es un proyecto de investigación, que ninguna imagen que subamos se va a quedar almacenada, lo cual está muy bien. Aceptamos los términos y con eso podemos ver todas las fotografías que ellos ponen a nuestra disposición para probar la herramienta. De todas ellas me gustan mucho los ejemplos con imágenes como esta, dibujos o arte, donde los objetos no son objetos tan cotidianos, puesto que están representados artísticamente, no son fotografías reales y eso me llama mucho la atención cómo Sam consiga aún así detectar los diferentes objetos y hacer la segmentación correctamente. Podemos marcar aquí, bueno, por los diferentes elementos que hay en el dibujo, podemos marcar. Podríamos ver el sol, lo tendríamos aquí, podríamos ir a las diferentes partes que está identificando, un poco por el estilo, por la forma o podríamos darle aquí a Everything para detectar todo de golpe y ver qué elementos diferencia. Aquí estaría haciendo el análisis y saca una segmentación de las diferentes flores, del suelo, de las diferentes partes de los muñecos, etc. Entonces lo que vamos a hacer es subir una imagen, vamos a hacer un par de ejemplos. Por ejemplo, aquí estoy subiendo una imagen de un robot que he hecho con midjourney y si os fijáis ha habido una primera etapa donde lo que ha hecho Sam ha sido analizar toda mi imagen. Está analizando todos los elementos, está creando un vector de embedding que va a almacenar la información visual de lo que ha observado y a partir de ahí luego con ese vector de embedding es con lo que se va a generar todo el tema de la segmentación. Entonces me gusta mucho esta imagen porque he intentado generar una imagen de un robot que está medio oculto por el humo, por una nube para ver si Sam consigue identificar toda la figura del propio robot. Si yo me voy moviendo por la escena vemos que uno de los elementos que identifica es el suelo, aquí no lo estaría marcando, nos está marcando también un poco toda la parte yo entiendo que de humo y si nos ponemos encima el robot, fijaos cómo consigue sacar una máscara perfecta donde incluso las patas, sería esta parte de aquí, justamente esta parte. Pues como las patas está consiguiendo detectarla a través del humo, algo que no es tan sencillo para un algoritmo de este tipo. Además la forma de operar de Sam pues le permite no solo hacer segmentación del objeto completo, sino que si te vas moviendo pues puedes ver cómo también tenemos la cabeza por separado y podríamos movernos también al ojo, a las diferentes partes del ojo, es decir, podríamos segmentarlo todo. De hecho, aquí hay una herramienta que es multi más que te permite seleccionar un objeto y que Sam pueda sacar diferentes partes de lo que está seleccionando, no te predice diferentes máscaras para un punto que haya seleccionado. Podríamos, por ejemplo, marcarle un ojo y aquí veríamos cómo nos saca pues la pieza del ojo, el ojo y la cabeza por completo. Y esta es otra foto con la que quería probar, que es una foto con mayor complejidad, con mayor nivel de detalle. Es en la frutería más antigua de Madrid, por lo que he podido leer y esta persona se llama Félix y nos va a ayudar ahora pues para hacer un poco de segmentación sobre esta imagen. Hay mucho detalle, así que lo que voy a probaritarle a la función Everything, que va a sacar una segmentación de muchos objetos que haya en la escena. Lo que va a hacer es trazar una malla de diferentes puntos y luego va a seleccionar aquellos que crea que son puntos de interés. No creo que saque todos los objetos por separado, porque la resolución del algoritmo no es tan alta como la que hemos podido ver en el paper con más de 500 máscaras, pero vemos que consigue un resultado bastante bueno delimitando muchos de los objetos de interés de esta escena. Vemos que cada tomate en este sexto está delimitado. Creo que todas las cestas aquí estarían perfectamente delimitadas. Esta se ha quedado fuera. Vemos que diferentes cestas de frutas, pues también están marcadas con algunas frutas más diferenciadas, no sé si porque el color sea distinto. Y vemos también que nuestro amigo Félix aquí también ha sido perfectamente segmentado en este proceso. Fijaos además que si nos vamos moviendo con el cursor, pues Sam va actuando y va intentando segmentar aquel punto donde marcamos y podemos llegar a algunas de las máscaras que antes no se han seleccionado con la función de Everything, porque como hemos dicho, Everything realmente no cubre todo, sino que traza una malla de puntos y donde cae, cae. Pero fijaos que aquí, donde antes teníamos seleccionado todos los elementos, si nos movemos un poquito, pues podemos llegar incluso a diferenciar las diferentes frutas cuando vamos pasando el cursor por encima. Luego además tenéis la opción de marcar un objeto con una bounding box. En vez de hacerlo con un punto de interés, pues podríamos decirle, mira, aquí en esta caja que te estoy marcando, hay un objeto de interés, intenta adivinar cuál es. Y entonces te consigue predecir perfectamente que el objeto, el concepto que cae dentro de este cuadrado, pues tendría que ser nuestro amigo Félix. Y con esto pues ya tendríamos las presentaciones hechas. Félix ya conoces a Sam, Sam ya conoces a Félix y vosotros pues ya conocéis esta herramienta. Y la pregunta es, ¿por qué es tan importante un modelo como Sam? Pues hay tres motivos. Primero, porque es muy versátil. Versátil porque Meta ha diseñado su arquitectura pensando en aceptar diferentes tipos de prompts, que van a permitir al usuario interactuar de distinta forma con el modelo. Por ejemplo, como hemos visto, podemos marcar puntos que marquen qué objeto queremos segmentar y qué parte no queremos segmentar, para diferenciar a nuestro objeto de otros elementos. Pero también este modelo acepta el uso de bounding boxes que delimiten la región de la imagen donde tiene que encontrar un objeto de interés y con la misma dinámica también podríamos utilizar máscaras. Ahora, de todos los inputs, el más interesante, aunque es algo experimental que solo muestran en el paper, es el uso de un prompt de texto donde tú puedes indicar una descripción donde digas quiero que se segmente la rueda del coche y boom, automáticamente la IA lo segmenta. Y esto último, aunque es algo experimental que están haciendo con el modelo clip, me parece una de las funcionalidades más interesantes porque podría servir de puente con modelos como GPT-4, donde tú puedas dejar a la IA, GPT-4, que ya sabemos que puede operar bastante bien con este tipo de herramientas, que elija qué elemento segmentar de una imagen para completar una tarea mayor. Y seguramente no tardaremos mucho en ver implementaciones de este tipo. ¿Por qué? Pues por el segundo motivo por el que Sam es un modelo tan interesante. Y es que es un modelo open source. Y aquí es imposible no hacer la semejanza de este modelo con whisper de OpenAI. Son modelos completamente diferentes. Uno sirve para transcripción de audio, este para segmentar imágenes, pero en ambos casos representan modelos que vienen a dar una solución a un problema clásico de deep learning, transcripción de audio, segmentación de imágenes, problemas muy típicos donde, bueno, ya existía una gran variedad de modelos anteriores que eran capaces de hacer esto. Pero de repente aparecen, llegan, son open source y barren por completo cualquier investigación y cualquier empresa que pudiera ser competencia. Ya lo sabéis que whisper fue un enorme regalo para la comunidad y ahora Sam llega también a nuestras manos para que cualquiera de vosotros pueda empezar a jugar con él, a usarlo y a construir. Tendréis abajo en la caja de descripción el enlace al repositorio de GitHub, donde podréis encontrar el modelo y todas las instrucciones para que empecéis a jugar con él y ver que también funciona, aunque esto ya os lo digo yo, funciona muy bien. Y aquí el tercer punto. Sam es importante por esto mismo, porque funciona muy bien. En el paper podemos encontrar algunos ejemplos bastante impresionantes, donde la hayas capaz de identificar más de 500 máscaras de objetos, cada uno de ellos segmentado correctamente. Y esto evidentemente Meta lo enseña para mostrar músculo, pero aquí lo que me parece interesante no es tanto el músculo, sino el cerebro, la utilidad real que un sistema de este tipo podría tener. Y por eso me alegra mucho encontrarme en redes ejemplos de profesionales que están indicando el salto cualitativo que en sus primeros usos les está suponiendo esta herramienta. Y pensad en lo que hemos dicho antes, las capacidades SiruShot van a permitir a Sam poder hacer bien su labor incluso sobre datos que no haya visto previamente, datos como imágenes satelitales tal y como podemos ver aquí o aplicados a datos capturados por un sonar. Pensad que en muchos trabajos este proceso que estamos viendo suele ser un proceso de etiquetado manual, donde el usuario tiene que estar con su ratón, clicando en las imágenes y segmentándolas, o en el mejor de los casos asistido por un modelo que ha tenido que ser entrenado específicamente para esa tarea, algo que no siempre es posible o no siempre funciona bien. Pero ahora y repito de nuevo, Sam recién salido de la caja es capaz de desplegar sus habilidades sin necesidad de entrenarlo para ello. Y bueno, si no funcionara, podrías utilizarlo de base para hacer un fine tuning con tu conjunto de datos. Es lo bueno de que este modelo sea open source. Y en este punto quiero que te des cuenta de algo muy interesante que está sucediendo y es que cuando entendemos que en el mundo del deep learning dos de los factores fundamentales para entrenar a IAS más potentes son, bueno, la escala de los modelos, ok, pero también el volumen de datos que utilizamos, pues se están dando una serie de avances que creo que pueden influir muchísimo en esto. Recientemente un paper demostró que utilizando modelos como chat GPT, pues se puede automatizar el proceso de etiquetados de un dataset con una calidad superior a lo que antes se conseguía utilizando a grandes equipos humanos y a una fracción del coste. También con tecnologías como whisper de transcripción de audio hemos conseguido desbloquear todo ese texto que estaba atrapado detrás de las ondas de audios, podcast, vídeos y que ahora podemos sumar también al dataset de entrenamiento. Y ahora sumando la tecnología SAM a este equipo, pues tenemos una nueva herramienta que es potentísima, haciendo a SAM un modelo que no es solo valioso por sí mismo, sino por lo que va a facilitar el entrenamiento de futuros modelos de visión por computador. Y esto, como decía antes, es lo que le interesa a Meta. ¿Por qué? Bueno, porque ellos son Meta de Metaverse, son el Metaverse. Y una de sus divisiones más potentes en la empresa es todo lo que tiene que ver con su hardware de realidad virtual y realidad aumentada. Hardware que se beneficia enormemente de todos los avances que se van dando en el campo de la visión por computador. Y poca broma con esto porque creo que Meta está muy bien posicionada en la próxima revolución que se va a vivir de todo el hardware de realidad aumentada a VR combinado con todo lo que estamos viviendo ahora en inteligencia artificial. Y de hecho ellos mismos han mostrado algunos ejemplos del uso de SAM para captura de vídeo en primera persona, donde el punto central al que se orienta la visión del usuario pues queda segmentada por este modelo, dándole una visión clara y estructurada de lo que el usuario está viendo. ¿Entendéis ahora cómo van encajando todas las piezas? Estamos en un punto donde la realidad empieza a aproximarse de nuevo a la ciencia ficción. Y a partir de aquí, con el ritmo de desarrollo que lleva la IA, pues a ver dónde acaba todo esto. Estamos viviendo tiempos interesantes. Y si te gusta todo esto, si quieres estar al día con la revolución de la inteligencia artificial, si te gusta este contenido, como explico, como te enseño todo lo que está pasando, pues si no lo has hecho todavía, suscríbete a este canal que cada semana estamos hablando con nuevos vídeos de toda la actualidad, también a través de directos. Y si no lo has hecho y quieres apoyar este proyecto y agradecer todo lo que estamos divulgando aquí en abierto para todo YouTube, pues puedes hacerlo a través de Patreon, que es una plataforma donde con una pequeña aportación pues podéis agradecer todo este contenido que os traigo aquí al canal. Nuevamente agradecer a Hostinger por haber patrocinado este vídeo. Tenéis abajo el código de descuento en la caja de descripción y por aquí dos vídeos que son muy interesantes y que deberíais de echarle un ojo. Chicos, chicas, nos vemos en el próximo vídeo. Adiós.
