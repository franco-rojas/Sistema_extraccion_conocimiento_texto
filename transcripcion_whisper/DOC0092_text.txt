 En este video veremos una explicación detallada y muy completa de lo que son las máquinas de soporte vectorial o máquinas de vectores de soporte, que es uno de los algoritmos clásicos del Machine Learning y que en la actualidad tiene un uso muy extendido, por ejemplo, en combinación con arquitecturas más complejas como las redes neuronales o las redes convolucionales. Así que sin más preámbulos, comencemos. El principal uso de las máquinas de soporte vectorial se da en la clasificación binaria, es decir, cuando queremos separar un set de datos en dos categorías diferentes. Para entenderlo, veamos de forma intuitiva este concepto. Supongamos que queremos conformar nuestro propio equipo de ciclismo y que estamos en el proceso de selección de nuestros corredores y que cada ciclista puede pertenecer a una de dos categorías. Escalador, es decir, especialista en la montaña o embalador, es decir, especialista en las llegadas con terreno plano. Para asignar la categoría a la que pertenece cada ciclista, es decir, para clasificarlo, podemos usar varias características. Consideremos una primera característica, el peso. Generalmente, los corredores más livianos son buenos en la montaña y los que tienen un peso un poco mayor son buenos para el embalaje. En este caso, podemos definir un umbral y al momento de la clasificación simplemente definimos la categoría, dependiendo de si el peso del ciclista está a la izquierda o a la derecha de este umbral. Pero para mejorar la precisión de nuestra clasificación, podemos agregar más características. Por ejemplo, si añadimos ahora la potencia que desarrolla el ciclista en cada pedalazo, entonces ahora en dos dimensiones podemos trazar una línea que divida a estos dos grupos. Si además del peso y la potencia incluimos, por ejemplo, la capacidad pulmonar, entonces tendremos una distribución de puntos en tres dimensiones, es decir, tres características. Y en este caso tendremos un plano que separa una categoría de otra. Y con cuatro o más dimensiones no resulta fácil dibujarlo, pero la idea es que en todos los casos idealmente tendremos una frontera de decisión que permite determinar la categoría a la que pertenece cada ciclista. En adelante vamos a llamar a esta frontera el hiper plano y en la explicación que viene ahora nos vamos a enfocar únicamente en dos dimensiones porque resulta más fácil entender los conceptos gráficamente. Pero este mismo concepto de hiper plano se aplica para una, tres, cuatro o más dimensiones. Ok, volviendo a dos dimensiones, nos enfocaremos en dos características. El peso y la potencia de cada pedalazo. Si volvemos a nuestro grupo inicial de ciclistas, podemos ver que los dos grupos están separados por un hiper plano, que en este caso es simplemente una línea recta. Si analizamos esta línea recta, veremos que cualquier punto que pertenezca a ella tiene una característica importante. Al reemplazarlo en la ecuación, el resultado será exactamente igual a cero. Pero si tomamos, por ejemplo, los puntos en los cuales están ubicados los embaladores, veremos que al reemplazarlos en la ecuación del hiper plano, todos son mayores que cero. Mientras que al hacer lo mismo para los puntos donde están los escaladores, se obtienen valores menores a cero. Así que partiendo de esto, podemos definir un algoritmo muy sencillo para la clasificación de los datos. En primer lugar, tenemos que obtener la ecuación del hiper plano, es decir, calcular los coeficientes. Y en segundo lugar, usar esta ecuación para reemplazar el dato que queremos clasificar en dicha ecuación y dependiendo del signo que obtengamos, lo clasificamos en una o en otra categoría. Si miramos en detalle este sencillo algoritmo, vamos a encontrar que el problema realmente se reduce a encontrar la ecuación de ese hiper plano. Y es aquí donde entran precisamente las máquinas de vectores de soporte. Para entender cómo funciona este algoritmo, debemos comprender un concepto muy importante, el del mejor hiper plano. Al intentar separar nuestro set de datos, podemos obtener diferentes líneas o hiperplanos y todos ellos logran dividir correctamente el set en dos categorías. Pero cuál de ellas es mejor? Vemos que las líneas uno y dos están demasiado cerca de una de las categorías, mientras que la línea tres está en un punto intermedio entre las dos agrupaciones. Esta línea es precisamente el hiper plano óptimo, pues es la que se encuentra más alejada de todas las observaciones. Y esto hace que al momento de clasificar un nuevo dato, no exista un sesgo hacia una u otra categoría, pues el algoritmo de máquinas de soporte vectorial permite precisamente obtener este hiper plano óptimo. Y aunque existen diferentes maneras de implementarlo computacionalmente, en esencia lo que en el fondo logra hacer este algoritmo es primero detectar los puntos más cercanos entre una clase y otra. Luego encuentra la línea que los conecta y finalmente tras una frontera perpendicular que divide esta línea en dos. La línea que se obtiene es precisamente el hiper plano óptimo. Y acá debemos resaltar tres definiciones importantes. Los vectores de soporte, que son precisamente los puntos más cercanos entre una clase y otra y son los que le dan el nombre al algoritmo. El margen, que es la distancia entre el hiper plano y los vectores de soporte y el mismo hiper plano óptimo, que es la frontera de separación que consigue el mayor margen posible. Así que podemos pensar en el algoritmo de máquinas de soporte vectorial como un método que nos permite trazar una carretera con dos carriles entre nuestros datos. El hiper plano óptimo es el separador entre esos dos carriles y el ancho de cada uno de esos dos carriles, que es el mismo en ambos casos, se conoce precisamente como el margen. Y la idea de las máquinas de soporte vectorial es precisamente obtener los carriles con el máximo ancho posible. Bien, y con esto ya tenemos el principio básico de cómo funcionan las máquinas de vectores de soporte. Pero este algoritmo funciona solo para el caso ideal, es decir, cuando tenemos nuestros datos lo suficientemente separados de manera tal que podemos trazar una línea recta con el margen más amplio posible, es decir, cuando podemos obtener una frontera de decisión óptima. Pero volvamos a nuestro ejemplo. Qué pasa si llega a nuestro equipo un corredor excepcional, es decir, que es muy bueno en la montaña, pero que también se destaca como envalador. En este caso, ese nuevo corredor se comporta como un outlier, es decir, un valor atípico, pues no obedece el comportamiento esperado para un escalador. Si aplicamos el algoritmo explicado anteriormente, veremos que se obtendrá un hiper plano óptimo, pero que el margen será muy pequeño. Esto se debe a que los vectores de soporte están más cerca y esto puede llevar al overfit, es decir, que si introducimos un dato nuevo que no haya sido nunca antes visto por el algoritmo, muy probablemente será clasificado incorrectamente porque el margen es muy reducido y no hay una adecuada separación entre las clases. Así que el algoritmo que vimos inicialmente conocido como hard margin o margen duro no resulta muy flexible y no se puede utilizar cuando tenemos outliers, como el caso de nuestro corredor excepcional. La manera de resolver esto es ensanchando el margen, es decir, buscando que la mayoría de los datos sean correctamente clasificados y aceptar la posibilidad de que existan unos cuantos errores al momento de la clasificación. Esto se logra introduciendo una modificación al algoritmo para el cálculo del hiper plano óptimo. Originalmente, en el clasificador hard margin se buscaba maximizar el margen y para esto se modificaban únicamente los coeficientes del hiper plano. Ahora, para contrarrestar el efecto del outlier se incluye un término adicional a esta función, lo que permite flexibilizar el margen. Este término depende de un parámetro C, un hiperparámetro que yo como diseñador elijo durante el entrenamiento. La idea es que un C relativamente pequeño permite generar margenes más amplios y a medida que su tamaño aumenta, el margen se va reduciendo poco a poco. Este parámetro se escoge de manera empírica, analizando el error que se obtiene en la clasificación comparado con diferentes valores de C. Y a este algoritmo de máquina de vectores de soporte se le conoce como soft margin. Bien, en este punto ya tenemos un algoritmo mucho más versátil sobre el cual tenemos un control al momento de obtener el ancho de ese margen o de esos carriles. Sin embargo, todavía estamos considerando una situación bastante ideal porque nuestros datos están lo suficientemente separados y basta con trazar una línea recta para clasificar correctamente la mayor parte de ellos. Pero en aplicaciones reales la situación es más complicada porque podemos tener fronteras de división que no son necesariamente líneas rectas y que tienen formas mucho más complejas. Y el problema es que hasta donde hemos visto, las máquinas de soporte vectorial solo permiten obtener hiperplanos o fronteras de decisión lineales. Pero entonces, ¿qué se puede hacer en este caso? Una alternativa sería agregar más dimensiones a cada dato. Es decir, ¿qué pasaría si encontramos una forma de agregar una o más dimensiones adicionales para así lograrse separar las dos categorías? En este caso podríamos usar el mismo algoritmo de máquinas de vectores de soporte para clasificar los datos en esas tres o más dimensiones. Pero ¿cómo logramos agregar más dimensiones a los datos para así lograr su clasificación? Pues el método usado en las máquinas de soporte vectorial se conoce como el truco del kernel. Básicamente consiste en tomar el set de datos original, que no es separable, y mapearlo a un espacio de mayores dimensiones usando una función no lineal. En un momento veremos las funciones más usadas. La idea es que con esta transformación el dataset ahora será linealmente separable. Es decir, que se puede usar una máquina de vectores de soporte, tipo SoftMargin, como la que vimos anteriormente, para obtener el hiperplano óptimo. Una vez lo hayamos obtenido hacemos la transformación inversa para volver al espacio original y así llevar a cabo la clasificación. En la práctica el truco del kernel no implementa todos estos pasos, sino que hace el mapeo y el cálculo del hiperplano, usando algo de álgebra lineal para obtener de forma simplificada estos cálculos y hacer más rápido el algoritmo. Las transformaciones más usadas en este truco del kernel se logran con dos tipos de funciones. Las polinomiales, que implican obtener combinaciones de los vectores de características usando potencias mayores que uno o usando funciones gaussianas, con forma de campana que se conocen como funciones de base radial. Aunque las máquinas de soporte vectorial fueron desarrolladas a comienzos de los años 90, en la actualidad son muy usadas porque permiten separar de manera óptima un set de datos en dos categorías diferentes, es decir, obteniendo una frontera de decisión que está equidistante a los dos grupos que yo quiero separar. Esto hace que una máquina de vectores de soporte tenga usualmente una precisión más alta que una neurona o un perceptrón convencionales. De hecho, esto ha permitido que en la actualidad muchos sistemas de deep learning que usan redes convolucionales para el procesamiento de imágenes, como por ejemplo para el reconocimiento de rostros, extraigan características de las imágenes usando estas redes convolucionales y luego alimenten estas características a una máquina de soporte vectorial para realizar la clasificación final del rostro. Bien, y con esto espero que tengan una idea completa y detallada de qué son y cómo funcionan las máquinas de soporte vectorial. En un próximo video veremos cómo usarlas para resolver un problema de machine learning en Python. Y si les gusta el contenido del canal, los invito a ver los videos que les estoy sugiriendo aquí en la pantalla y nos vemos en el próximo video.
