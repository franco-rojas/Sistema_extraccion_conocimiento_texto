 Ok, sigamos con los Transformers. Noticia de hace unos días. Nvidia y Microsoft presentan un nuevo modelo del lenguaje que triplica en número de parámetros al famosísimo GPT-3, un modelo que, siguiendo el juego del nombre de los Transformers, ha sido bautizado como Megatron Turin, y cuyas capacidades podemos ver en esta demo en directo del pasado GTC. Y es gracias a una arquitectura como los Transformers que estamos consiguiendo desarrollar tecnologías tan impresionantes como estas. Enormes modelos de lenguaje tipo GPT-3 o Megatron Turin que han revolucionado por completo el campo del procesamiento del lenguaje natural, sistemas como Alphafol para la predicción del plegamento de proteínas, o cosas tan impresionantes como Dali o VQ Gammas Clip. De hecho, casi cualquier revolución que hemos visto en 2021 en el campo del deep learning tiene por detrás funcionando a un Transformer. ¿Por qué? No, de verdad, quiero que lo penséis. ¿Cuál es ese aspecto técnico que hace que los Transformers tengan el rendimiento que realmente demuestran? ¿Qué ingrediente secreto hace que esta tecnología esté suponiendo una revolución dentro de la revolución? En el vídeo anterior de esta serie ya os presenté uno de los componentes principales, los mecanismos de atención. Un mecanismo que lata en el interior de esta arquitectura y que permite a la red poder aprender a prestar atención a los datos que usamos para el entrenamiento. Como digo, este mecanismo de prestar atención es una de las piezas fundamentales en el funcionamiento del Transformer. Pero, ¿y si te dijera que la clave no está ahí? Que en realidad, el motivo por el cual el Transformer es tan espectacular no es porque los mecanismos de atención sean tan buenos que tan bien, sino por otro motivo que hoy te vengo a explicar. Quédate y ponte cómodo porque hoy sí vas a conocer por fin qué aspecto de los Transformers hacen que funcionen tan bien. Aunque para llegar a esto antes tendremos que solucionar unos cuantos problemitas. Si buscáramos el origen de los Transformers en el famoso paper de donde surgió, Attention is all you need, nos encontraríamos con el siguiente diagrama. Un diagrama que creo que ya ha pasado a la historia del universo del deep learning y que hoy, bueno, pues que hoy lo vamos a trocear. Y de todas las partes en la que me quiero centrar hoy es en esta de aquí, la codificación posicional. ¿El motivo? Bueno, pues porque considero que esta piecita de aquí resuelve uno de los procesos fundamentales que hacen del Transformer una arquitectura tan impresionante. ¿Por qué? Bueno, eso ya lo sabrás al final del vídeo. Antes tenemos que entender cómo este módulo de procesamiento actúa exactamente sobre nuestras secuencias de entrada. Ya sabemos que cuando hablamos de Transformers realmente de lo que hablamos es de analizar secuencias de datos. Secuencias de acciones de un jugador que ejecuta en su teclado, secuencias de píxeles que conforman una imagen o secuencias de palabras que conforman a una frase, una frase como esta. Pues vamos a ver dónde comenzaría el viaje dentro de un Transformers. Como ocurre con toda red neuronal, el primer paso lo encontramos en su entrada, donde introducimos el input. Pues aquí, en la entrada del encoder, donde ocurre esta primera transformación. En nuestro caso, la frase que se va a procesar ya sabemos que realmente no viene representada por palabras como tal, sino que estas primeras son reconvertidas a vectores numéricos. Ya sabéis que tenéis este vídeo aquí para saber más sobre esto. Y aquí es donde viene una de las principales diferencias. Y es que si antes, cuando trabajamos con redes neuronales recurrentes, cada palabra era procesada una tras otra, ahora el Transformer se come toda la frase entera al mismo tiempo. Todas las palabras a la vez. Algo así como leer un libro ya no secuencialmente, palabra por palabra, sino absorbiendo de golpe todas las palabras de una página. Claro, siendo esto así, ¿cómo podríamos saber cuál es el orden en el que estas palabras están siendo presentadas? Me explico. Con las redes recurrentes, recordaréis que lo que hacíamos era conectar el output del procesamiento anterior con el input de la siguiente palabra. Así, la palabra número 3 de nuestra frase recibía información de la palabra número 2, que se había procesado anteriormente. Y esta de la primera. Es decir, hay una dependencia entre palabras que ya establece un orden. ¿Lo ves? El problema es que si ahora decidimos que no, que metemos todas las palabras a la vez, este orden se pierde, desaparece. Y lo mismo le daría al Transformer que la frase fuera esta de aquí o esta otra de aquí. Pero claro, creo que todos estaremos de acuerdo en que la forma en la que se ordenan las palabras dentro de una frase es muy importante para poder entender su significado. Entonces, ¿qué hacemos? Pues, puesto que estamos trabajando con vectores numéricos, la cosa no sería tan complicada, ¿no? Por ejemplo, si este vector es la primera palabra, podríamos sumarle otro vector de igual tamaño con todas sus componentes marcando la posición que ocupa. Aquí todos a 1, aquí todos a 2, aquí todos a 3, y así sucesivamente para darle de alguna forma información a la red neuronal de cuál es la posición que ocupa cada palabra. Hemos agregado información posicional y esto es una opción, pero no es la correcta. Lo que hemos propuesto hasta ahora es lo que se conoce como un posicionamiento absoluto, donde cada palabra tiene asociado un vector con justamente la posición que ocupa. 1, 2, 3, 4, 5, 6, y así hasta el número de palabras que tengamos en esa secuencia. Pero claro, esto es un problema porque si tuviéramos secuencias muy largas, por ejemplo de 200 palabras, el último vector que estaríamos sumando con información posicional sería un vector que estaría con todas sus componentes a 200. Estaríamos sumando esto al vector de dicha palabra y si nos damos cuenta por comparación relativa, la información del vector original acabaría siendo poco relevante. Bueno, vale, ok, si sumar vectores que tengan números muy grandes puede ser un problema, vamos a buscar una alternativa. La alternativa podría ser, bueno, pues que si tienes cinco palabras en toda tu secuencia, lo que vamos a hacer ahora es coger las componentes de los vectores posicionales y dividirlo por ese número. En este caso lo que estaríamos haciendo es normalizar estas componentes obteniendo los siguientes porcentajes que irían en el rango de 0 a 1. En la primera palabra sumaríamos 1 partido 5, en la segunda 2 partido 5, en la tercera 3 partido 5 y así sucesivamente. Pues solucionado, ahora los últimos vectores no tendrían valores tan grandes, serían pues cercanos a 1 y ya podríamos tener capturada la información posicional de cada palabra. Perfecto. Pues tampoco y es que siempre hay algún problema perjudicándonos por el camino. En este caso fijaros que para esta secuencia esto funciona perfectamente. Pero y si ahora introducimos una secuencia de longitud diferente, ¿qué pasaría? Para esta estrategia una secuencia de tres palabras tendría una codificación posicional de dos tercios de 0,66 para la segunda palabra. Sin embargo, para una secuencia de seis palabras, pues la palabra en cuarta posición tendría su vector posicional a cuatro sextos que también es 0,66. Entonces ¿qué le estaríamos diciendo a la red? ¿Que 0,66 es la segunda posición o la cuarta? Y así podríamos seguir probando codificaciones encontrándonos diferentes problemas en el camino hasta encontrar a la codificación ideal que sería la siguiente. Ok, volvamos para atrás. Sí, vamos a utilizar posicionamiento absoluto. Vamos a utilizar 1, 2, 3, 4, 5, 6 para identificar cuál es la posición de cada palabra. Pero en este caso si antes el problema era que componentes con valores muy altos podían afectar a la información del vector original, lo que vamos a hacer ahora es trabajar únicamente con unos y ceros. Por ejemplo con codificación binaria. Si antes lo que hacíamos para que por ejemplo este vector representara la posición número 3 era colocar todas sus componentes con dicho número 3, lo que vamos a hacer ahora es que este vector codifique en binario al número 3. 0000000111 ¿Quieres el número 5? Pues 0000000101. Y así lo único que tendríamos que hacer pues sería seguir codificando cada una de estas posiciones en binario. Y aquí cualquiera que haya tenido alguna vez que completar alguna de estas tablas de verdad con muchas posiciones representadas en binario se habrá fijado en el siguiente patrón. Pues que el primer dígito del número en binario, según vamos aumentando posiciones, irá alternando su valor de 0 a 1 y luego a 0 y luego a 1 y así sucesivamente. Y con el segundo dígito ocurre igual. Pero ahora alternando cada dos posiciones. 00110011. Y en la tercera posición lo mismo, pero ahora cada cuatro posiciones. 00001111. 00001111. Y este fue el típico truquillo que a mí me salvó mucho tiempo durante los exámenes de la carrera de informática. Porque al final esta alternancia que se produce a diferentes frecuencias era un atajo muy rápido con el cual podías construir todas las posiciones codificadas en binario. Y con esto ya parecería que lo tenemos todo solucionado. Ya tenemos codificada cuál es la posición de cada palabra. Y además cada vector no incluye números muy grandes que van a distorsionar la información del vector original. Solamente trabajamos con 0 y 1. ¿Es esta la solución correcta? Pues parece que sí. Así que corre, vete a contárselo a todo el mundo, ponlo en redes sociales, llama a tu familia y cuéntase, lo ponlo en los grupos de whatsapp. No hay que ser discreto. Este chiste es muy malo. No hay que ser discreto. ¿A qué me refiero con todo esto? Bueno, que no hay que ser discreto matemáticamente. Es decir, ahora mismo estamos viendo cómo existe un patrón discreto donde cada componente de nuestros vectores posicionales alterna su estado de forma discreta. 0 1 0 1 apagado encendido apagado encendido o en esta columna pues 0 0 1 1 apagado apagado encendido encendido. Estados discretos que se van alternando. Pero la naturaleza continua de las redes neuronales nos invitan a representar estos cambios de estados de otra forma, con ondas. Si te das cuenta, este patrón que hemos identificado en nuestra codificación binaria en realidad se podría representar de forma continua como ondas que vibran a diferentes frecuencias. 0 1 0 1 0 1 es una onda que vibra a una frecuencia muy alta y 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 una onda que vibra a menor frecuencia. Ondas cuya frecuencia de vibración varía según qué componente del vector posicional estemos mirando. Esta intuición que hemos desarrollado hasta ahora nos lleva un resultado compacto y elegante y es que toda la codificación posicional que nos indicará la posición de cada palabra en nuestra secuencia se podría calcular con una única función sinusoidal como esta. La idea es la misma que antes cuando trabajamos con 0 y 1, pero ahora todo esto se resume en esta fórmula de aquí. En el paper original del transformer, si lo buscáis, esto en realidad es un poquito más complejo con una combinación de senos y cosenos, pero donde la intuición de lo que hemos visto hoy se mantiene. Quiero remarcar que todavía existen algunas consideraciones más que se han tenido en cuenta en el diseño de esta codificación posicional, pero que no vamos a entrar a ver en este vídeo porque ya implicaría entrar a estudiar algunas matemáticas un poquito más pesadas. Os voy a dejar abajo en la caja de descripción un enlace a un artículo donde está todo perfectamente detallado al completo y que podéis leer si queréis profundizar más en todo este tema. Pero para hoy me basta con que entendáis que es este token posicional con el cual podemos informar al transformer de cuál es el orden de los diferentes tokens, de por ejemplo las diferentes palabras que componen a nuestra frase, a nuestra secuencia y que esto es algo fundamental. Una formulación basada en senos y cosenos que permiten a la red neuronal de nuevo poder entender cuál es el orden de las palabras suministradas como input. Y esto considero yo que dentro del transformer es uno de los elementos clave. ¿Por qué? Porque una secuencia sin orden es caos y esta ordenación en los modelos recurrentes se conseguía como hemos visto a través de la dependencia de procesar una palabra, luego otra, luego otra, luego otra y así sucesivamente. Aquí está el problema. Lo que estamos viendo es que en una red neuronal recurrente existe una dependencia por la cual cada palabra tiene que esperar a que se hayan procesado las palabras anteriores para llegar a su procesamiento. Algo que difiere por completo a lo que ocurre en un transformer. Y ahí es donde el transformer gana la partida, porque al procesar todas las palabras al mismo tiempo este procesamiento se puede paralelizar. Podemos sacar rendimiento a nuestras GPUs, a nuestras TPUs, a nuestras no sé qué PUS. Podemos sacar el rendimiento de la alta paralelización de los procesadores modernos haciendo que estas arquitecturas sean capaces de procesar una mayor cantidad de datos, un mayor número de secuencias y ser mucho más eficientes en su entrenamiento. Creo que ahora ya entenderás mejor de dónde surgen estos enormes modelos del lenguaje como GPT-3 o Megatron Turing. Enormes modelos del lenguaje que tienen una gran cantidad de parámetros y que han sido entrenados pues con enormes datasets. Unos entrenamientos masivos que no hubieran sido posibles sin una arquitectura tan paralelizable como el transformer. Algo que con las redes recurrentes en 2015-2016 no era posible. Dejar atrás a las redes recurrentes para apostar por arquitecturas altamente paralelizables basadas en mecanismos de tensión pues implicaba entonces perder esta información posicional que venía por la dependencia secuencial del procesamiento. Algo que se ha solucionado a través de la codificación posicional que hemos visto hoy. Así ya sabes que la cosa va de ondas. La próxima vez que veas el diagrama de los transformers ya podrás entender qué significa este simbolito de aquí y que ha permitido que en el enorme potencial que se esconden dentro de los mecanismos de tensión de un transformer todavía pueda existir algo de orden. La verdad es que es maravilloso poder entender cuáles son las matemáticas que están tras tecnologías tan impresionantes como GPT-3 o modelos que pueden por ejemplo aprender a programar. Y sobre este último tema sobre inteligencias artificiales que aprenden a programar y cuáles son sus implicaciones es de lo que voy a estar hablando la semana que viene. Me han invitado a participar en el evento anual para desarrolladores que organiza el equipo de Samsung Dev Spain. El 18 de noviembre por la tarde voy a estar dando una ponencia sobre este tema junto a una agenda completísima donde se va a estar hablando de mucha tecnología. Si queréis ver estas charlas, la mía y la del resto de compañeros y todas las sorpresas que va a haber en este evento os informo de que los registros ya están abiertos, es gratuito y simplemente tenéis que acudir a la página de Samsung, os la dejo abajo en la caja de descripción y os apuntáis y listo. Vale, la tarde del 18 de noviembre tenemos una cita así que apuntadlo bien en el calendario y disfrutemos de la tecnología como hacemos siempre juntos. Si te ha gustado este vídeo recuerda que puedes apoyar la actividad de este canal a través de Patreon, es la forma más directa que tienes de apoyar todo el contenido que hago, sería el equivalente invitarme a una caña pero versión digital. Tenéis el enlace abajo en la cajita de descripción y es el apoyo de todos vosotros lo que permite que este canal pues siga estando vivo y siga trayendo contenidos como este. Vuestro apoyo se convierte en estos vídeos. Mola. Chicos, chicas, me despido, nos vemos con más inteligencia artificial la próxima semana. Podéis ver mientras este vídeo de aquí que está muy chulo, no sé de qué va, te lo recomendado YouTube a ti porque te conoces mejor que yo. Este vídeo te va a gustar mucho.
