 Muy buenas, mi nombre es Abraham Requena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una empresa multinacional española. Bueno, durante este vídeo vamos a hacer una pequeña introducción a los fundamentos de Hadoop. Nos podemos preguntar ¿qué es Hadoop? Bueno, pues Hadoop no es más que un framework que nos permite el procesamiento distribuido de grandes cantidades de datos, usando modelos de programación simple sobre un cláster de máquinas. ¿Cuáles son las características básicas que posee Apache Hadoop? Bueno, en primer lugar, su procesamiento distribuido. La idea de Hadoop es poder distribuir los datos, poder paralelizar el tratamiento de los datos de forma que cada nodo de nuestro cláster de máquinas procese una parte de los datos. De esta manera, estaremos ganando velocidad. Luego, es un proceso eficiente porque consigue procesar los datos en poco tiempo. Es un framework económico porque se puede escalar fácilmente de manera horizontal. Si se nos queda pequeño nuestro sistema, lo que tenemos que hacer únicamente es añadir un nuevo nodo que lo añadiremos a nuestro cláster de máquinas y ya no tendremos problemas. ¿Qué más? Bueno, es un sistema tolerante a fallos porque usa la alta disponibilidad y además usa la replicación. Los datos suelen estar replicados con replicación 3 en el HDFS, que es el sistema de almacenamiento de Hadoop. De forma de que si un nodo cae tendremos el dato en el resto de nodos. Y por último y no menos importante es que es un proyecto open source, es un proyecto de código abierto. ¿Cuál es la arquitectura básica que sigue Hadoop? En Hadoop se distinguen cuatro módulos principales. El Common Utilities, el YARN, el HDFS y los procesos MapReduce. El Common Utilities lo forman todos los JAR y todas las librerías que son necesarias para ejecutar Hadoop. El YARN es el gestor de recursos de Hadoop. Como hemos dicho, Hadoop es un sistema distribuido en distintas máquinas, por lo cual debe de haber un gestor de recursos que vaya gestionando esto durante en todas las máquinas. Luego tenemos el HDFS, este es el sistema de archivo distribuido de Hadoop. Este sistema de archivo está instalado en cada uno de los nodos de nuestras máquinas y los procesos MapReduce de arriba siempre se apoyarán en el HDFS para poder coger los datos paralizados, para poder paralizar el procesamiento. Y por último, como hemos dicho, los procesos MapReduce. Esto es una manera de implementar el software que lo que conseguimos es paralizar de nuevo los datos. Siempre vamos al mismo fin, para la paralización de los datos. Entonces tendremos que crear unos procesos Map y unos procesos Reduce de forma que primero iremos agrupando los datos y luego haremos cálculo con esos grupos de datos. Todo esto lo veremos durante el curso. Y como hemos dicho, la configuración habitual de Hadoop es tenerlo en un clúster de máquinas, de forma que tengamos una máquina maestra y tengamos N máquinas esclavas. La idea es que la máquina maestra gestionará todas las tareas y las enviará a las máquinas esclavas. Esas máquinas esclavas van a realizar todos los procesamientos de los datos y finalmente terminarán informando de nuevo a la máquina maestra. Bueno, ¿qué opciones tenemos para trabajar con Hadoop? Bueno, pues tenemos tres opciones. La primera de ellas es Microsoft Azure. Microsoft Azure es un servicio que nos proporciona Microsoft y que nos permite tener nuestras máquinas en la nube. Entonces tendríamos nuestras máquinas en la nube y pagaríamos según la cantidad de máquinas que tengamos y las características de las máquinas. Es un sistema bastante interesante. Bueno, luego tenemos Hortonworks. Hortonworks es la última distribución que ha salido acerca de Hadoop. Que bueno, con Hortonworks nos podremos descargar una máquina virtual que abrimos con un cliente de virtualización y tendremos todo nuestro sistema Hadoop instalado con todos los servicios. Y por último, que es la que utilizaremos en el curso, tendremos Cloudera. Cloudera fue la primera distribución acerca de Hadoop y igualmente nos permite descargar una máquina virtual con la cual podremos gestionar todo. Descargamos una máquina virtual que abrimos con un cliente de virtualización y ahí ya tenemos todo Hadoop instalado y también tenemos la Cloudera Manager que es un asistente para gestionar todo el clúster de máquinas. Y bueno, esto es todo. Y nada, os animo a continuar con el curso porque creo que va a ser un curso bastante interesante en el que vamos a poder implementar Big Data y crear procesos de MapReduce.
