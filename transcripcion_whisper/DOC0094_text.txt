 Los árboles de decisión son uno de los algoritmos de aprendizaje supervisado más importantes del Machine Learning. En el video anterior vimos cómo usar el algoritmo CART para implementar un árbol de decisión capaz de clasificar un set de datos. En este video veremos la segunda parte de este algoritmo, es decir, cómo implementar árboles de decisión para una tarea de regresión. Vamos a ver en detalle cómo funciona paso a paso el algoritmo, cómo se construye el árbol, cómo usarlo para realizar predicciones y cómo se realiza la poda en caso de que sea necesario simplificarlo. Así que sin más preámbulos, comencemos. En una tarea de regresión lo que buscamos es entrenar un modelo que sea capaz de predecir el valor de una variable continua. Por ejemplo, podríamos usar características como el área de un inmueble, el número de habitaciones y de bañas y la antigüedad para predecir una variable continua que es el costo del inmueble. Si hablamos de medicamentos podríamos usar como características la dosis en miligramos del medicamento, el peso y la edad del paciente para intentar predecir la variable continua que es el porcentaje de efectividad del medicamento. Para simplificar las cosas asumamos un problema de regresión con una sola característica. Con esto va a ser suficiente para entender los árboles de regresión y el algoritmo CART. Y luego simplemente será repetir esto paso a paso cuando tengamos dos o más características. Volviendo al problema de los medicamentos vamos a suponer que la única característica es la dosis en miligramos y que la usamos para predecir el porcentaje de efectividad. Si la relación entre estas dos variables es lineal, es decir, a mayor dosis mayor efectividad, el problema se resuelve fácilmente usando el algoritmo de regresión lineal que vimos en un video anterior. Pero compliquemos las cosas intencionalmente porque en el mundo real es así. Supongamos que la relación entre la efectividad y la dosis tiene el comportamiento que vemos en la figura. Para un rango de dosis la efectividad es baja, para otro rango tiene un valor alto y para otro tiene un valor medio. Acá necesitamos un modelo que sea capaz de tener en cuenta estos diferentes rangos y que logre predecir con precisión el porcentaje de efectividad para una dosis en particular. Y es aquí donde precisamente entran los árboles de decisión y el algoritmo CART. En la regresión con árboles de decisión de manera iterativa vamos a comenzar a generar particiones binarias sobre el espacio de características que en este caso es nuestro eje horizontal y que corresponde a la dosis en miligramos del medicamento. La idea es que estas particiones generen las agrupaciones de datos con los niveles de efectividad del medicamento lo más uniformes posible. Veamos esto en detalle. Si establecemos por ejemplo una condición de una dosis menor o igual a 37.5 miligramos, los puntos a la izquierda no estarán muy dispersos y en promedio tendrán una efectividad del 10%. Pero los del lado derecho que no cumplen la condición estarán más dispersos y por tanto el promedio de la efectividad no será una representación muy precisa de esta región. Así que dividimos esta región usando como criterio una dosis menor o igual a 60 miligramos. La partición resultante del lado izquierdo tiene baja dispersión y una efectividad promedio del 90%. Pero la del lado derecho aún está muy dispersa así que la dividimos nuevamente usando como condición una dosis menor o igual a 80 miligramos. Y acá resultan dos nuevas agrupaciones un poco más homogéneas. La de la izquierda con un promedio de 38.3% y la de la derecha con uno de 11.7%. Y con este simple ejercicio acabamos de construir nuestro primer árbol de regresión. El nodo inicial se conoce como la raíz, los demás nodos se conocen como nodos internos y las hojas son las terminaciones, es decir donde no hay más particiones. En este caso decimos que nuestro árbol tiene una profundidad igual a 3 que es la distancia máxima entre la raíz y la hoja más alejada. Bueno ya tenemos una idea general de cómo se entrena un árbol de regresión pero todavía nos quedan varias preguntas por responder. Por ejemplo ¿cómo lo usamos para realizar la regresión? ¿O cómo se construye de forma automática este árbol? ¿O cómo definimos en qué momento se dejan de hacer más particiones? En el caso de la regresión, que en nuestro caso consiste en predecir el porcentaje de efectividad del medicamento para un nuevo dato, basta con tomar la dosis correspondiente y recorrer el árbol entrenado evaluando en cada nodo la condición establecida hasta llegar a una de las hojas. En este caso la predicción será simplemente el valor promedio de los datos que pertenecen a esta hoja y que fue obtenido durante el entrenamiento. Ahora respondamos la segunda pregunta, es decir ¿cómo construir automáticamente este árbol? Y acá es donde precisamente entra en acción el algoritmo CART. El primer paso es organizar ascendentemente los valores de la característica y luego calcular el punto medio entre cada par de características consecutivas. A estos puntos los llamaremos umbrales candidatos. La idea ahora es seleccionar el mejor de estos umbrales candidatos, es decir el que genere las particiones con la menor dispersión posible de la variable continua, que en este caso es la efectividad. Para entender cómo medir esta dispersión analicemos el primer umbral, una dosis menor o igual a 10 mg. Al usar esta condición la región izquierda tendrá un solo punto y su dispersión será nula, porque su promedio será exactamente igual al valor de la efectividad, es decir del 10%. Pero la región de la derecha tiene un comportamiento diferente, pues tendrá 11 puntos con una efectividad promedio del 40%. Sin embargo la mayor parte de estos puntos se encuentra alejada del promedio, así que habrá una alta dispersión. Para medir esta dispersión podemos simplemente promediar la diferencia existente entre el porcentaje de efectividad de cada punto y el valor de efectividad promedio de la región. Pero para evitar que algunas diferencias positivas se anulen con diferencias negativas, elevaremos al cuadrado estas diferencias y luego si las promediaremos. Y esta métrica que acabamos de definir se conoce como el error cuadrático medio. Un error cuadrático medio igual a 0 es la condición ideal, es decir una dispersión nula, mientras que entre más alto sea este error mayor será el grado de dispersión. Si volvemos a las dos regiones que acabamos de obtener y calculamos su error cuadrático medio, veremos que la de la izquierda tiene un error igual a 0, mientras que la de la derecha tiene una dispersión bastante alta, pues su error es mucho mayor, de 1127.3. Ahora conociendo el grado de dispersión de cada una de las agrupaciones podemos calcular un puntaje para el umbral seleccionado, lo que llamaremos en adelante la función de costo. Para esto primero calculamos la dispersión promedio en cada región, es decir tomamos el valor del error cuadrático medio y lo multiplicamos por la fracción de los datos que se encuentran en cada agrupación con respecto al número original de datos antes de realizar la partición. Así por ejemplo, la región izquierda tenía un error cuadrático medio igual a 0 y en total había uno de los 12 datos iniciales, lo que equivale a una dispersión promedio igual a 0. En el lado derecho el error era igual a 1127.3 y se tenían 11 de los 12 datos iniciales, lo que equivale a una dispersión promedio igual a 1033.4. Y con esta información calculamos la función de costo para este umbral, que será simplemente la suma de las dos dispersiones promedio que acabamos de obtener, es decir igual a 1033.4. Perfecto, ya tenemos una métrica para evaluar qué tan buena o no es una partición. La idea es ahora repetir el mismo procedimiento del cálculo de la función de costo para cada umbral y una vez hecho esto tomar el umbral con la menor función de costo posible, que será precisamente el que genere las particiones con la menor dispersión. En este caso particular veremos que la mejor condición de todas es una dosis menor o igual a 37.5 miligramos. Y bien con esto ya tenemos una primera partición, lo que podemos seguir haciendo es ahora refinar cada una de estas regiones haciendo más y más particiones. Si por ejemplo nos enfocamos en la región del lado derecho veremos que no se puede subdividir, así que repetimos el algoritmo anterior. Definimos los umbrales candidatos para esta región de interés y luego calculamos la función de costo de cada uno y elegimos el umbral con el menor costo posible, que en este caso equivale a una dosis menor o igual a 60. Y con esto ya tenemos la base del algoritmo CART para la regresión, que se puede resumir de la siguiente forma. Primero calcular los umbrales candidatos, que corresponden al punto intermedio entre dos valores consecutivos de las características. Segundo para cada umbral candidato obtener las dos particiones y calcular sus errores cuadráticos medios. Tercero calcular la función de costo de cada umbral candidato y elegir aquel con el menor costo. Y cuarto repetir los pasos 1 a 3 de forma iterativa hasta cumplir con un criterio de parada. Pero nos quedaba una pregunta por responder, que era ¿cómo sabemos en qué momento dejamos de hacer más particiones? Pues la respuesta está en el término que acabamos de mencionar, el criterio de parada. Si volvemos al árbol que acabamos de entrenar veremos que algunas hojas tienen algo de dispersión, así que podríamos seguir subdividiéndolas. Pero al hacerlo estaríamos obteniendo regiones cada vez más pequeñas, que se ajustarían prácticamente a un solo dato de entrenamiento, pero que no funcionarían también cuando intentemos hacer la predicción. Es decir, que el árbol estaría haciendo overfitting, se ajustaría perfectamente al set de entrenamiento, pero no funcionaría de forma adecuada cuando tenga nuevos datos que nunca antes había visto. Para evitar que haya overfitting, es decir, que el árbol crezca de forma indiscriminada, usamos precisamente un criterio de parada, que básicamente es una condición que establece hasta dónde se va a subdividir un nodo. El criterio de parada más usado es el mínimo número de datos por hoja. Por ejemplo, en el árbol que acabamos de entrenar este criterio era igual a 3, lo cual quiere decir que no se realizarán más particiones a un nodo que alcance este mínimo número de datos. Si por ejemplo cambiamos este número a 2, tendremos un árbol con unas hojas, mientras que si es igual a 4, el tamaño del árbol será menor. Este criterio de parada también se conoce como prepoda, pero también existe otra forma de controlar el tamaño del árbol, que es primero entrenarlo y después eliminar algunas hojas, lo que se conoce precisamente como post-poda. El algoritmo más usado para realizar este recorte es la poda de complejidad de costos, que funciona de la misma forma como lo vimos en el video anterior para el caso de la clasificación con árboles de decisión. La idea es que se usará un parámetro alfa que controla el nivel de poda. Con un alfa igual a cero no se elimina ninguna hoja, y a medida que aumenta se eliminarán más hojas y se controlará por tanto el tamaño del árbol de regresión. Bien, y con esto acabamos de ver detalladamente cómo funciona el algoritmo CART para entrenar y usar un árbol de decisión para una tarea de regresión. El principio de funcionamiento es bastante sencillo, pero más importante aún el árbol de decisión resultante es fácil de interpretar, porque para entender cómo hizo la predicción basta con mirar las condiciones establecidas en cada uno de los nodos del árbol. Y aunque para facilitar la explicación vimos el funcionamiento del algoritmo para una sola característica, la misma idea la podemos extender de forma iterativa a dos o más características. Si combinamos lo que acabamos de ver en este video con lo que vimos en el video anterior sobre los árboles de clasificación, tenemos dos de los algoritmos más poderosos del Machine Learning, que son la base de un algoritmo incluso aún más poderoso que se conoce como los bosques aleatorios, y que será el tema del próximo video. Así que si les gustó este video no olviden darle un pulgar hacia arriba de me gusta, porque esto me ayudaría un montón para que la red neuronal de YouTube le recomiende este mismo contenido a otras personas. Por el momento esto es todo, los invito a continuar viendo los videos que les voy a sugerir de este lado, les envío un saludo y nos vemos en el próximo video.
