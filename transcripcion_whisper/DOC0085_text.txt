 Los grandes modelos de lenguaje como GPT-4 o BART, en realidad no procesan el texto como lo vemos nosotros los seres humanos, y en lugar de ello lo que hacen es transformarlo a una representación numérica que se conoce como embeddings, así que internamente estos modelos lo que hacen es procesar y generar embeddings para posteriormente realizar diferentes tareas de procesamiento del lenguaje natural. Entonces en este vídeo vamos a entender qué son estos embeddings y cómo se utilizan para desarrollar diferentes tipos de aplicaciones de procesamiento y generación del lenguaje natural usando estos grandes modelos de lenguaje. Pero antes de comenzar los invito a visitar codificandovids.com, en donde encontrarán la academia online con cursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Para entender qué son los embeddings comencemos con un ejemplo sencillo, supongamos que queremos describir la apariencia de dos personas diferentes usando algunas características. Podemos decir por ejemplo que la primera persona tiene una altura de unos 72 centímetros, el cabello corto y de color negro, su peso es 67 kilogramos y sus ojos son de color café. Y la segunda persona tiene una altura de 183 centímetros, el cabello largo y de color castaño, su peso es 78 kilogramos y sus ojos son de color verde. Así que lo que hemos hecho ha sido tomar el dato original, es decir cada persona, y lo hemos representado a través de cuatro características. Y si organizamos estas cuatro características en un listado, lo que tendremos será un vector de cuatro elementos. Ahora veamos cómo lograríamos hacer algo similar pero usando redes neuronales, que son la base de los grandes modelos de lenguaje. Por ejemplo, si queremos hacer un sistema de verificación de identidad, lo que necesitamos es comparar la imagen del rostro de una persona con imágenes de referencia en una base de datos, para determinar si la persona es o no quien dice ser. Pues en estos sistemas realmente no comparamos directamente las imágenes, en lugar de ello lo que se hace es construir una red convolucional, que es un tipo de red neuronal, que procesa estas imágenes, y que a la salida genera una representación vectorial de la imagen de entrada. Y esta representación vectorial, que es simplemente un listado o un arreglo de números, es precisamente lo que se conoce como un embedding. Y la idea es que imágenes con rostros similares tendrán embeddings similares, e imágenes con rostros diferentes tendrán embeddings diferentes. Y este principio nos permitirá construir precisamente el sistema de verificación de identidad. Y este mismo principio lo podemos aplicar para el procesamiento y análisis del lenguaje natural. Así que en resumen y para lo que nos interesa en este video, un embedding es una representación vectorial del dato de entrada y que se obtiene tras el entrenamiento de una red neuronal. Como acabamos de ver el texto también puede ser procesado por algún tipo de red neuronal para generar embeddings y desarrollar aplicaciones como por ejemplo el análisis de sentimientos, la generación de texto o los chatbots. Y un enfoque utilizado hace algunos años era generar embeddings a partir de palabras. Es decir que la idea era tomar cada palabra dentro del texto, generar un token que es una representación numérica inicial de esa palabra, de esos tokens hablamos anteriormente en algún video, y después tomar ese token y convertirlo precisamente en un embedding o una representación vectorial de esa palabra. Con este tipo de embeddings, usualmente palabras con significados similares, como por ejemplo manzana y pera, tendrán embeddings similares, pero que a su vez son diferentes de los embeddings para las palabras perro o gato, que tienen un significado diferente. Sin embargo en el lenguaje natural las palabras no se encuentran aisladas, sino que se encuentran en un contexto, en una frase por ejemplo. Y dependiendo de ese contexto, una palabra determinada podría tener diferentes significados. Por ejemplo en las frases debo ir al banco a retirar dinero y estoy sentado en el banco, la palabra banco tiene significados completamente diferentes. Y ese significado lo establecemos nosotros los humanos a partir precisamente del contexto, leyendo la totalidad de la frase. Esto quiere decir que el uso de embeddings a nivel de palabra no es capaz de capturar la información del contexto y por tanto en las frases anteriores la palabra banco tendría el mismo embedding en ambos casos, lo cual no resulta ideal. Así que como alternativa al uso de embeddings a nivel de palabra, los grandes modelos de lenguaje existentes actualmente hacen uso de las redes transformer que permiten generar embeddings capaces de capturar la información del contexto, es decir de la totalidad del texto. De estas redes transformer hablo en detalle en un video anterior, pero para resumir la idea principal podemos decir que una red transformer es un tipo de red neuronal diseñada específicamente para procesar secuencias como el texto y que incorpora algo que se conoce como un mecanismo atencional. Al momento de decidir cuál será el embedding para una palabra o una frase, lo hace analizando la relación entre la palabra o la frase y los demás elementos del texto a diferentes niveles. Así que de alguna manera los embeddings usados por estos grandes modelos de lenguaje intentan imitar la forma como nosotros los seres humanos interpretamos el texto. Con la diferencia de que estos modelos generan internamente representaciones vectoriales, es decir, arreglos de números correspondientes a esa palabra, a esa frase o a ese texto que se está procesando. La ventaja de esto es que con los embeddings generados, frases o textos que tengan significados similares, tendrán representaciones vectoriales similares. Por ejemplo, frases como me gustaría conocer las tarifas de vuelos entre Madrid y Milán o quiero encontrar tiquetes de primera clase ida y vuelta de Nueva York a Miami tienen embeddings similares, pues ambas se refieren, por ejemplo, al concepto de tiquetes de avión. Mientras que frases como que aerolíneas vuelan de Los Ángeles a Tokio o de todas las aerolíneas que llegan a Hong Kong, cuál es la más económica, se refieren a un concepto relacionado con el anterior, pero que es ligeramente diferente, pues se enfoca más en la temática de aerolíneas. Lo anterior quiere decir que embeddings que son numéricamente similares equivalen a frases semánticamente similares, es decir, con significados muy parecidos. En la práctica, los diferentes grandes modelos de lenguaje disponibles utilizan diferentes tamaños para esos embeddings, dependiendo de la forma como hayan sido entrenados. Por ejemplo, los últimos modelos de OpenAI usan embeddings de 1536 o 2048 elementos, mientras otros, como el LLAMA por ejemplo, tienen embeddings que oscilan entre los 4096 y los 8192 elementos. Bien, como hemos visto, hasta ahora los embeddings del texto son la materia prima de los grandes modelos de lenguaje y son lo que ha permitido el desarrollo de muchas de las aplicaciones de procesamiento de lenguaje natural que hemos visto recientemente. Y los tipos de aplicaciones que se pueden construir con este concepto van más allá incluso de las aplicaciones más conocidas, como por ejemplo, ChatGPT. Por ejemplo, podemos usar los embeddings para realizar lo que se conoce como Es decir, que podemos tomar un texto fuente y obtener su representación a través de embeddings y luego un usuario puede escribir una búsqueda en lenguaje natural, es decir, tal como lo haría por ejemplo en una conversación, que también se representa con embeddings. Luego el sistema realiza la comparación vectorial de ambos embeddings y genera la respuesta a la búsqueda también en lenguaje natural. También se pueden realizar las compasiones de los dos embeddings también se pueden realizar tareas de clustering o agrupamiento. Es decir, que podemos usar los embeddings del texto para pedirle a uno de estos modelos de lenguaje que encuentre diferentes temáticas o tópicos o agrupaciones de conceptos que se encuentren dentro del texto. O podemos usar los embeddings para generar sistemas de síntesis de texto, es decir, que extraigan las ideas principales de un texto y generen a la salida un texto más corto, es decir, un resumen, pero usando el lenguaje natural. O también podríamos implementar aplicaciones de recomendación de contenido. Por ejemplo, con base en los artículos que más busca o que más lee un usuario, podríamos recomendar nuevo contenido similar en la web para su lectura. E internamente todo este procesamiento se puede hacer precisamente a través de los embeddings. Así que el rango de aplicaciones que tienen los embeddings generados por los grandes modelos de lenguaje es realmente inmenso y en los próximos vídeos de esta serie comenzaremos a ver algunas de esas aplicaciones. Muy bien, acabamos de ver que son los embeddings, que es la forma como los grandes modelos de lenguaje aprenden a representar el texto al momento de procesarlo. En últimas, un embedding es simplemente un vector, es decir, un arreglo de números que captura la información esencial de la totalidad del texto y que permite no solo que ese texto sea procesado por un computador, sino también desarrollar múltiples aplicaciones de procesamiento del lenguaje natural. Si quieren entender internamente cómo es este proceso de generación de los embeddings a partir de una Red Transformer que es la base de los grandes modelos de lenguaje, les sugiero ver el vídeo sobre Red Transformer en donde explico esto detalladamente. Y como siempre, si tienen alguna duda, comentario o sugerencia sobre esta serie de vídeos, lo pueden dejar abajo en los comentarios. Además, si les gustó el vídeo, no olviden darle un pulgar hacia arriba de me gusta y compartirlo con sus amigos y conocidos, pues esto me ayudará a seguir llevando este tipo de contenido cada vez a más y más personas. Y si aún no lo han hecho, los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique un nuevo vídeo. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo vídeo.
