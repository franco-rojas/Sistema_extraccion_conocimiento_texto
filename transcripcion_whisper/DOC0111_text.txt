 El aprendizaje reforzado es una de las áreas más excitantes y más prometedoras del Machine Learning en la actualidad, porque tiene la capacidad de desarrollar agentes o máquinas inteligentes que pueden ejecutar tareas de forma muy similar a como lo hacemos nosotros los seres humanos. Pero ¿en qué consiste el aprendizaje por refuerzo? ¿Cuál ha sido su evolución? ¿Cuál es su potencial? ¿Qué tiene que ver el Machine Learning con todo esto? ¿Y cuáles son sus aplicaciones? Pues en este video les traigo una guía definitiva en donde les voy a aclarar todas estas dudas. Así que si quieren tener un panorama completo sobre lo que es el aprendizaje por refuerzo, no se pierdan ningún detalle. Así que listo, comencemos. En 1952, el matemático norteamericano Claude Shannon fue tal vez uno de los primeros en desarrollar una aplicación del aprendizaje reforzado. Creó un ratón artificial llamado Teseus que a través de prueba y error logró aprender a atravesar un laberinto, recordando la ruta más exitosa con la guía de imanes ubicados en el piso. Con el paso del tiempo se lograron otros avances, pero más que todo en la teoría. Pero en el año 2013 se dio inicio a una verdadera revolución. Los investigadores de DeepMind crearon un sistema capaz de aprender a jugar prácticamente cualquier juego de Atari desde cero y capaz también de superar a los humanos, usando solo como entrada los pixeles de cada escena, sin tener ningún conocimiento previo de las reglas mismas de estos juegos. Y esta fue la primera de una serie de logros cada vez más impresionantes, que continuaron en mayo de 2017 con AlphaGo, un agente inteligente que fue capaz de vencer al campeón mundial de Go, un juego de mesa extremadamente complejo inventado en China hace más de 2000 años. Pero ¿cómo lograron desarrollar este sistema? Pues la idea fue bastante simple, combinaron el poder de las redes neuronales, que es un área específica del Machine Learning, con las técnicas básicas de aprendizaje reforzado que se venían desarrollando desde los años 50. Y con esto precisamente nació el Aprendizaje Reforzado Profundo, que es una de las áreas del Machine Learning que tiene aplicaciones potenciales no solo en el tema de videojuegos, sino también en la robótica, en la automatización industrial, e incluso en áreas como el desarrollo de nuevos medicamentos para tratar diferentes enfermedades. Pero un momento, para entender lo que es el Aprendizaje Reforzado Profundo, primero tenemos que entender lo básico. Es decir, ¿qué es esto del Aprendizaje Reforzado? ¿Cómo es que una máquina inteligente puede aprender y empezar a ejecutar tareas similares a las que hace el ser humano? Para entender este concepto básico del Aprendizaje Reforzado, vamos a partir de un ejemplo intuitivo de los videojuegos. Supongamos que vamos a enseñar a un humano a jugar punk, el clásico juego de Atari. Si le mostramos por primera vez el juego a una persona, le podríamos dar una instrucción como esta. Con el teclado puede controlar una paleta que se mueve hacia arriba o hacia abajo. Su tarea es golpear la bola hasta que su oponente no logre alcanzarla. Cada vez que haga esto obtendrá un punto, y gana el jugador que logre obtener más puntos al final de la partida. Lentamente, el nuevo jugador humano aprenderá a controlar la paleta, a golpear la bola y a marcar puntos, a través de un proceso de prueba y error, y eventualmente logrará vencer al oponente. ¿Y cómo lograríamos esto con un computador? Pues en primer lugar deberíamos crear un programa, que en adelante vamos a llamar Agente, que logre hacer varias cosas. En primer lugar debe entender los elementos del juego, es decir, que hay por ejemplo dos oponentes, un tablero, una bola y dos paletas. Es decir, debe entender el entorno. Después de esto, el agente debe entender lo que está sucediendo en el entorno, es decir, por ejemplo, la dirección de la bola o los movimientos de su oponente. A esto lo llamaremos Estado. Dependiendo del estado, el agente deberá aprender a moverse en el tablero para tratar de vencer a su oponente. A esto lo llamaremos Acción. La manera de saber si lo hizo bien o mal, será a través del puntaje. Si el agente lo hace mal, recibirá un punto en contra, una penalización, pero si lo hace bien, recibirá un premio. A este premio o castigo les daremos el nombre genérico de recompensa. Así que con todas estas ideas, ya podemos empezar a dar una definición mucho más precisa de lo que es el aprendizaje reforzado. La idea en el aprendizaje reforzado es que un agente aprenderá de su entorno, mediante la observación de su estado y mediante su interacción a través de una serie de acciones por las cuales va a recibir una recompensa. Al final, la idea es que el agente aprenda a ejecutar la mejor acción posible, dependiendo del estado observado, lo que a la larga le permitirá obtener la mayor recompensa positiva posible. En el contexto de PUNK, esto equivale simplemente a ganar el juego. Y esta misma definición la podemos aplicar a diferentes conceptos. Por ejemplo, si pensamos en un robot, el agente será el programa que controla su movimiento, el entorno va a ser el mundo real, y los estados son los posibles obstáculos que vaya encontrando en el camino. Las acciones serán los movimientos que ejecuta el robot y la recompensa puede ser positiva o negativa. Positiva si llega la meta final al destino y negativa si se pierde en el camino o si de pronto cae con un obstáculo. Bien, con todo esto ya tenemos una idea general de lo que es el aprendizaje reforzado, pero nos queda una pregunta fundamental. ¿Cómo lograr que este agente logre aprender a través de su interacción con el entorno? ¿Y qué tiene que ver en todo esto el Machine Learning? En esencia, hay dos maneras de hacerlo, dependiendo si el agente conoce en detalle el entorno o solo una parte de él. Cuando se conoce en detalle el entorno y todas sus reglas de juego, lo que tenemos es el aprendizaje reforzado basado en modelos. Un ejemplo clásico de este tipo de aprendizaje reforzado es por ejemplo el juego Go. Con antelación en este juego, el agente puede conocer las reglas, los movimientos que puede realizar y el tamaño del tablero. Con este modelo, con estos detalles, el agente puede planear con antelación su siguiente movida y puede analizar las implicaciones de este movimiento o elegir otras alternativas. Alpha 0, desarrollado precisamente por DeepMind en 2017, es un ejemplo de un algoritmo de aprendizaje reforzado basado en modelos. El problema de este tipo de algoritmos es que solo en contadas ocasiones se tiene acceso completo a la información detallada del entorno para poder construir este modelo. En la mayoría de las aplicaciones reales solo se tiene acceso parcial. Y en este caso hablamos de aprendizaje reforzado libre de modelos, al que pertenecen la mayoría de los algoritmos que se usan en la actualidad. En este caso, el agente tiene que aprender a tomar decisiones por prueba y error, porque realmente no conoce todos los detalles del entorno. En realidad tiene acceso solo a dos elementos, los estados de este entorno y la recompensa que obtiene por sus acciones. Un ejemplo de esto es precisamente la inteligencia artificial que en 2013 desarrolló DeepMind y que fue capaz de vencer al ser humano en varios juegos de Atari. Pero para entender cómo funcionan los diferentes algoritmos de aprendizaje reforzado libres de modelos, necesitamos hablar de política. Pero no, no, no, no es la política tradicional y aburrida de nuestros países. En este caso se refiere al cerebro de nuestro agente, es decir, al programa de computador que le permite decidir qué acciones tomar dependiendo del estado observado. Por ejemplo, si tenemos un juego hipotético en el cual el agente debe recolectar un diamante y obtener al final el puntaje más alto posible, en este caso la política le permitirá determinar la ruta más adecuada para evitar la mayor cantidad de penalizaciones y así lograr al final la recompensa positiva más alta. Así que en el aprendizaje reforzado libre de modelos, la idea es poder contar con un método o un algoritmo que permita calcular esta política y que esto de su vez le permita al agente desplazarse o desenvolverse de la forma más óptima posible en este entorno. Y para esto existen esencialmente dos algoritmos que son los pilares fundamentales del aprendizaje reforzado moderno, las políticas de gradientes y el Q-Learning. Hablemos primero de la política de gradientes. La idea en este caso es que dado un estado en particular, el algoritmo sea capaz de predecir la acción a realizar, maximizando de esta forma la recompensa total. Volvamos a nuestro juego hipotético. En este caso, el agente puede ejecutar cuatro posibles acciones. Supongamos que diseñamos un algoritmo de política con cuatro parámetros, cada uno indicando la probabilidad de que el agente se desplace en una de esas direcciones. Una forma de entrenar al agente es definiendo, por ejemplo, un set inicial de valores para estos cuatro parámetros, donde cada valor indica la probabilidad de que el agente se mueva en dicha dirección. Luego, debemos hacer que el agente se mueva hasta que llegue a la meta, es decir, hasta el final del episodio y calcular la recompensa total obtenida. Después, modificamos ligeramente los parámetros, ejecutamos el episodio y calculamos la recompensa obtenida. Y repetimos una y otra vez hasta lograr afinar los parámetros de forma tal que se obtenga la recompensa más alta posible. Así que con la política de gradientes, la idea es reajustar iterativamente los parámetros de este algoritmo y la dirección en la que nos tenemos que mover con este ajuste es precisamente la de la máxima variación o el máximo gradiente de la recompensa calculada finalmente. Aunque para nuestro ejemplo hipotético, esta forma de actualizar los parámetros funciona bastante bien en un caso real, es decir, un agente que puede ejecutar múltiples acciones. Y en un escenario con muchísimos más estados, el problema se hace más complejo y es casi como buscar una aguja en un pajar. Afortunadamente, una solución a esta inconveniente no se puede realizar precisamente en el Machine Learning, pero esto lo vamos a ver un poco más adelante. Por ahora hablemos del segundo algoritmo más importante para entrenar nuestro agente, el Q-Learning. En ese caso, el algoritmo Q-Learning no generará directamente una predicción de la acción a realizar. En su lugar, el método permite calcular para cada par de estados y acciones la máxima recompensa que se obtendrá. Para entender esto, volvamos al caso de nuestro pequeño juego. Que nuestro agente se encuentra en el estado inicial y que a partir de ese estado puede ejecutar una de tres posibles acciones. Lo ideal sería seleccionar la acción que resultase en el máximo puntaje posible al final del juego. Si tuviéramos una función mágica que con antelación, es decir, muchas jugadas atrás, nos permitiera predecir la acción más inmediata a tomar, pues el agente tendría altas probabilidades de culminar exitosamente ese juego. Pues a esta función mágica la vamos a llamar la función Q, de quality o calidad. Y va a representar qué tan buena será la acción que tomará el agente en un estado determinado. Así que lo que hace el Q-Learning es encontrar iterativamente esta función a medida que el agente se va moviendo por el entorno paso a paso. Volviendo a las recompensas predefinidas para nuestro juego, vemos que las casillas blancas entregan una penalización de menos un punto, la que contiene fuego menos 100, mientras que la que corresponde al diamante dará un premio de 50 puntos. Este esquema de puntuación predefinido permitirá que el agente aprenda a moverse por las casillas blancas, que tienen una penalidad menor, a que en lo posible evite el fuego con mayor penalidad, y a que busque la ruta más corta, es decir, la que tenga menos penalización, para llegar al diamante con la máxima recompensa. Para lograr esto, en cada una de estas visitas se va llenando una tabla que tendrá tantas filas como estados allá disponibles, y tantas columnas como acciones puede ejecutar el agente. Al final, cada celda va a contener el valor máximo de la recompensa esperada para cada estado y cada acción. En nuestro juego se tendrá entonces una tabla con siete filas, que corresponden a los siete estados del juego, y cuatro columnas que corresponden a los cuatro posibles movimientos del agente. Al comienzo, todos los valores de la tabla serán cero, pues el agente no ha explorado el entorno. Luego comienza a desplazarse por el tablero, y en cada caso almacena el puntaje en la posición correspondiente de la tabla. Al comienzo del algoritmo se moverá de manera aleatoria, pero cuando el procedimiento se repite muchas veces, poco a poco nuestro agente irá descubriendo el patrón existente. Los valores más altos de la tabla corresponderán a los pares estado-acción que arrojen el mayor puntaje posible. Este algoritmo de QLearning usa un enfoque de fuerza bruta, es decir, se tienen que visitar todos los posibles estados y para cada estado calcular todas las posibles acciones, y el método funciona bastante bien para nuestro juego hipotético, porque teníamos tan solo 28 celdas, pero para un caso práctico realmente no resulta viable. Así que la gran desventaja del algoritmo del QLearning es similar a la de política de gradientes, que no es un algoritmo escalable, es decir, entre más acciones y más estados se tengan, más difícil va a resultar entrenar el agente. Afortunadamente, muchos de estos inconvenientes se pueden resolver usando el Machine Learning. Así que ahora vamos a ver cómo combinar estos dos algoritmos básicos con todo el potencial que tienen las redes neuronales. Una red neuronal es básicamente una arquitectura de Machine Learning que es capaz de generalizar conocimiento, es decir, que es entrenada con una serie de datos, a partir de ese entrenamiento es capaz de detectar unos patrones y luego es capaz de transferir ese conocimiento para detectar patrones en datos que previamente no ha visto. Si quieren aprender más sobre las redes neuronales tienen que ver por acá este video que les voy a dejar donde les explico en detalle cómo es que funciona. Así que si en los dos algoritmos reemplazamos los bloques funcionales por redes neuronales, en cada caso tendremos un agente con un cerebro más potente. Con este aprendizaje por refuerzo profundo, es como DeepMind y AlphaZero lograron vencer al ser humano. Al usar las redes neuronales en combinación con estos sistemas de aprendizaje reforzado, ahora es posible analizar escenarios mucho más complejos, tanto continuos como discretos, incluso con muchos más estados o muchas más acciones. Y todo esto gracias a la capacidad que tienen las redes neuronales de generalizar y de encontrar patrones en los datos, que para nosotros los seres humanos es difícil percibir a simple vista. El potencial del aprendizaje por refuerzo profundo es inmenso, porque ahora tenemos agentes que son capaces de aprender de forma autónoma de su entorno simplemente observándolo e interactuando con él, y que son capaces de desarrollar tareas relativamente complejas que incluso muchas veces superan la capacidad del ser humano. Y esto no se reduce simplemente a los videojuegos o la robótica, como hemos visto hasta el momento. El aprendizaje reforzado profundo tiene, por ejemplo, un potencial inmenso en la industria, en donde se pueden entrenar agentes capaces de optimizar el consumo de energía o el uso de materia prima, o mejorar el transporte en las bodegas y las cadenas de suministro, o logrando desarrollar robots capaces de ejecutar autónomamente tareas en entornos hostiles. Incluso resulta posible desarrollar sistemas autónomos que aceleren el proceso de desarrollo de medicamentos. Así que el aprendizaje reforzado y en particular el aprendizaje por refuerzo profundo es tal vez una de las áreas más prometedoras del machine learning en la actualidad. Y probablemente en los próximos años nos va a traer desarrollos que cada vez se van a parecer más a lo que muchas veces vemos solo en ciencia ficción, es decir, la capacidad de desarrollar máquinas inteligentes que aprendan de la experiencia de forma muy similar a como lo hacemos nosotros los seres humanos. Y bien, esto es todo por ahora. Espero que les haya gustado lo que les acabo de contar. Si todavía no se han suscrito los invito a suscribirse y nos vemos en el próximo video.
