 En los videos anteriores hablamos de los árboles de decisión y de cómo usarlos para resolver problemas de regresión y clasificación. El entrenamiento de estos árboles se hace generando de forma recurrente particiones sobre el espacio de características, para obtener agrupaciones de datos cada vez más uniformes. Y esta uniformidad se mide con el índice Gini en el caso de la clasificación y con el error cuadrático medio en el caso de la regresión. Y para clasificar un dato nuevo o para realizar la regresión simplemente verificamos las condiciones para ese dato en cada nodo del árbol, hasta que sea asignado a una hoja en particular. Pues a pesar de que estos árboles de decisión son arquitecturas muy poderosas, tienen una gran desventaja, el overfitting. Esto quiere decir que funciona muy bien cuando los estamos entrenando, pero no tanto cuando queremos hacer predicciones con nuevos datos. Y es esta limitación la que precisamente dio origen a los bosques aleatorios, que son uno de los algoritmos más usados y más poderosos del Machine Learning. Entonces en este video hablaremos de esos bosques aleatorios, de cómo entrenarlos y validarlos y de cómo usarlos para hacer una predicción, de cómo elegir los parámetros para su entrenamiento y de las cosas en común y las ventajas que tienen frente a los árboles de decisión. Para entender todos los detalles les recomiendo ver los dos vídeos anteriores, en donde les explico cómo funcionan los árboles de clasificación y los árboles de regresión. Pero antes de entrar en todo esto les quiero contar que el próximo 10 de julio voy a hacer el lanzamiento de la Academia Online de Cursos de Machine Learning y Ciencia de Datos. Así que si están interesados pueden visitar codificandovids.com y diligenciar el formulario para que puedan recibir más información a vuelta de correo. Bien, hablemos primero del gran problema de los árboles de decisión, el compromiso entre el bias o sesgo y la varianza. Los árboles de decisión tienen la característica de que al momento de entrenarlos se ajustan bastante bien a ese set de entrenamiento, lo que se conoce como un bias bajo, pero al momento de hacer la predicción el error o varianza es relativamente alto y esto se debe a que son muy sensibles a los datos de entrenamiento. Una ligera variación en tan solo algunos de ellos puede dar origen a un árbol totalmente diferente, lo que dificulta aún más hacer predicciones con datos nuevos. Y cuando tenemos un bias bajo y una varianza alta estamos precisamente ante un problema de overfitting, pues resulta que los bosques aleatorios permiten resolver este problema, preservando lo mejor de los árboles de decisión que es su bias bajo pero además logrando reducir su varianza. Bueno, ¿y cómo se logra esto? Pues esencialmente introduciendo dos variantes a estos árboles de decisión, la primera es que en lugar de entrenar un único árbol se entrenan varios, usualmente decenas o cientos, de ahí precisamente el término bosques, pero esto no es suficiente porque si entrenamos cada árbol del bosque con el mismo set de datos, seguiríamos teniendo el mismo problema de la varianza alta. Con la aleatoriedad cada árbol será entrenado con un subset diferente, así que si el set de entrenamiento tiene algo de ruido, probablemente este afectará a algunos árboles pero no a la totalidad del bosque. Además, al agregar los resultados para generar la predicción, los árboles que no funcionan también no tendrán un impacto significativo en ese resultado final, así que al combinar estos dos elementos, la aleatoriedad y la agregación se logra reducir la varianza de los árboles individuales. Entonces un bosque aleatorio es como el equipo de jueces que evalúa a los clavadistas en los Juegos Olímpicos, habrá jueces poco exigentes o demasiado estrictos, es decir que generarán una alta varianza que asignarán puntajes o muy altos o muy bajos a los participantes, pero al tener 7 u 8 jueces y obtener la puntuación de forma conjunta, es decir agregando los resultados, evitamos tener puntajes excesivamente altos o demasiado bajos. Bien, ya tenemos una idea general de cómo funcionan estos bosques aleatorios, ahora veamos cómo hacer el entrenamiento, y para eso vamos a suponer que vamos a resolver una tarea de clasificación, aunque si la tarea es de regresión el procedimiento será equivalente. Partamos de un set de datos con 6 ejemplos de entrenamiento, es decir 6 filas, cada uno con 4 características, es decir 4 columnas. El primer paso es definir el número de árboles que tendrá el bosque, para facilitar la explicación supongamos que este número es 5, aunque en realidad se usan decenas o cientos de árboles, pero de esto hablaremos más adelante. El segundo paso es crear el subset de entrenamiento de cada árbol, introduciendo precisamente un componente aleatorio, como tenemos 5 árboles la idea es crear 5 subsets de entrenamiento a partir del set original, para lograr esto debemos hacer algo que se llama el muestreo con reemplazo o bootstrapping, en cada caso vamos a tomar al azar observaciones, es decir filas, del set original hasta completar un total de 6 observaciones, el término reemplazo hace referencia a que una misma fila podrá aparecer varias veces en el subset que estamos creando, por ejemplo digamos que para crear el primer subset se toman aleatoriamente las observaciones 1, 2, 5, 1, 1 y 3, y acá el reemplazo implica que la primera observación se repite 3 veces, para el segundo subset tomaremos al azar y con reemplazo otro grupo de observaciones, por ejemplo las filas 6, 3, 2, 5, 6 y 2, de nuevo el reemplazo implica que las filas 6 y 2 aparecerán repetidas en el subset, y así repetimos este procedimiento hasta obtener los 5 subsets de entrenamiento, después de esto viene el entrenamiento que consiste en tomar cada subset y realizar de forma recurrente las particiones en el espacio de características, para crear cada uno de los árboles, y aquí es donde introducimos un segundo elemento de aleatoriedad, porque en lugar de tomar todas las características o variables disponibles, vamos a tomar aleatoriamente solo una parte, así que para obtener cada nodo de cada árbol, es decir las particiones, en lugar de escoger las cuatro columnas que tenemos disponibles, al azar vamos a escoger dos o tres de ellas, supongamos que para este ejemplo en particular fijamos ese número en 2, pero más adelante vamos a ver cuáles son los criterios para determinar cuál es el número más adecuado de características a extraer, si vamos al primer árbol para determinar su primer nodo, seleccionamos entonces de forma aleatoria dos de las características, supongamos que la 1 y la 4, y obtenemos la mejor partición, para el segundo nodo nuevamente escogemos de forma aleatoria otras dos características, supongamos que la 2 y la 4, y calculamos la mejor partición, y repetimos este procedimiento una y otra vez hasta construir el árbol completo y alcanzar el criterio de parada, y esto lo repetimos para cada uno de los árboles que conforman el bosque, y con esto tenemos listo el entrenamiento, que es muy similar a lo que se hace convencionalmente con los árboles de decisión, con la única diferencia que en lugar de tomar todas las características, escogemos solo unas cuantas de ellas de forma totalmente aleatoria, con el bosque aleatorio ya entrenado es fácil realizar la predicción, simplemente se introduce el nuevo dato a cada árbol, se realiza la clasificación individual, y se escoge la categoría asignada por la mayoría de los árboles, si la tarea fuese de regresión el procedimiento sería muy similar, se realiza la regresión individual y el valor final sería simplemente el promedio de la predicción hecha por cada árbol, acá es importante que resumamos los dos mecanismos esenciales detrás del funcionamiento de estos bosques aleatorios, el bootstrapping durante el entrenamiento y la agregación de los resultados durante la predicción, si miramos en detalle el bootstrapping nos daremos cuenta de que no todas estas observaciones seleccionadas aleatoriamente quedarán incluidas al final en los subsets de entrenamiento, en realidad aproximadamente una tercera parte de ellas quedará por fuera y no será usada durante el entrenamiento, así que a diferencia de otros algoritmos del machine learning donde al inicio el set de datos se divide entre entrenamiento y validación, en el caso de los bosques aleatorios no ocurre lo mismo, porque el set de validación se extrae precisamente de las muestras que no fueron tomadas aleatoriamente del set de entrenamiento original, estos ejemplos de entrenamiento se conocen como muestras por fuera de la bolsa o out of back samples, pero si nos devolvemos al ejemplo que acabamos de ver nos quedan todavía tres preguntas por resolver, ¿cómo sabemos cuántos árboles usar? ¿cómo sabemos hasta dónde hacer crecer los árboles? y ¿cómo sabemos cuántas características seleccionar aleatoriamente durante el entrenamiento? Para ajustar estos hiperparámetros podemos tomar las muestras que están fuera de la bolsa, introducirlas al bosque aleatorio y mirar cómo se comporta el error de la predicción con el cambio de estos parámetros. Podemos primero fijar el número de características y el criterio de parada, y entrenar múltiples bosques cambiando progresivamente el número de árboles que lo conforman, en este caso veremos que a medida que se tienen más y más árboles el error irá disminuyendo, usualmente en aplicaciones prácticas se usan entre 100 y 200 árboles. Con el número de árboles ya determinado fijamos el criterio de parada y repetimos el procedimiento cambiando el número de características y escogemos aquel que genere el menor error. Finalmente con el número de árboles y de características fijo variamos el criterio de parada y escogemos el que arroje el menor error, recuerden que el criterio de parada puede ser por ejemplo el mínimo número de datos de una hoja. Otra de las ventajas interesantes de estos bosques aleatorios es que, al igual que con los árboles de decisión, podemos definir el orden de importancia de las características, para lo cual podemos tomar por ejemplo el índice Gini en el caso de la clasificación o el error cuadrático medio en el caso de la regresión, porque en ambos casos ellos miden precisamente la calidad con que se está realizando esa partición. Así que si durante el entrenamiento del bosque aleatorio mantenemos un registro de todas estas puntuaciones, al final podremos obtener el puntaje individual de cada característica y podremos determinar cuales de ellas generan las mejores particiones y cuales no. Y esta información puede resultar súper útil al momento de desarrollar un modelo de Machine Learning, porque podemos seleccionar las características más relevantes y descartar otras que tengan menor importancia. Bien, y con esto ya tenemos un panorama completo y detallado de qué son y cómo funcionan estos bosques aleatorios, uno de los algoritmos más importantes del Machine Learning y que en la práctica es de los más usados para resolver una gran variedad de problemas. En resumen, lo que hace este método es usar el bagging, es decir el bootstrapping y la agregación para entrenar múltiples árboles y luego combinar sus resultados individuales, logrando así un bosque aleatorio que va a tener un bias y una varianza mucho más bajos. Bien, y esto es todo. Como siempre, los invito a compartir este video con sus amigos y conocidos y a darle un pulgar hacia arriba de me gusta. Les envío un saludo y nos vemos en el próximo video.
