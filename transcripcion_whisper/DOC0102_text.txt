 En un video anterior hablamos de la matriz de confusión y de cómo utilizarla para caracterizar el desempeño de un clasificador. Sin embargo vimos que cuando el set de datos está desbalanceado, la matriz de confusión no es suficiente para caracterizar ese desempeño. Así que en este video vamos a partir de esos conceptos vistos acerca de la matriz de confusión y vamos a hablar de dos nuevas métricas de desempeño, el precision y el recall, que permiten caracterizar un clasificador cuando tenemos un set de datos desbalanceado. Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la Academia Online y servicios en ciencia de datos y machine learning. Además en codificandovids.com me podrán contactar si están interesados en servicios de formación a la medida para personas o empresas o en el desarrollo de proyectos y asesorías en las áreas de ciencia de datos, machine learning o inteligencia artificial. Y ahora sí comencemos. Entonces recuerden que el notebook que vamos a usar en este video lo van a tener disponible el enlace en la descripción del video. Entonces de qué vamos a hablar acá, vamos a ver una breve introducción en donde hablaremos de lo comentado en el video anterior acerca de la exactitud y la matriz de confusión. Luego vamos a ver limitaciones de esas dos métricas o de esas dos herramientas para evaluar el desempeño de un clasificador. Luego vamos a hablar de la primera métrica de importancia en este caso que es el precision y vamos a ver cómo calcularla. Luego hablaremos del recall y veremos cómo calcularla. Luego analizaremos en qué situaciones es mejor usar el precision y en cuáles es mejor usar el recall. Y dependiendo de esto entonces encontraremos que en ocasiones las dos métricas son igualmente importantes. Entonces ahí hablaremos en el punto 6 del F score y cómo calcularlo. Y luego veremos todas estas ideas cómo llevarlas fácilmente a clasificadores multiclase. Y finalizaremos entonces con una conclusión de los puntos más importantes a tener en cuenta al momento de usar el precision, el recall y el F score. Entonces como lo vimos en un video anterior, en ese caso hablábamos de la exactitud y de la matriz de confusión. Cuando teníamos un clasificador que el clasificador podía ser binario o multiclase, pero podríamos usar entonces esas dos métricas o esas dos herramientas. Entonces ¿qué es la exactitud? Pues básicamente mide la cantidad de aciertos o del total de datos que tenemos, cuántos fueron clasificados correctamente, la proporción de aciertos. Eso independientemente de la categoría. La matriz de confusión nos permite ver de forma un poco más detallada el número de aciertos y desaciertos para cada categoría. Permite, digamos, a través de una tabla diferenciar esos aciertos y desaciertos en cada una de esas categorías. Sin embargo, cuando tenemos datos desbalanceados, es decir, por ejemplo, en el caso que tengamos dos categorías clasificación binaria, si tenemos muchos más datos de una categoría que de otra, en esos casos no resultan lo suficientemente adecuados la exactitud y la matriz de confusión. Entendamos eso un poco más en detalle. De esto hablamos en un video anterior, pero acá lo vamos a resumir. Entonces, por ejemplo, acá tenemos un set de datos desbalanceados. Supongamos que tenemos una gran cantidad de sujetos normales. Supongamos que estamos hablando de una aplicación donde queremos clasificar al sujeto como normal o anormal, dependiendo, digamos, de su ritmo cardiaco. Entonces acá tenemos una gran cantidad de sujetos normales. Tenemos 90 sujetos normales, los de color verde, y tenemos tan solo 10 sujetos, entre comillas, anormales. Entonces, si llevamos eso a un clasificador, supongamos que ese clasificador nos entrega para los sujetos normales 89 aciertos y tan solo un desacierto. Y para los sujetos anormales, los 10 anormales, tenemos un solo acierto y 9 desaciertos. Si calculamos la exactitud de este clasificador, pues simplemente tenemos que sumar el número de aciertos que tendríamos 89 y 1, es decir, 90 aciertos en total de la cantidad total de datos que serían en total 100. Entonces tendremos una exactitud de .9 o del 90%. ¿Qué nos diría esa exactitud? Que en el 90% de los casos, el sujeto va a quedar clasificado correctamente. Lo cual no es cierto porque como estamos poniendo todo en una misma bolsa, no estamos diferenciando normales de anormales, sino que simplemente estamos calculando el número de aciertos, pues acá ese 90% nos va a enmascarar el comportamiento inadecuado que tiene el clasificador con los sujetos anormales. Porque en el caso de los anormales está clasificando incorrectamente el 90% y solo el 10% de ellos está siendo clasificado correctamente. Pero esto no lo podemos ver a través del accuracy o de la exactitud. Si construimos la matriz de confusión, por ejemplo acá tendríamos un caso de una matriz de confusión, podemos ver un poco más en detalle esos comportamientos, entonces podemos ver de los que realmente son normales, cuántos fueron clasificados como normales, lo que está en verde, este 89, y de los que son anormales, cuántos fueron clasificados correctamente como anormales, digamos que fueron 8. Y tenemos entonces los falsos positivos y los falsos negativos, es decir, de los normales, ¿cuántos fueron clasificados incorrectamente como anormales? ¿Eso serían los falsos negativos? Si suponemos que los normales son los positivos y los anormales son los casos negativos, tendríamos dos falsos negativos. y acá tenemos un anormal que fue clasificado incorrectamente como normal esto sería un falso positivo, entonces ahí tenemos la matriz de confusión, nos permite ver un poco más en detalle esa situación pero no nos permite determinar o cuantificar si este número de falsos negativos o este número de falsos positivos es adecuado o no es adecuado entonces ahí es donde aparecen las dos métricas de precision y recall hablemos del precision inicialmente y veamos entonces cómo calcular entonces partamos de algunas definiciones previas, lo que acabamos de mencionar en el contexto del problema que estemos resolviendo, acá todo esto va a ser referencia inicialmente a clasificadores binarios es decir, sólo tenemos dos categorías y luego en la parte final del video veremos cómo extender esto a la clasificación multiclase suponiendo que tenemos dos categorías, entonces una primera definición es hablar de los positivos y los negativos eso depende mucho de la aplicación que estemos desarrollando si volvemos a este ejemplo vamos a suponer que los positivos entonces van a ser los sujetos normales que no tienen problemas cardíacos y los negativos son los sujetos anormales ¿cuáles son entonces los verdaderos positivos? los que eran normales, los que sabemos que son normales y fueron clasificados correctamente como normales ¿cuáles son los falsos positivos? los que eran anormales y que incorrectamente fueron clasificados como normales esos son los falsos positivos, los verdaderos negativos entonces eran o serán la cantidad de anormales que fueron clasificados correctamente como anormales y los falsos negativos son aquellos sujetos normales que fueron clasificados incorrectamente como anormales entonces aquí tenemos esas cuatro posibles situaciones y esto es importante porque estas definiciones las necesitamos para poder calcular el precision y el recall entonces, calculemos el precision ¿qué es el precision? veamos primero una definición entonces de todo lo que haya sido clasificado como positivo o sea como normal en nuestro caso particular ¿qué es realmente positivo? entonces si volvemos acá a nuestra tabla ¿qué es lo que tenemos clasificado como positivo? acá tenemos las categorías predichas y positivo para nosotros es normal entonces todo esto, esta columna del lado izquierdo es todo lo que ha sido clasificado como positivo por el modelo este 89 y este 1 pero de todo esto, acá vemos que hay una porción lo que está en color rojo en esa columna que realmente no es positivo porque es un sujeto anormal que fue clasificado incorrectamente como normal un sujeto negativo que fue clasificado incorrectamente como positivo entonces acá la idea del precision es mirar de todo esto que fue clasificado como positivo ¿qué es lo que realmente, qué proporción realmente es positivo? en nuestro caso normal entonces ¿cómo se calcula? el precision se calcula simplemente dividiendo los verdaderos positivos sobre la suma de los verdaderos positivos más los falsos positivos, la suma de los verdaderos positivos más los falsos positivos es precisamente todo aquello que fue clasificado como positivo y eso es un número que está entre 0 y 1 o que si lo representamos como un porcentaje va de 0 a 100 idealmente el valor máximo debería ser 100% si fuese 100% quiere decir que no tenemos falsos positivos y que todo lo que fue clasificado como positivo realmente es positivo ese sería el valor ideal acá veamos un cálculo simple entonces vamos con non-py vamos a introducir esa matriz de confusión que tenemos acá en el dibujo introducimos la matriz de confusión y simplemente entonces acá vamos a calcular de manera individual los verdaderos positivos entonces los verdaderos positivos son normales, clasificados como normales es decir el primer elemento de esta matriz es decir el elemento 00 y los falsos positivos son los anormales que fueron clasificados como normales es decir, segunda fila, primera columna de esa matriz que la indexamos con 1, 0 y entonces acá tendremos 89 verdaderos positivos y un falso negativo y el precision es simplemente entonces calcular la relación de todos los que fueron clasificados como casos positivos que proporción realmente son datos positivos y entonces tendremos acá un precision del 98.9% y esta es entonces una primera métrica una segunda métrica es el recall entonces si el precision se enfocaba acá en tener o en medir los falsos positivos fíjense que indirectamente está midiendo los falsos positivos el recall se va a enfocar en los falsos negativos entonces ¿qué es lo que ocurre con el precision? que no nos dice nada acerca del desempeño del clasificador frente a esas predicciones que fueron negativas los falsos negativos entonces ¿cómo definimos el recall? de todo lo que sabemos que es positivo en nuestro caso por ejemplo sujetos normales ¿qué proporción fue clasificada realmente como positivo? entonces volvamos acá a nuestra tabla ¿qué cantidad de datos sabemos que son normales o positivos? son esta fila todo lo que sabemos que es normal tenemos 89 que fueron clasificados correctamente y 2 que fueron clasificados incorrectamente como anormales pero estos 91 datos sabemos que realmente son positivos entonces de estos 91 datos positivos ¿cuántos fueron realmente clasificados como positivos? estos 89 entonces esa proporción de 89 entre el 91 que es el total de positivos es lo que es precisamente el recall entonces ¿cómo se calcula ese recall? la tasa de positivos o los verdaderos positivos sobre los verdaderos positivos más los que fueron clasificados incorrectamente como negativos y nuevamente esto tiene un rango de valores de 0 a 1 donde el valor ideal es 1 o 100% que sería cuando tenemos una tasa de falsos negativos exactamente igual a 0 es decir todos los que eran positivos fueron clasificados correctamente como positivos entonces veamos acá nuevamente cómo sería el cálculo simplemente tenemos que calcular los falsos negativos porque los verdaderos positivos ya los tenemos del cálculo del precision entonces los falsos negativos simplemente van a ser este número 2 de acá es decir los normales que fueron clasificados incorrectamente como anormales que es la fila 1 columna 2 de esta matriz de confusión que se indexa precisamente como 0,1 entonces esos son los falsos negativos vamos a imprimir acá en pantalla los falsos negativos los verdaderos positivos y calculamos el recall y entonces al hacer eso pues tenemos nuestra matriz de confusión tenemos acá este primer elemento son los verdaderos positivos y este elemento del lado derecho son los falsos negativos y el recall nos dio 97.8% un valor relativamente alto si esto es un ejemplo hipotético entonces tenemos un precision de 98.9 y un recall de 97.8% entonces cuál de los dos es mejor el precision o el recall es decir cuando yo esté caracterizando un modelo que ya entrenamos para clasificar datos binarios medimos el precision, medimos el recall pero cuál de los dos es mejor eso depende eso depende de la aplicación entonces ahí tendremos que mirar el uso que le vamos a dar a ese clasificador entonces acá por ejemplo si lo que nos interesa es minimizar la cantidad de falsos positivos es decir lo que nos interesa es reducir la cantidad de anormales en nuestro caso particular por ejemplo que sean detectados como normales es decir nos interesa reducir este número de falsos positivos entonces observemos que acá el precision está midiendo indirectamente el número de falsos positivos si reducimos el número de falsos positivos este precision tiende a ir hacia arriba se incrementa entonces la idea es para minimizar esos falsos positivos deberíamos entonces enfocarnos en el precision y en obtener el precision más alto posible por el contrario si en la aplicación que estemos desarrollando lo que nos interesa es reducir la tasa de falsos negativos este Fn que aparece acá entonces vemos que el recall se relaciona con esos falsos negativos en nuestro caso particular que serían los normales que fueron detectados como anormales entonces en ese caso si queremos minimizar esa cantidad de falsos negativos nos enfocamos en el recall y deberíamos buscar que ese recall sea lo más alto posible si es alto es porque precisamente estamos reduciendo el número de falsos negativos pero puede haber situaciones en la que de pronto nos interese reducir esas dos métricas es decir, reducir tanto los falsos positivos como los falsos negativos entonces no podemos darle más importancia o al precision o al recall sino que tenemos que darle igual importancia a los dos entonces acá es donde entra precisamente el Fscore que es una métrica que combina el precision y el recall en un solo valor entonces ese Fscore tiene una ecuación para el cálculo que depende de un parámetro beta y ese parámetro beta como lo vemos acá entonces simplemente lo que hace es aparece un valor constante que depende de ese valor beta que escojamos y vamos a tener la relación entre la multiplicación del precision por el recall en el numerador sobre beta al cuadrado veces el precision más el recall entonces entendamos como algunos valores extremos por ejemplo, que es el significado de beta entonces beta controla el nivel de importancia que le vamos a dar al precision o al recall por ejemplo si hacemos en esta ecuación del Fscore hacemos beta igual a cero pues acá simplemente nos quedaría un coeficiente de 1 y si hacemos beta igual a cero en este denominador desaparece el precision y nos queda precision por recall sobre el recall entonces nos queda simplemente el precision quiere decir que en este caso cuando beta es igual a cero vamos a descartar por completo el recall y vamos a enfocarnos o darle importancia únicamente al precision si ponemos por ejemplo un beta igual a 0.5 y lo reemplazamos en esta ecuación entonces tendremos esta combinación de acá 1.25 veces el precision por el recall y en el denominador entonces tenemos 0.25 veces el precision y el recall multiplicado por un factor de 1 entonces acá le estaremos dando más importancia al recall que al precision en ese caso particular de 0.5 y el caso especial es cuando beta es igual a 1 cuando beta es igual a 1 que es como la métrica más usada la llamamos el F1 score y es F1 score precisamente porque beta es igual a 1 y entonces en ese caso observemos que acá nos aparece el precision y el recall en el numerador y en el denominador nos aparecen el precision y el recall ponderados por un factor de 1 ambos tienen igual importancia entonces esta sería la métrica a usar cuando queremos darle la misma importancia al precision y al recall y en ese caso entonces en el caso del ejemplo que estamos analizando cómo lo calculamos por ejemplo si calculamos el F1 score entonces simplemente escribimos esta ecuación de acá dos veces el precision que ya calculamos anteriormente por el recall sobre la suma de estos dos y comparemos entonces esos valores entonces el precision nos daba 98.9 el recall nos daba 97.8 y el F1 score nos da 98.3 entonces si queremos que el precision y el recall tengan igual importancia nos enfocamos en esta métrica y tendríamos que buscar hacerla lo más alta posible si logramos hacerla lo más alta posible quiere decir que estamos reduciendo tanto los falsos positivos del precision como los falsos negativos del recall estamos incrementando esas dos métricas ¿cómo extendemos estas ideas entonces a los clasificadores multiclase? pues muy sencillo simplemente entonces ya no tendremos dos categorías sino que podremos tener 3 o 5 o 10 entonces construimos nuestra matriz de confusión nuestra matriz de confusión siempre va a ser una matriz cuadrada entonces si tenemos por ejemplo 5 categorías nuestra matriz será de 5 filas por 5 columnas la diagonal de esa matriz de confusión va a contener los aciertos y lo que esté por fuera de la diagonal va a contener los desaciertos entonces ahí lo que podemos hacer que esa matriz de confusión multiclase de esa ya hablamos en el vídeo anterior entonces construimos esa matriz de confusión multiclase y simplemente podemos calcular el precision o el recall para la categoría correspondiente para cada categoría recordemos que el precision si tenemos la matriz conformada de esta forma el precision es analizar por ejemplo por columnas y el recall es analizar precisamente por filas entonces es aplicar la misma idea pero a cada categoría de manera individual si tenemos 5 categorías calculamos 5 diferentes precision o 5 diferentes recalls o 5 diferentes F1 scores para poder caracterizar entonces el comportamiento del clasificador multiclase en cada una de esas categorías entonces esa es como la idea del precision y el recall cosas importantes a tener entonces acá en cuenta cuando tenemos un set de datos desbalanceados lo más recomendable es no usar la exactitud o el accuracy porque nos pueden mascarar el comportamiento del desempeño del modelo para diferentes categorías en lugar de eso entonces podemos usar el precision y el recall el precision recordemos entonces que es una métrica que indirectamente está midiendo la tasa de falsos positivos mientras que el recall tiene en cuenta la tasa de falsos negativos si queremos reducir la tasa de falsos positivos deberíamos darle prioridad al precision si queremos reducir la tasa de falsos negativos debemos darle prioridad al recall y si queremos darle el mismo nivel de importancia tanto al precision como al recall lo más recomendable es usar el F1 score que combina estas dos métricas en una sola cantidad numérica que es la métrica de desempeño para ese clasificador muy bien acabamos de ver cómo usar el precision y el recall para caracterizar el desempeño de un clasificador y vimos además que la métrica a utilizar depende de si nos interesa minimizar el número de falsos positivos o de falsos negativos sin embargo en algunas aplicaciones vamos a encontrar que este número de aciertos y desaciertos va a depender de un umbral que podemos fijar así que dependiendo del valor de este umbral irán cambiando precisamente los valores del precision y el recall para encontrar el umbral más adecuado dependiendo de nuestra aplicación debemos recurrir a otra herramienta que se conoce como la curva ROC o también podremos usar lo que se conoce como la curva precision recall de las cuales hablaremos en el próximo video recuerden que en la descripción de este video van a encontrar el enlace para poder descargar el notebook y no olviden también dejarme sus dudas y comentarios acerca de este video si les gustó también los invito a compartirlo con sus amigos y conocidos y a darle un pulgar hacia arriba de me gusta y si no se han suscrito los invito a hacerlo pues esto me seguirá motivando a desarrollar este tipo de contenido y a llegar cada vez a más y más gente así que les envío un saludo y nos vemos en el próximo video
