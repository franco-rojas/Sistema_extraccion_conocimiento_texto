 Los árboles de decisión son tal vez el algoritmo más sencillo, pero a la vez uno de los más poderosos del Machine Learning y son muy usados cuando tenemos sets de datos relativamente complejos, pues en este video veremos en detalle qué son y cómo usar los árboles de decisión para resolver un problema de clasificación. En particular vamos a hablar del algoritmo CART, que es el más usado en el Machine Learning para implementar en la actualidad árboles de decisión. Así que sin más preámbulos, comencemos. En la clasificación buscamos entrenar un modelo que sea capaz de determinar la categoría a la que pertenece un dato en particular. La idea es que el modelo aprenda a calcular una frontera de decisión que permita asignar el dato a una u otra categoría. Un ejemplo típico de este tipo de tarea es la clasificación de correos electrónicos entre normales y spam. En videos anteriores vimos cómo por ejemplo la regresión logística o las redes neuronales podrían ser usadas para clasificar sets de datos relativamente complejos. Sin embargo el problema de la regresión logística es que nos permite clasificar datos que son solo linealmente separables, es decir, donde la frontera de decisión es una simple línea recta en dos dimensiones o un plano o un hiper plano en tres o más dimensiones. Mientras que el gran inconveniente de las redes neuronales es que son generalmente modelos de caja negra, que son excelentes clasificadores pero no son fáciles de interpretar. Es decir que usualmente resulta difícil explicar en términos simples por qué la red clasificó un dato de una forma o de otra. Precisamente los árboles de decisión son una solución a este inconveniente porque además de permitirnos clasificar datos relativamente complejos, también nos permiten interpretar fácilmente los resultados de esa predicción. El algoritmo más usado en la actualidad para implementar árboles de decisión se conoce como CARP o árboles de clasificación y regresión por sus siglas en inglés. Lo que vamos a ver de aquí en adelante es en detalle cómo funciona este algoritmo para las tareas de clasificación y en un próximo video veremos cómo funciona para una tarea de regresión. Partamos de un ejemplo sencillo para entender cómo funciona el algoritmo. Supongamos que somos fanáticos del anime y que queremos diferenciar demonios de asesinos de demonios. Cada uno de estos personajes posee ciertas características como el nivel de fuerza o el nivel de letalidad y cada una en una escala de 0 a 20. Para representar esto gráficamente dibujaremos un plano en dos dimensiones, con el eje horizontal igual al nivel de fuerza y el eje vertical igual al nivel de letalidad. Los demonios estarán representados con puntos rojos y los asesinos de demonios con puntos verdes y en total tendremos 20 personajes. A los niveles de fuerza y de letalidad los llamaremos características y aunque por simplicidad estamos considerando únicamente dos características, es importante tener en cuenta que los árboles de decisión funcionan igualmente con sets de datos mucho más complejos, con tres o más características. La idea es encontrar una forma de separar los demonios de los asesinos de demonios, es decir, calcular unas fronteras de decisión que permitan posteriormente clasificar nuevos personajes en una de estas dos categorías. La idea de la clasificación con árboles de decisión es bastante simple. Iterativamente se irán generando particiones binarias, es decir, de a dos agrupaciones, sobre la región de interés, buscando que cada nueva partición genere un subgrupo de datos lo más homogéneo posible. Así, primero se establece una condición y dependiendo de si los datos cumplen o no la condición, tendremos una primera partición en dos subregiones, de ahí el término particiones binarias. Y luego se repite el procedimiento anterior una y otra vez, hasta que al final se obtengan agrupaciones lo más homogéneas posible, es decir, con puntos que pertenezcan en lo posible a una sola categoría. Y esta serie de particiones la podemos representar precisamente a través de un árbol de decisión. El punto de partida del árbol se conoce como la raíz y contiene la primera condición. Este nodo genera la primera partición binaria, lo que se representa gráficamente como dos flechas indicando si se cumple o no la condición. Luego tenemos los nodos internos que corresponden a condiciones adicionales para continuar realizando la partición. Y también las hojas que corresponden a subregiones más allá de las cuales no realizaremos más particiones. La profundidad del árbol es simplemente la trayectoria más larga entre la raíz y una de las hojas, que en nuestro caso corresponde a un árbol de profundidad 3. Ok, hasta acá tenemos una idea general de cómo funciona un árbol de decisión, pero todavía nos quedan por responder algunas preguntas de fondo. Por ejemplo, ¿cómo se construye de forma automática este árbol? ¿Y cómo lograr que en este proceso se generen las agrupaciones lo más homogéneas posibles? Pues aquí precisamente es donde entra en juego el algoritmo CART, que es capaz de generar automáticamente este tipo de particiones. Para medir esta homogeneidad se usa el índice Gini, que mide el grado de impureza de un nodo. Índices Gini iguales a cero indican nodos puros, es decir, con datos que pertenecen a una sola categoría, mientras que índices mayores que cero y con valores hasta de 1 indican nodos con impurezas, es decir, con datos de más de una categoría. Por ejemplo, volviendo a nuestro set de datos, analicemos dos posibles particiones iniciales, X0 menor o igual que 6.5 y X1 menor o igual que 11. Para el primer umbral veremos que la partición del lado izquierdo contendrá únicamente 4 puntos rojos y ningún punto verde, mientras que la partición del lado derecho tendrá 6 puntos rojos y 10 verdes. Esto corresponde a un nodo izquierdo totalmente puro, es decir, un nodo hoja con un índice Gini igual a cero y a un nodo derecho con impurezas con un índice Gini de 0.469. Para el segundo umbral veremos que el nodo izquierdo será puro, pues contendrá 2 puntos rojos y ninguno verde, mientras que el nodo derecho contendrá impurezas, 8 puntos rojos y 10 verdes, que equivalen a un índice Gini igual a 0.494. Para saber cuál de estas dos particiones es la mejor, el algoritmo CART define una función de costo, que asigna un puntaje al nodo padre dependiendo de los valores de los índices Gini individuales de sus nodos hijos. Para entender esto veamos nuevamente las dos posibles particiones. Para calcular la función de costo en cada caso debemos obtener el valor promedio ponderado de la impureza de los nodos hijo a izquierda y derecha. Esto se calcula tomando el índice Gini correspondiente a cada nodo y multiplicándolo por el resultado de dividir el número de datos que pertenecen a la nueva agrupación entre el número total de datos antes de la partición. Así para la primera partición el nodo hijo del lado izquierdo tiene un índice Gini igual a cero, el número de datos resultantes de la partición es 4 puntos rojos y cero verdes y el número total de datos antes de la partición es simplemente el set de datos original, así que para este nodo hijo se tiene una impureza ponderada igual a cero. Para el nodo del lado derecho el índice Gini era 0.469, tras la partición se obtuvieron 6 puntos rojos y 10 verdes y el número total de puntos antes de la partición sigue siendo 20, lo que nos arroja una impureza ponderada de 0.375. Al sumar estos valores ponderados de las impurezas de cada uno de los nodos obtenemos el valor de la función de costo del nodo padre que es igual a 0.375. Si repetimos el mismo procedimiento para la segunda opción, es decir si calculamos las impurezas ponderadas individuales de los nodos hijos y las sumamos, obtenemos un valor para la función de costo igual a 0.446. Y ahora sí tenemos un criterio para definir cuál de las dos particiones es mejor, simplemente tomamos la que tenga el menor valor posible para la función de costo, lo que indica un menor nivel de impureza y por tanto una mejor clasificación. Y con esto ya tenemos la base del algoritmo CART para la clasificación con árboles de decisión, ahora la idea es aplicar este mismo principio de funcionamiento de forma iterativa para clasificar los datos hasta que tengamos las agrupaciones lo más homogéneas posible. Así que si volvemos al nodo raíz seleccionado vemos que el nodo hijo de la izquierda es una hoja y por tanto no haremos más particiones, sin embargo el nodo de la derecha aún contiene impurezas y podemos intentar hacer más particiones. Si repetimos el procedimiento anterior y analizamos todos los umbrales posibles, veremos que la condición X0 menor o igual a 17 es la que generará el menor costo posible. Al aplicar esta condición se generará un nodo izquierdo impuro con tres puntos rojos y diez puntos verdes y un nodo derecho totalmente puro con tres puntos rojos y ningún punto verde. El costo promedio de este nuevo nodo padre será de 0.288 y vemos que es inferior al costo del nodo raíz con lo cual podemos verificar que progresivamente estamos logrando obtener agrupaciones cada vez más homogéneas. Como el nodo izquierdo recién obtenido es impuro podemos continuar dividiéndolo. Si repetimos el mismo procedimiento veremos que la condición X1 menor o igual a 11 generará un nodo padre con el menor costo posible. Esta nueva partición generará un nodo hijo izquierdo impuro con un punto rojo y diez puntos verdes y un nodo derecho totalmente puro con tres puntos rojos y cero puntos verdes. Al calcular la función de costo para el nodo padre recién obtenido tendremos un valor de 0.139 que de nuevo es inferior a la de los nodos anteriores y podríamos continuar subdividiendo este último nodo para mejorar la precisión pero podríamos llegar al extremo de tener overfitting en este árbol de decisión, aunque de esto les voy a hablar en un momento. Pero por ahora resumamos las principales ideas de este algoritmo CART. Para crear la raíz del árbol, es decir la primera partición, se toman todas las características y para cada una de ellas se definen todos los posibles umbrales a que haya lugar. Cada umbral será simplemente el punto intermedio entre dos valores consecutivos de cada característica. Por ejemplo en el caso particular de nuestro set de datos tenemos dos características X0 y X1. Como en total tendremos 20 datos, por cada característica existirán 19 umbrales, así que en total tendremos 38 umbrales por evaluar. Para cada uno de estos umbrales se calcula la partición, nodo izquierdo y nodo derecho, y para cada nodo hijo se calcula el índice GINI. Con esto se calcula la función de costo del nodo padre, que es el promedio ponderado de los índices GINI de sus hijos. Y se toma el umbral o nodo padre resultante, que tendrá la función de costo con el menor valor posible, indicando que la partición obtenida es la más homogénea de todas las analizadas. Una vez se haya realizado esta partición, se repite el mismo procedimiento de forma iterativa para los nodos resultantes, exceptuando los que sean nodos hoja. Bien, una vez entrenado nuestro árbol de decisión podemos hacer una predicción, es decir podemos clasificar un nuevo personaje. Supongamos que tenemos uno con un nivel de fuerza igual a 13 y un nivel de letalidad igual a 17. Para clasificarlo con el árbol ya entrenado simplemente llevamos estas dos características al modelo y comenzamos a evaluar una a una las condiciones en cada uno de los nodos. La categoría en la que será clasificado el personaje será simplemente el nodo en el cual resulte ubicado después de evaluar estas condiciones. Ok, pero recuerdan que en el árbol que acabamos de entrenar teníamos al final un nodo impuro, ¿por qué no continuamos haciendo más particiones en ese nodo? Pues perfectamente podríamos hacerlo, pero el problema es que esto nos llevaría al overfitting. Es decir, en caso de dividir este nodo estaríamos obteniendo dos nuevos nodos que mejorarían la clasificación. Pero las regiones obtenidas serían muy pequeñas y es muy probable que al introducir un nuevo dato para ser analizado por el modelo, este resulte clasificado incorrectamente. Es decir, que el árbol de decisión se ajustará perfectamente a sed de entrenamiento, pero al momento de hacer predicciones no lo hará tan bien. Para evitar este overfitting tenemos esencialmente dos opciones. Una de ellas es restringir el crecimiento del árbol durante el entrenamiento y la segunda es, después del entrenamiento, eliminar algunos nodos, es decir, de alguna forma podar ese árbol. Para restringir el crecimiento del árbol durante el entrenamiento lo que podemos hacer es usar unos hiperparámetros, es decir, unas variables numéricas que definimos al inicio del algoritmo cuando lo estamos programando. Podemos por ejemplo definir la profundidad máxima o el mínimo número de datos que debe tener un nodo o el mínimo número de datos de una hoja. Con estos hiperparámetros podemos evitar la aparición descontrolada de nodos en el árbol. La otra forma de evitar el overfitting en nuestro árbol de decisión se hace después del entrenamiento y consiste en podar o eliminar algunos nodos. Uno de los métodos más usados es el de poda de complejidad de costos, que consiste en definir un hiperparámetro alfa que controla el nivel de overfitting. Con un alfa igual a cero tendremos el árbol de decisión sin ningún recorte y por tanto con un alto overfitting, mientras que a medida que aumenta alfa se eliminarán algunos nodos del árbol, hasta lograr un balance adecuado entre la precisión con el set de entrenamiento y la que se logra con el set de validación. Además de tener la posibilidad de clasificar sets de datos relativamente complejos, una de las grandes ventajas de los árboles de decisión comparado por ejemplo con la regresión logística o las redes neuronales, es la facilidad con que se pueden interpretar los resultados que arroja el modelo. Volviendo a nuestro ejemplo resulta muy sencillo entender la serie de reglas que hacen que un dato sea clasificado en una categoría o en otra, basta simplemente con mirar en detalle cada una de las condiciones en los nodos del árbol de decisión, claro esto resulta sencillo siempre y cuando el árbol y el número de características sean relativamente pequeños. Y otra gran ventaja es que una vez entrenado el árbol podemos determinar fácilmente cual o cuales fueron las características que tuvieron mayor impacto, mayor relevancia al momento de la clasificación. De nuevo si miramos el árbol de decisión obtenido podremos ver que dos de sus tres nodos usan X0, el nivel de fuerza del demonio o del asesino, mientras que solo uno usa X1, el nivel de letalidad, así que podemos decir que la primera característica es más relevante que la segunda al momento de la clasificación, de hecho existen métricas que permiten cuantificar estos niveles de importancia, pero esto lo veremos en detalle en el video tutorial sobre cómo programar árboles de decisión para clasificación en Python. La grandes ventajas es que CART es un algoritmo codicioso, para calcular el umbral óptimo en cada partición evalúa todas las posibles opciones, para nuestro caso fue sencillo porque solo teníamos dos características y 20 datos, pero entre más características y más datos tengamos el algoritmo requerirá más tiempo de entrenamiento. Bien, espero que con esta explicación hayan entendido todos los detalles de cómo funcionan los árboles de decisión y cómo usar el algoritmo CART para la clasificación de datos, de hecho estos mismos árboles de decisión son la base de algoritmos mucho más potentes del Machine Learning, como los bosques aleatorios o los métodos del Gradient Boosting o potenciación del gradiente, que veremos en próximos videos. También en un próximo video veremos un tutorial en Python de cómo usar estos árboles de decisión para clasificar un set de datos. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
