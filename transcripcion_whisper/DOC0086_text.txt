 El término ventana de contexto es un concepto muy muy importante en los grandes modelos de lenguaje, pues en últimas determina la cantidad de texto que estos modelos pueden procesar y generar, así que es fundamental entender este concepto si queremos desarrollar aplicaciones usando estos grandes modelos de lenguaje. Entonces en este video vamos a entender, ¿qué es la ventana de contexto? ¿qué ocurre cuando superamos los límites establecidos por esta ventana? y ¿qué alternativas tenemos cuando queremos procesar texto que es más extenso que el límite establecido por esta ventana de contexto? Pero antes de comenzar los invito a visitar codificandovids.com en donde encontrarán la academia online con cursos de inteligencia artificial, ciencia de datos y machine learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Para entender qué son las ventanas de contexto necesitamos primero entender qué significa el concepto de contexto cuando hablamos de los grandes modelos de lenguaje. Recordemos que en un video anterior vimos que estos grandes modelos de lenguaje se basan en las redes transformer, un tipo de red neuronal especializada en el procesamiento de secuencias, como lo es precisamente el texto. Y recordemos que al momento de procesar el texto estas redes no analizan cada palabra de manera individual, sino que por el contrario tienen en cuenta la relación de cada palabra con las otras palabras del texto. Y dependiendo de esto codifica numéricamente esta información para luego realizar el procesamiento. Por ejemplo, en la frase el perro está jugando mientras que el gato acaba de comer y ahora duerme plácidamente. Para determinar a qué sujeto se refiere la palabra duerme, la red transformer debe analizar la totalidad del texto y las relaciones entre palabras a diferentes niveles para determinar en últimas que el sujeto es el gato y no el perro. Así que en últimas el contexto es la relación que existe entre las diferentes palabras y que permite interpretar y codificar adecuadamente la información del texto analizado por el modelo. Teniendo claro este sencillo concepto, ahora sí podemos ver qué son las ventanas de contexto. De forma sencilla podemos definir la ventana de contexto de un gran modelo de lenguaje como la cantidad de tokens que el modelo puede procesar al momento de interpretar una secuencia de texto. Y acá es necesario que veamos algunos detalles de esta definición. En primer lugar hablemos del término tokens, al cual le dediqué un video anterior. Recordemos que un gran modelo de lenguaje antes de procesar el texto lo divide en pequeños segmentos que pueden corresponder a una palabra o a una porción de una palabra. Estos segmentos son precisamente los tokens y la cantidad de tokens que se obtendrá a partir de un texto depende del idioma. Por ejemplo para GPT-4 en promedio un token equivale a 7 caracteres en inglés a 9 en español y a 12 en chino. En segundo lugar el término procesamiento se refiere a la cantidad de tokens que el modelo puede recibir a la entrada y generar a la salida. Así que en últimas la ventana de contexto es el número total de tokens que el modelo de lenguaje podrá procesar en un momento determinado. Entendamos esto con un ejemplo. El modelo GPT-3.5 de OpenAI, que es la base de la aplicación ChatGPT tiene una ventana de contexto de 4097 tokens. Esto quiere decir que si escribo una pregunta y espero una respuesta de esta aplicación al sumar el texto introducido y generado este no podrá sobrepasar los 4097 tokens que son poco menos de 37 mil caracteres en español. Así que la ventana de contexto nos impone un límite en la cantidad de texto que podemos introducir y esperar a la salida del modelo. Y esto es súper importante cuando queremos desarrollar aplicaciones con este tipo de modelos pues simplemente nos indica que no podemos introducir una cantidad arbitraria de texto y esperar que mágicamente el modelo nos genere la respuesta esperada. Bien, en este punto ya tenemos claro que es la ventana de contexto pero qué pasaría por ejemplo si en una aplicación determinada introducimos al modelo un texto que supere ese límite establecido por la ventana de contexto pues simplemente lo que ocurriría es que en primer lugar el modelo no podría ni siquiera procesar el texto de entrada y mucho menos generar una respuesta. Por ejemplo, supongamos que queremos usar ChatGPT para que nos genere el resumen del texto completo de Don Quijote de la Mancha. Entonces copiamos y pegamos el texto completo y le pedimos que haga el resumen. Y al hacer la solicitud de ChatGPT vemos que nos genera un mensaje de error indicando que el texto introducido es demasiado extenso. Es decir que ni siquiera nos genera una respuesta. Lo que ocurre en este caso es que el texto completo de Don Quijote de la Mancha contiene más de 2 millones de caracteres, es decir más de 226 mil tokens. Y como la ventana de contexto de GPT 3.5, el modelo usado por ChatGPT es de 4096 tokens pues simplemente no resulta posible procesar esa cantidad de texto. Y también lo que puede ocurrir generalmente es que cuando ese texto supere el límite de la ventana de contexto al ingresarlo al modelo pues este va a truncar el texto, es decir lo va a recortar para garantizar que quepa dentro de esa ventana de contexto y que haya un espacio suficiente para generar la respuesta. Sin embargo en este caso lo más probable es que la respuesta del modelo no sea la adecuada porque el texto de entrada está incompleto. Acabamos de ver que es clave que el texto a procesar o a generar por parte del modelo sea acorde con el tamaño de la ventana de contexto. Pero también puede ocurrir que tengamos un texto demasiado extenso y que queramos procesar ese texto con un modelo determinado. Así que el primer paso es asegurarnos de que el modelo que vamos a utilizar para procesar ese texto tenga una ventana de contexto del tamaño adecuado para la longitud del texto que queremos procesar. Por ejemplo, modelos como GPT 3.5 y 4.0 tienen ventanas de contexto que van de los 4096 a los 32768 tokens. Pero recientemente modelos como Cloth de la empresa Anthropic tienen ventanas de contexto de hasta 100.000 tokens. Así que en principio existen diferentes alternativas y tamaños de ventanas de contexto que se podrían ajustar a nuestras necesidades. Sin embargo muchas veces el texto que queremos procesar puede incluso sobrepasar ese límite de los modelos existentes. Así que en este caso podemos usar otras alternativas para intentar hacer este procesamiento. La primera de ellas consiste simplemente en dividir el texto en pequeños fragmentos, lo que se conoce como chunking, donde cada fragmento no supera el tamaño de la ventana de contexto del modelo que estemos usando. Otra alternativa es combinar el chunking con la generación de resúmenes. Por cada fragmento de texto se genera un breve resumen y luego todos los resúmenes se concatenan en un nuevo texto de menor extensión que el original y que puede caber en la ventana de contexto. Y una tercera alternativa es el uso de bases de datos vectoriales. En un próximo video hablaré de este tipo de bases de datos, pero la idea general es que lo que se hace es tomar el texto extenso y representarlo usando lo que se conoce como embeddings, que son simplemente vectores o arreglos de números. De estos embeddings también hablé en un video anterior, pero lo importante acá es que al usarlos, logramos representar el texto de forma compacta a través de vectorias. Y generalmente con estas técnicas lograremos evitar las limitaciones impuestas por las ventanas de contexto. Muy bien, acabamos de entender que son las ventanas de contexto de estos grandes modelos de lenguaje y por qué resultan importantes cuando queremos construir aplicaciones con este tipo de modelos. En últimas la idea general es que todos los modelos de lenguaje tienen un límite máximo de tokens que pueden procesar y generar. Así que no podemos ni introducir ni generar ni generar así que no podemos ni introducir ni esperar que generen una cantidad de texto totalmente arbitraria. Y este límite lo pone precisamente la ventana de contexto. Entonces si sobrepasamos este límite lo que generalmente hará el modelo será truncar el texto de entrada y por tanto terminará generando una respuesta que no es la que estamos esperando. Así que al momento de desarrollar una aplicación lo que les sugiero es primero determinar el tamaño del texto que queremos introducir y generar, ese tamaño la idea es medirlo en tokens y con base en esto es coger un modelo de lenguaje que tenga una ventana de contexto adecuada para ese tamaño. Y si en todo caso el texto supera ese límite de la ventana de contexto pues podemos usar alguna de las técnicas que les mencioné anteriormente para lograr hacer ese procesamiento. Recuerden que acá abajo me pueden dejar sus dudas y comentarios de este video y también recuerden que si les gustó le pueden dar un pulgar hacia arriba de me gusta y compartirlo con todos sus amigos y conocidos pues esto me ayudará a seguir llevando este tipo de contenido cada vez a más y más personas. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique un nuevo video. Por ahora esto es todo les envío un saludo y nos vemos en el próximo video.
