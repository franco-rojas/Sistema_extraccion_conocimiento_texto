 Meta lo acaba de hacer. Acaban de sacar la segunda versión de su modelo Llama. Han sacado Llama 2, en un movimiento que rompe por completo todo el mercado de los modelos de generación de texto. Cogiendo la puerta que poco a poco durante los últimos años, OpenEJ ha ido cerrando y abriéndola de una patada por completo. Y además haciendo esto de la mano de Microsoft, quien ahora parece ver un aliado nuevo para la batalla de las Cías en Meta. ¿Por qué y quién se beneficia de todo esto? Y bueno, ¿cómo salí ganando vosotros en esta lucha de grandes empresas? Hoy vamos a estar comentando por qué la salida de Llama 2 es toda una revolución en el campo de la inteligencia artificial. Y hablando de grandes modelos del lenguaje y de todo el impacto que va a tener esta tecnología en el futuro inmediato, tengo que hablar de este máster de aquí, el máster ejecutivo en inteligencia artificial del IEA, el Instituto de Inteligencia Artificial, que ya hemos comentado aquí en el canal veces anteriores y que llega con una sexta edición, una edición que va a comenzar en el mes de octubre, que podéis matricular ya. Las plazas son limitadas, así que tenéis que ser rápidos. Y este es un máster que está muy bien orientado si queréis aprender sobre todo a esta revolución de la IEA desde un enfoque no técnico, vale, no hace falta ningún conocimiento previo para poder matricular, porque este máster está orientado sobre todo a aprovechar en proyectos reales, a saber cómo utilizar todo el potencial de esta tecnología en vuestros proyectos, a conocer las arquitecturas más importantes que están saliendo ahora y todo esto de la mano de gente muy experta, gente que sabe muy bien trabajar con toda esta tecnología como Andrés Torrubia, quien ha estado en el canal un montón de veces y que ya conocéis. Y luego también pues una serie de expertos que van a estar participando y que son gente protagonista de toda esta revolución que está sucediendo ahora. Tenemos gente como Cristóbal Valenzuela, CEO de Runway, ya sabéis qué empresa es. Tenemos gente de Hanging Face, tenemos gente de Twitter, tenemos gente de todo tipo de todas las especialidades, pues abogados, empresarios, inversores, toda la gente que pueda estar relacionado con toda esta revolución que está ocurriendo, todo aunado en este máster. Y bueno, luego a todos ellos, pues yo me sumo a la charla inaugural. Voy a estar dando esta ponencia de salida y ahí también tendremos oportunidad de conocernos. El máster es online, dura seis meses, tenéis la posibilidad de matricularos ya. Y además si lo hacéis, pues tenéis un código de descuento de 300 euros con el código DOTCCV300. Aprovechad la oportunidad porque este máster merece la pena. Para entender bien por qué Llamados es tan importante, primero tenéis que entender cuál ha sido el desarrollo, la historia, el culebrón que ha ido ocurriendo en los últimos meses. Estamos en febrero de 2023, hace unos meses, y ChadGPT ya lleva un tiempillo en nuestras vidas. Y mientras que OpenAI nos tiene impresionados por las capacidades de su modelo, hay gente en internet que se empieza a hacer la siguiente pregunta. ¿Podría la comunidad open source crear un modelo como ChadGPT pero que esté abierto y disponible para que todo el mundo lo use como quiera? Y la pregunta era legítima, ya que la moral de la comunidad estaba bastante arriba después de lo que había ocurrido en el año 22. Recordemos que cuando en abril de 2022 aparecieron los primeros modelos comerciales de generación de imágenes como Dali2, creíamos que 1. Estos modelos solo los podían hacer las grandes empresas como OpenAI o Google, y 2. Que bueno, de tener un modelo así disponible aún así sería imposible de utilizar y ejecutarlo en nuestro hardware habitual. En agosto de 2022, una empresa, Stability AI, movió ficha e hizo la inversión de entrenar a StableDiffusion, y lo compartió para uso y disfrute de la comunidad y el resto es historia. Entonces, con esa perspectiva ya en febrero de este año, la comunidad online estaba muy motivada para repetir la hazaña que se había logrado con StableDiffusion y Dali2, pero en este caso con ChadGPT. Y de ahí surgieron muchas iniciativas como la de Open Assistant que, como muchos recordaréis, estuvimos apoyando aquí desde este canal. Pero, claro, si StableDiffusion nació fue porque una empresa, Stability AI, decidió pagar la fiesta y hacer la inversión de pagar toda la computación necesaria. Y si ahora queremos tener a nuestro ChadGPT, pues alguien tenía que hacer esa inversión. Y aquí es donde aparece Meta y le pone la mano en el hombro a la comunidad y le dice, Tranquila, aquí tienes a tu modelo, aquí tienes a Yama. Bueno, en realidad el mensaje de Meta no fue tan épico y de hecho la historia que hay detrás de esto es bastante rocambolesca. En el caso de Yama, cuando Meta lo presenta, realmente lo que dicen es, por seguridad no vamos a liberar este modelo en internet, sino que solo lo compartiremos con aquellos investigadores que se registran en este formulario y de forma súper segura mandaremos el archivo por aquí. Una semana. Una semana tardó en filtrarse el modelo de Yama en internet. Y para darle más comedia al asunto, la forma en la que se filtró fue bastante de coña. Lo que pasó es que en el repositorio de GitHub, oficial de Meta, donde se compartía información del modelo, un usuario hizo un pull request donde daba una alternativa a eso de rellenar el formulario de seguridad para descargar de los servidores de Meta. Lo que ofrecía era un enlace a un torrent donde podías descargarte el modelo, tú y cualquier persona, según decía, para ahorrar consumo de ancho de banda. Internet es maravilloso. Y hay quien discute si realmente el modelo se filtró o se filtró de forma intencionada por Meta. No lo sabemos. Pero lo que sí sabemos es que Yama ya estaba en internet y ahora era turno de la comunidad open source. Y a todo esto a lo mejor te estarás preguntando, Carlos, ¿qué es Yama? Pues Yama es un enorme modelo del lenguaje, el LLM por sus siglas en inglés. Una inteligencia artificial entrenada con muchísimo texto y cuya tarea es aprender a predecir cuál es el siguiente trozo de palabra. Es decir, aprender el lenguaje y aprender a escribir. Y ojo, Yama no es chat GPT. Yama sería equivalente a un modelo como GPT-2, como GPT-3, como Palm de Google. Sería un modelo del lenguaje cuya única tarea es aprender a predecir la siguiente palabra. Lo bueno es que no es un chat GPT, pero para construir un chat GPT, tener acceso a este tipo de modelos es algo fundamental. Ya que luego, como vimos en este vídeo de aquí, una vez tienes a uno de estos modelos capaces de generar lenguaje y escribir correctamente, entrenarlo un poco más para que cumpla este rol de chatbot amigable que cumple instrucciones y que se rige dentro del marco de lo correcto y lo moral, pues no es tan complicado. GPT-3 o Yama serían estos modelos base, estos modelos fundacionales capaces de generar lenguaje, pero que todavía no serían ese chatbot funcional como chat GPT. Y aquí es donde a mediados de marzo aparecen modelos como Alpaca y Vicuña. Los primeros modelos que basados en Yama se empiezan a reentrenar para que cumplan instrucciones y actúen como el chatbot que a todos nos gustaría tener. Y si te das cuenta, todo esto estaba ocurriendo en el mes de marzo, al mismo tiempo que OpenAI estaba dando otro salto de gigante con la salida de GPT-4, y empresas como Anthropic pues estaban presentando a su modelo Claude. Modelos muy impresionantes, pero privados, que alejaban aún más el objetivo a conseguir. Y aunque la comunidad estaba consiguiendo tecnología impresionante para trabajar, pues Yama, Alpaca, Vicuña, si te das cuenta todavía estos modelos dependían de que grandes organizaciones como Meta, como Stanford, que tenían recursos computacionales suficientes, pues hicieron este preentrenamiento o reentrenamiento a posteriori. Seguíamos dependiendo de la computación para poder avanzar, algo que cambiaría con la llegada de Lora. Lora es una técnica que quizás usó en uno de los vídeos de Stable Diffusion, y es que se trata de una técnica que ganó bastante popularidad el año pasado al permitir coger un modelo como Stable Diffusion y reentrenarlo, hacerle fine tuning, a un costo computacional mucho más reducido que de la forma tradicional. Cuando tú tienes una red neuronada y quieres reentrenarla actualizando sus parámetros, tener que actualizar los millones y millones de parámetros que constituyen a un modelo como Yama puede llevar mucho tiempo y dinero. Y aquí la técnica de Lora lo que consigue es un reentrenamiento semejante, pero dedicando la computación solo actualizará un número muy inferior de parámetros. ¿Y qué ganamos con esto? Pues tiempo de reentrenamiento mucho más bajos y a menor costo. Estamos hablando que si quisieras hacer un fine tuning de un modelo como GPT-3 que tiene 175 mil millones de parámetros, Lora te permitiría hacer algo similar solamente actualizando 17 millones de parámetros, lo que sería una reducción de 10 mil veces. Esto, evidentemente, Lora Mola se merece una hola, un tsunami, e, e, no, eso no entra. Y no solo esto, sino que la agilidad de la comunidad Open Source también permitió que en semanas se pudiera implementar avances que a otras empresas le habían llevado meses. La integración con modelos de visión para implementar la famosa multimodalidad que todavía estamos esperando, el uso de herramientas semejante a los plugins de ChatGPT, ventanas de contexto más grandes o optimizaciones que permitían ejecutar estos modelos en tu propio ordenador o en un hardware más limitado como el de un móvil. Y tú ahora estarás pensando, pero Carlos, ¿esto no son enormes modelos de lenguaje? ¿Esto no son modelos con miles de millones de parámetros que ocupan muchísimo espacio en la memoria de la GPU y que por tanto no podríamos ejecutar en una tostadora? El poder ejecutar estos modelos en nuestro ordenador se está consiguiendo, y esto gracias a que la comunidad Open Source optimiza muy bien. Y aquí otra de las claves de los últimos meses están en las técnicas de cuantización. Esta es una técnica que te permite tomar los parámetros de tu red neuronal, que como sabéis son números decimales que ocupan un cierto espacio en tamaño de bits en memoria, y cambiarlo a otro tipo de dato que ocupe menos espacio, pues a lo mejor una cuarta parte menos. Así, a costa de algo de precisión y de rendimiento del modelo, podemos conseguir mejoras sustanciales en cuanto a ocupación de memoria, reduciendo fácilmente en 4 o en 8 el tamaño original del modelo, y permitiendo que cada vez más GPUs puedan ejecutar. Así, si os dais cuenta, el margen de separación entre los modelos Open Source y los modelos privados, como el chat GPT o VAR, en cuestión de meses se ha ido cerrando radicalmente, algo que quedó patente en este artículo de aquí, titulado No Tenemos Ventaja Competitiva y Tampoco OpenAI la Tiene, una carta presuntamente filtrada por un trabajador de Google, donde se reconocía que ni Google ni OpenAI iban a ser capaces de sostener en el tiempo la ventaja competitiva frente a la comunidad Open Source, principalmente motivada por la salida de llama en febrero de este año. Porque pensadlo, al ritmo al que se está moviendo la comunidad Open Source, no sería una locura que el año que viene tengamos un modelo como GPT-4 pero de libre acceso. Y en ese momento, imaginad que sois una empresa, que tenéis que decidir si queréis contratar los servicios de OpenAI, de Google, para mandar vuestros datos a una empresa de terceros, donde utilizar a uno de estos modelos privados. ¿Preferiríais eso o utilizar un modelo que podéis ejecutar en vuestro hardware, en vuestro ordenador, en vuestra empresa, cumpliendo la privacidad de vuestros datos y de vuestros clientes? Pues la pregunta no es tan fácil de responder. Y es que llama tiene un problema, y es que las empresas no podrían utilizarla porque la licencia no permitía uso comercial de este modelo. Por lo tanto, puesto que la licencia con la que se había liberado a llama no permitía el uso comercial, no tenemos ni llama, ni alpaca, ni vicuña, ni nada. Este era el gran problema de llama. Y por suerte, en los últimos meses, en toda esta explosión de enormes modelos de lenguaje, hemos tenido muy buenas alternativas con licencias que sí permitían su uso comercial, como el modelo MPT de 30.000 millones de parámetros o el modelo Falcon de 40.000 millones. Modelos que en rendimientos se acercaban a lo que ofrecía llama, pero que todavía ninguno conseguía superar. Y yo creo que ahora todos entendéis la importancia de lo que ha sucedido esta semana. La llama quería ser libre. Y Meta así lo entendió. Con la salida de la segunda versión de llama, a unos pocos meses de la primera versión, lo que Meta ha regalado a la comunidad de Deep Learning es un modelo más potente que la primera versión, y ahora sí disponible para uso comercial. Aún así, hay varias incógnitas que resolver, y la más importante, la que más me ha descolocado es ¿qué pinta Microsoft en todo esto? ¿Y por qué de repente empresas que parecían que estaban compitiendo aparecen aliadas para anunciar a este modelo? Recordemos que Microsoft es el principal aliado comercial de OpenAI, creadores de chat GPT. La primera cosa que quiero empezar con es OpenAI. Simplemente, Microsoft ama OpenAI. Y uno de los principales beneficios que ellos sacaban de esta alianza comercial era poder integrar mucha de la tecnología de OpenAI en sus productos. Por ejemplo, BinChat, Windows Copilot, Microsoft 365. Entonces, si esto es así, ¿qué sentido tiene ahora estar aliándose con la competencia quien está proponiendo además romper el mercado por completo al liberar una alternativa Open Source? Pues analizando la posteriori diría que es un movimiento bastante inteligente. La ventaja de liberar tu tecnología es que ahora tienes a toda la comunidad online trabajando sobre ella, optimizándola, mejorándola, a un ritmo que ninguna otra empresa ni laboratorio de inteligencia artificial puede igualar. Mejoras en tu tecnología que luego tú como empresa, como empresa, te vas a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento que te va a dar la oportunidad de hacer un movimiento tú como empresa te puede beneficiar al integrar esta tecnología en tus productos y servicios. Aquí Meta se beneficiará cuando integren a Yama2 en su Instagram, en su WhatsApp, en su Facebook y de la misma forma su aliado comercial Microsoft también se beneficiará de ello cuando lo integran en Windows, en Office y en todos sus servicios. Por tanto, tú y tus socios comerciales se podrán beneficiar de los avances en esta tecnología. Pero Carlos, no se supone que si el modelo es de uso comercial, pues cualquier empresa se podría beneficiar de los avances que se produzcan en Yama2. Y la respuesta es que sí, pero con un asterisco. Y es que aquí Meta ha incluido una cláusula muy curiosa en su licencia, donde si eres una empresa con más de 700 millones de usuarios activos, ahí sí que tienes que pedirle permiso a Meta para poder utilizarlo. Una cláusula que evidentemente está colocada para limitar el acceso comercial al modelo de sus grandes competidores. Y hablando del modelo, recuperemos el objetivo original. Es mejor que el chat GPT por respuesta corta, es comparable a chat GPT 3 y no llega al nivel de chat GPT 4. La nueva versión de Yama viene en tres tamaños medido por su número de parámetros, donde el modelo mayor será el que mejor rinda y el menor el que más rápido y menos requerimiento de hardware necesitará y que posiblemente veamos integrado en muchos dispositivos móviles a lo largo del año. Además, en esta ocasión no solo están compartiendo el modelo del lenguaje base Yama2, sino también un modelo reentrenado, como era Vicuña, para actuar como un chatbot y así tener una variante ya más orientada a ser como chat GPT. El modelo tiene una ventana de contexto de 4000 tokens, lo cual lo hace equivalente al modelo GPT 3.5. Y como se puede ver en esta tabla, algunas de las evaluaciones que se ha hecho de la inteligencia de este modelo nos muestra que su nivel está a la altura de la versión gratuita de chat GPT, la versión 3. Pero hay una pega y quiero que lo veáis bien en esta tabla. Fijaos bien en la diferencia que existe en la evaluación de human evals. Esta evaluación lo que mide es la capacidad del modelo de poder generar código que sea funcional, de poder programar. Y aquí se puede ver lo que sería una de las grandes carencias de este modelo, y es que parece que no lo han entrenado con el objetivo de generar código en mente. Algo que es extraño, porque una cosa que se ha comprobado en los últimos meses es que estos enormes modelos del lenguaje, cuando los entrenas con código de programación, no sólo mejoran evidentemente en sus capacidades de programar, sino que también mejoran en sus capacidades de razonamiento lógico y resolución de problemas a través del lenguaje natural. Es decir, lo vuelve más inteligente y por algún motivo meta descartado esto. Y creo que esto ilustra bien por qué creo que OpenAI pues por ahora puede estar tranquila y es que todavía no existe un modelo ni privado ni de libre acceso que ponga entre las cuerdas a GPT4, que es el rival a batir. Recordemos que la punta de lanza y quien ha detonado toda esta revolución ha sido OpenAI a través de chat GPT, quienes son los que han puesto algo innovador sobre la mesa. Y me imagino que Microsoft esto no lo va a olvidar tan rápido. Entonces sí, OpenAI creo que puede estar tranquila, pero no se deberían de relajar. Y es que algo que ha cambiado es la narrativa. OpenAI lleva desde hace unos años, desde la salida de GPT2, pues metiendo esta idea en la cabeza de que estos modelos no se deberían de liberar tan a la ligera por motivos de seguridad. Un discurso que les ha permitido ir cerrando poco a poco esa puerta que estaba muy abierta en el mundo del deep learning de compartir modelos, compartir paper y que ha instaurado un secretismo comercial muy raro en los últimos años. Pues ha llegado a meta y ha dicho mira Google, OpenAI, yo es que no vengo aquí a pelearme por a ver quién es el modelo más potente. Yo creo que esto debería estar en abierto, rompo el mercado, llego con mi modelo, lo hago open source, saco Sam, saco Dino, saco todo lo que tengo y pues ha cambiado el discurso. Quién sabe si esto ahora motivará a OpenAI a seguir compartiendo modelos en abierto. Y por último, qué esperar de todo esto? Qué va a pasar ahora? Por qué es tan revolucionario? Pues lo que podéis esperar ahora es una explosión de chatbots, de servicios conversacionales, de optimizaciones en el modelo de llama que van a llegar desde ya. Seguramente tengáis curiosidad por saber si el modelo cabe en vuestra GPU, si podéis ejecutarlo en vuestro móvil y todas estas cosas. Pero mi consejo es que esperéis un poco a que la comunidad trabaje. Esto se va a mover muy, muy rápido. Ahora vamos a ver en cuestión de días cómo organizaciones van a empezar a reentrenar a sus modelos basados en llama dos y los van a compartir. Veremos gente que se anime a optimizar y a cuantizar el modelo para que quepa mejor en memoria. Veremos personalizaciones y fine tunings entrenadas con Lora. Y bueno, cuánto ha pasado? Dos, tres días. Pues ya hay gente en Twitter que está compartiendo que han conseguido ampliar la ventana de contexto a 8000 tokens con técnicas que ya se conocían. Es decir, todo se va a mover muy, muy rápido. Aún así, si queréis testear el modelo, hay un montón de opciones y os voy a dejar abajo en la caja de descripción un par de enlaces para que podáis echarle un ojo. Dicho esto, quiero que te deis cuenta de que estamos viviendo tiempos excepcionales, donde si por una parte ya la generación de imágenes estaba explotando con la llegada de StableDiffusion, ahora la llegada de llama dos introduce un nuevo tsunami, una nueva ola y una nueva corriente que vamos a estar viendo cómo se desarrolla en los próximos meses. Chicos, chicas, ya sabéis que toda la actualidad y todo el conocimiento que os puedo brindar sobre inteligencia artificial lo tenéis aquí en mi canal de YouTube dot Csv. Tendremos más en el próximo vídeo. Muchas gracias y hasta la próxima.
