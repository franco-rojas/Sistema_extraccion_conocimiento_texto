 No os quiero hacer un spoiler, pero es Python. Muy buenas digitales y bienvenidos al videoblog de la informática corporativa. Y hoy continuamos con el tema de Big Data, que sí, que hace muchos meses que os prometo más vídeos sobre Big Data y ciencia de datos. Pero es que me cuesta mucho encontrar la fórmula para hacer vídeos, porque yo siempre hago vídeos y artículos sobre mi día a día. Y mi día a día sobre estos temas, pues engloban muchas tecnologías, mucha planificación, mucha coordinación con mucha gente. Y cuesta mucho sintetizarlo todo en un solo vídeo que dure 20 minutos. Pero ahora creo que he encontrado la fórmula, con lo cual vamos a tirar de este hilo a ver qué tal. Hoy vamos a continuar un post, un artículo que hice la semana pasada, donde explicaba los comandos bash más usados o las mejores herramientas bash para trabajar con ficheros muy grandes. Ficheros de terabytes, por ejemplo. En este sentido, pues explicaba lo básico. Leer las líneas de un fichero, cortar ficheros, dividir ficheros, comprimir ficheros, etcétera, etcétera, etcétera, que es la operativa básica pues para trabajar con Big Data en bash. Al final de este artículo había un pequeño script en bash para procesar un fichero JSON y convertirlo a CSV. Y lo que vamos a hacer ahora va a ser un paso más. Vamos a crear scripts para procesar grandes ficheros de gigabytes y ver cuál es el mejor lenguaje para hacer este tipo de trabajos. En el artículo usábamos bash porque era de lo que iba el artículo y porque es súper práctico. Un script de bash prácticamente no tienes que instalar nada o cuatro cositas para empezar a trabajar con ello. A partir de aquí ya tendríamos que tener algún lenguaje para crear scripts. En este sentido, Python es uno de los lenguajes que se ha impuesto más en tema de ciencia de datos. No es para menos. Y es que tiene cantidad de opciones out of the box para trabajar con estructuras de datos, trabajo con ficheros. El rendimiento que tiene es bueno, etcétera. Y lo compararemos también con Java, que es el otro lenguaje que yo utilizo muchísimo para aplicaciones corporativas. De hecho, muchos proyectos para trabajar con tema de Big Data y ciencia de datos están desarrollados sobre Java. Por ejemplo, Hadoop, Spark, etcétera. Todo ello funciona sobre Java, con lo cual es un buen lenguaje para hacer este tipo de trabajos. Y los vamos a comparar los tres para ver cuál es la mejor solución. Todos tienen sus pros y todos tienen sus contras. No me voy a enrollar más. Nos vamos a la pantalla. Muy buenas, digitales. Pues aquí estamos en la pantalla. Y aquí está mi blog, donde, como siempre, os hago un poco de spam. Y os digo que aquí tenéis toda mi lista de servicios, por si creéis que os puedo ayudar en vuestras empresas. Tema consultoría, formación, desarrollo full stack. Lo que necesitéis, que este 2021 está siendo duro, ya os digo. Aparte de esto, lo que os comentaba, este es el artículo que tuvo bastante éxito, donde os enseño cómo, por ejemplo, ver los recursos que tenéis disponibles para trabajar. Superimportante. No empecéis a hacer procesos en Big Data sin saber si tenéis bastante memoria RAM, que no es que no tengáis mucho CPU y que todo va bien. Incluso espacio en disco, que también es lo suyo. Y os explico cómo leer ficheros, las cabeceras, filas e incluso jq, tal, tal, tal. Y aquí, al final, os puse un pequeño script para procesar un fichero de texto, donde cada línea era un registro en JSON. JSON, este tipo de ficheros, donde cada línea es un registro en JSON, es muy difícil, sobre todo en Big Data, porque nos permite guardar datos complejos. Si, por ejemplo, usásemos CSV y quisiésemos guardar facturas, es muy complicado en un solo registro tener los datos del cliente, las líneas de la factura, los productos y líneas de descuento, formas de pago, etcétera, etcétera. En cambio, en JSON es muy fácil de moldear. Por eso se utiliza tanto este formato en Big Data. Pues lo que vamos a hacer hoy va a ser, este mismo script lo vamos a implementar usando Bash, usando Java, usando Python y vamos a ver pros y contras de usar cada solución. Mirad, aquí tengo una carpeta y aquí tengo un fichero que si lo miramos, menos LH, ocupa 17 GB. En el proyecto que estoy trabajando ahora, estoy procesando 8 ficheros como este. ¿Vale? Porque no lo tienes en un solo fichero, porque si no sería muy complejo de procesar, así que lo divides en varios ficheros. Para que os hagáis una idea de la magnitud. Son casi 8 millones de registros super largos. Entonces, lo primero que vamos a hacer va a ser hacer una porción más pequeña de este fichero. ¿Vale? Porque esto al final es un vídeo de YouTube y todo va a funcionar a la primera o casi, pero lo habitual es que cuando estás haciendo un script para procesar algo, pues te falle varias veces. Y si cada prueba te lleva 20 minutos, pues vas a tardar mucho más a tener optimizado el script que si cada prueba te dura un minuto. Entonces, lo que haces primero es siempre procesar una parte más pequeña de los datos para ir más rápido a tener todo el proceso bien moldeado. ¿Cómo hago esto? Pues, por ejemplo, como yo me voy a coger los 1500 primeros registros. Para hacer esto hago hit menos 1500, que son los registros que quiero coger, del fichero 1.data y lo voy a meter en el fichero 1.short.data.json. Esto es Bash puro y duro. Si queréis que haga más vídeos de Bash, incluso algún curso que tengo ganas de hacerlo, por favor decímelo en los comentarios para saber si realmente hay gente interesada o no. Le doy al enter y ahora ya menos lh, ya tengo los dos ficheros, uno de 17 gigas y el otro con 153 megas, mucho más ligero de procesar. Para que os hagáis una idea, cada uno de estos registros tiene esta pinta. Si cojo uno de los registros a la azar, por ejemplo, el 15, pues hago hit menos 15, tail menos 1, del fichero 1.short.data. Es un fichero super largo, si le quiero dar formato jq, todo esto es lo que tiene nuestro registro. Registros con cantidad de información. Yo lo que quiero es convertir cada uno de estos registros en un fichero CSV con solamente tres campos. Porque es lo único que me interesa para cargarlo en la base de datos que me va a funcionar en mi aplicación. Entonces iremos primero al primer script, que es el script que tenemos en Bash. Este script tiene esta pinta, bash barra to CSV. Este es mi script en Bash, que de hecho es el mismo que tenemos en el artículo. Básicamente este script tiene dos parámetros. El parámetro 1, que es el fichero de entrada y el parámetro 2, que es el fichero de salida. Y yo, si quiero generarme o transformar este CSV, este JSON a CSV, lo que hago es bash barra to CSV. Y le paso el fichero de entrada, que es 1.short.taltaltalt, y le voy a llamar a la salida 1.bash.csv. Le doy al enter y perfecto. Ha tardado un montón. Lo que pasa es que he cortado para no estar aquí como 40 segundos. Hago ls-l y aquí tengo mi fichero bash, que si, para que os hagáis una idea, los tres primeros registros de 1.bash tienen esta pinta. Como veis, mucho más reducido. Pero hemos dicho que queríamos comparar, porque hasta ahora hemos visto que el script bash es práctico, porque es un fichero que le das permisos de ejecución y lo ejecutas. Es lo que tenemos aquí. Pero vamos a ver cómo de rápido es. Para esto vamos a usar el comando time. Time es un comando que tenemos en bash también, que cuando le pasas o cuando ejecutas un comando, te dice lo que ha tardado en ejecutarse. Entonces, lo único que voy a hacer va a ser, esto es mi script en bash, le pongo time. Perfecto. Y ya fijaros, me da el tiempo real. Ha tardado 26 segundos en ejecutarse, de los cuales en código de usuario ha estado 23 segundos y en código del core del kernel ha estado 3 segundos. A ver, la métrica que nos interesa es el tiempo real, que es lo que tarda desde que lo ejecutas hasta que se para el proceso, 26 segundos. Eso está bien, pero claro, en Big Data una diferencia o esto para procesar solamente 1500 registros, teniendo en cuenta que tenemos con más de 7 millones, casi 8 millones, pues puede ser un auténtico desastre. Quizás necesitamos una alternativa mejor. Y si probamos con Python, Python es un lenguaje que se utiliza muchísimo en ciencia de datos. De hecho, no os podéis dedicar a esto sin saber Python, es como ser cocinero y no saber usar una olla. Es un lenguaje interpretado, eso quiere decir que solamente que tengamos Python instalado en la máquina, escribimos un fichero y lo ejecutamos, casi es como un script bash. Y vamos a ver qué pinta tiene este script. En este caso tenemos el script que está en Python2CSU.py. Esta es la pinta que tiene nuestro script. De hecho, fijaros, importamos tres paquetes, el paquete CSV para exportar a CSV, el paquete JSON para leer datos en JSON y el SIS que lo vamos a usar para leer los parámetros que le pasamos por línea de comandos. Básicamente lo que hacemos es leer el fichero de entrada que es el que le pasamos en el primer parámetro, declaramos también el fichero de salida que es el que leemos en el segundo parámetro y en el fichero de salida empezamos grabando la cabecera del fichero y luego hacemos todo el recorrido. En Python no tenemos ni puntos y comas, ni corchetes, no es como JavaScript o Java, sino que es tabulado. Él entiende que la tabulación, todo lo que va tabulado es lo que va dentro del for. Y básicamente lo que hace es leer la línea, la traspasa a JSON y en la línea que queremos insertar le metemos los datos. Cuando ha terminado guardamos la línea y cuando ha hecho todas las líneas cerramos el fichero. Como veis Python es un lenguaje diseñado para que sea fácil de leer. Vamos a ver ahora qué tal se comporta. Pues aquí hacemos Python, de hecho voy a coger el mismo o el otro que tenía. Pongo el time directamente para que me de los datos de los tiempos que ha usado. En este caso voy a usar python.to.csv.py y voy a procesar el fichero uno short data JSON y la salida la voy a llamar uno python.to.csv. Le doy al enter y perdonad, hemos dicho que era interpretado, con lo cual tengo que poner Python y que me interprete el script. Le doy al enter y esto fijaros ha tardado tan solo dos segundos frente a los 26. Estamos hablando de que ha tardado menos de un 10% del tiempo. Aquí cuando tenemos casi 8 millones de registros se nota mucho. Y si hacemos el ls-lh vemos que tanto el de bash como el de python ocupan 50k solamente y la información es la misma. Pongo hit menos cinco registros del 1.py. Y aquí tenemos los datos. No habría problema. Perfecto. Ahora vamos a hacer la prueba con Java. Java es el lenguaje corporativo por excelencia. Ahora en las empresas están empezando a entrar otros lenguajes porque hoy en día ya no se hace todo con un solo lenguaje. Pero cuando todo se hacía con un solo lenguaje y tenías una aplicación monolítica donde todo estaba escrito en el mismo lenguaje, se ejecutaba en el mismo CREA, todo, todo, todo. Los reyes de la informática corporativa eran Java y C Sharp por el otro lado. En este sentido crear scripts en Java es bastante más complejo. Primero porque lo tienes que compilar. Y bueno, esto ya te obliga a que el proceso de pruebas y demás sea bastante más largo. Vamos a ver en Java. Además tenemos que instalar las dependencias y luego ver el script. En este sentido podemos hacer cat java build punto properties para tener las dependencias. En este caso utilizaremos JSON y OpenCSUV para procesar los datos y nuestro script queda de la manera siguiente. En este sentido tenemos el paquete en el que está esto, los imports de todo lo que vamos a usar y todo lo que es el proceso. Hacemos el escáner del fichero que le pasamos como primer parámetro, declaramos el fichero de salida también, guardamos la cabecera y tenemos el bucle donde procesa por cada línea. Pues esto, lee la línea, lo convierte a JSON y guardamos todas las columnas y guardamos la fila y cuando acaba hacemos el flush para que termine de guardar todos los datos. Java, si Python está diseñado para ser ágil y legible, o sea Java está diseñado para no tener errores como esto se tiene que compilar, pues si hay problemas de sintaxis y demás nos daremos cuenta no durante la ejecución sino en el momento de compilar, etcétera, etcétera, etcétera. Son lenguajes totalmente distintos. Y vamos a hacer exactamente lo mismo. En este caso pongo el time y para ejecutar esto, yo ya lo tengo compilado, para ir más rápido en este vídeo, hacemos java menos jar y le ponemos el jar donde está compilado esto que os acabo de enseñar, está en JSON.jar, primer parámetro el 1.short.data.json y lo vamos a meter en el fichero para continuar con el mismo formato 1.java.csv. Le doy al enter y vamos a ver cuánto tarda esto en procesar. Perfecto, mira, ha tardado 8 segundos, bastante más de lo que ha tardado Python, lo que podría parecer que Java es más lento que Python. Esto en realidad no es bien así. Ahora lo que tendríamos que hacer es hacer lo mismo, no con una porción pequeña del fichero sino con todo el fichero. Yo esto ya lo he hecho para que no se nos haga muy largo y las conclusiones están aquí. Fijaros, tenemos el script de Bash que para 1500 registros en mi prueba ha tardado 26, en la que hemos hecho antes no sé lo que ha tardado. El de Java en la prueba que he hecho antes ha tardado 7 segundos, más o menos lo que nos ha tardado ahora y el de Python 2,65 segundos, también más o menos lo que nos ha tardado ahora. Java es un 70% más rápido que Bash, brutal, o sea, Bash no es un buen lenguaje para procesar gran cantidad de datos y luego el tema está entre Java y Python. Realmente Python es más rápido y además nos permite crear scripts de una manera más cómoda, por lo que comentábamos porque es interpretado, no hay que compilar, etcétera, etcétera. Entonces el lenguaje para trabajar para este tipo de cosas es Python, sin lugar a dudas. Estoy pensando que quizás me faltaría aquí la comparativa con Go, a ver si realmente Go es mucho más rápido que Python. Entonces nos lo podríamos plantear, lo dejo para más adelante. Fijaros, a la hora de procesar gran cantidad de datos, cuando hacemos la prueba con los 17 GB, Bash ha tardado 2892 segundos, una burrada. Creo que tarda como 40 minutos, por eso no lo hago aquí delante en el vídeo. Java ha tardado 682 segundos, un 76% menos, fijaros que a más cantidad de trabajo más rápido ha sido Java. Y Python ha tardado 603 segundos, un 11% menos que Java. Fijaros en la diferencia, cuando hay pocos datos, Python es mucho más rápido, a medida que crece la cantidad de datos, Java le va ganando posiciones. Esto es por temas de cómo trabajan internamente los dos lenguajes, procesan los datos, etcétera. Incluso, seguramente yo podría haber optimizado un poquito más el script en Java. En cualquier caso, Python es ahora mismo el líder indiscutible, por las dos variables estas que os comento. Por un lado, facilidad de crear los scripts y por el otro lado, velocidad. Aunque Python fuese un poquitín más lento, el hecho de poder trabajar más ágilmente lo hace mejor candidato, atención, para crear scripts, para procesar datos grandes, ficheros grandes, incluso pequeños también. Creo que no me he dejado nada, espero que haya quedado claro cómo funciona todo esto. Mirad el artículo, porque está bien, os explica cosas como ver el número de líneas que hay en un fichero. Un fichero wc-l y el 1.short tiene 1500 líneas. Si nos miramos el de Java, csv tiene 1501, porque tiene la primera línea que es la cabecera, que les hemos puesto a todos. Y bueno, esto es todo, creo que no me he dejado nada. ¿Qué os ha parecido? Ha estado super interesante. Ahora, después de haberlo hecho, quizás me ha faltado el ejemplo con Go, a ver si realmente este lenguaje nos aporta muchísima más velocidad. Si me lo pedís mucho en los comentarios, os haré no un vídeo, sino un artículo con el mismo script que hemos probado en Python y en Java, usado en Go, a ver si nos aporta grandes beneficios o simplemente no aporta nada nuevo. Y tanto por tanto, pues ya está. Lo de siempre, suscribiros si no lo habéis hecho ya. Manita arriba, por favor, muchos comentarios y compartid, porque es la manera como estamos creciendo muchísimo. Estos vídeos ayudan a muchísima gente, la gente no para de decírmelo, que ha aprendido mucho, que ha encontrado trabajo, que tal, tal, tal, gracias a estos vídeos. Y es una pena que no estemos creciendo más, porque el estilo de YouTube es otro y nos penaliza muchísimo. Pero si le metemos ganas, si compartimos mucho, creceremos igualmente y demostraremos que los vídeos bien hechos y con calidad y con I más D detrás y tal, tal, tal, también crecen. Así que, venga, muchísimas gracias por compartir y hasta la próxima.
