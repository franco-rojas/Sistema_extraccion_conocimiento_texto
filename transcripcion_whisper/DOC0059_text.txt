 Muy buenas a todos, mi nombre es Abraham Requena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una multinacional española. Bueno, aquí me gustaría haceros una pequeña introducción acerca del mundo del Big Data. El Big Data es un término que escuchamos a diario y que, bueno, creo que su definición aún no está muy definida. Entonces vamos a intentar definir el Big Data. Mira, aquí os muestro una primera definición que a mí me gusta mucho, que dice que el Big Data es como el sexo de los jóvenes, que todo el mundo habla de él, nadie sabe realmente cómo se hace, todo el mundo piensa que el resto lo está haciendo, por lo cual todo el mundo dice que lo está haciendo y efectivamente es un poco así, es un término que todo el mundo quiere hacer, pero bueno, creo que aún está por definir. Si vamos a una definición un poco más teórica, la Wikipedia nos define el Big Data como un término que hace referencia a una cantidad de datos tal que supera la capacidad del software convencional para ser capturados, administrados y procesados en un tiempo razonable y el volumen de los datos masivos va a crecer constantemente. Esta definición me gusta bastante más y es cierto, el Big Data a mí me gusta definir el Big Data como la capacidad de tratar grandes volúmenes de datos en un tiempo aceptable. Bueno, en Big Data vamos a tener lo que se conoce como las cuatro V. El Big Data tiene la V del volumen, tiene la V de la variedad, tiene la V de la velocidad y tiene la V del valor y normalmente también se suele añadir una quinta V que es la velocidad. Veamos que cada uno de ellos. Bueno, el volumen es eso, es que en Big Data vamos a tratar con volúmenes de datos muy grandes, vamos a tratar con datos del orden del terabyte o del petabyte y esos datos van a ser muy variados, es decir, vamos a trabajar con imágenes, con ficheros secuenciales, con ficheros de texto, etcétera. Otra V del Big Data era la velocidad y bueno, como su definición dice, el Big Data consiste en tratar grandes volúmenes en un tiempo aceptable, es decir, con una velocidad adecuada. Y por último, conseguir valor, porque tú en Big Data vas a tener unos datos en bruto, unos datos crudos, a los cuales haciendo una serie de transformaciones vamos a ser capaces de conseguir información. Esa información va a tener un valor útil para la empresa. Y bueno, la quinta V es la veracidad, nosotros podemos desde unos datos brutos conseguir una información, pero bueno, ¿cómo deberá es esa información? Pues ahí es donde entra la quinta V. Bueno, definiciones aparte, lo que no podemos negar es que la información está creciendo constantemente a día de hoy. En este gráfico podemos ver cómo cada 60 segundos se están escribiendo más de 98 mil tweets o se están creando más de 695 mil estados de Facebook o se están escribiendo 168 millones de correos, es decir, estamos generando casi 2 terabytes de datos por segundo. Toda esta información se puede utilizar. Toda esta información cruda debemos de ser capaces de tratarla para conseguir una información, ¿vale? Entonces es aquí donde aparece Apache Hadoop. Con Apache Hadoop lo que vamos a ser capaces de procesar esa cantidad de datos tan grande de una manera distribuida en distintos nodos que formarán nuestro clúster de máquinas en un tiempo digno, de manera que seamos capaces de convertir, que seamos capaces de convertir esos datos en bruto en información. Pues bueno, esta es una pequeña introducción al Big Data y durante el curso veremos cómo podemos implementar nosotros mismos Big Data.
