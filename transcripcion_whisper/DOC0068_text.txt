 Pido que compartan con sus grupos de Discord, de WhatsApp, sus redes sociales haciendo un salto mortal hacia atrás. Mierda, no que me duele la espalda. Muy buenas digitales y bienvenidos al Videoblog de la Informática Corporativa. Hoy vamos a usar la Stream API o la funcionalidad esta que tiene Spark para trabajar con Streams. En realidad Spark tiene dos APIs para trabajar con Streams, lo que sería la Structured Data Stream y los DStreams. La diferencia entre cada uno de ellos vendría a ser que la Structured Data Streams es para trabajar básicamente con Streams usando DataFrames, que es lo que vimos en el primer vídeo, os lo dejo por aquí para que lo veáis, donde podremos trabajar con los datos usando pues esto, esta API de los DataFrames y la SQL y este tipo de cosas que nos lo hacen más fácil, pero es menos escalable y no podemos trabajar con realmente Big Data de verdad usando solamente DataFrames. Podemos trabajar con cantidad de datos, pero tiene un límite. Los DStreams en cambio nos permiten trabajar con Streaming usando el API de los RDD, lo que vimos en el segundo vídeo, os lo dejo por aquí, que esta API es muchísimo más escalable, procesa los datos entre varias máquinas, no como los DataFrames, que los procesa en una sola máquina y este tipo de cosas. En este vídeo nos centraremos en los DStreams, los Distributed Streams. Como veréis es una alternativa, en este canal hemos hablado de otras alternativas como por ejemplo Apache Kafka con su Stream API, que era super potente. Seguramente para muchas cosas será mucho mejor utilizar Apache Kafka y para otras usar Apache Spark, ya lo veremos. Es muy importante entender cómo funciona cada herramienta para escoger la mejor herramienta para cada caso, porque una mala herramienta te puede mandar atrás del proyecto entero. Por eso es tan importante el trabajo de los arquitectos y meter a la gente que sabe al principio de todo y no al final, porque muchas veces cuando ya tienes el marrón es cuando llamas al que sabe y luego es complicado de arreglar. Pero bueno, esas son otras historias. Así que, ya estoy dentro de la pantalla. Como veis aquí tengo mi blog www.sabercronado.com donde tenéis cantidad de artículos. Os recomiendo mucho que les echéis un buen vistazo, tenéis las redes sociales, mi currículum, mis servicios de formación, de consultoría, de desarrollo, etcétera, etcétera. Y la comunidad Patreon para los héroes que quieren apoyar de verdad el canal, que de verdad que los necesitamos. Cada vez hay más gente y esto tiene pinta de despegaría. También nos podéis apoyar puntualmente vía PayPal, por si no queréis estar involucrados dentro de la comunidad patria. Y ya está, lo que vamos a hacer hoy va a ser lo de siempre. A través del terminal arrancaremos primero el Apache Spark en modo stand-alone, es decir, que tendremos el master y un solo worker en la misma máquina. Para ello me meto en la carpeta de Spark, la carpeta S-DIN. Aquí hago start master. Esto nos abre un master. Perdón, el master lo tendremos también, nos abre un dashboard súper útil porque nos dice dónde está escuchando. Si tenéis problemas para seguir lo que estoy haciendo, sobre todo revisad los anteriores vídeos, súper potente. Y aquí lo que hago es startworker.sh y aquí le paso el master al que se tiene que conectar. Y aquí ya tenemos una instalación con su worker que es el que va a trabajar. Perfecto. Me meto en la carpeta BIN, que es donde tenemos Spark Submit para lanzar las aplicaciones. Pero primero vamos a crear nuestra aplicación. Para ello con Nano, que es mi editor favorito en modo texto, me creo una aplicación que va a ser SD de D-Streams o DS de D-Streams.py. Acordaros en estos vídeos estamos creando todas las aplicaciones usando Python, que es una gran herramienta que se utiliza muchísimo en ciencias de datos, Big Data, este tipo de cosas, pero que podemos hacerlo en Java, Scala o R también. Vamos a inicializar nuestra aplicación importando pues los paquetes, las librerías. En este caso será Spark Context y Streaming Context. Esto ya nos viene inicializado cuando estamos trabajando con la consola, pero no cuando estamos creando una aplicación. Lo siguiente va a ser inicializar el contexto. Primero creamos el contexto de Spark, donde le ponemos el nombre de la aplicación y el master y después inicializamos el contexto de Streaming. En este caso a partir del contexto de Spark y cada cuántos segundos se van a procesar los datos, porque esta API lo que hace es conectarse a un origen de datos, un socket, un fichero o lo que sea, va leyendo los datos que le van llegando por Streaming y cada x segundos hace todo el proceso de datos. Esta es la lógica de negocio y en este caso le estamos diciendo que cada 10 segundos procese los datos que le vayan llegando. Y aquí lo tendríamos. Siguiente paso, inicializar el socket. En este caso le decimos, metemos en la variable lines las líneas que irá leyendo y aquí lo que creamos es un socket text stream. Aquí podríamos crear un file text stream y otros orígenes de datos al que le pasamos el host y le pasamos también el puerto al que va a conectarse. O sea, importante, él no estará escuchando en este host y en este port, sino que va a conectarse. Por lo tanto tenemos que poner algo a funcionar en este puerto. Para ello vamos a usar netcat, que es una aplicación que viene normalmente en todas las distribuciones Linux y que es una especie como de navaja suiza para para trabajar con red. Nos permite crear servidores, incluso tomar el control remoto de otras máquinas, pero esto es otra historia. Hacemos netcat menos lk 90 90 y esto lo que nos abre es un socket, es un server que queda ahí escuchando para que se conecten otros. En este caso yo puedo poner aquí hola mundo y esto lo va a mandar al que se conecte. Ya tenemos nuestro servidor que va a ir mandando datos y aquí tenemos nuestro programa que se va a conectar a este servidor. Ahora vamos a hacer toda la lógica de procesar los datos. Esto lo haremos de esta manera, fijaros. Esta API, la de destreams, es calcada o la misma API de los RDDs. Acordaros que aquí lo que podemos hacer es aplicar un map reduce o cantidad de funciones. El otro día nos lo expliqué, pero mirad aquí en la documentación de Spark tenéis todas las transformaciones de los destreams y aquí también podéis encontrar las transformaciones de los RDDs y este tipo de cosas. Podemos aplicar un mapeo, un flat map, un filtro, repartition, union, count, reduce, este tipo de cosas. En este caso, al igual como hicimos en el anterior vídeo, cogemos el RDD y le aplicamos un flat map. En este caso no le aplicamos una función sino que le aplicamos una lambda. En el anterior vídeo hicimos las dos cosas, aplicamos una función y una lambda. Acordaros que en Python normalmente en la práctica siempre acabas usando funciones porque en las lambdas de Python no puedes poner más de una instrucción, y es complicado. En este caso hacemos un flat map y por cada línea que nos llegue lo que le haremos será un split para que nos transforme cada línea. Cada registro que es una línea va a ser un registro una palabra. Y después lo siguiente que hace es un mapeo para transformar cada palabra y le pone a cada palabra el peso de 1. Y finalmente aplicamos la función de reducción, esto es un map reduce de cajón y lo que hace es agrupar todas las palabras y hace por cada palabra que encuentra les hace la suma de 1 más 1 pues 2, 3, 4 y así va sumando las distintas palabras y esto lo hace en paralelo. El motivo de que lo hagamos usando es el modelo este de map reduce es porque queremos paralelizar el trabajo y que en lugar de hacerlo calcularlo linealmente que esto al final tiene el límite de la potencia de la máquina. Y finalmente cogemos el wordcount y le hacemos un pprint que básicamente lo que hace es imprimir los 10 primeros registros más que suficiente. Una vez hemos acabado con esto simplemente arrancamos el contexto de streaming y esperamos a que se termine. Y ya está, no tiene nada más le decimos guardar y vamos a ver si esto funciona hacemos un spark-submit master le pasamos el master que sea aquí voy a buscar cuál es la url del master copiar perfecto y le ponemos de ese punto pi que es nuestra aplicación le dimos enter y esto ahora perfecto fijaros esto se ha quedado a la espera y a los 15 segundos hará el primer procesado perfecto y ya ha procesado el hola mundo que yo le he pasado aquí aquí yo le ahora le puedo poner hola mundo hola mundo adiós mundo y él vale me ha pillado el primer hola mundo y luego me ha pillado el hola mundo adiós vale otra vez hola mundo hola mundo adiós mundo perfecto y ahora me va pues espero que a procesar esto tarda los 15 segundos yo le he metido dos olas tres mundos y un adiós que es lo que teníamos aquí uno y dos olas dos tres mundos y un adiós y esta es la funcionalidad de trabajar con datos en streaming utilizando spark como veis si nuestro procesado va a ser muy simple es muchísimo más práctico utilizar apache Kafka os dejo el vídeo por aquí de apache Kafka para que le echéis un vistazo si queréis procesar o simplemente os queréis ahorrar el hecho de tener que copiar los datos en cada una de las máquinas workers podéis usar esta esta esta API que realmente es útil y bueno y esto es todo vale espera espera espera ya estoy fuera fuera de la pantalla vale como comentaba esto es todo o esta es toda la lógica que hay detrás de apache spark en lo que respecta al tema de streaming en próximos vídeos le vamos a meter mucha caña a cosas súper interesante como es la API de machine learning o la API de graph x que nos permite trabajar con grafos y nos permite hacer cosas tan peligrosas como una vez tenemos un usuario por perdón si estamos procesando la navegación de los usuarios no y tenemos un usuario que ha comprado algo o que ha hecho clic en un anuncio podemos ver la navegación de este usuario compararla con la navegación de otros usuarios parecidos y así enseñarles el mismo anuncio o el mismo producto para que hagan clic en lo que hace pues facebook y todas las redes sociales grandes y estos serán futuros vídeos como veis hasta ahora nos hemos quedado en todo lo que sería ingeniería de datos vale para procesar datos sacar report este tipo de cosas y ahora empezaremos a trabajar con realmente con las APIs que les interesan a los científicos de datos a los que están ahí mirando a ver qué que petróleo pueden sacar así que está atentos a los próximos vídeos por cierto que la semana que viene no habrá vídeo el motivo estoy de vacaciones vale me voy unos días y me voy a olvidar de crear vídeos así que como siempre por favor manita arriba y comentar decirme que os parece que os gustaría ver vale y bueno suscribiros en la web hecho ya eso ya ni se debería de decir y sobre todo compartir por favor porque es como crecemos vale compartir con vuestros grupos de discord de en foro coches vale a ver si alguien me comparten foro coches vale y tengo ahí un nicho ahí de seguidores en whatsapp vale en las redes sociales en todas partes con vuestros compañeros de universidad con vuestros compañeros de trabajo a ver si hacemos crecer esto vale qué buena falta me hace muchísimas gracias y hasta la próxima
