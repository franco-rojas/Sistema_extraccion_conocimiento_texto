 Muy buenas, mi nombre es Abraham Raquena y soy ingeniero informático. Actualmente estoy trabajando como desarrollador Big Data en una multinacional española. Durante el curso también vamos a hablar acerca de cuándo usar Hadoop y cuándo no usar Hadoop. Aquí hablamos un poco acerca de ello. ¿Cuándo debemos de usar Hadoop? Bueno, pues Hadoop está pensado para cuando debemos de procesar ficheros de texto muy, muy, muy grandes. Cuando hablo de muy grandes, estoy hablando del orden del terabyte o del orden del petabyte. ¿Cuándo más? Bueno, cuando nosotros tenemos una previsión de crecimiento de nuestros datos. Puede ocurrir que nuestro proyecto inicialmente contenga pocos datos, a lo mejor del orden del giga, estamos generando datos continuamente de forma que tengamos, por ejemplo, un crecimiento exponencial de los datos. Entonces, en este caso también es bastante útil utilizar a pase Hadoop porque llegará el momento en que necesitaremos dividir nuestros datos para conseguir velocidad. Además también porque Hadoop nos va a permitir un escalado muy fácil. Ya veremos que simplemente añadiendo nuevos nodos a nuestro cláster de máquina podremos aumentar el rendimiento de nuestro sistema. Bueno, también cuando tengamos tipos de datos variados, es decir, en Hadoop, en el HDFS, que es el sistema de almacenamiento de Hadoop, se pueden almacenar cualquier tipo de datos, ya sean imágenes, sean ficheros secuenciales, sea texto, ficheros más grandes, ficheros más pequeños, etcétera. Ya veremos cómo almacena todos estos ficheros el HDFS y cómo lo va dividiendo en bloques, que será lo que se irá almacenando. Y por supuesto debemos de asegurarnos que tenemos la capacidad de paralelizar nuestros datos. Al fin y al cabo Apache Hadoop lo que consiste es de tener grandes ficheros los cuales iremos dividiendo en pequeños grupos que se tratarán de manera paralela y con eso es con lo que ganaremos la velocidad. Por ello debemos de asegurarnos que podemos paralelizar nuestros datos. Bien, vayamos al polo opuesto. ¿Cuándo no debemos utilizar Apache Hadoop? Bueno, pues también veremos durante el curso que Apache Hadoop no es útil cuando necesitamos un análisis de tiempo, un análisis de datos en tiempo real. Hadoop es capaz de tratar muchos datos en un tiempo aceptable, pero tenemos que tener en cuenta que Hadoop trabaja en disco y el disco no es tan rápido, por lo cual puede ser que tengamos procesos más reduced que nos tarden horas o incluso días. Por lo cual si necesitamos el análisis en tiempo real, Hadoop no va a ser una buena solución. Durante el curso vamos a ver cómo Apache Spark nos puede solucionar este problema, ya que Apache Spark trabaja en memoria, lo que nos da una latencia mucho más baja y nos permite el procesamiento de datos en tiempo real. Vale, Apache Hadoop tampoco está pensado para cuando tengamos un sistema de datos, un sistema con una base de datos relacional, es decir, cuando tengamos un modelo de nuestros datos muy complejo, en el que necesitemos usar join, filter, uniones, etc. Si nuestro proyecto necesita de eso, Apache Hadoop en principio no va a ser una buena lección. ¿Cuál es la solución que se le puede dar? Bueno, durante el curso vamos a ver una herramienta que se conoce como Apache Hive y que lo que nos permite es lanzar consultas SQL sobre el HDFS. De esta manera tendremos nuestros ficheros alojados en el HDFS y luego tendremos con Apache Hive que crearles como una pequeña estructura de esos datos para poder ser consultados con un lenguaje muy similar a SQL que se denomina HiveQL. Bien, tampoco es útil Apache Hadoop cuando queramos modificar nuestros datos. ¿Por qué? Porque el HDFS utiliza la política de Great ones, rare many, es decir, escribimos una vez el fichero y lo podemos leer muchísimas veces, pero un fichero una vez que está escrito en el HDFS no puede ser modificado. Lo único que podemos realizar es añadir contenido al final del fichero o eliminarlo. ¿Qué ocurre si tenemos que modificar el fichero? Pues tendríamos que revisar un poco todo nuestro proceso porque deberíamos de coger el contenido que hay en el fichero, borrar ese documento y crear un documento nuevo con todo lo que había más lo que queramos añadir. Pero vamos, no es un proceso trivial y no podemos añadir en un punto aleatorio de un fichero del HDFS. Y por supuesto cuando no se pueda paralelizar nuestro trabajo. Como ya veremos, una de las características principales de Hadoop es la paralelización, es la capacidad de poder distribuir los datos a lo largo de los nodos de nuestro cluster. Por lo cual si nuestros datos tienen que ser tratados de una manera completamente secuencial y no se pueden paralelizar, Apache Hadoop no nos va a dar ninguna ventaja a la hora de tratar dichos datos. Y por último también vamos a ver durante el curso algunas malas prácticas, algunas prácticas que no debemos usar cuando implementamos Big Data con Apache Hadoop. Alguna de ellas es tener cientos de ficheros pequeños en el HDFS. Ya veremos que el HDFS está pensado para para para alojar ficheros muy muy grandes del orden del terabyte o del petabyte y almacenará esos ficheros en bloques de 128 megas. Por lo cual si tendremos, si tenemos muchos muchos ficheros pequeños con poco tamaño, esto no es útil en el HDFS. Otra mala práctica es tener muchos procesos map con poca duración. Ya veremos lo que son los procesos map reduce, pero la parte del map es la parte donde vamos a paralelizar el trabajo, donde vamos a dividir los datos. Entonces si tenemos demasiados procesos map, es decir, hemos paralelizado más de la cuenta y eso tampoco es útil porque todo lo que conlleva la ejecución del map no nos ayudaría, estaríamos perdiendo tiempo en ese punto. Luego otra mala práctica es tener demasiados pocos reviews para ficheros muy grandes. Si tenemos ficheros de 2 gigas y sólo tenemos 2 reviews, cada reviews estaría comiendo un giga completo de datos. Son muchos millones de líneas, por lo que la idea es poder paralelizar mucho más los datos. Si tenemos 10 máquinas, pues tendremos cada máquina con una parte del fichero. Y por último otra mala práctica es generar muchas salidas y pequeñas de los reviews. Ya veremos cómo en los procesos map reduce finalmente vamos a generar unos ficheros de salidas que serán ya nuestra información útil, que tendrá la V del valor que comentábamos del big data. Entonces si esos reviews generan muchas salidas y muy pequeñas, también estaríamos perdiendo tiempo, porque los reviews necesitan un tiempo para crear los ficheros y para escribir en el HDFS. La idea es tener menos ficheros de más tamaño. Y bueno, aquí os he mostrado un poco cuándo debemos usar cadub y cuando no debemos usar cadub. Ahora es el momento de suscribiros al curso y comenzar a ver la herramienta.
