 Cuando queremos resolver un problema de Machine Learning tenemos a nuestra disposición tantos algoritmos que a veces resulta difícil decidir cuál de ellos usar. Así que en este video haremos un tour por las principales familias de algoritmos de Machine Learning organizadas dependiendo de su principio de funcionamiento. Para cada familia veremos también los algoritmos más representativos y en la descripción del video encontrarán el enlace de descarga con este mapa que le servirá como guía para el desarrollo de sus proyectos. Pero antes de comenzar los invito a visitar la Academia Online de Codificando Vídeos donde encontrarán cursos para construir su carrera en ciencia de datos, inteligencia artificial y Machine Learning. Este mes estamos con el curso Python Nivel Avanzado donde aprenderemos el manejo de excepciones así como el procesamiento de diferentes tipos de archivo y todo lo relacionado con la programación orientada a objetos. Recuerden que el acceso a este y a todos los cursos de la Academia es a través de una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. Como punto de partida supongamos que ya hemos determinado que el Machine Learning es la ruta adecuada para resolver nuestro problema. De hecho en la descripción les dejo un video donde explico cómo hacerlo. También es importante tener en cuenta que existen muchísimos algoritmos de Machine Learning así que en esta guía no tendremos un listado exhaustivo sino más bien los algoritmos más representativos de cada familia. Comencemos entonces hablando de lo que significa resolver un problema de Machine Learning. En un problema de Machine Learning lo que buscamos es crear un modelo que sea capaz de tomar unos datos de entrada, encontrar ciertos patrones en esos datos y con base en ello que logre realizar una predicción a partir de dichas propiedades. Por ejemplo podemos pensar algo tan simple como un modelo alimentado con imágenes y que buscamos que logre clasificarlas en diferentes categorías. Y para que el modelo aprenda a detectar patrones usamos lo que se conoce como un set de entrenamiento. Así una vez entrenado el modelo lo podremos poner a prueba con datos que nunca ha analizado, es decir el set de prueba. Y este proceso de aprendizaje puede ser supervisado, no supervisado o por refuerzo. En el aprendizaje supervisado tenemos un grupo de datos de entrada y para cada uno de ellos conocemos con antelación los atributos que deseamos predecir. Por ejemplo para entrenar un sistema de clasificación de imágenes debemos tener varias imágenes y para cada una de ellas conocer la categoría a la que pertenecen. O por ejemplo para un sistema de puntuación de crédito tenemos datos de potenciales clientes de un banco pero también tenemos que conocer con antelación la probabilidad de que cada cliente adquiera un producto crediticio con el banco. Por otra parte en el aprendizaje no supervisado solo tenemos los datos pero desconocemos el atributo que queremos predecir. Por ejemplo podemos tener una base de datos de clientes con diferentes comportamientos financieros pero desconocemos cuáles de ellos tendrán más o menos probabilidad de adquirir por ejemplo un crédito con el banco. En este caso los algoritmos de aprendizaje no supervisado pueden detectar diferentes patrones de comportamiento y de forma automática agruparlos en ciertas categorías para que podamos posteriormente interpretar estos resultados y tomar decisiones. Y por último está el aprendizaje por refuerzo en donde lo que se busca es usar algoritmos que permitan a un agente aprender a interactuar con su entorno. Como cuando por ejemplo queremos que un robot aprenda por sí solo a ejecutar acciones y a moverse de forma autónoma en un espacio determinado. Este aprendizaje por refuerzo tiene un espectro tremendamente amplio de algoritmos así que en este vídeo no hablaremos de este enfoque aunque en el canal podrán encontrar un vídeo introductorio a este tema. Ahora sí estamos listos para comenzar a explorar las diferentes familias y sus algoritmos más representativos. Estas familias están conformadas por grupos de algoritmos que obedecen al mismo principio de funcionamiento aunque hay que tener en cuenta que no son totalmente excluyentes es decir que algunos algoritmos bien podrían pertenecer a diferentes familias. Pero más allá de este criterio lo importante es que estoy intentando darle un cierto orden lógico a la gran cantidad de algoritmos existentes. Y como se trata de una guía lo que haremos será describir las principales características de cada familia y mencionar sus algoritmos más representativos. Sin entrar a mirar en detalle el funcionamiento de cada uno de ellos. Comencemos con los algoritmos de regresión que son algoritmos de aprendizaje supervisado y donde lo que buscamos es entrenar un modelo capaz de predecir el valor de una variable continua. Por ejemplo podemos entrenar un modelo que aprenda a tomar la edad, el género y el peso de una persona e intente predecir su altura. La regresión lineal, la regresión ordinaria por mínimos cuadrados y la regresión logística son algunos de los principales algoritmos de esta familia. Los algoritmos basados en árboles de decisión también son de aprendizaje supervisado y se pueden usar para tareas de regresión o clasificación. El nombre de esta familia se debe a que usan una estructura básica, los árboles precisamente, que con simples reglas de decisión aplicadas sobre los datos les permite generar predicciones. Por ejemplo podemos entrenar un árbol para que aplique de forma secuencial reglas de decisión sobre diferentes características relacionadas con el comportamiento financiero de un cliente y con esto prediga si adquirirá o no un producto. Entre los principales algoritmos tenemos los árboles de clasificación y los árboles de decisión que usan como base lo que se conoce como el algoritmo CART. Otra familia son los algoritmos probabilísticos que intentan construir una distribución de probabilidades de las características de los datos para realizar tareas de clasificación y regresión o también de aprendizaje no supervisado. Por ejemplo pensemos en un simple clasificador de imágenes para determinar si la fruta en una imagen es una banana o un kiwi. Si se analiza la forma y el color de cada fruta se tendrán distribuciones diferentes y con esta información el clasificador probabilístico podrá diferenciar una de la otra. Los algoritmos más representativos son Knifebase, las redes vallesianas y los modelos de mezcla gausiana. Muchos modelos de Machine Learning sufren en mayor o menor grado el problema de overfitting que hace referencia a que el modelo funciona bastante bien con el set de entrenamiento pero no tanto con el set de prueba. Para reducir este overfitting se puede intentar controlar el impacto que tiene cada una de las características del dato que está siendo procesado por el modelo. Por ejemplo volviendo al sencillo clasificador de tipos de fruta puede ser que le de más importancia al color que a la forma al realizar la clasificación y tal vez el desempeño no sea el adecuado. Lo ideal sería que ambas características tuviesen niveles de importancia similares. Los algoritmos de contracción buscan reducir el overfitting contrayendo el peso o la importancia de algunas características imponiendo una penalidad sobre su tamaño. Entre los algoritmos más importantes encontramos la Regresión Reach, la Regresión Lazo y ElasticNet que pueden ser usados para tareas tanto de regresión como de clasificación. Los algoritmos de agrupamiento permiten realizar tareas de aprendizaje no supervisado para lo cual generan agrupaciones basadas en el grado de similitud entre los datos de forma tal que datos similares pertenecerán a una misma agrupación. Por ejemplo podemos usar algoritmos de clustering para encontrar diferentes perfiles de cliente de una entidad bancaria. Aquellos más interesados en adquirir productos crediticios o aquellos con un perfil de inversionistas. Los principales algoritmos en esta familia son K-means, el agrupamiento espectral y los mapas auto-organizados. En los algoritmos combinados se construye un modelo a partir de múltiples modelos más simples con lo cual se busca mejorar el desempeño del modelo en tareas de regresión o clasificación. Por ejemplo podemos tener un árbol de decisión que tiene un desempeño regular como clasificador pero si construimos un modelo con múltiples árboles, unos diferentes de otros, podremos aprovechar las fortalezas de cada árbol para que el modelo combinado realice una mejor clasificación que cada modelo individual. Los principales algoritmos en esta categoría son los bosques aleatorios y los algoritmos de bagging y boosting. Los algoritmos de aprendizaje basado en instancias se usan en tareas de aprendizaje supervisado y en lugar de construir un modelo almacenan instancias o ejemplos de los datos de entrenamiento para posteriormente comparar el dato procesado con dichas instancias. Por ejemplo podemos intentar determinar si una imagen corresponde a un producto en buen estado o defectuoso comparando sus características con las de múltiples objetos almacenadas previamente. Los algoritmos más usados en esta familia son los K-vecinos más cercanos y las máquinas de soporte vectorial. Los algoritmos de reducción de dimensionalidad de aprendizaje no supervisado permiten analizar los patrones presentes en los datos para simplificar su representación pero preservando la información que nos interesa. Por ejemplo, supongamos que tenemos una población de sujetos y para cada uno tomamos los datos de su altura y su peso. Estas dos variables no son totalmente independientes y en su lugar están correlacionadas. Es de esperar que a mayor altura se tenga mayor peso y viceversa. Así que podríamos pensar que para describir al sujeto podríamos usar una sola variable que combine la información tanto del peso como de la altura. Al usar una en lugar de dos variables lo que estamos haciendo es reducir la dimensionalidad de nuestro set de datos pero sin pérdida de información. Entre los algoritmos más usados tenemos el análisis de componentes principales o el análisis discriminante lineal. Hasta este punto hemos hablado de los algoritmos clásicos del machine learning que funcionan bastante bien cuando tenemos datos estructurados, es decir que vienen representados en formato tabular o cuando tenemos relativamente pocos datos, es decir unas cuantas decenas o cientos de miles. Pero cuando tenemos datos no estructurados, es decir como las imágenes, el audio, el video o el texto nos resulta mucho más difícil usar estos algoritmos clásicos para realizar tareas de aprendizaje supervisado o no supervisado. En este caso la alternativa más adecuada son las arquitecturas del deep learning que han tenido una evolución impresionante en los últimos 10 años y que se derivan de una estructura básica que son las redes neuronales. Estas arquitecturas requieren una gran cantidad de datos de entrenamiento del orden de cientos de miles o millones y son adecuadas cuando tenemos datos no estructurados. Los principales representantes de esta familia son las redes neuronales, las redes convolucionales, las redes recurrentes y LSTM y las redes transformer. Muy bien, ya tenemos un panorama general de las principales familias y algoritmos del Machine Learning. Aunque tengamos en cuenta que este listado de algoritmos es mucho más amplio y lo que he hecho en esta guía ha sido incluir aquellos más representativos de cada familia. De muchos de estos algoritmos ya hemos hablado en detalle y encontrarán videos asociados en el canal, pero si no los encuentran me pueden dejar abajo sus comentarios para incluir próximamente un video sobre ese tema. También recuerden que en la descripción del video podrán encontrar el enlace de descarga de esta guía Y si les gustó el video no olviden darle un pulgar hacia arriba de me gusta y compartirlo con sus amigos y conocidos porque esto me ayudará a seguir creando este tipo de contenido y también los invito a suscribirse al canal si aún no lo han hecho. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
