 En las etapas experimentales de desarrollo de un modelo de Machine Learning podemos implementar el prototipo simplemente en un computador personal, pero si queremos ya llevarlo a la etapa de desarrollo o a la etapa de producción, probablemente necesitamos o la colaboración de un grupo de trabajo, o tendremos que montar esta aplicación en la nube para darle acceso a diferentes usuarios. Pero esta aplicación muy probablemente no será definitiva, porque en este caso el Machine Learning es un proceso iterativo, los datos pueden cambiar, el modelo mismo puede cambiar, y por tanto cambiará el producto final. Así que durante todo este proceso empiezan a surgir varios inconvenientes, porque tenemos que garantizar que esta aplicación, que fue construida con ciertas especificaciones, pueda ser ejecutada por los colaboradores del proyecto, o desde un servidor, independientemente del hardware o del software que utilicen estos equipos remotos. Una alternativa a este problema es empaquetar la aplicación, usando algo que se conoce como un contenedor. Y en la actualidad Docker es una de las plataformas más usadas para esto. Pues si están interesados en conseguir empleo en el área del Machine Learning, definitivamente Docker es una de esas herramientas que deben saber manejar. Pues en este video veremos qué es Docker y para qué se requiere en el Machine Learning Engineering. Y al final también les voy a mostrar una guía sencilla para que puedan fácilmente incorporar Docker en su flujo de trabajo cuando sea necesario. Así que sin más preámbulos, comencemos. Partamos de un ejemplo hipotético para entender por qué estos contenedores de software resultan esenciales en el Machine Learning Engineering. Supongamos que estamos desarrollando una aplicación de Machine Learning inicialmente en nuestro entorno local, es decir, en nuestro computador. Instalamos todas las librerías requeridas, optimizamos el código para que aproveche la CPU y la GPU de nuestro computador, entrenamos el modelo y lo tenemos listo y funcionando a la perfección. Pero luego compartimos el desarrollo con otro colega o lo llevamos a otro computador o intentamos desplegarlo en la nube, usando entornos con una configuración similar. Pero resulta que no funciona. Comienzan a aparecer errores, conflictos con las versiones de las librerías o con el hardware y nos damos cuenta que realmente resulta muy muy complicado depurar todos estos errores y poner a funcionar el prototipo en un entorno diferente al nuestro. Incluso es posible que funcione, pero supongamos que en el equipo remoto de repente se modifica por ejemplo un driver y la aplicación o deja de funcionar o comienza a correr más lento de lo normal. Y el problema es aún más complejo si estamos en un entorno laboral, donde tenemos un equipo de desarrolladores y queremos hacer varias pruebas sobre el modelo en la etapa de desarrollo, antes de llevarlo a la etapa de producción y montarlo en un servidor para dar acceso a los usuarios. Pero ¿por qué tantos inconvenientes? Pues si analizamos por un momento la aplicación nos daremos cuenta que el código desarrollado es sólo la punta del iceberg, pero en realidad el problema es más complejo de lo que parece. Porque el código está montado sobre una API que puede ser por ejemplo PyTorch o TensorFlow que evoluciona constantemente, pero a su vez esta API depende de otras librerías que fueron desarrolladas de manera independiente como SciPy o NumPy o Python incluso y que también están evolucionando constantemente. Y para enredar aún más las cosas estas librerías están compiladas usando unas rutinas que son propias y específicas para la CPU local o incluso para la GPU o para los drivers de la GPU local. Así que cuando movemos el código del equipo local al de nuestros colaboradores o a un cluster en la nube, lo que estamos haciendo es introducir múltiples puntos de falla. Lo que quiere decir que es altamente probable que no exista coincidencia entre las librerías y dependencias usadas por el equipo local y el remoto. Para resolver todos estos inconvenientes necesitamos una herramienta que cumpla al menos estas tres condiciones. En primer lugar que nos dé resultados consistentes, es decir que nos permita ejecutar la misma aplicación de Machine Learning en diferentes computadores y obteniendo siempre los mismos resultados. En segundo lugar que nos dé portabilidad, es decir que nos permita empaquetar la aplicación y luego desplegarla por ejemplo en la nube en un servidor. Y en tercer lugar que se encargue en su totalidad del manejo de dependencias, es decir que podamos empaquetar la aplicación, pero también las librerías y todas las dependencias exactamente de la misma forma como funcionan en el computador local. Y acá es donde entra el rescate Docker, que es actualmente la plataforma más usada para la creación de contenedores de software. Hacemos una metáfora para entender lo que hace Docker. Cuando un barco de carga transporta mercancía lo hace usando varios contenedores. Cada contenedor tiene un producto diferente y esto se hace para evitar que diferentes productos de diferentes proveedores y con diferentes características se mezclen y se arme un desorden. Pues Docker hace lo mismo que estos contenedores del barco, solo que en lugar de productos tenemos el código y todas las dependencias asociadas a nuestra aplicación. En detalle lo que hace Docker es crear un contenedor que encapsula por completo no solo el código sino la totalidad de las dependencias, llegando hasta las librerías que interactúan con el hardware para acceder por ejemplo a los puertos de red o a recursos de CPU y GPU. Y con esto se logra estandarizar el entorno, porque se configura solo una vez y se puede distribuir todas las veces que sea necesario a diferentes hosts sin necesidad de reconfigurar una a una las librerías o el entorno del hardware. Y esta idea de encapsular la aplicación y todas sus dependencias en un contenedor resulta súper útil en el Machine Learning Engineering, porque en un proyecto el equipo puede usar Docker para encapsular diferentes módulos del proyecto, lo que facilita la distribución no solo a otros miembros del equipo sino que también reduce el tiempo de desarrollo y además en últimas facilita todo el proceso de despliegue y producción del modelo de Machine Learning. Y esto es mucho mejor que usar máquinas virtuales, pues por cada máquina virtual se requiere la instalación de un sistema operativo por cada aplicación que queramos ejecutar, mientras que con Docker cada aplicación estará en un contenedor, pero todos los contenedores estarán montados sobre un solo sistema operativo. Esto hace que un contenedor necesite menos recursos, sea más liviano, mucho más portátil y se pueda ejecutar más rápido que una máquina virtual. Bien, teniendo ya una idea detallada de lo que es Docker, veamos una guía que les permitirá fácilmente incorporarlo en sus proyectos de Machine Learning. La creación de un contenedor en Docker es un proceso de tres fases. Primero debemos crear un archivo Docker que contiene simplemente un listado de instrucciones para empaquetar la aplicación. Este archivo se almacena en el directorio del proyecto bajo el nombre Docker File. En segundo lugar está la imagen Docker, que es el paquete de software que contendrá el código, las librerías y todas las dependencias. Es lo que se distribuye a múltiples equipos remotos. Y en tercer lugar está precisamente el contenedor Docker. El contenedor es simplemente el resultado de ejecutar la imagen en un equipo remoto. Es decir que la imagen es una sola, pero puede haber múltiples contenedores si la ejecutamos en múltiples equipos remotos. Veamos en detalle este flujo de trabajo. Primero creamos el Docker File, que debe estar ubicado en la misma carpeta local del proyecto. Este archivo no tiene ninguna extensión y puede ser creado en un simple editor de texto. El formato usado es una instrucción en mayúscula seguida de una serie de argumentos. Usualmente la primera instrucción es FROM, con la cual le indicaremos a Docker la imagen base sobre la cual construiremos el contenedor. Por ejemplo, si nuestra aplicación está desarrollada localmente sobre una versión de Anaconda Python, entonces debemos especificar esto precisamente en el argumento. Otra instrucción muy usada es COPI, con la cual le indicaremos a Docker que tome todo el contenido de la carpeta local del proyecto y lo copie en el directorio que especificemos para el contenedor. Si por ejemplo nuestra aplicación tiene una interfaz a través del navegador, tendremos que habilitar un puerto de red. Para esto usamos la instrucción EXPOSE, seguida del número del puerto de red que queremos usar. Ahora debemos indicarle a Docker en qué directorio del contenedor se encontrará el archivo ejecutable de nuestra aplicación. Para esto usamos la instrucción WORKDIR, seguida de la ruta completa del directorio en el contenedor. Como nuestra aplicación muy probablemente requerirá varias librerías de Python, debemos crear un archivo de texto con estos requerimientos y luego desde Dockerfile incluir una instrucción para que estas librerías sean instaladas. Para eso usamos RUN, seguido del comando para instalar las librerías. Finalmente debemos especificar cuál comando se debe ejecutar al momento de correr la aplicación. Así que usamos CMD y como nuestra aplicación está construida en Python, el comando será simplemente Python, seguido del nombre del archivo que contiene el ejecutable de la aplicación. Y listo, con esto ya tenemos nuestro set de instrucciones, es decir el Dockerfile, con toda la información necesaria para construir primero la imagen y luego el contenedor de Docker. El siguiente paso es entonces construir la imagen, para lo cual tenemos que abrir el terminal de Docker, que viene ya incluido en la aplicación de Docker instalada en nuestro computador local. Nos movemos al directorio local en donde se encuentra nuestro Dockerfile y escribimos el comando Docker, seguido de la palabra clave build, del nombre que queremos darle a la imagen y de la ruta en donde se encuentra el Dockerfile. Y listo, ya tenemos creada la imagen en donde hemos encapsulado absolutamente todo, el código fuente, las librerías y todas las dependencias que se necesitan para poder correr la aplicación remotamente. Finalmente el último paso sería distribuir esta aplicación para ejecutarla remotamente, y aquí existen básicamente tres alternativas. La primera, que de hecho no es muy usada, consiste en crear un archivo comprimido de la imagen local, luego compartirlo con un usuario remoto y allí desempecarlo y luego ejecutarlo. La segunda es distribuirlo a través de Docker Hub, el servicio de Docker para compartir imágenes. Simplemente en este caso usamos Docker push y los usuarios remotos pueden descargar esta imagen con Docker pull y luego ejecutarla con Docker run. Y finalmente está la opción más usada en Machine Learning Engineering, que consiste en crear la imagen local y luego distribuirla y también ejecutarla en un servicio en la nube, como Google Kubernetes, Amazon Elastic Container o Azure Kubernetes. En este caso la imagen se distribuye nuevamente con Docker push, pero la ejecución depende de las particularidades de cada uno de estos servicios. En resumen, Docker es una plataforma que permite fácilmente encapsular y distribuir una aplicación de Machine Learning usando algo que se conoce como contenedor. Y este tipo de herramientas resulta muy útil en diferentes fases del proceso de desarrollo de un proyecto de Machine Learning, pero especialmente cuando queremos desplegarlo y llevarlo finalmente a producción. Así que si están interesados en desempeñarse profesionalmente como ingenieras o ingenieros de Machine Learning, definitivamente Docker es una de esas herramientas que considero deberían conocer y saber manejar. Como les mencioné hace un momento, comenzaremos a hablar de plataformas en la nube, como las ofrecidas por Google, Amazon o Microsoft, que son tal vez las más usadas en la actualidad para desplegar y llevar a producción modelos de Machine Learning. Por ahora esto es todo, los invito a continuar viendo otros videos del canal que les voy a compartir de este lado, les envío un saludo y nos vemos en el próximo video.
