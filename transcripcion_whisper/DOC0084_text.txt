 Muchos hemos oído hablar del término token cuando nos referimos a los grandes modelos de lenguaje, como por ejemplo GPT-4, o cuando usamos aplicaciones como ChatGPT. Y este concepto de los tokens resulta fundamental para poder usar este tipo de modelos, pues lo que hacen en lugar de procesar directamente el texto es recibir y generar precisamente tokens. Así que en este video veremos todos los elementos esenciales relacionados con este concepto de los tokens. Veremos qué son, cómo se generan y por qué resultan esenciales cuando queremos usar estos grandes modelos de lenguaje. Así que prepárense porque en este video vamos a ver varios conceptos que resultan muy sencillos, pero que a la vez son fundamentales para poder construir aplicaciones basadas en estos grandes modelos de lenguaje. Pero antes de comenzarlos invito a visitar codificandobits.com, en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning, que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Además se podrán poner en contacto conmigo si están interesados en asesorías para el desarrollo de proyectos o cursos de formación personalizada. Así que listo, comencemos. Como lo vimos en el primer video que encontrarán acá en esta playlist, los grandes modelos de lenguaje están basados en las redes Transformer, que son un tipo de red neuronal capaz de procesar datos, pero estos datos deben estar precisamente en formato numérico. Sin embargo estos modelos están diseñados para procesar el lenguaje natural, es decir que a la entrada reciben texto y a la salida generan texto. Así que es necesario que internamente haya algún tipo de procesamiento que permita tomar ese texto que introduce el usuario, convertirlo al formato numérico para que esta red neuronal lo pueda procesar, luego esta red neuronal generará unas predicciones también en formato numérico y esos números tendrán que ser convertidos a texto para que nosotros podamos entender la respuesta generada por el modelo. Esta equivalencia de texto a números o de números a texto se conoce como el vocabulario del modelo y existen diferentes formas de construir este vocabulario. Una primera alternativa sería simplemente representar cada carácter en el texto con un número e ingresar esta secuencia de números al modelo, es decir codificar el texto a nivel de caracteres. Pero este enfoque tiene varios inconvenientes. Uno de los principales problemas es que si el texto de entrada es extenso, es decir, si tiene miles o millones de caracteres, tendríamos una secuencia de números inmensa que requeriría una gran capacidad de cómputo y de memoria para que el modelo pueda procesarla. Es decir, que en términos simples no resulta eficiente representar cada carácter con un número. La segunda alternativa es representar cada palabra con un número, es decir, codificar el texto a nivel de palabras. Este enfoque es un poco más eficiente que la codificación a nivel de caracteres. Por ejemplo, en el texto, el gato duerme plácidamente, tenemos cuatro palabras y un total de 27 caracteres, incluyendo espacios en blanco. La codificación a nivel de palabras requeriría solo cuatro números, mientras que a nivel de caracteres requeriría 27 números. Sin embargo, esta codificación a nivel de palabras tampoco es la mejor solución. Uno de los problemas es que los lenguajes naturales tienen una inmensa cantidad de palabras únicas, así que al tener muchas palabras seguiríamos necesitando un vocabulario inmenso. Otro problema es qué hacer con palabras que no estén en el vocabulario. Si dentro del texto aparece una palabra desconocida que no esté inicialmente en el vocabulario, el modelo no podrá procesarla adecuadamente. Y otro problema es que en el lenguaje natural muchas palabras están relacionadas a través de lo que se conoce como las variantes morfológicas. Por ejemplo, las palabras gato, gata, gatito, gatos, gatitos, tienen una secuencia de caracteres que es común a todas, las letras G, A y T. Y de alguna forma el vocabulario usado debería aprovechar estas similitudes para hacer la codificación. Así que la idea al momento de representar el texto numéricamente a través del vocabulario es que esta codificación sea lo más compacta posible, para evitar presentarle demasiados datos al modelo. Y además que esta codificación aproveche las particularidades del lenguaje. Y acá aparece precisamente el concepto de tokens, del cual vamos a hablar en detalle a continuación. Acabamos de ver que no siempre la codificación del texto a nivel de caracteres o a nivel de palabras resulta siendo la más adecuada. Así que en los grandes modelos de lenguaje lo que se usa es la codificación del texto a través de tokens, que son simplemente el resultado de tomar el texto y partirlo en pequeños pedazos, teniendo en cuenta las particularidades del lenguaje que se está usando. Entonces un token puede ser una palabra, una sub-palabra, es decir, una parte de una palabra o por ejemplo un signo de puntuación. Es decir que esta codificación con tokens es un punto intermedio entre la codificación a nivel de caracteres y la codificación a nivel de palabra de la cual hablamos hace un momento. Entendamos este concepto con algunos ejemplos. La palabra una es tokenizada, es decir, es convertida a tokens como una. Es decir, es exactamente igual a un token. Pero la palabra puntos es tokenizada con dos tokens, punt y os. Y esto se debe a que la secuencia de caracteres punt aparecen varias palabras del español y por tanto por ser relativamente común se codifica como un solo token. Por ejemplo podemos encontrar las palabras puntillas y puntapié que tienen exactamente la misma secuencia inicial de caracteres, P, U, N y T. Así dependiendo del lenguaje y de la palabra en particular un token puede ser exactamente una palabra o una porción de una palabra. Y esta codificación termina siendo más eficiente que la codificación a nivel de palabra o a nivel de caracteres y resuelve inconvenientes como la aparición de vocabularios inmensos que requieren más capacidad de memoria y más capacidad de procesamiento o problemas como la presencia de palabras desconocidas que no se encuentren en el vocabulario. Entonces lo que hacen los algoritmos que permiten obtener estos tokens es analizar una gran cantidad de texto que se conoce como el corpus y buscar pequeñas secuencias de caracteres que se repitan dentro de ese texto. Y estos tokens son precisamente la unidad fundamental de información que es procesada y generada por estos grandes modelos de lenguaje. La tokenización es precisamente el proceso de convertir el texto en tokens. Así que entendamos este proceso de tokenización a través de varios ejemplos. En primer lugar es importante tener en cuenta que la tokenización implica el uso de un vocabulario. Es decir que cada uno de los tokens tendrá un número equivalente y este es el número que precisamente será procesado de forma interna por la red transformer o la red neuronal que conforma este modelo de lenguaje y que luego de procesarlo generará un nuevo token también numérico que posteriormente tendrá que ser convertido al formato de texto a través de ese vocabulario para que nosotros podamos entender la respuesta generada por el modelo. Así en el texto el gato duerme tendremos un total de seis tokens cuyos equivalentes numéricos son 417, 308, 5549, 7043, 263 y 1326. En segundo lugar es importante tener en cuenta que la tokenización se lleva a cabo incluyendo los espacios en blanco. Por ejemplo en el texto este y este la primera palabra este no contiene un espacio en blanco al comienzo y por tanto es un token diferente al de la segunda palabra este que sí contiene un espacio en blanco al comienzo. Además como lo mencioné anteriormente una palabra puede representarse con varios tokens. Por ejemplo la palabra esto se representa con dos tokens EST el primer token y O el segundo token. También muchas veces en el texto escrito podremos encontrar emojis. Estos emojis también son representados a través de tokens y al igual que con el texto el número de tokens usados dependerá de qué tan frecuente sea el uso del emoji en el lenguaje de interés. Por ejemplo el emoji de la cara sonriente requiere dos tokens para ser representado mientras que el emoji con la mano hacia arriba requiere cuatro tokens. Y además es posible encontrar números en el texto escrito. En este caso durante la tokenización lo que puede ocurrir es que secuencias de números encontradas comúnmente en el texto sean representadas con un token. Por ejemplo la secuencia de números 1 2 3 4 5 6 7 8 9 y 0 es representada con cuatro tokens. Uno para el grupo 1 2 3 otro para el grupo 4 5 otro para el grupo 6 7 8 y otro para el grupo 9 y 0. Además la equivalencia texto tokens depende del idioma. Así por ejemplo la frase Pone una canción que me haga sonreír tiene 35 caracteres y 7 palabras. En español esta frase es representada con 14 tokens pero en inglés requeriría tan sólo 8 mientras que en chino se necesitan 15 tokens. En general esta equivalencia dependerá del lenguaje en el que se haga el procesamiento y del tipo de modelo que estemos usando pues cada modelo usa un algoritmo de tokenización en particular. Por ejemplo para GPT-4 en promedio cada token equivale a 7 caracteres en inglés 9 caracteres en español y 12 caracteres en chino. Y entender este concepto de tokens resulta esencial para poder construir aplicaciones con estos grandes modelos de lenguaje que se encuentran disponibles. Y la primera razón es porque estos modelos tienen un límite máximo de tokens a la entrada y a la salida. Por ejemplo, ChatGPT se basa en el modelo GPT 3.5 Turbo el cual tiene un límite de 4096 tokens. Esto quiere decir que si sumamos los tokens de entrada y de salida al momento de usar la aplicación el valor máximo será precisamente de 4096. Así que si por ejemplo tenemos un texto en español de 25000 caracteres a la entrada de ChatGPT esto equivaldrá aproximadamente a 2800 tokens. Por tanto a la salida la aplicación podrá generar máximo 1296 tokens es decir, unos 11600 caracteres. Y la segunda razón por la cual resulta importante entender que son los tokens es porque muchos de estos modelos tienen precisamente un costo asociado al número de tokens que usemos o que introduzcamos a la entrada y que esperemos generar a la salida de estos modelos. Es decir, que el costo en dinero de utilizar el modelo se va a incrementar a medida que usemos más tokens es decir, más texto tanto a la entrada como a la salida. Por ejemplo, supongamos que queremos usar el modelo GPT 3.5 Turbo en nuestro sitio web para crear una aplicación de chat para nuestros usuarios. En este caso tendremos que pagar a OpenAI por el uso de este modelo. Si un usuario ingresa a nuestro sitio web e introduce un texto en español de 10.000 caracteres a la aplicación lo que es aproximadamente igual a unos 1100 tokens y el modelo genera una respuesta también de unos 10.000 caracteres entonces el costo de procesamiento será según los precios actuales de uso establecidos por OpenAI 0.0015 dólares por cada 1000 tokens de entrada. Es decir, que el costo de procesar los 1100 tokens de entrada serán 0.00165 dólares y 0.002 dólares por cada 1000 tokens de salida. Es decir, que el costo para generar los 1100 tokens de salida serán 0.0022 dólares. Así que para esta solicitud simple el costo total serían apenas 0.0038 dólares. Esto parece muy económico pero tengamos en cuenta que estamos considerando solo una pregunta del usuario y una respuesta generada por el modelo. Pero si la conversación tiene 10 preguntas y 10 respuestas y asumimos que cada una tiene más o menos la misma cantidad de tokens ya estaríamos hablando de 0.038 dólares. Y si asumimos que en lugar de un usuario tenemos por ejemplo 50 usuarios interactuando diariamente con la aplicación el costo sería de casi 2 dólares diarios. Así que es evidente que entre más solicitudes hagamos al modelo tendremos más tokens procesados que se sumarán al costo total de uso del modelo. Muy bien, acabamos de revisar los conceptos esenciales acerca de los tokens y esto nos permite entender por qué resultan fundamentales al momento de usar estos grandes modelos de lenguaje. Hemos visto que en el lenguaje natural un token puede ser una palabra, una subpalabra o también nos puede permitir representar absolutamente cualquier elemento dentro del texto escrito como por ejemplo emojis, números o signos de puntuación. También hemos visto que el número total de tokens usados para representar el texto va a depender en últimas del idioma y que hay idiomas para los cuales el número promedio de tokens es menor como por ejemplo en el caso del inglés y otros para los cuales el número de tokens es mucho mayor como por ejemplo el chino. Y finalmente es importantísimo entender que el número de tokens determinará en última instancia el costo que pueda tener el uso de estos modelos para el procesamiento y generación de texto. Y que además los tokens determinan ese límite máximo de datos que podremos ingresar al modelo y que podremos esperar como respuesta del modelo al momento de la generación de texto. Si tienen alguna duda de lo que acabamos de ver en este video no olviden dejarla abajo en los comentarios y recuerden que si les gustó el video los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con todos sus amigos y conocidos pues esto me ayudará a llegar a cada vez más y más personas con este tipo de contenido. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido. Por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
