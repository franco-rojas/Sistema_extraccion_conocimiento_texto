 Este vídeo está patrocinado por la Universitat Politécnica de València. Veamos. Concentración. Si tuvieras que enfrentarte a cómo resolver este problema, ¿cómo lo harías? No digo de saber cómo usar redes neuronales o alguna técnica que conozcas ya, sino realmente, ¿a qué tipo de solución intuitiva hubieras llegado tú si a mediados del siglo XX hubieras tenido que enfrentarte al problema de que una máquina entendiera el contenido de una frase expresada de manera natural? Por ejemplo, para saber si el sentimiento de esta frase es positivo o negativo. En mi caso, mi primer acercamiento a este problema pues sería el siguiente y es que básicamente el lenguaje que estoy utilizando para comunicarme pues está aplicando una serie de reglas gramaticales que podría ser interesante tener en cuenta y codificarlas, programarlas. Y así, poquito a poco y con mucha paciencia, iremos construyendo todo nuestro lenguaje codificándolo en pues cubriendo todas las combinaciones, todos los matices, todas las enviguedades, todos los... Intentar traducir a reglas formales todas nuestra forma de comunicación es una tarea no sólo increíblemente difícil, sino que además deberá de ser actualizada, puesto que el lenguaje está en continua evolución, significando esto que el humano encargado de codificar todas estas reglas pues va a tener que dedicarle bastante tiempo. Pero en este punto ya hemos estado, ¿no? Al final, esto es similar al por qué no tenemos un algoritmo que instrucción a instrucción te vaya describiendo formalmente cuáles son los pasos para conducir correctamente un coche o clasificar un gran dataset de imágenes con gran precisión. A ver, ¿no contaremos ya con algún tipo de tecnología que nos pueda automatizar todo este proceso y que pueda aprender automáticamente todas estas reglas? Dice mi nombre. Machine Learning. ¡Tú eres bien! Y este es el primer punto a entender. Y es que históricamente el campo del Natural Language Processing se ha desarrollado entre dos mundos, en el que por un lado tenemos las técnicas que se apoyan más en la codificación manual de todas aquellas reglas gramaticales que son interesantes a utilizar para la resolución del problema que tengamos. Y por el otro lado contamos con técnicas de Machine Learning que directamente se basan en el análisis de grandes corpus de texto para dejar que todas estas reglas sean aprendidas. Y en medio de estos dos polos lo que nos encontramos pues es un abanico de muchísimas técnicas y algoritmos que se han ido desarrollando a lo largo de las últimas décadas, pero que bueno, como es de esperar, pues en los últimos años ha ido ganando más importancia aquellas más cercanas al campo del Machine Learning. Y son en esta donde vamos a poner nuestro foco. Y es que por suerte en los últimos años hemos ido perfeccionando unos potentes algoritmos, las redes neuronales, que son capaces de tomar datos de entrada, como por ejemplo texto, y mostrándole el resultado que queremos conseguir supervisando su aprendizaje, pues podemos dejar que sea el propio algoritmo el que aprenda. A ver, ¿qué pasa aquí? ¿Por qué no entra el texto? ¡Eh tú! ¿Qué problema tienes con el texto? Eh, que no, que no lo digiero bien. O sea, internamente yo solo proceso cosas numéricas. Ya sabes, vectores bajos en grasas, numeritos más digestivos. ¿Qué es probar uno de estos caracteres? Y es que no sé cómo digerirlos. ¡Todo el día con los parámetros revueltos! Pues... en realidad tiene razón. Y es que, como ya sabemos, pues una red neuronal en realidad es una máquina de analizar datos, representados evidentemente de manera numérica. Es decir, cuando hablamos de dar como input una imagen, realmente lo que le estamos pasando a la red neuronal es una matriz de valores numéricos donde cada número representa las intensidades de un pixel. O cuando insertamos una tabla de datos, pues aquellas variables categóricas representadas con etiquetas también las transformamos a valores numéricos. O en un fichero de audio, las ondas analógicas del sonido pues ya han sido previamente digitalizadas. Es decir, necesitamos representar a nuestro texto de forma numérica para que así nuestras redes neuronales lo puedan procesar. ¿Se te ocurre algún plan? Pues, ¿a poco que sepas cómo funcionan internamente nuestros sistemas operativos, quizá ya tengas una primera idea. Y es que, por ejemplo, sabrás que en nuestros ordenadores cada carácter que tecleamos y almacenamos realmente viene representado por un código numérico donde cada símbolo está asociado con una etiqueta, donde el igual es el 61, la L mayúscula el 76 o este símbolo de aquí el 185. Bueno, y esta estrategia es bastante legítima pues podemos utilizarla para representar también nuestros textos. Pero antes de avanzar voy a aclarar una cosa, y es que antes estábamos hablando de palabras y ahora estamos hablando de codificar caracteres. ¿Cuál de la tos es realmente la buena? Pues realmente depende. Depende del problema que quieras resolver y cómo lo quieras afrontar. Cada una de estas opciones te va a dar ventajas y desventajas. Por ejemplo, como te podrás imaginar, trabajando con un vocabulario de palabras ya formado pues es mucho más fácil generar texto que sea realista. Pero la contraparte es que si directamente te basas en generar cada palabra como una secuencia de caracteres, pues realmente tienes la posibilidad de que tu algoritmo pueda aprender incluso a generar palabras que no existen. Como ves, pues cada una tiene su ventaja. E incluso podemos encontrar estrategias intermedias para dividir a nuestro texto. Por ejemplo, por subpalabra. Método que como curiosidad te cuento es el que utilizan los modelos GPT-2 y GPT-3 que hemos visto en el canal. Es a cada uno de estos bloques que conforman nuestra secuencia de datos, ya sea palabras, caracteres o subpalabras, lo que se le denomina token. Y a este proceso de división, tokenización. En nuestro caso continuaremos el resto del vídeo hablando a nivel de palabra. Es decir, cada token es una palabra. Y como habíamos dicho que lo vamos a representar así, asignándole una etiqueta numérica a cada uno de estos tokens. Así que lo que tenemos que hacer es, eligiendo el criterio que no te la gana, incluso poniendo etiquetas aleatorias, asociar cada una de las palabras con un número. Y lo único que tenemos que tener en cuenta es que cada palabra siempre venga con la misma etiqueta. Y con esto ya habríamos solucionado el problema. Nuestro texto ha sido convertido, ha sido representado a una forma numérica. ¿Verdad? ¿Verdad? ¿Verdad? Pues no. Agárrate porque vienen curvas. Y es que hay un pequeño problema. A ver, no es un problema grave. Si quisieras podrías empezar ya a utilizar esta codificación para analizar tu texto. Pero estamos desvirtuando un poco el problema. Fíjate en lo siguiente. Imagínate que no estuviéramos trabajando con texto, sino con un dataset sobre personas. Tú, yo, el de atrás y el de la esquina. Y uno de los atributos que estuviéramos mirando fuera la edad. Encontraríamos una columna en la tabla de datos donde vendría representada la edad. Y por ejemplo uno podría tener 15 años, otro 20 años y otro 30 años. Y bueno, nuestra red neuronal se comería esos valores y básicamente al analizarlo entendería las diferencias relativas entre un valor y otro sobre la dimensión de esta columna. Es decir, entendería que este valor de aquí es cuantitativamente menor que este otro valor de aquí. Y que por ejemplo este es el doble de este. Y toda esta información sería la que la red utilizaría para resolver cual sea el problema que estuviéramos resolviendo. Pero claro, la red solo ve números. Pero realmente no sabe que esta columna representa la edad o el número de ojos que tiene esa persona o que esto realmente sea la etiqueta asociada a una palabra. Esto genera un problema, porque a lo mejor numéricamente este 15 es el identificador de la palabra teléfono. Y este 20 el de la palabra tostadora y este 30 el de la palabra pangolín. Y claro, haciendo esto lo que le estamos diciendo a la red es que un pangolín esto veces mayor que un teléfono o que una tostadora se encuentra a mitad de camino entre un pangolín y un teléfono. Algo que no tiene realmente sentido. Y si esto es así, ¿se te ocurre otra solución? Pues fíjate. La solución viene por buscar una representación diferente. Donde en vez de asignarle una etiqueta numérica a cada palabra, lo que haremos será asignarle un vector. Un vector con tantas posiciones como palabras queramos tener en el vocabulario de nuestro problema. Mil palabras, veinte mil palabras, cien mil palabras, las que queramos especificar. Cada una de las posiciones de este vector representará a cada una de las palabras de nuestro vocabulario y será marcando con un uno en la posición que queramos con la que especificaremos que es esa la palabra que estamos representando. Era el teléfono el que tenía la etiqueta 15, pues buscamos la componente 15 del vector y la marcamos. Para entender por qué esto es interesante, lo mejor será verlo a nivel geométrico. Imagínate que en nuestro caso, pues todo nuestro vocabulario estuviera conformado solo por estas tres palabras. Teléfono, tostadora y pangolin. En este caso nos limitamos solamente a tres palabras, pues porque geométricamente nuestros obtusos cerebros de primates son incapaces de imaginarse más de tres dimensiones. Así que nos quedaremos con estas tres palabras. Teléfono tendrá la etiqueta cero, tostadora la etiqueta uno y pangolin la etiqueta dos. Y esto reconvertido vector sería su nueva representación. Lo que estamos haciendo ahora, al pasar su representación de esto a esto, es convertirlo en vectores. Vectores de tres componentes que si lo visualizamos en un espacio tridimensional se colocarían de la siguiente manera. ¿Lo ves? Lo que está ocurriendo es que en este caso cada palabra ocupa su propia dimensión. Y ahora si calcularamos la distancia entre cada palabra geométricamente, veríamos que la distancia siempre es la misma. Es constante. Y por tanto ya no tenemos el problema de representación que teníamos anteriormente. ¿Lo entiendes? Y a lo mejor estés pensando, oye Carlos, ¿no es esto un gasto innecesario? Al final, para una frase que contenga veinte palabras en un vocabulario de tamaño mil, pues esto se convierte en una matriz de veinte por mil, veinte mil elementos, donde además la mayoría de números van a ser ceros. Esto sería como querer almacenar en todo un diccionario la definición de una única dejando el resto de páginas sin definiciones, en blanco, un diccionario por palabra. Algo que, bueno, pues parece que es una representación que ocupa demasiado espacio. No lo podemos hacer mejor. Y bueno, no solo eso, está muy bien esta representación para evitar tener algún tipo de ordenación donde tostadora sea menor que pangolín y mayor que teléfono. Pero también es cierto que una tostadora y un teléfono son palabras que conceptualmente están más próximas de lo que lo estarían de un pangolín. Y aquí estamos asumiendo que la diferencia entre todas ellas es la misma. ¿No tendría más sentido algo como esto? Pues sí. Y esta solución vendrá dada por un concepto interesantísimo denominado embeddings. Un concepto que realmente en nuestra serie será el primer punto de encuentro del campo del natural language processing con el campo del deep learning. Un concepto que veremos, sí, pero lo veremos en el próximo vídeo. Video que por cierto se verá muy pronto en el canal. Lo estoy preparando para tenerlo listo este mes. Y de hecho, mira, si conseguimos que este vídeo tenga más de 10 mil likes. Wow, Carlos, eres todo un youtuber. Lo sacaré la próxima semana. Igualmente, si quieres apoyar el canal, ya sabéis que lo podéis hacer a través de Patreon. Tenéis un Patreon hermosísimo disponible donde podéis apoyar todo el contenido que hago y que además si participáis, pues podéis entrar directamente a un grupo de telegram donde hay muchísima gente hablando también de machine learning y discutiendo de muchas cosas que les puede interesar. Por mi parte, nada más. Muchas gracias por ver el vídeo entero y nos vemos en la siguiente parte.
