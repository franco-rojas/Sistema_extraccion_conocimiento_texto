 Hola a todos y bienvenidos a Codificando Bits, el lugar donde aprenderán toda acerca de la inteligencia artificial, el deep learning y la ciencia de los datos. Les recuerdo que si quieren ver más detalles pueden visitar mi sitio web codificando bits.com. Hoy hablaremos del algoritmo del gradiente descendente que es el algoritmo esencial en la etapa de aprendizaje de la mayor parte de los modelos de deep learning que veremos más adelante. Así que comencemos. Primero, una idea general de qué es el gradiente descendente. El gradiente descendente es un algoritmo que permite de forma automática, es decir, una vez programado de forma automática encontrar el valor mínimo de una función. Acá por simplicidad les he mostrado pues una función muy sencilla, una función que llamamos E, que depende de una variable que se llama W, está en el eje horizontal y esa función tiene una forma de una parabola. Entonces el gradiente descendente permite calcular o encontrar de forma automática ese punto de color verde que estamos dibujando ahí que corresponde precisamente al mínimo de la función que les mostraba anteriormente. He dibujado tres puntos y al lado de cada uno de esos puntos he dibujado unas líneas rectas, unas líneas tangentes a la gráfica de la función. En el primer caso, en el primer punto vemos una línea de color gris con una inclinación, esa inclinación recibe el nombre también de pendiente, o sea la pendiente es una medida de qué tan inclinada está esa línea recta en ese punto y aquí lo que podemos decir es que para ese punto 1 la pendiente es positiva. Para el punto 2 vemos que la línea es totalmente horizontal, no tiene ningún tipo de inclinación, entonces ahí hablamos de que la pendiente es exactamente igual a cero. Y en el punto 3 vemos que la pendiente tiene una dirección opuesta a la del punto 1, es decir que estamos hablando de que la pendiente en ese punto 3 tiene un valor negativo. Entonces ¿qué es el gradiente? Pues el gradiente simplemente es una medida de qué tan grande o qué tan pequeña o el signo que tiene la pendiente. Otros nombres que recibe ese gradiente o esa pendiente, matemáticamente eso se conoce como la derivada. Entonces si yo conozco la expresión matemática para esa parábola que vemos en la gráfica del lado izquierdo, yo puedo aplicar la operación matemática de la derivada y con eso entonces tengo otra función matemática que me permite calcular la pendiente o el gradiente para diferentes valores de W. Ahora bien, ¿esto para qué nos sirve? Entender ese concepto del gradiente o la pendiente, para entender precisamente cómo funciona el método del gradiente descendente. Es muy muy sencillo, recuerden que el objetivo de ese método, el gradiente descendente, es encontrar el mínimo de una función matemática. En este caso la función que estamos considerando es esta parábola que está acá dibujada del lado izquierdo y el método consiste en lo siguiente. Inicialmente el usuario tiene que definir dos parámetros. Un parámetro que se llama la tasa de aprendizaje, ya vamos a ver qué papel juega esa tasa de aprendizaje en inglés de learning rate y define el número de iteraciones, o sea cuántas veces se va a repetir el mismo procedimiento en el algoritmo. Esos son los dos únicos parámetros que la persona tiene que introducir al momento de programar el algoritmo. Entonces, ¿en qué consiste? Antes de iniciar las iteraciones el algoritmo de forma aleatoria escoge un punto cualquiera de la gráfica. Es decir, define un valor aleatorio para este parámetro W, que en nuestro caso es el que controla o el que define los valores de la función E. Entonces escoge aleatoriamente un valor inicial de W, puede ser muy grande, puede ser muy pequeño, totalmente aleatorio y a medida que se van ejecutando las iteraciones el algoritmo lo que hace es usar la ecuación que vemos acá en la parte superior derecha y es en cada iteración el valor W se va actualizando obedeciendo a la siguiente ecuación. Se toma el valor actual de W y a ese valor se le resta el gradiente en ese punto W multiplicado por la tasa de aprendizaje que fue el parámetro que introdujo el usuario al programar el algoritmo. Entendamos un poco esta ecuación. Entonces digamos que en la primera iteración el algoritmo escoge aleatoriamente este punto que estamos mostrando acá en el lado izquierdo, el punto 0. Ese punto 0 lo escoge aleatoriamente, es decir, el define de forma arbitraria el valor de W sub 0, el algoritmo, y fíjense que en este punto he dibujado una línea blanca que corresponde a la pendiente de la parábola en ese punto 0. Esa pendiente como veíamos anteriormente es una pendiente negativa, eso es un número negativo. Entonces, ¿qué pasa? Cuando vamos a la siguiente iteración y aplicamos la ecuación que está del lado derecho, el siguiente valor de W, es decir W sub 1, ¿a qué va a ser igual? Va a ser igual al W sub 0 que tenemos acá definido inicialmente, menos una cierta cantidad de veces, que es el learning rate, que es un valor positivo, una cierta cantidad de veces el gradiente. Como en este punto el gradiente es negativo, al multiplicar ese gradiente negativo por la tasa de aprendizaje que es positiva, sigo obteniendo un número negativo y al multiplicar ese resultado por el menos de la ecuación acá del lado derecho, lo que tengo realmente es un número positivo. Entonces lo que me dice este algoritmo es que en la siguiente iteración a este valor W sub 0 original le voy a sumar en últimas un pequeño valor, luego el valor W sub 1 va a estar ubicado del lado derecho del punto inicial. Entonces, observen lo que ocurre en esta primera iteración, en últimas estamos pasando del punto 0 al punto 1 y si miramos la altura del punto 0 y la altura del punto 1, lo que estamos viendo es que tras una iteración obtenemos un valor de la función más pequeño que el que teníamos originalmente. Si repetimos el mismo procedimiento, es decir, si el W sub 2 que vamos a calcular es el W1 que tenemos acá dibujado del lado izquierdo, menos alfa veces el gradiente, en este punto 1 el gradiente sigue siendo negativo, luego menos alfa veces el gradiente nos da una cantidad positiva, luego lo que vamos a hacer es a W1 sumarle un número positivo y por tanto el W2 va a estar ubicado más a la derecha. Entonces, fíjense que poco a poco y progresivamente nos vamos acercando a ese valor mínimo de la función E que es el que estamos buscando. Si repetimos el procedimiento para 1.3 y para 1.4 pues ya era un momento en la iteración que ya llegamos al mínimo de esa función. ¿Qué pasa si el algoritmo al definir el valor inicial de W lo hubiese escogido del lado derecho? Porque recuerden que este valor inicial se define de forma aleatoria. Hubiera ocurrido exactamente lo mismo, aquí la diferencia ¿cuál es? Que en ese punto inicial la pendiente en este caso es positiva, esa línea recta de color blanco dibujada al lado del punto cero es una pendiente positiva. Entonces ese término alfa por el gradiente es una cantidad positiva y al agregar el signo menos lo que vamos a hacer es que al W0 le vamos a restar una cierta cantidad. Al restarle una cierta cantidad lo que estamos diciendo es que ese W0 se va a mover ahora en la siguiente iteración hacia el lado derecho. Y fíjense que nuevamente entonces poco a poco desde el otro extremo vamos a comenzar a acercarnos a ese valor mínimo. Si repetimos las iteraciones vamos a ver que progresivamente vamos llegando a ese valor mínimo. Entonces esta es la idea general del algoritmo del gradiente descendente, este es un ejemplo sencillo para una función que depende de una sola variable que en este caso se llama W, pero el algoritmo se aplica a cualquier tipo de función que dependa de una de dos o de más variables, es decir que sea de dos o de tres o de cuatro o de más dimensiones y también funciones que tengan diferentes formas. Esta función es muy sencilla porque es evidente que tiene un único mínimo que es el punto cuatro que estamos aquí ilustrando, pero si la función tiene otras características, otra forma, otro comportamiento también se va a poder aplicar el algoritmo del gradiente descendente. Entonces para resumir este algoritmo del gradiente descendente permite detectar el mínimo de una función, de una función matemática de forma automática, lo único que requiere al momento de ser programado son dos parámetros, la tasa de aprendizaje o learning rate y el número de iteraciones o la cantidad de veces que yo quiero ejecutar el algoritmo para poco a poco ir llegando a obtener ese valor mínimo de esa función. Bien esto ha sido todo, gracias por haberme acompañado y no olviden suscribirse al canal y visitar mi sitio web codificando bits.com. Hasta luego.
