 En el video anterior hablamos del análisis exploratorio de los datos, y allí mencionamos que en cualquier proyecto de Machine Learning o de ciencia de datos nos tenemos que enfrentar a una situación ineludible, y es que los datos no son ideales, pues en este video nos vamos a enfocar en uno de esos problemas ineludibles y que es una tarea fundamental del análisis exploratorio de datos, que es el manejo de datos faltantes. Así que veremos en detalle las dos grandes técnicas para el manejo de datos faltantes, la eliminación y la imputación. En la descripción del video les voy a dejar un enlace para que puedan descargar una guía con todo lo que vamos a hablar ahora, para que fácilmente puedan utilizarla en sus proyectos cuando sea necesario. Pero antes de entrar en todo esto les quiero contar que el próximo 10 de julio haré el lanzamiento de la Academia Online de cursos de Machine Learning y Ciencia de Datos, así que si están interesados, pueden entrar a codificandovids.com y diligenciar el formulario con sus datos para que a vuelta de correo yo les envíe más detalles de estos cursos. Para saber cuál técnica podremos usar para el tratamiento de estos datos faltantes, primero tenemos que determinar cuál es el mecanismo detrás de esta pérdida de datos, y estos mecanismos se dividen en tres. Primero están los datos faltantes completamente aleatorios, y en este caso la razón de la falta de los datos es ajena a los datos mismos. Para entender esto supongamos que tenemos un set de datos y que los datos faltantes aparecen tanto en la categoría A como en la B o en la C, y los valores faltantes pueden ser altos o bajos. Esto quiere decir que estos datos faltantes no dependen ni de la categoría ni del valor mismo de los datos, por lo que podemos decir que el mecanismo es completamente aleatorio. El segundo mecanismo, es decir, los datos faltantes no aleatorios, es totalmente opuesto al anterior. Esto quiere decir que la razón detrás de la falta de esos datos depende precisamente de los datos que hemos recolectado. De nuevo esto se entiende mejor con un ejemplo. Volviendo a nuestro set de datos hipotético, podemos ver que sistemáticamente los datos con valores menores a 100 faltan, tanto para las categorías A como B como C. Es decir que los valores faltantes dependen de la variable B2, y por tanto la razón de la falta de datos no es aleatoria. Este mecanismo es el más complicado de todos, porque como la pérdida de datos es sistemática, es decir hay una razón de fondo, tenemos que encontrar esa razón, intentar corregir el problema y muy probablemente adquirir los datos nuevamente. Y el tercer mecanismo, es decir, el de los datos faltantes aleatorios, es un punto intermedio entre los dos anteriores. Esto quiere decir que la razón detrás de la falta de estos datos no depende de los mismos datos faltantes, pero sí puede depender de otras variables o otras columnas dentro del mismo set de datos. De nuevo en el ejemplo vemos que los datos faltantes corresponden únicamente a datos en la categoría B, y que estos datos faltantes van desde los más pequeños a los más grandes. Esto quiere decir que los valores faltantes dependen sólo de la variable B1, la categoría, y no de la propia variable B2. Para este mecanismo y para el primero también podremos usar las técnicas que vamos a ver a continuación. Estas técnicas se dividen en dos grandes grupos, que son el descarte y la imputación. El descarte consiste simplemente en eliminar los registros que contengan datos faltantes, mientras que en la imputación lo que buscamos es estimar el valor del dato faltante, bien sea usando la información de los registros vecinos o la información de las columnas o variables que hacen parte de nuestro set de datos. En términos generales, estas técnicas las podemos aplicar a datos faltantes aleatorios o completamente aleatorios, es decir que en el fondo asumen que existe algún grado de aleatoriedad en este mecanismo responsable de la pérdida de los datos. Sin embargo, para el caso de datos faltantes no aleatorios, no es aconsejable usar estas técnicas, porque como les comenté antes, en este caso la pérdida de datos es sistemática. Entonces ahora sí hablemos de los algoritmos más usados en cada caso cuando implementamos un proyecto de Machine Learning o de ciencia de datos. En el caso del descarte, la eliminación se puede hacer de dos formas, la primera se conoce como eliminación de la lista, y consiste en remover del set de datos las filas que contengan datos faltantes, con la desventaja de que al eliminar una fila completa eliminaremos también algunos datos existentes, lo que puede llevar a una pérdida significativa de información. La segunda forma es la eliminación por pares, que es un método menos agresivo que el anterior, pues en lugar de eliminar la fila completa se quitarán únicamente las casillas con el dato faltante. La ventaja es que preservaremos los datos conocidos, pero la desventaja es que podremos tener características, es decir columnas, con diferente cantidad de datos, lo que puede complicar el entrenamiento de un modelo de Machine Learning, pues el número de datos debe ser el mismo para cada característica. De todos modos les sugiero usar estos métodos con pinzas, porque si tenemos demasiados datos faltantes, generalmente un poco más del 10%, lo que puede ocurrir es que al eliminarlos estaremos cambiando la distribución de nuestros datos originales, y esto puede afectar el modelo de Machine Learning que entrenemos más adelante, o el análisis que hagamos posteriormente. Por ejemplo, si estamos hablando de un problema de clasificación, al final con esta eliminación podríamos tener más datos de una categoría que de otra. Para evitar una pérdida significativa de datos, lo mejor es usar la imputación. Y ojo, porque esto no es lo mismo que inventar datos. Cuando inventamos no tenemos en cuenta ningún criterio y el valor asignado es totalmente arbitrario, pero en la imputación lo que hacemos es mirar los valores de los datos vecinos para tener una estimación aproximada del valor del dato faltante. Idealmente esta imputación no debería cambiar la distribución de nuestros datos, así que si originalmente teníamos una distribución normal con forma de campana, entonces después de la imputación se debería mantener esta forma original. Para esta imputación podemos usar dos técnicas, la imputación simple y la imputación múltiple, y en este caso sugiero, al igual que con el descarte, usarlas solo si estamos seguros que los mecanismos detrás de la pérdida de datos corresponden a datos faltantes aleatorios o completamente aleatorios. En la imputación simple se usa un algoritmo para hacer una única estimación, y el valor obtenido se usa para reemplazar el dato faltante correspondiente. En este caso las tres técnicas más usadas en el Machine Learning y la ciencia de datos son la imputación por la media o la mediana, la imputación por regresión y la imputación hotdeck. La imputación por la media o la mediana es la más sencilla de todas. Simplemente se toman los valores conocidos en la variable donde están los datos faltantes, se calculan la media o la mediana y se reemplazan estos datos faltantes con cualquiera de estos dos valores. Aunque es muy fácil de implementar, este método tiene la desventaja de que al reemplazar muchos datos faltantes con un único valor estaremos cambiando la distribución de los datos. Una alternativa es hacer la imputación por regresión, en este caso cada dato faltante es reemplazado con el valor predicho por un modelo de regresión. Para esto primero se combina la información de la columna con los datos faltantes con columnas en donde los datos están completos, para así ajustar un modelo de regresión y luego se usa este modelo para predecir los datos faltantes. Este método es mejor que la imputación por la media o la mediana, porque los datos faltantes no serán reemplazados con un único valor en todos los casos, lo cual nos permite preservar la distribución original de los datos. La desventaja es que para poder realizar la regresión, lineal, logística o polinómica, debemos garantizar que hay algún tipo de correlación entre los datos que usamos para construir ese modelo de regresión. El tercer método de imputación simple es la imputación hotdeck. En este caso el dato faltante es reemplazado con valores tomados de datos cercanos a ese dato faltante. Dentro de esta categoría el método más usado es el de k vecinos más cercanos o knn por sus siglas en inglés. Este algoritmo busca los k valores más cercanos, donde k es un número entero como 2, 3 o 10 por ejemplo y reemplaza el valor faltante con el promedio de estos vecinos. La ventaja de este método es que es mucho más preciso que la media o la mediana e incluso lo podemos usar en lugar de la regresión cuando no podemos garantizar que haya correlación entre los datos. La desventaja es que si tenemos demasiados datos el tiempo de cómputo es elevado porque tenemos que calcular para cada dato faltante su distancia con respecto a cada uno de los demás datos del set. Una alternativa más robusta que todas las técnicas que vimos anteriormente es precisamente la imputación múltiple que de hecho es una de las técnicas más usadas en la actualidad y es que todas las técnicas de imputación siempre tienen un gran problema y es que para reemplazar el dato faltante se fían de una única estimación. Es como intentar reemplazar la altura desconocida de una persona con el promedio de otras mil personas, muy probablemente esa estimación va a estar alejada del valor real. Así que en lugar de hacer una sola estimación en la imputación múltiple como su nombre lo indica se hacen múltiples estimaciones y luego todas ellas se combinan en un único valor que es el que reemplazará el dato faltante con lo cual se evita el sesgo precisamente en esta estimación. El método de imputación múltiple más usado es el algoritmo de imputación múltiple con ecuaciones encadenadas o MICE por sus siglas en inglés. En este algoritmo de forma iterativa se harán progresivamente cada vez mejores estimaciones de los datos faltantes. Inicialmente la primera estimación no es muy buena y se hace con la imputación por la media que vimos anteriormente, pero en los pasos restantes se aplica una regresión lineal entre pares consecutivos de columnas y el procedimiento se repite una y otra vez hasta completar un número predefinido de iteraciones. La idea es que progresivamente las estimaciones sean cada vez más precisas y se acerquen más y más al valor real. Como les decía este algoritmo MICE es mucho más robusto que cualquiera de los de imputación simple y en la práctica no cambia la distribución obtenida. Sin embargo tampoco es perfecto porque para poderlo utilizar tenemos que garantizar que las variables están relacionadas linealmente. De no ser así debemos usar modelos estadísticos más sofisticados o incluso entrenar un modelo de machine learning para que pueda hacer esa regresión. Bien y con esto ya tenemos una guía completa de los métodos más usados para el manejo de los datos faltantes. En resumen recordemos que el primer paso es determinar el mecanismo que da origen a estos datos faltantes. Si este mecanismo es aleatorio o completamente aleatorio entonces podemos usar las técnicas que les acabo de mencionar, pero si el mecanismo es no aleatorio en este caso lo que les recomiendo es recolectar más datos porque si usamos cualquiera de las técnicas anteriores con este tipo de datos podemos introducir un sesgo en la distribución del data a ser resultante. Si miramos en detalle las técnicas creo que las más robustas son la de cabecinos más cercanos para la imputación simple y obviamente el algoritmo MICE para la imputación múltiple. En todo caso la técnica que escojan va a depender mucho del tipo de datos que ustedes tengan en su proyecto. Recuerden que en el enlace que les dejo en la descripción del video van a poder descargar la guía completa con las recomendaciones para que puedan utilizar estas técnicas en sus proyectos de machine learning o ciencia de datos. También tengan en cuenta que existen otras técnicas para el manejo de datos faltantes cuando estamos hablando de series de tiempo, pero estas son totalmente diferentes de las técnicas que acabamos de ver hace un momento, así que si están interesados déjenme sus comentarios para preparar un video sobre este tema. Por ahora esto es todo, si les gustó el video les agradezco un montón darle un pulgar hacia arriba y compartirlo con sus amigos y conocidos y si aún no lo han hecho suscribirse también al canal. Les envío un saludo y nos vemos en el próximo video.
