 Cuando queremos hacer algún tipo de análisis de datos, sea hallar algún estadístico, hacer alguna visualización, algún filtrado o entrenar un modelo de Machine Learning, usualmente cargaremos nuestros datos desde un archivo almacenado en nuestra computadora hacia un DataFrame de Pandas, y aquí gracias a la magia de Pandas vamos a poder hacer el análisis que querramos. Esta magia y rapidez de Pandas se debe a que los DataFrames se cargan en la memoria RAM de sus computadoras una sola vez. Luego de esta carga inicial, cualquier análisis o operación ejecutada sobre el DataFrame se hace con los datos ya cargados en la memoria RAM. Lo interesante es que las memorias RAM son muy rápidas al momento de operar sobre datos que allí se encuentran, pero ¿qué pasa cuando mis datos no entran en mi memoria RAM? Teniendo en cuenta que hoy en día la mayoría de computadoras personales tienen entre 8 y 16 GB de RAM, esto podría suceder muy fácilmente. En este punto vamos a obtener un error si queremos cargar todo en un DataFrame de Pandas y nos tocará implementar una solución en donde nuestro código lea y opere sobre nuestros datos por partes. Estas múltiples lecturas al disco duro son lo que va a matar la eficiencia y rapidez en nuestro proceso, ya que en comparación a la memoria RAM, leer datos del disco duro es de 10 a 100 veces más lento dependiendo de la calidad del disco. Sin embargo, ¿qué pasa si tenemos más de una máquina a nuestra disposición? Por cada máquina tendríamos una memoria RAM adicional que podríamos utilizar para que este procesamiento siga siendo rápido, y por sobre todo que soporte grandes cantidades de datos. Con esta premisa nacen algunos problemas, ¿cómo dividimos los datos en las memorias RAM de varias máquinas? ¿Cómo controlamos qué datos están en cuál máquina? ¿Cómo unimos los datos cuando queremos hacer algún análisis como una agrupación? Con estos problemas nace una solución, Apache Spark. Spark es un framework para procesar grandes cantidades de datos, cuya idea principal es distribuir los datos en las memorias RAM de varias máquinas y que cada máquina se encargue del procesamiento de la parte de los datos que está en ella. A este conjunto de computadoras se le denomina clusters, y a cada computador en el cluster se le llama nodo. Por cada cluster existe un nodo maestro o master node y varios nodos trabajadores o worker nodes. El nodo maestro se encarga de recibir las tareas que tienen que ejecutarse y dividir y repartir estas tareas a los nodos trabajadores, los cuales van a ejecutar estas tareas en paralelo sobre su parte de los datos utilizando el paradigma de programación map reduce, que en pocas palabras es un divide y conquista para grandes cantidades de datos. Y en este punto las tareas pueden ser, por ejemplo, cargar los datos, hacer algún filtrado, alguna agrupación, una somatoria o incluso entrenar un modelo de machine learning. Cuando los datos se cargan, estos se distribuyen, idealmente de manera uniforme a lo largo de las memorias RAM de los nodos workers. Y el nodo maestro conoce hasta cierto punto cómo se encuentran hechas estas divisiones de los datos, de modo que el nodo maestro pueda en algunos casos saber a qué nodos trabajadores asignar ciertas tareas. Por ejemplo, si el usuario nos pide mostrar los cinco primeros registros de un dataset, en este ejemplo solo el nodo trabajador 1 y 2 van a recibir tareas, evitando el trabajo innecesario de los otros nodos. Los resultados de cada tarea son retornados por cada worker directamente a la aplicación, donde existe un ente que se encarga de unirlos y presentarlos al usuario. La magia es que para nosotros, como usuario final, todo esto funciona detrás de cámaras. Nosotros nunca nos enteramos de cómo están distribuidos los datos ni de lo que está haciendo cada nodo. Nosotros solamente manejamos nuestros datos a un alto nivel como si estuvieramos manejando un data frame de pandas. Podemos cargar datos directamente de un archivo CSB, Excel o JSON y Spark se va a encargar de dividir los datos de manera eficiente a lo largo de los worker nodes. Luego podremos hacer consultas y análisis en los datos utilizando una sintaxis muy parecida a consultas SQL. Con Spark también podemos entrenar modelos de machine learning con la librería MLib y ejecutar algoritmos de análisis de grafos con la librería GraphX, librerías que trabajan encima de Spark y hacen uso de su procesamiento distribuido para hacer estas operaciones de forma eficiente. Spark es rápido gracias a este procesamiento paralelo y distribuido de los datos y el hecho de que los datos a procesar casi siempre estarán en memoria RAM, con lo que la velocidad de tareas de procesamiento de grandes cantidades de datos se puede incrementar en órdenes de magnitudes comparado con frameworks más convencionales como Hadoop. Pero ojo, esto no sucede por arte de magia, tendremos que ser eficientes también a la hora de escribir código para poder sacar el mayor provecho de Spark. Cabe aclarar que Spark no es un lenguaje de programación ni una librería, es un framework, un marco de trabajo sobre el cual trabajamos. Esto quiere decir que lo podemos utilizar en muchos lenguajes de programación, incluyendo Python, gracias a la librería de PySpark, siempre y cuando tengamos un clóster de computadoras con Spark instalado y configurado. Lo cual puede llegar a ser muy complejo. Para nuestra suerte existen algunos proveedores de servicios en la nube como AWS que nos dan herramientas que con un par de clicks ya tenemos configurado un clóster de máquinas para ejecutar nuestro código de Spark sobre nuestros datos. Spark también implementa un formato de archivos optimizado para la rápida lectura y guardado, denominado Parquet, así como también implementa diversas estrategias para hacer joins o manejar transformaciones de datos cuyo resultado ya no entra en la RAM. Pero todo eso es un tema para otro video.
