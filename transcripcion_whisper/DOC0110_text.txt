 En esta primera lección del curso haremos un repaso de los conceptos esenciales del aprendizaje por refuerzo vistos en el curso anterior, es decir que hablaremos de los elementos de un sistema de aprendizaje por refuerzo, de los procesos de decisión de Markov, del agente y de las ecuaciones de Bellman. Luego introduciremos algunos conceptos fundamentales como los problemas basados en modelos y libres de modelos, así como los problemas de predicción y de control, lo que nos permitirá cerrar la lección con un panorama de los algoritmos clásicos de aprendizaje por refuerzo que veremos a lo largo del curso y su relación con estos conceptos fundamentales. Comencemos entonces recordando los elementos principales que conforman un problema de aprendizaje por refuerzo, los componentes. Entonces tenemos dos elementos esenciales que son el agente y el entorno y estos dos interactúan constantemente enviando y recibiendo una serie de señales. Entonces cuando el agente ejecuta una acción, entonces esa es una señal que ese agente envía al entorno, el entorno recibe esa señal y como respuesta a esa señal de entrada entonces genera lo que es un cambio de estado y una recompensa que le indica entonces o juntos le indican entonces al agente qué tan buena o mala fue la acción que ejecutó y esto se repite de manera cíclica y de esta forma el agente aprende a interactuar con ese entorno. También hay un elemento importantísimo entonces en el agente que es esta política, todo esto lo vemos en detalle en el curso anterior, entonces la política del agente es como el cerebro que le permite ejecutar acciones con base en el estado que le está proporcionando el entorno. Y entonces para recordar también el objetivo principal de resolver un problema de aprendizaje por refuerzo pues es lograr en esencia que el agente encuentre una secuencia de acciones que maximice el retorno, cuál es el retorno es como la puntuación total que la gente obtendría al sumar precisamente todas las recompensas individuales de cada una de las interacciones con el entorno, entonces ese sería el retorno, entonces el objetivo es maximizar ese retorno y cómo se logra eso pues encontrando la secuencia de acciones más adecuada y esa secuencia de acciones pues está muy relacionada precisamente con esa política, entonces resolver un problema de aprendizaje por refuerzo en el fondo está muy relacionada con esa política del agente que es la que le permite tomar decisiones frente a las variables que le está presentando el entorno. Recordemos además entonces que en este proceso de interacción entre el agente y el entorno podemos obtener una representación matemática compacta de todas las variables que hacen parte de ese problema y eso se conoce como un proceso de decisión de Markov que esto también lo vimos en el curso anterior, entonces ese proceso de decisión de Markov contiene algo que se conoce como el espacio de estados que son todos los posibles estados que puede tener el entorno, el espacio de acciones que es el conjunto de todas las posibles acciones que puede ejecutar el agente, la función de recompensa que son esencialmente todas las posibles recompensas que puede obtener el agente tras ejecutar una acción y tras recibir como estímulo de entrada un estado, acá tenemos entonces también una función de transición que en últimas nos indica la probabilidad de que el entorno en el que se encuentra el agente alcance un nuevo estado dependiendo del estado anterior y de la acción misma que esté ejecutando el agente y por último entonces tenemos algo que se conoce como el factor de descuento que nos permite ponderar como en cada interacción el agente está recibiendo una recompensa que puede ser positiva o negativa, puede tener diferentes valores es una cantidad numérica que le entrega precisamente como señal el entorno, entonces al sumar todas esas recompensas pues vamos a tener lo que es el retorno pero entonces para optimizar ese proceso de interacción lo que hacemos es penalizar esas recompensas con un factor de descuento y entonces ese factor de descuento pues resulta siendo clave en el cálculo del retorno de la suma total de las recompensas y en el proceso de optimización de esa interacción como lo vimos en el curso anterior entonces estos cinco elementos hacen parte de lo que es el proceso de decisión de Markov que sintetiza todas las variables que hacen parte de nuestro problema de aprendizaje por refuerzo, también como parte de lo que vimos en el curso anterior entonces lo que mencionábamos era la política que es el cerebro que define la forma como va a interactuar el agente con el entorno dependiendo de las señales que reciba y en particular entonces esa política la estamos denotando con la letra pi y estamos usando una notación también de probabilidades es decir que lo que estamos diciendo es que la política por una parte es estocástica es decir casi siempre tendremos un grado de incertidumbre en ese proceso de interacción entre el agente y el entorno no siempre tendremos clara cuál es la acción que va a ejecutar el agente partiendo de un estado en particular sino que eso tendrá un componente relativamente aleatorio y por eso es una política estocástica y por eso se usa una notación de probabilidades también entonces teníamos una forma o definíamos unas funciones para cuantificar qué tan bueno es un estado en el que se encuentra la gente en un momento o en un instante de tiempo en particular con respecto al objetivo que quiere alcanzar o qué tan buena es la acción que ejecuta la gente en un instante de tiempo en particular tomando como referencia el objetivo al que quiere llegar entonces teníamos la función estado valor y la función acción valor que en últimas pues medían esas bondades y que tiene pues una ecuación matemática que tiene que ver directamente con esa ese retorno que se está obteniendo en cada una de esas interacciones y que lo vimos en detalle pues en el curso anterior y también entonces podemos introducir esto no lo vimos en el curso anterior pero tiene una definición sencilla y es entonces la función acción ventaja entonces además de medir la bondad de un estado o la bondad de una acción también entonces podemos definir la función acción ventaja que simplemente tomar la función acción valor y restarle la función estado valor y que nos mide o nos cuantifica la ventaja de tomar una acción definida por la política por eso aparece acá el sub índice pi en lugar de cualquier otra acción que no tenga relación alguna con la política entonces tenemos esas funciones valor que nos cuantifican esas decisiones que se toman dentro del proceso y finalmente como parte de este repaso entonces tenemos las ecuaciones de bellman de las cuales hablamos en el curso anterior que simplemente son una representación matemática un poco más detallada de estas funciones estado valor y acción valor que veíamos anteriormente entonces acá ya se involucra de manera explícita la política por ejemplo en el caso de la función estado valor y en el caso de ambas funciones estado valor y acción valor se involucra de manera explícita también la función de transición que tiene nuestro proceso de decisión de marcó pero además de eso tienen un componente importante y es que nos permiten descomponer esos cálculos en dos elementos un elemento que contiene la recompensa inmediata y que aparece en ambas ecuaciones y otro elemento que contiene los valores futuros observemos que este s prima hace referencia al estado futuro al siguiente estado que va a alcanzar el agente pero estamos incluyendo el descuento o la penalización todo relacionado precisamente con las tareas de optimización entonces acá tendremos los valores futuros con descuento en ambas ecuaciones de bellman entonces ese es un elemento importante del cual va a partir precisamente el primer algoritmo que veremos en la próxima lección que es el de programación dinámica y ahora sí entonces vamos a introducir un primer concepto que es los problemas o los algoritmos que involucran modelos o sin modelos entonces si volvemos acá al caso del proceso de decisión de marcó donde teníamos los cinco elementos que hacen parte de ese conjunto cuando hablamos de un modelo es porque en ese proceso de interacción con el entorno conocemos la función de recompensa del entorno y conocemos la función de transición entonces estos dos elementos hacen referencia precisamente al modelo del entorno y ese modelo del entorno pues en esencia nos da todas las reglas de juego como podrá interactuar el agente con ese entorno y cuál será la respuesta de ese entorno a esa interacción o esas acciones que toma la gente entonces si conocemos esos detalles decimos que conocemos el modelo del entorno y entonces dependiendo de eso podemos tener dos tipos de problema o dos familias también grandes de algoritmos que son los problemas o algoritmos basados en modelos acá colocamos el término en inglés porque en la bibliografía que se encuentra comúnmente todo esto se encuentra en inglés entonces son los model based y tenemos los problemas o algoritmos libres de modelos o model free cuáles son los problemas basados en modelos entonces donde tenemos absolutamente toda la información de nuestro proceso de decisión de markov entonces tenemos el espacio de estados el espacio de acciones tenemos el factor de descuento pero también tenemos el modelo del entorno y un ejemplo de eso pues es precisamente el caso del tablero bidimensional el pequeño juego que vimos en el curso anterior donde conocíamos todos los detalles del proceso incluso sabíamos las probabilidades de transición de un estado a otro y en cada caso que recompensa se iba a obtener cuando la gente se desplazaba de una casilla a otra entonces como conocíamos las recompensas y las probabilidades de transición de una casilla a otra pues teníamos el modelo completo de ese juego en ese caso lo que veamos era que la gente debía decidir cuál era la mejor secuencia de acciones si partía del inicio cuál era la mejor secuencia de acciones que le permitiese llegar a la meta entonces como era como ya digamos teníamos todos los detalles del modelo y del proceso de decisión de marcó se convertía todo esto esencialmente en un proceso de planeación encontrar la mejor ruta posible para ir del inicio a la meta y esa era una tarea entonces de planeación pero en casos prácticos en la mayor parte de los casos reales realmente no podemos conocer esta función de transición y esta función de recompensa no tenemos esa información completa entonces como no tenemos esa información completa tendremos problemas o algoritmos que tienen un enfoque libre de modelos en ese caso tendremos por ejemplo acceso a los diferentes estados del entorno a las diferentes acciones que puede ejecutar el agente y tendremos acceso entonces al factor de descuento pero no tendremos la información del modelo un ejemplo de esto entonces es un robot acá tenemos un robot que se desplaza por ejemplo por una bodega y entonces en ese caso cuando el robot interactúa con la bodega generalmente no conoce con antelación por ejemplo cuál es la penalidad o la recompensa que va a obtener por una acción determinada que está ejecutando y no conoce tampoco con respecto a esa bodega que es el entorno la probabilidad de que ese entorno cambie dependiendo de ciertas acciones es decir la función de transición y como no conoce eso entonces el agente tiene que comenzar a interactuar con el entorno e ir aprendiendo a identificar de manera progresiva la manera más adecuada de desplazarse por ese entorno como un proceso de prueba y error es decir que tiene que ejecutar acá en este problema de libre de modelos tiene que ejecutar una tarea de aprendizaje entonces en la mayor parte de los problemas de aprendizaje por refuerzo no nos vamos a enfocar tanto en la planeación sino en el aprendizaje porque efectivamente el objetivo del aprendizaje por refuerzo es que el agente por esa interacción que se da con el entorno progresivamente vaya aprendiendo a interactuar con él mismo entonces la mayor parte de los problemas de aprendizaje por refuerzo están enfocados en este tipo de problemáticas y usarán ese tipo de algoritmos con ese enfoque y también entonces tenemos dos tipos de tareas o dos tipos de problemas o dos tipos de algoritmos que son los de predicción o de control entonces en el caso de los algoritmos de predicción lo que tendremos como entrada será entonces ya la política del agente es decir con antelación de alguna manera ya tendremos la política del agente el cerebro del agente que le indica entonces cuál es la probabilidad de ejecutar una cierta acción partiendo de un cierto estado y entonces en esas tareas de predicción lo que se tendrá que hacer es calcular el retorno es decir la suma de todas las recompensas que se tienen ejemplo típico volvemos al tablero bidimensional entonces si ya definimos una política unas reglas de juego para que la gente se mueva por ese tablero pues entonces teniendo esa política lo que tenemos es que predecir cuál será el retorno del puntaje que obtendrá tomando diferentes rutas eso será una tarea entonces de predicción que es una tarea relativamente sencilla porque ya el problema fundamental se resolvió y es que se conoce la política pero entonces tenemos otra tarea otro problema otro tipo de enfoque para los algoritmos que es el enfoque de control y el enfoque el enfoque de control entonces es que podemos tener múltiples políticas es decir múltiples cerebros que le van a indicar al agente diferentes maneras de interactuar con su entorno y lo que tenemos entonces es que determinar de todas esas posibles políticas cuál va a ser la mejor de ellas es decir vamos a ponerle acá una anotación con asterisco que nos va a indicar entonces cuál es la política óptima es decir cuál de todas las políticas posibles es la más adecuada para permitir esa interacción del agente con el entorno entonces ahí tendremos una tarea de control que lo que busque es entonces determinar la política más adecuada de un abanico de posibles políticas existentes entonces esto es un problema mucho más complejo y que realmente es como el núcleo principal del aprendizaje por refuerzo encontrar la política más adecuada y ya después de eso entonces podremos hacer tareas de predicción y entonces teniendo en cuenta este tipo de tareas basadas en modelos o libres de modelos o tareas de predicción y de control pues es donde tenemos los algoritmos clásicos del aprendizaje por refuerzo que sustentan digamos toda la teoría de los algoritmos contemporáneos que se usan en la actualidad incluso los que involucran algoritmos de deep learning entonces para entender eso pues en este curso nos vamos a enfocar precisamente en esos algoritmos clásicos sobre los cuales se construye la teoría de algoritmos mucho más avanzados y esos algoritmos clásicos pues los podemos organizar dependiendo de si tenemos tareas de planeación es decir basadas en modelos o libres de modelos donde tendremos entonces tareas de aprendizaje y también entonces si tenemos problemas de predicción y de control y entonces dependiendo de esas categorías podremos tener un primer grupo grande lo que está acá en verde que son todos los algoritmos de la familia de la programación dinámica que fueron digamos los primeros algoritmos de aprendizaje por refuerzo que se desarrollaron que en la actualidad digamos tienen un uso limitado pero que es necesario que los veamos en este curso porque son la base de algoritmos más elaborados como los de monte carlo y las diferencias temporales que veremos también en este curso y de algoritmos más avanzados que veremos también en el próximo curso entonces estos no son muy usados en la práctica pero son esenciales para entender los algoritmos que vinieron después entonces en el caso de tener modelos algoritmos basados en modelos y que ejecuten tareas de predicción entonces tendremos un algoritmo de programación dinámica que se conoce como evaluación de la política y tendremos en del lado del control también basados en modelos entonces la iteración de la política y la iteración de valores y todos estos algoritmos de programación dinámica requieren que tengamos o que conozcamos en detalle el modelo del entorno con el cual está interactuando el agente si no tenemos una información completa del modelo es decir si no conocemos la función de recompensa en la función de transición pues entonces tendremos algoritmos enfocados en el aprendizaje o libres de modelos y acá tenemos dos grandes familias monte carlo y diferencias temporales si usamos tareas de si abordamos tareas de predicción entonces tendremos la predicción con monte carlo o el aprendizaje por diferencias temporales y si ejecutamos tareas de control entonces tendremos el control por monte carlo y dos variantes también de las diferencias temporales que son el algoritmo sarza y el algoritmo que learning todos ellos entonces los veremos precisamente en detalle a lo largo de este curso y veremos también entonces cómo implementarlos computacionalmente usando librerías específicas en python muy bien acabamos de ver un panorama de los principales algoritmos clásicos para la solución de problemas de aprendizaje por refuerzo y que se dividen en tres grandes familias la programación dinámica monte carlo y las diferencias temporales y cada una de estas familias puede ser usada para abordar problemas de predicción o de control o problemas donde tengamos o no la información del modelo del entorno así que con esta introducción ya estamos listos para comenzar a ver en detalle cada una de estas familias entonces en la primera sección del curso nos enfocaremos en los algoritmos de programación dinámica y en particular en la próxima lección veremos el primero de estos algoritmos de predicción para la evaluación de la política.
