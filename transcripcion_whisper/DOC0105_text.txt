 Cuando abordamos un proyecto de Machine Learning, usualmente debemos entrenar múltiples modelos y elegir cuál de ellos generará las mejores predicciones posibles. Y generalmente lo que hacemos es entrenar esos múltiples modelos, ponerlos todos a prueba para medir su desempeño, seleccionar el que tenga el mejor desempeño, pero a la vez garantizar que este modelo seleccionado funcione correctamente con datos que nunca antes ha visto. Y para lograr esto, usualmente debemos usar tres sets de datos diferentes. El de entrenamiento, el de validación y el de prueba. Así que en este video veremos por qué son necesarios estos sets de datos al momento de implementar una solución de Machine Learning. Pero antes de comenzar, los invito a visitar codificandobits.com en donde encontrarán la Academia Online con cursos de Inteligencia Artificial, Ciencia de Datos y Machine Learning que les permitirán construir su carrera en estas áreas y todo por una suscripción mensual de tan solo 10 dólares. Así que listo, comencemos. En un video anterior hablamos en detalle de los parámetros e hiperparámetros de un modelo, conceptos que resultan fundamentales para entender que son estos sets de entrenamiento, validación y prueba. En esencia los parámetros son los coeficientes internos del modelo, es decir, son unas cantidades numéricas que nosotros no podemos modificar y que en su lugar el modelo las aprende durante el proceso de entrenamiento. Y después del entrenamiento el modelo usa estos parámetros para generar predicciones. Y por otro lado tenemos los hiperparámetros que son unos coeficientes externos al modelo y que debemos definir al momento de programar el algoritmo de entrenamiento. Por ejemplo, en un bosque aleatorio los parámetros son los umbrales internos que usa el modelo para clasificar un dato en una o en otra categoría. Mientras que los hiperparámetros son, por ejemplo, el número de árboles que contendrá el bosque o el número máximo de características usado en la clasificación. Y con estos conceptos claros ya estamos listos para comenzar a ver en qué consisten los sets de entrenamiento, validación y prueba. Partamos de un ejemplo hipotético que nos permitirá entender el papel que juegan los sets de entrenamiento, validación y prueba en el desarrollo de un modelo. Supongamos en este sencillo ejemplo que hemos recolectado datos de 10.000 sujetos con información sobre su peso, su nivel de glucosa en sangre y si la persona es o no es hipertensa. Y supongamos que a partir de este set de datos queremos entrenar un modelo de clasificación que tome como entrada las variables peso y nivel de glucosa en sangre y que prediga si una persona es o no es hipertensa. Al intentar resolver este problema nos enfrentamos a una situación que usualmente encontraremos en diferentes proyectos de Machine Learning, que es elegir cuál es el modelo más adecuado para generar las predicciones. En este caso podríamos preguntarnos qué tipo de modelo resultaría más adecuado si por ejemplo una red neuronal, un bosque aleatorio o un modelo de regresión logística. Y realmente no podemos saber esto con antelación, así que lo que nos queda es entrenar todos estos modelos y posteriormente ponerlos a prueba. Así que veamos para este ejemplo hipotético cuál sería el procedimiento a seguir usando precisamente los sets de entrenamiento, validación y prueba. ¿Cómo se puede entrenar los tres modelos? Entonces supongamos que para este problema entrenaremos los tres tipos de modelos mencionados anteriormente. Una red neuronal, un bosque aleatorio y un modelo de regresión logística. Y entonces las primeras dos preguntas que debemos resolver es cómo encontramos los parámetros de estos modelos y cuáles datos usamos para encontrar esos parámetros. Pues si usamos la totalidad de los datos tendríamos un inconveniente porque la idea es no solo encontrar los parámetros del mejor modelo posible sino también ponerlo a prueba en un escenario real. Es decir que una vez entrenado ese mejor modelo lo que nos interesa es presentarle unos datos que no haya visto previamente para determinar si logra clasificar correctamente a un sujeto como hipertenso o no hipertenso. Así que en lugar de entrenar los modelos candidatos con la totalidad de los datos lo que haremos será una primera partición. Tomaremos aleatoriamente por ejemplo un 70% de los datos y se los presentaremos a cada modelo durante el entrenamiento para que logre calcular sus parámetros. Mientras que el 30% restante lo mantendremos por ahora oculto. Y a este set usado para obtener los parámetros de cada modelo durante el entrenamiento lo llamaremos precisamente el set de entrenamiento. Pero recordemos que cada modelo tiene unos hiperparámetros y que dependiendo de los valores que escojamos para esos hiperparámetros podremos mejorar o empeorar el desempeño de cada clasificador. Y recordemos además que nos interesa escoger el mejor modelo de los tres que estamos considerando. Así que en este segundo paso debemos resolver dos preguntas. Cómo elegir los hiperparámetros de cada modelo para que tenga el mejor desempeño posible y cómo elegir cuál es el mejor modelo de los tres que estamos considerando. De nuevo para responder estas preguntas necesitamos recurrir a los datos. Pero no podemos usar el set de entrenamiento pues al hacerlo tendríamos varios inconvenientes. En primer lugar si afinamos los hiperparámetros de cada modelo con el mismo set de entrenamiento podríamos llegar a una situación de sobreajuste. Cada modelo tenderá a memorizar los datos de entrenamiento. Aparentemente tendrá un buen desempeño pero en realidad no generará buenas clasificaciones cuando le presentemos datos que nunca antes ha visto. En segundo lugar para elegir de todos los modelos posibles cuál es el mejor deberíamos tomar cada modelo entrenado y con sus hiperparámetros afinados presentarle los datos y generar predicciones con cada clasificador y simplemente elegir aquel que tenga el mayor porcentaje de aciertos. Pero al usar el set de entrenamiento para generar estas predicciones tendríamos el mismo inconveniente mencionado anteriormente. El desempeño obtenido por cada modelo no sería una medida fiable de su verdadero comportamiento. De nuevo la idea es ponerlo a prueba con un set de datos que no haya visto previamente. Así que la forma adecuada de afinar los hiperparámetros de cada modelo y de elegir el mejor modelo posible es usando un set de datos que no haya visto previamente ninguno de los modelos. Y recordemos que en el problema que estamos resolviendo habíamos ocultado el 30% de los datos. Así que vamos a tomar este subset, lo vamos a mezclar aleatoriamente y lo vamos a partir en dos. Y vamos a tomar la mitad de estos datos, es decir el 15% del set de datos total y lo vamos a usar para afinar los hiperparámetros y para elegir el mejor modelo. Este subset se conoce precisamente con el nombre de set de validación y nos permitirá obtener los mejores hiperparámetros de cada modelo y seleccionar el mejor modelo de todos los que hayamos entrenado y de una manera objetiva. Muy bien, hasta este punto ya hemos entrenado cada uno de los modelos usando precisamente el set de entrenamiento y ya hemos afinado sus hiperparámetros y encontrado el mejor modelo de los tres que estamos considerando usando el set de validación. Así que ya hemos seleccionado el modelo que podría generar las mejores predicciones para nuestro problema en particular. Y entonces sólo nos queda resolver una pregunta, que es ¿cómo se comportaría el modelo seleccionado con datos que nunca antes ha visto? Es decir que la idea es poder usar nuestro modelo en el mundo real, así que si tenemos nuevos sujetos la idea sería recolectar información de su nivel de glucosa en sangre, de su peso, llevar esa información al modelo entrenado e intentar predecir con el mayor grado de fiabilidad si ese sujeto es hipertenso o no hipertenso. Y para poner a prueba este modelo seleccionado no podemos usar ni el set de entrenamiento ni el set de validación porque se trata de datos que ya ha analizado previamente ese modelo. Así que debemos ser totalmente imparciales y objetivos en este punto, para que al poner a prueba el modelo tengamos una medida certera de cuál sería su verdadero desempeño con datos que no ha procesado previamente. ¿Y cuáles son esos datos que aún no ha visto el modelo seleccionado? Pues recordemos que habíamos tomado un 70% de los datos para el entrenamiento y un 15% como set de validación, así que aún tenemos un 15% de los datos ocultos. Pues podemos precisamente usar estos datos restantes para poner a prueba el modelo y determinar cuál sería su desempeño en condiciones reales. Y este set se conoce precisamente con el nombre de set de prueba. Y acá es importante hacer hincapié en que este set lo debemos mantener en todo momento oculto al modelo y solo se lo debemos presentar al final de todo el proceso. Cuando ya hayamos realizado el entrenamiento, seleccionado el mejor modelo y afinado sus hiperparámetros. Si no hacemos esto podríamos obtener una medida poco fiable del desempeño real del modelo y esto podría afectar su fiabilidad al momento de hacer predicciones con datos totalmente nuevos. Muy bien, acabamos de ver que son los sets de entrenamiento, validación y prueba muy usados convencionalmente cuando entrenamos y construimos modelos de Machine Learning. En esencia el set de entrenamiento nos permite obtener los parámetros de cada modelo candidato, mientras que el set de validación nos permite encontrar los hiperparámetros más adecuados de cada uno de esos modelos candidatos y elegir de entre todos estos modelos que estamos considerando cuál es el que tiene el mejor desempeño para el problema que estamos resolviendo. Y una vez elegido el mejor modelo lo que podemos hacer es ponerlo a prueba precisamente con el set de prueba, lo que nos daría una medida de cuál sería su desempeño cuando llevemos el modelo a producción y comencemos a presentarle datos que nunca antes había visto. Esta partición del set de datos en entrenamiento, validación y prueba es un enfoque muy usado especialmente en el Deep Learning, cuando contamos con cientos de miles o incluso millones de datos y las proporciones usadas convencionalmente son 70, 15, 15 u 80, 10, 10. Sin embargo este enfoque no resulta adecuado cuando tenemos un set de datos pequeño, con unos cuantos cientos o miles de datos. En estos casos es recomendable usar lo que se conoce como la validación cruzada, de la cual vamos a hablar en detalle en un próximo video. Y bien, espero que les haya gustado este video y si es así los invito a darle un pulgar hacia arriba de me gusta y a compartirlo con todos sus amigos y conocidos para seguir llevando este contenido a cada vez más y más personas. Y si tienen alguna duda de este video no olviden dejarla abajo en los comentarios. Y si aún no lo han hecho los invito a suscribirse al canal y activar la campanita para que YouTube les notifique cada vez que publique nuevo contenido en el canal. Así que por ahora esto es todo, les envío un saludo y nos vemos en el próximo video.
